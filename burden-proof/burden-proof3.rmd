---
title: "Likelihood ratio and decision thresholds"
author: "Marcello Di Bello and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        - Rafal_latex6.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: [../references/referencesMRbook.bib]
csl: [../references/apa-6th-edition.csl]
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



\section*{SAMLE CHAPTER PLAN UPDATE}

I am now realizing that perhpas the 
structure of the chapter 
could be further broken down into 
three chapters:

1. A chapter that shows how probabilistc thresholds are good as analytical tools, despite implementation or practical difficulties with them. This chapter would include discussions of expected utility, minimizing errors, signal detection theory, etc. A lot of this stuff is already in the extended version of the SEP entry. So main claim of this chayer is: yes, probabilistic threshold are not good practically, but they can still be good as analytical tools. Title: "Probability Thresholds as Analytical Models of Trial Decision Making"

2. Two chapters that looks at the two theoretical 
difficulties (naked stats and conjunction paradox, ad also problem 
of priors). One chapter on naked statistical evidence and our informal solutions to it, based either on LR or on specific narratives (this should be followed by another chapter with the formal details).

3. Another chapter on conjunction paradox and our informal solution to it, maybe in terms of LR, BF or narratives (followed by another chapter in which the formal details are spelled out).

4. A chapter that formally addresses the two theoretical difficulties, perhaps 
using Bayesian Networks. This need not be included in the 
sample chapters we sent out.  Title: "Adressing the Proof Paradoxes with Bayesian Networks". 


\section*{SAMPLE CHAPTER PLAN}

In rethinking the sample chapter, 
we should perhaps stick to 
a simpler structure, trying to offer a 
more focused and compelling argument. 
Right now 
I think we have too many 
possible accounts under consideration, and the structure 
is not very tight or cohesive. 
It feels more like a literature review, 
especially the first few sections. 

So here is how I proposed we do it:

\begin{enumerate}

\item Begin by stating the simplest probabilistic account based on a threshold for the 
posterior probability of guilt/liability. The threshold can be variable or not. Add brief description of decision-theoretic ways to fix the threshold. (Perhaps here we can also 
talk about intervals of posterior probabilities or imprecise probabilities.) 


\item Formulate two common theoretical difficulties against ths posterior 
probability threshold view: (a) naked statistical evidence and (b) conjuction.
(We should state these difficilties before we get 
into alternative probabilistic accounts, or else the reader might 
wonder why so many different variants are offerred of probabilistic accounts). 

R: Yes. That's what I thought.


We might also want to add a third difficulty: (c) the problem of priors (if priors cannot be agreed 
upon then the posterior probability threshold is not functionally operative). Dahlman I think has quite a bit of stuff on the problem of priors. 

\item  As a first response to the difficulties, articulate the likelihood ratio account. 
This is the account I favor in my mind paper. Kaplow seems to do something similar. So does Sullivan. So it's a  popular view, worth discusing in its own right. You say that Cheng account is one particular variant of this account, so we can talk about Cheng here, as well.

\item Examine how the likelihood ratio account fares against the two/three difficulties above. One could make an argument (not necessarily a correct one) that the likelihood ratio account can address all the two/three difficulties. So we should say why one might think so, even thought the argument will ultimately fail. I think this will help grab the reader's attention. This is what I have in mind:

4a: the LR approach solves the naked stat problem because LR=1 (Cheng, Sullivan) or L1=unknown (Di Bello). 

4b: the LR approach solves the conjuction problem because -- well this is Dawid's point that we will have to make sense of the best we can

4c: the LR approach solves the priors problem b/c LR do not have priors.


\item Next, poke holes in the likelihood ratio account:

against 4a: you do not believeLR=1 or LR=unknown , so we should  talk about this

against 4b: this is your cool argument against Dawid

against 4c: do you believe the arguemt in 4c? we should talk about this 

In general, we will have to talk to see where we stand. As of now, I tentatively believe that the likelihood ratio account can solve (a) and (c), and you seem to disagree with that. Even if I am right, the account is still not good enough becaue it cannot solve (b).

\item Articulate (or just sketch?) a better probabilistic account overall. 
Use Bayesian networks, narratives, etc. I am not sure if this 
should be another paper. That will depend on how much we'll 
have to say here. 


\end{enumerate}


\tableofcontents




# SAMPLE CHAPETR TITLE - "Probability Thresholds as Analytical Models of Trial Decision Making"


# Introduction


<!-- \label{subsec:legal-background} -->
<!-- ## Burden of pleading, production and persuasion -->
<!-- ## Proof standards in the law -->

After the evidence has been presented, examined and cross-examined at trial, 
trained judges or lay jurors must reach a decision. 
In many countries, the decision criterion is defined by law and consists of 
a standard of proof, also called the burden of persuasion. So long 
as the evidence against the defendant meets the requisite proof standard, the defendant should be found liable. 

 

In criminal proceedings, the governing standard is 'proof beyond a reasonable doubt.' 
If the decision makers are persuaded beyond a reasonable doubt that the defendant is guilty, 
they should convict, or else they should acquit. In civil cases, the standard is 
typically 'preponderance of the evidence.' The latter is less demanding than the former, so the same body of evidence may  meet the preponderance standard, but not meet the beyond a reasonable doubt standard. A vivid example of this difference is the 1995 trial of O.J. Simpson, who was charged with the murder of his wife. He was acquitted of the criminal charges, but when the family of the victim brought a lawsuit against him, they prevailed. O.J.\ Simpson did not kill his wife according to the beyond a reasonable doubt standard, but he did according to the preponderance standard. An intermediate standard, called 'clear and convincing evidence,' is sometimes used for civil proceedings in which the decision is particularly weighty, for example, a decision whether someone should be committed to a hospital facility. \todo{Not sure if it is clear what you mean by this.}

How to define standards of proof---and whether they should be even defined in the first place---remains  contentious [@diamond90; @newman1993; @Horowitz1996; @laudan2006truth; @walen2015]. Judicial opinions offer different, sometimes conflicting, paraphrases of what these standards mean. The meaning of `proof beyond a reasonable doubt' 
is the most controversial. It has been equated with 'moral certainty' 
or 'abiding conviction'  (Commonwealth v. Webster, 59 Mass. 295, 320, 1850) or with 'proof of such a convincing character that a reasonable person would not hesitate to rely and act upon it in the most important of his own affairs' (US Federal Jury Practice and Instructions, 12.10, at 354, 4th ed.\ 1987). But courts have also cautioned that there is no need to define the term because 'jurors know what is reasonable and are quite familiar with the meaning of doubt' and attempts to define it only 'muddy the water' (U.S. v. Glass, 846 F.2d 386, 1988).

To further complicate things, differences 
between countries and legal traditions exist. The tripartite distinction of proof standards---beyond a reasonable doubt; preponderance; clear and convincing evidence---is common in Anglo-american jurisprudence. It is not universal, however. Different countries may use different standards. 
France, for example, uses the standard of 'intimate conviction' 
for both civil and criminal proceedings. Judges deciding cases 'must search their conscience in good
faith and silently and thoughtfully ask themselves what impression the evidence
given against the accused and the defence's arguments have made upon them' (French Code of Criminal Procedure, art.\ 353). German law is similar. Germany's Code of Civil Procedure, Sec.\ 286, states that 'it is for the court to decide, based on its personal conviction, whether a factual claim is indeed true or not.'\todo{R: check the formulation in Poland}

While there are inevitable differences between 
legal traditions, the question of how strong the evidence should 
be to warrant a finding of civil or criminal 
liability has universal appeal. Any system of adjudication whose decisions 
are informed by evidence will confront this question in one way or another. 
Not all legal systems will explicitly formulate standards 
of proof for trial decisions. Some legal systems may specify rules 
about how evidence should be weighed without formulating decision criteria such as standards 
of proof. But even without explicit proof standards, the triers of 
facts, judges or jurors, will have to decide 
whether the evidence is sufficient to 
judge the defendant legally liable. 

\todo{Need to revise this when the chapter is done.}
We will not survey the extensive legal literature 
and case law about proof standards. We will instead examine 
whether or not probability theory can bring conceptual clarity 
to an otherwise heterogeneous legal doctrine. 
This chapter outlines 
different probabilistic approaches, formulates 
the most common challenges against them, 
and offers a number of responses from the perspective 
of legal probabilism.
The legal and philosophical 
literature has focused on the 
theoretical and analytical challanges. We will 
do the same here. We will focus on two key theoretical challanges 
that have galvanized the philosophical literature: the problem of naked statistical evidence 
and the conjunction paradox. One reason\todo{Here you sound like you're gonna list a bunch of reasons but you give only one. Consider adding reasons or reformulating this bit.} to choose these two in particular 
 is that it would be desirable to be able to handle basic 
 conceptual difficulties before turning to more 
 complex issues or attempting to implement 
 probabilistic standards of proof in trial proceedings.  
 
 

# Probability thresholds


Imagine you are a trier of fact, 
say a judge or a juror, who is expected 
to make a decision about the guilt of a defendant who faces 
criminal charges. The prosecution presents evidence 
to support its accusation, and the defense 
offers counterevidence. 
As a trier of fact, you are confronted with the question whether
the totality of the evidence presented at trial warrants a conviction. 
More specifically, the question 
is whether the evidence as a whole establishes the defendant's 
guilt beyond a reasonable doubt.


## The basic idea

Legal probabilists have proposed 
to interpret proof beyond a reasonable 
doubt as the requirement that the defendant's probability of guilt, given the evidence presented at trial, meet a threshold [see @Bernoulli1713Ars-conjectandi; @Laplace1814;  @Kaplan1968decision; @Dekay1996; @kaye79; @laudan2006truth]. On this interpretation, so long as the defendant's guilt 
is established with a sufficiently high probability, say 95\%, guilt is 
proven beyond a reasonable doubt and the defendant should be convicted. If the 
probability of guilt does not reach the requisite threshold,
the defendant should be acquitted. This intepretation can be spelled out 
more formally by means of conditional probabilities. 
That is, a body of evidence $E$ establishes 
guilt $G$ beyond a reasonable doubt if and only if 
$\pr{G\vert E}$ is above the threshold. 
 

This interpretation is, in many respects, plausible. From a legal standpoint, the requirement that guilt be 
established with high probability, still short of 100\%, accords 
with the  principle that proof beyond a reasonable doubt is the most stringent standard 
but does not require---as the Supreme Court of Canada put it---'proof to an absolute certainty' and thus 
'it is not proof beyond any doubt' (R v Lifchus, 1997, 3 SCR 320, 335).
The plausibility of a probabilistic intepretation 
is further attested by the fact that such an intepretation 
is tacitly assumed in empirical studies about 
people's understanding of proof beyond a reasonable doubt [@dhamiEtAl2015]. 
This research examines where decision-makers set the bar for convictions, say 
at 80\% or 90\% probability, 
but does not question the assumption that standards of proof 
function as probabilistic thresholds of some kind.

Reliance on probability is even more explicit 
in the standard 'preponderance of the evidence'---also called 'balance of probabilities'---which governs decisions in civil disputes. This standard can be interpreted as the requirement that the plaintiff---the party making the complaint against the defendant in a civil case---establish their version of the facts with greater than 50\% probability. The 50\% threshold, as opposed to a more stringent threshold of 95\% for criminal cases, reflects the fact that  preponderance is less demanding than proof beyond a reasonable doubt. The intermediate standard 'clear and convincing evidence' is more stringent than the preponderance standard but not as stringent as the beyond a reasonable doubt standard. Since it lies in between the other two, it can be interpreted as the requirement that the plaintiff establish their versions of the facts with, say, 75-80\% probability.

<!--
the defendant's guilt is identified as equivalent to a certain factual statement $G$ and that somehow you succeeded in properly evaluating $\pr{G\vert E}$ -- the probability of $G$ given the total evidence presented to you, $E$ (and perhaps some other relevant probabilities). For various reasons, some of which  will be mentioned soon, this is an idealized situation. One question that arises in such a situation is: \emph{when should you decide against the defendant? when is the evidence good enough?} 
-->

<!--
What we are after here is a condition or set of conditions $\Psi$, formulated in (primarily) probabilistic terms, 
such that the trier of fact, at least ideally, should accept any relevant claim (including $G$) just in case $\Psi(A,E)$. The requirement that the condition should apply to any relevant claim whatsoever (and not just a selected claim, such as $G$) will be called the \textbf{equal treatment requirement}.\footnote{The requirement is not explicitly mentioned in the discussion, but it is tacitly assumed, so it is useful to have a name for it. Moreover, it will turn out crucial when it comes to finding a resolution of the difficulties, but further details need to wait till the last section of this paper.}
-->


## Mixed reactions from legal practitioners

When appellate courts have examined the question whether standards of proof can be quantified using probabilities, they have often answered in the negative. One of the clearest opposition to quantification was formulated by Germany's Supreme Court, the Federal Court of Justice, in the case of Anna Anderson who claimed to be a descendant of the Tsar family. In 1967, the Regional Court of Hamburg ruled that Anderson failed to present sufficient evidence to establish that she was Grand Duchess Anastasia Nikolayevna, the youngest daughter of Tsar Nicholas II, who allegedly escaped the murder of the Tsar family by the Bolsheviks in 1918. (Incidentally, DNA testing later demonstrated that Anna Anderson had no relationship with the Tsar family.) 
Anderson appealed to Germany's Federal Court, complaining that the Regional Court had set too demanding a proof standard. Siding with the lower court, the Federal Court made clear that '[t]he law does not presuppose a belief free of all doubts', thus recognizing the inevitable fallibility of trial decisions. The Court warned, however, that it would be 'wrong' to think that a trial decision could rest on 'a probability bordering on certainty' (Federal Court of Justice, February 17, 1970; III ZR 139/67). 

The Anderson decision is all the more interesting as it applies to a civil case. The German court did not think trial decisions could rest on a probability, not even in a civil case. The same conclusion would be less surprising if it applied to a criminal case. For example, @buchak2014belief has argued that an attribution of criminal culpability is an ascription of blame which requires a full belief in someone's guilt, and a proposition that is highly probable on the evidence, no matter how high its probability, cannot amount to a full belief.  One is left wondering, however. If a high probability of guilt short of 100\% isn't enough and certainty cannot be required either, how else could the standard of proof be met? The question becomes more pressing in civil cases if we replace 'guilt' with 'civil liability'. Anticipating this worry, Germany's Federal Court in the Anderson case endorsed a conception of proof standards that acknowledges the inveitable fallibility of trial decisions while at the same time maintaining the need for certainty. The Federal Court wrote that a judge's decision must satisfy 'a degree of certainty which is useful for practical life and which makes the doubts silent without completely excluding them' (Federal Court of Justice, February 17, 1970; III ZR 139/67).  

The words of Germany's Federal Court echo dilemmas 
that bedeviled early theorists of probability and evidence law. 
When Jacob Bernoulli---one of the pionerres of probability theory---discusses the requirement 
for a criminal conviction in his \textit{Ars Conjectandi} (1713), he writes that 'it might be determined whether 99/100 of probability suffices or whether 999/1000 is required' (part IV). This is one of the earliest suggestions that the criminal standard of proof be equated with a threshold probability of guilt. A few decades later, the Italian legal penologist Cesare Beccaria in his celebrated treatise \textit{On Crimes and Punishments} (1764) remarks that the  certainty needed to convict is 'nothing but a probability, though a probability of such a sort to be called certainty' (chapter\ 14).  This suggestive yet admittedly elusive remark indicates that the standard of decision in criminal trials should be a blend of probability and certainty. But what this blend of probability and certainty should be like is unclear. At best, Beccaria's suggestion brings us back to paraphrases of proof beyond a reasonable doubt such as 'moral certainty' or 'abiding conviction'.

Not all legal practitioners, however, resist a probabilistic interpretation 
of standards of proof. Some actually find such interpretation plausible, even obvious. 
For example, Justice Harlan of the United States Supreme Court writes:

\begin{quote}
\dots in a judicial proceeding in which there is a dispute about the facts of some earlier event, the factfinder cannot acquire unassailably accurate knowledge of what happened. Instead, all the factfinder can acquire is a belief of what probably happened. The intensity of this belief -- the degree to which a factfinder is convinced that a given act actually occurred -- can, of course, vary. In this regard, a standard of proof represents an attempt to instruct the factfinder concerning the degree of confidence our society thinks he should have in the correctness of factual conclusions for a particular type of adjudication.\footnote{In re Winship, 397 U.S. 358, 370 (1970). This is a landmark decision by the United States Supreme Court establishing  that the beyond a reasonable doubt standard must be applied to both adults and juvenile defendants.}
\end{quote}

\noindent
After this methodological premise, Justice Harlan explicitly 
endorses a probabilistic interpretation of standards of proof, 
using the expression 'degree of confidence' instead of 'probability': 

\begin{quote}
Although the phrases 'preponderance of the evidence' and 'proof beyond a reasonable doubt' are quantitatively imprecise, they do communicate to the finder of fact different notions concerning the degree of confidence he is expected to have in the correctness of his factual conclusions.
\end{quote}

\noindent
Justice Newman of United States Court of Appeals for the Second Circuit 
proposes a similar definition. He worries that words such as 'probability' or 'likelihood' may confuse jurors, as these words often qualify predictions about future events that may or may not occur. 
Instead, Newman prefers the expression 'degree of certainity'. In chracterizing 
proof beyond a reasonable doubt, he writes:

\begin{quote}
Were I the trier of fact, I would think about my own degree of certainty about the defendant's guilt, 
and, with a scale of 0 to 100 in mind, not vote to convict unless my degree of certainty 
exceeded 95 on that scale (p. 269) CITED FROM: Jon O. Newman (2006), Quantifying the standard of proof beyond a reasonable doubt: acomment on three comments. Law, Probability and Risk. 5, pp. 267-269
\end{quote}

\todo{You only talk about Harlan; it would be nice to have more examples 
of people embracing probabilistic explications. M: GOOD POINT. ADDED MORE. 
NEED TO ADD EVEN MORE.}

OTHER EXAMPLES TO CHECK. SEE REFERENCES BELOW:

FRANKLIN, J. (2006) Case Comment-United States v. Copeland, 369 F. Supp. 2d 365 (E.D.N.Y. 2005): quantification of the ‘proof beyond reasonable doubt’ standard. Law, Probability and Risk, 5, 159-165.

TILLERS, P. and GOTTFRIED, J. (2006) Case Comment-United States v. Copeland, 369 F. Supp. 2d 365
(E.D.N.Y. 2005): A Collateral Attack on the Legal Maxim That Proof Beyond a Reasonable Doubt Is
Unquantifiable? Law, Probability and Risk, 5, 135-157.

WEINSTEIN, J. B. and DEWSBURY, I. (2006) Comment on the meaning of ‘proof beyond a reasonable doubt’.
Law, Probability and Risk, 5, 167-173.



## Implementation and idealization  

The remarks by Justice Harlan, Newman and others 
notwithstanding, legal practioners seem in general 
opposed to quantifying standards of proof 
probabilistically. This resistance has many causes. One key factor 
is the conviction that a probabilistic intepretation of 
legal standards of proof is unrealistic because its implementation would 
face unsurmountable challenges. How can the relevant probabilities---such as the probability of someone's guilt---be quantified? How will the triers of facts apply probabilistic thresholds? Should the application of the thresholds be automatic---that is, if the evidence meets the threshold, the triers of fact should find against the defendant (say, convict in a criminal trial) and otherwise find in favor of the defendant? The challenge, in general, is to articulate how probabilistic thresholds can be operationalized as part of trial decisions. This is by no means obvious. Judges and jurors do not weigh evidence in an explicitly probabilistic manner. Nor do they explicitly use 
probability thresholds to guide their decisions. CITE EMPIRICAL EVIDENCE HERE FOR NARRATIVE AND PLAUSIBILITY THEORY AGAINST PROBABILISTIC APPROACH. And even if judges and jurors were to change the way they assess, and reasona about, the evidence presented at trial there would remain the problem of computainal tractability. 
How could all the variables needed to assess the relevant probabilities 
be taken into acount in a computionally tractable way? CITE ALLEN. SAY MORE ABOUT COMPUTIONAL 
TRACTBILITY EVEN FOR BAYESIAN NETWORK.


To alleviate the force of these worries, the probabilistic interpretion of proof standards can be broken 
down into two separate claims, what we might call the 'quantification claim' and the 'threshold claim'.
In a criminal trial, these claims would look as follows:

 \begin{tabular}{lp{8.5cm}}
 \textsc{Quantification Claim} & a probabilistic quantification of the defendant's guilt can 
 be given through an appropriate weighing of all the evidence available (that is, of all the evidence against, and of all the evidence in defense of, the accused).\\
 \textsc{Threshold Claim} & an appropriately high threshold guilt probability, say 95\%, 
 should be the decision criterion for criminal convictions.
  \end{tabular}

\noindent
Those worried about implementation might reason thusly.
If guilt cannot be quantified probabilistically---for example, in terms 
of the conditional probability of $G$ given the total evidence $E$---no probabilistic 
threshold could ever be used as a decision criterion. 
Since the quantification claim is unfeasible and the threshold 
claim rests on the quantification claim,
the threshold claim should be rejected. 

One way to answer this objection
is to bite the bullet. Legal probabilists 
can admit that probabilistic thresholds constitute a 
revisionist theory. If they are to be implemented in trial proceedings, they 
will require changes. Jurors and judges will have to become 
familiar with probabilistic ideas. They will have to evaluate 
the strength of the evidence numerically, 
even for evidence that is not, on its 
face, quantitative in nature. But this response will simply highten 
the resistance toward a probabilistic 
intepretation of proof standards, or at least, 
the likelihood of success of such a program of radical reform 
of trial proceedings is uncertain. 
But there is a less radical 
way for legal probabilists to respond, one 
that admits that legal probabilism tacitly assumes 
a certain degree of idealization. 


Legal probabilists can admit
they are not---at least, not yet---engaged with 
implementation or trial reform. 
More specifically, the quantification claim 
can be interpreted in at least two different ways. One interpretation 
is that a quantification of guilt---understood as an actual reasoning process---can be effectively carried out by the fact-finders. The quantification claim can also be understood as an idealization or a regulative ideal. 
For instance, the authors of a book on probabilistic inference in forensic science write:

> the \dots [probabilistic] formalism should primarily be considered as an aid to structure and guide one's inferences under uncertainty, rather than a way to reach precise numerical assessments\dots
\hspace*{\fill} [@taroni2006bayesian, (p.\ xv)]



\noindent
Even from a probabilist standpoint, the quantification of guilt can well be an idealization which has, primarily, a heuristic role.  MAYBE ALSO ADD THAT RONALD ALLEN WOULD NOT OBJECT TO THIS, AS HE THINKS THAT PROBABILITY ARE TOOLS IN PLAUSIBILITY REASONING. ADD CITATION. 

Just as the quantification claim can be interpreted in two different ways, the same can be said of the threshold claim. For one thing, we can interpret it as describing an effective decision procedure, as though 
the fact-finders were required to mechanically convict whenever the defendant's probability of guilt happened to meet the desired probabilistic threshold. But there is a second, and less mechanistic, interpretation of the threshold claim. On the second interpretation, the threshold claim would only 
describe a way to understand, or theorize about, the standard of proof or the rule of decision.  The second interpretation of the threshold claim---which fits well with the 'idealization interpretation' of the quantification claim---is less likely 
to encounter resistance.

Lawrence Tribe, in his famous 1971 article 'Trial by Mathematics', expresses 
disdain for a trial process that were mechanically governed 
by numbers and probabilities. He claims that under this scenario 
judges and jurors would forget their humanizing function. He writes:
 

>Guided and perhaps \textit{intimidated by the seeming inexorability of numbers}, 
induced by the persuasive force of formulas and the precision of 
decimal points to perceive themselves as performing a largely 
mechanical and automatic role,  \textit{few jurors ... 
could be relied upon to recall, let alone to 
perform, [their] humanizing function}.  \hspace*{\fill} [@tribe1971trial]

\noindent
But this worry does not apply if we interpret the threshold claim in a non-mechanistic way. 
This is the interpretation we shall adopt in this chapter. To avoid setting the 
bar for legal probabilism too high, we will not be concerned with practical issues that arise if we wanted to deploy a probabilistic threshold directly. We will grant that, at least for now, successful implementation of such thresholds is not viable. For the time being, probabilistic thresholds are best understood as offerring a theoretical, analytical model of trial decisions. The fact that this theoretical model cannot be easily operationalized does not mean that the model is pointless. There are multiple ways in which such a model, even if unfit for direct deployment in trial proceedings, can offer insights into trial decision-making. 
 
<!-- Once we have a probabilistic model, a vast array of mathematical results pertaining to probability can be used to deepen our understanding of the rationality of legal decisions. If at least in abstraction  adequate, the model  could be useful for diagnosing various types of biases that humans are susceptible to in such contexts; it could be useful as a measuring stick against which various qualitative inference patterns are assessed, and it could be useful as a source of insights about various aspects of legal decisions and evidence presentation methods. Just as understanding physics might be useful for deepening our understanding of how things work, and for building things or moving them around without performing direct exact calculations, a general probabilistic model -- again, if adequate -- could help us get better at understanding and  making legal decisions without its direct deployment in practice.  
-->

## Minimizing expected costs: higher cost ratio, higher threshold

Let's start with the simplest illustration of how 
the probabilistic interpretation of proof standards can serve as a theoretical, 
analytical tool for conceptual clarification. Standards of proof are usually ranked 
from the least demanding, such as preponderance of the evidence, to 
the most demanding, such as proof beyond a reasonable doubt, even though 
this distinction  has not always been around.\footnote{See e.g. United States v. Feinberg, 140 F.2d 592 (2d Cir. 1944): `` to distinguish between the evidence which should satisfy reasonable men, and the evidence which should satisfy reasonable men beyond a reasonable doubt. While at times it may be practicable to deal with these as separate without unreal refinements, in the long run the line between them is too thin for day to day use'' (594).} 
Can we give a principled justification for the use of multiple standards 
and their ranking? A common argument is that more is at stake in a criminal trial than in a civil trial. A mistaken conviction will injustly deprive the defedant of basic liberties or even life. Instaed, a mistaken decision in a civil trial would not encroach upon someone's basic liberties since decisions in civil trials are mostly about imposing monetary compensation. This difference in the stakes warrants different standards of proof, more stringent for criminal than civil cases. This informal argument can be made  precise by pairing probability thresholds with expected utility theory, a well-establish paradigm of rational decision-making used in psychology and economic theory. 

At its simplest, decision theory based on the maximization of expected utility states that between a number of alternative courses of action, the one with the highest expected utility (or with the lowest expected cost) should be preferred. This theory can be applied to a variety of situations, including civil or criminal trials. To see how this works, note that trial decisions can be factually 
erroneous in two ways. A trial decision can be 
a false positive---i.e.\ a decision to hold the defendant liable (to convict, in a criminal case) even though the defedant committed no wrong (or committed no crime). A trial decision can 
also be a false negative---i.e.\ a decision not to hold the defendant liable (or to acquit, in a criminal case) even though the defendant did commit the wrong (or committed the crime). Let $\cost(CI)$ and $\cost(AG)$ 
be the costs associated with the two decisional 
errors that can be made in a criminal 
trial, convicting an innocent ($CI$) and acquitting a guilty defendant ($AG$). 
Let $\pr{G | E}$ and $\pr{ I|E}$ be the guilt probability and the 
innocence probability estimated on the basis of the evidence presented at trial. 
Given a simple decision-theoretic model [@Kaplan1968decision], a 
conviction should be preferred to an acquittal 
whenever the expected cost resulting from a mistaken conviction---namely, 
$\pr{I | E } \cdot \cost(CI)$---is lower than the expected cost 
resulting from a mistaken acquittal---namely, $\pr{G | E} \cdot \cost(AG)$.
That is, 

\[ \text{convict provided }  		 \frac{\cost(CI)}{\cost(AG)} < \frac{\pr{G | E}}{\pr{I | E }}.\footnote{This follows from $\pr{I | E } \cdot \cost(CI) <  \pr{G | E} \cdot \cost(AG)$.} \]

\noindent
For the inequality to hold, the ratio of posterior probabilities $\frac{\pr{G | E}}{\pr{I | E}}$ should exceed 
the cost ratio $\frac{\cost(CI)}{\cost(AG)}$. So long as the costs 
can be quantified, the probability threshold can be determined. For example, suppose mistaken conviction 
is nine times as costly as a mistaken acquittal. The corresponding probability threshold will be 90\%. 
On this reading, in order to meet the standard of proof beyond a reasonable doubt, the prosecution should 
provide evidence that establishes the defendant's guilt with at least 90\% probability, or in formulas, $\pr{G | E} > 90\%$. The higher the cost ratio $\frac{\cost(CI)}{\cost(AG)}$, the higher the requisite threshold. The lower the cost ratio, the lower the requiste threshold. For example, if the cost ratio is 99, the threshold would be as high as 99\%, but if the cost ratio is 2, the threshold would only be 75\%. 

The same line of argument applies to civil cases. Let a false attribution 
of liability $FL$ be a decision to find the defendant liable when the defendant committed no civil wrong 
(analogous to the conviction of an innocent in a criminal case). 
Let a false attribution of non-liability $FNL$ be a decision not to find the defendant  liable 
when the defendant did commit the civil wrong (analogous to the acquittal 
a factually guilty defendant in a criminal case). Let $\pr{L | E}$ and $\pr{ NL | E}$ be the liability probability and the non-liability probability given the evidence presented at trial. 
So long as the objective is to minimize the costs of erroneous decisions, 
the rule of decision would be as follows:

\[ \text{find the defendant civilly liable provided }  \frac{\cost(FL)}{\cost(FNL)} < \frac{\pr{L | E}}{\pr{NL | E}}.\footnote{This follows from $\pr{ NL | E } \cdot \cost(FL) <  \pr{L | E} \cdot \cost(FNL)$} \]

\noindent
If the cost ratio  $\frac{\cost(FL)}{\cost(FNL)}$ is set to 1, 
the threshold for liability judgments should equal 50\%, a common 
intepretation of the preponderance standard in civil cases. This means that 
$\pr{L | E}$ should be at least 50\% for a defedant 
to be found civilly liable. 

The difference between proof standards in civil and criminal cases lies 
in the different cost ratios. The cost ratio in civil cases,  $\frac{\cost(FL)}{\cost(FNL)}$, is typically 
lower than the cost ratio in criminal cases, $\frac{\cost(CI)}{\cost(AG)}$, because a false positive in a criminal trial (a mistaken conviction) is considered a more harmful error than a false positive in a civil trial (a mistaken attribution of civil liability). This difference in the cost ratio 
can have a consequentialist or a retributivist justification [@walen2015]. From 
a consequentialist perspective, the loss of personal freedom or even life can be 
considered a greater loss than being forced to pay an undue monetary compensation. 
From a retributivist perspective, the moral wrong that results from 
the mistaken conviction of an innocent person can be regarded as more 
egregious than the moral wrong that results from the mistaken attribution of civil liability. 
This difference in consequences or moral wrongs can be captured by positing a 
higher cost ratio in criminal than civil cases. 


Along similar lines, Justice Harlan of the United Supreme Court draws 
a clear differencce in the cost ratio 
between criminal and civil litigation:

\begin{quote}
In a civil suit between two private parties for money damages, for example, we view it as no more serious in general for there to be an erroneous verdict in the defendant's favor than for there to be an erroneous verdict in the plaintiff's favor \dots In a criminal case, on the other hand, we do not view the social disutility of convicting an innocent man as equivalent to the disutility of acquitting someone who is guilty. In Re Winship (1970), 397 U. S. 358, 371.
\end{quote}

\noindent
To underscore the  differences in the cost ratios, 
Harlan cites an earlier decision 
of the United States Supreme Court that emphasizes how a defendant's liberty 
has a transcending value:

\begin{quote}
[t]here is always in litigation a margin of error \dots, representing error in factfinding, which both parties must take into account \dots [w]here one party has at stake an interest of transcending value -- as a criminal defendant his liberty -- \dots this margin of error is \textit{reduced} as to him by the process of placing on the other 
party [i.e.\ the prosecutor] the standard of \dots persuading the factfinder at the conclusion of the trial of his guilt beyond a reasonable doubt. Speiser v. Randall (1958), 357 U.S. 513, 525-26.
\end{quote}

\noindent
Justice Newman of the United States Court of Appeals for the Second 
Circuit made a similar point by linking directly the ratio 
of errors and the degree of certainity required 
for a conviction:


\begin{quote}
... all must recognize that factfinders are
fallible and that any system of adjudicating guilt will inevitably run some
risk of both convicting the innocent and acquitting the guilty....Whatever ratio [of false conviction to false acquittals] we find acceptable, one of the major variables in achieving that ratio is the degree of certainty we impose on factfinders. (p. 980)
QUOTED FROM: Jon O. Newman (1993), Beyond Reasonable Doubt, New York University Law Review, 68(5), pp. 979-1002
\end{quote}


## Beyond just costs: the benefits of correct decisions

The analysis provided so far is limited since 
it only weighs the costs of mistaken decisions, but leaves out the benefits 
of correct decisions. A more comprehensive analysis should 
consider both. Even though the basic idea is the same---that is, 
trial decision-making is viewed as an instrument 
for maximizing overall social welfare [@Posner1973; @Dekay1996; @laudan2016law]---
a mre comprehensive analysis would afford a more nuanced understanding. 
It is therefore instructive to explore the implications 
of weighing the costs of incorrect decisions as well as the 
benefits of correct decisions. 

For simplicity, we will quantify costs 
and benefits with units of utility using the abbreviation $\ut(...)$. 
Benefits will correspond to positive numbers and 
costs to negative numbers.  In a criminal trial, the (negative) utility 
of a mistaken conviction should be weighed together the (negative) 
utility of an incorrect acquittal: $\ut(CI)$ v. $\ut(AG)$. In addition, 
the (positive) utility of a correct conviction should be weighed toghether with the (positive) 
utility of a correct acquittal: $\ut(CG)$ v. $\ut(AI)$. 
Given this set-up, a conviction 
would be justified provided its expected utility---that is,
$\pr{G | E} \cdot \ut(CG) + \pr{I | E } \cdot \ut(CI)$---exceeds the expected utility 
of an acquittal---that is, $\pr{G | E} \cdot \ut(AG) + \pr{I | E} \cdot \ut(AI)$. 
By elementary algebraic steps, 
the threshold is identified by the equation:

\[ \pr{G | E} > \frac{1}{1+\frac{\ut(CG)-\ut(AG)}{\ut(AI)-\ut(CI)}}.\footnote{SHOW COMPUTATIONS HERE}\]

In a number of cases, this new formula 
returns the same threshold as the earlier one. 
If the benefits of convictions and acquittals are zero, this inequality 
identifies the same threshold as the earlier inequality that only considered costs. 
For example, if the costs of a mistaken conviction is nine times the cost of a mistaken acquittal, that is, $\frac{\ut(CI)}{\ut(AG)}=9$, while the benefits are zero, the decision threhshold should be a guilt probability of $\frac{1}{1+(1/9)}=0.9$, as before. Or suppose the magnitude of the benefits resulting from acquitting an innocent defendant (say +9 units of utility) is the same as the magnitude of the costs resulting from convicting an innocent (-9 units of utility). Similarly, suppose the magnitude of the benefits resulting from convicting a guilty defendant (say +1 unit of utility) is the same as the magnitude of the cost resulting from acquitting a guilty defendant (say -1 unit of utility). Again, the threshold would be $\frac{1}{1+(1+1)/(9+9)}=0.9$. 

But consider now the following utility assignments: $\ut(CI)=-9$, $\ut(AG)=-1$, $\ut(CG)= 5$, and $\ut(AI)=5$. The correspoding threshhold would be $\frac{1}{1+(5+1)/(5+9)}=0.7$, significanly below the 90\% threshold. If the benefits of correct decisions are further increaesed, say at 7 units of utility each, the threhsold would be lowered further 
to 66% since $\frac{1}{1+(7+1)/(7+9)}=0.66$. So the the benefits of correct decisions are not at all inconsequential. In order to keep the threshold for criminal convictions releatively high the benefit of correct conviction would have to be close to zero (as seen before) or alternatively, the benefit of a correct acquittals would have to be significantly higher than the benefit of correct conviction. Say $\ut(CG)= 2$ but $\ut(AI)=18$, while still $\ut(CI)=-9, \ut(AG)=-1$, the threhhold would be $\frac{1}{1+(2+1)/(18+9)}=0.9$.

The new inequality shows that the threshold depends on the ratio of the difference 
between utilities, not so much the cost ratio or the benefit ratio. A given cost and 
benefit ratio may correspond to different thresholds. In the examples above, even though the cost ratio was 
fixed at 9:1 and the benefit ratio at 1:1, the threshold was 70% in one case
and 66% in the other. The difference in the threshold is due to the difference in the absolute magnitute of the benefits resulting from correct decisions, increased from +5 units of utility to +7 units of utility. 

A question suggests itself. How should utilities be assigned? 
The assignments may be a moral question (what the right conception of justice dictates), an empirical question 
(what the majority thinks the utilities should be), or 
a political question (how political ideologies affect utility 
assignments). For one thing, elected officials should set the appropriaate 
assignments of utilities, and elected officials should represent 
the will of the people. On other hand, it is curious 
that the standard of proof should be allowed to vary depending
on the political party who is is charge at the moment. Perhaps, 
standards of proof should be part of a country's constitution and 
not vary depending on the political party in power. 

We will now explore different strategies for assigning utility to correct and incorretc 
trial decisions. One stratgy is to identfy the main sources of harm 
and the main sources of benefits reslting from trial decision. The weight placed on each 
diferent harms or benefits is likely to be a matter of political idealogy. 

Let's start with a politically neutral assignment of utilities. Consider the loss resulting from convicting an innocent defendant. This loss includes: the inappropriate assignment of culpability (-1); damaged reputation (-1); loss of income (-1); severance from family members (-1); putting other citizens at risk of victimization by failing to convict the actual perpetrator (-1); weakening the deterrence function of the trial system by failing to apprehend the perpetrator (-1). This is a total utility loss of -6. What about the correct acquittal of an innocent? There woud be no inappropriate assignment of culpability (0); no reputional damage (0); no loss of income (0); no severance from familiy members (0). The actual perpetrator, however, could still victimize others (-1) and deterrence would be weakened (-1). The innocent defendant who is acquitted could still experience damaged reputation and other harms, but we shall leave these details aside. All in all, trying an innocent defendant, no matter the final decision, carries the baseline costs of failing to identify the true perpetrator (-1) and putting other citizens at risk of victimization (-1). This baseline cost increases when an innocent person is wrongly convicted. So, by adding everything up, $\ut(AI)-\ut(CI)=-2+6=+4$. Next, consider the loss resulting from acquitting a guilty defendant. This will include putting other citizens at risk of victimization by the perpetrator who is not convicted (-1) and possibly weaken the deterrence function of the trial system (-1). There would be no inappropriate assignment of culpability (0); no damaged reputation (0); no loss of income (0); no severance from family members (0). What about the conviction of a guilty defendant? The defendant would still experience damaged reputation (-1); loss of income (-1); severance from family members (-1). However, citizens would enjoy a lower risk of victimization (+1) and the deterrence function of the trial system would be reaffirmed (+1). There would also be no incorrect assertions about the citizen's culpabilty (0). All things considered, $\ut(CG)-\ut(AG)=-1+2=+1$. So, assuming the utilities are correctly assigned, the threshold for a criminal conviction should be $\frac{1}{1+\frac{1}{4}}=0.8$, lower than the earlier $90\%$ threshold. (Incidentally, if the correct assignment of culpability is counted as a 
positive benefits (say +1) from a correct conviction, then $\ut(CG)-\ut(AG)=0+2=+2$ and the threshold 
would be $\frac{1}{1+\frac{2}{4}}=0.6666$, an even lower threshold.)

The above analysis is---arguably---politically neutral because it takes into account the costs of a conviction, even for those who are factually guilty, such as loss of income and severance from familiy members. These are issues often emphasized by those on the left who are wary of the costs of criminalization and punitiveness, even for those who, strictly speaking, did committ a crime. At the same time, the above analysis also takes into account the negative consequences that result from failing to apprehend the actual perpetrator, such as hightened crime victimization for others, a point often made by people on the right who are concerned with so-called "law and order". 

Suppose someone is extremely concerned about risk of victimization for different reasons, 
such as more conservative political views or having grown 
up in a high crime area. Suppose the costs resulting from the 
risk of victimization resulting from a false acquittal or the trial of an innoocent 
are increased from -1 to -3.  Converserely, the benefits resulting from 
lower risk of of victimizatio associated with a correct conviction are also increased 
from +1 to +2. Then, $\ut(AI)-\ut(CI)=+4$, as before, but $\ut(CG)-\ut(AG)=0+3=+3$. The threshold would be $\frac{1}{1+\frac{3}{4}}=0.57$, significantly lower than before. Unsurprisingly, someone who is very concerned with the  risk of victimization will favor a lower threshold for conviction in criminal cases. (CITE LAUDAN and SAUNDERS HERE). 

Let's now explore the view of someone---perhaps ore progressive, liberal and left-leaning---who is 
more concerned about negative effects of criminalization, such as loss of income 
and severance from family members. Suppose the utility loss is increased from -1 to -2 for each of these items.
Then, $\ut(AI)-\ut(CI)=-2+8=+6$, as before, but $\ut(CG)-\ut(AG)=-3+2=-1$. The threshodl would be $\frac{1}{1+\frac{-1}{6}}=1.2$. Interestingly, under this assignment of utilities, convciting a defendant never maximizes expected utility unless the evidence establishes guilt with 120% probability. This is clearly impossible. In other words, if the costs of criminalization are so high, convicting 
anyone is never justified, not even someone whose guillt is 100% probable. 

This conclusion could show one of two things. First, it could show that the 
assignment of utility above is non-sensical, because it leads to the non-sensical conclusion that no one should ever be convicted. Alternatively, those who stand by that assignment of utilities (or one like that) will be committed to say that the practice of convicting people as we know it shoud be abolished, at least until the cost of criminalization become less burdensome. The latter view is by no means a non-starter, as it agrees with radical proposals about prison abolition. (CITE APPROPRIATE REFERENCES ABOUT PRISON ABOLITION).

As the above discussion shows, 
probabilistic thresholds, when paired with expected utility theory, provide an analytical framework 
to justify, or at least meaningfully debate, different degrees 
of stringency necessary for decision criteria---i.e. legal proof 
standards---in criminal trials. The same discusison could be 
had for civil trials.

This analytical framework allows for even 
finer distinctions, not explictily codified in the law.
The law typically  makes coarse distinctions between standards of proof, such as 'proof beyond a reasonable doubt' for criminal cases, 'preponderance of the evidence' for civil cases and `clear and convincing evidence' for a narrow subset of civil cases in which the accusation against the defendant is particularly serious. But for rather different crimes, associated with rather different punishments, say murder and grand theft, the same standard of proof is applied for both. It is not obvious why this should be so, except that a finer distinction may cause more confusion than there need be.  If the probability required for a conviction or a finding of civil liability against the defendant is a function of weighing the costs and benefits that would result from true and false positives (as well as true and false negatives), the stringency of the threshold should depend on costs and benefits, and thus different cases may require different thresholds. Cases in which the charge is more serious than others---say, murder compared to grand theft---may require higher thresholds so long as the cost of a mistaken decision against the defendant is more significant.
In countries that allow for the death penalty or life imprisonment for certain 
crimes but not others, the cost of a mistaken conviction would be more serious for crimes with harsher punishments, other things being equal. Thus, the threhold should be placed appropriately higher. We could even think that the threhsold should vary across individual cases even for defendants charged with the exact same crime, provided the costs are different for different individuals. However, whether or not standards of proof should vary in this way is debated [@kaplow2012; @picinali2013]. Ultimately, the question 
is what considerations should be admissible in the calculus of costs and benefits. 

The discussion so far might have proceeded from 
the wrong assumption. To weigh the costs and benefits 
of convictions and acquittals, correct and incorrect, is one thing. 
It is another to weigh the costs and benefits of punishment. So perhaps the calculus 
 should only strictly apply to trial decisions, not to what flows from them. 
 
 
 
 




NEEDS MORE EXPLANATION HERE

1) OTHER QUESTION TO ADDREES IS WHAT COST AND BENEFITS? MAYBE ONLY COSTS AND BENEFIT OF A CONVICTION (BLAME ATTRIBUTION), NOT EVERYTHING THAT FOLLOWS FROM A CONVICTION. 

2) RELATED POINT. SOME COSTS AND BENEFITS ARE ASSOCIATED WITH LITIGATION PER SE, OTHERS WITH PUNISHMENT, SO IN TALKING ABOUT COSTS AND BENEFITS OF TRIAL DECISION PERHPAS WE SHOULD TAKE A MORE NARROW APPROACH. WHAT ARE THE UNIQUE COSTS AND BENEFITS OF TRIAL DECISION?

3) SOME COSTS AND BENEFITS ARE SHARED BY MULTIPLE TYPES OF DECISIONS. E.G. CONVICTION ALL DEPRIVE THE DEFENDANT OF INCOME AND FAMILY TIES, WHETHER THE DEFENDANT IS GUILTY OR INNOCENT. DOES THIS MEAN THESE COSTS AND BENEFITS ARE IRRELEVANT FOR THE CALCULUS OF UILITY?

4) WHAT THE ANALYSIS IS MISSING A LARGER LOOK AT THE CONTEXT OF THE TRIAL, PLEAE BARGAINING, THE CRIMINAL JUSTICE SYSTEM AND SOCIETY MORE GENERALLY. MULTI STAGE ANALYSYS. THE MAXIMIZATION OF EXP UTILITY FRAMEOWKR OBSCURES THIS COMPLEXITY.

5) COMPLEXITY PROBLEM. ALLEN.

\subsection{SUGGESTION}

MARCELLO: IF WE END UP DIVIDING THIS CHAPTER INTO TWO OR THREE SEPARATE CHAPTERS, 
WE COULD CONTINUE THE DISCUSISON OF THE ANALYTICAL POWER OF THE PROBABILITSTIC APPROACH 
MORE IN DETAIL HERE, DRAWING ON SOME OF THE MATERIALS ALREADY IN THE LONGER VERSION 
OF THE SEP ENTRY. 

HERE IS A TENTATIVE IDEA OF WHAT TO DISCUSS:

(1) SIMPLE EXPECTED UTILITY MODEL  - \textbf{DONE, SEE ABOVE}

(2) LAUDAN MODEL, THIS IS A MORE COMPLICATED EXPECTED UTILITY MODEL, 
PARTLY BORROWED FROM LAPLACE  - \textbf{YET TO BE DONE}

(3) SIGNAL DETECTION THEORY MODEL - \textbf{YET TO BE DONE, ONLY PARTLY DONE}

(4) HAMER MODEL AND KAYE MODEL FOR ERROR MINIMIZATION (DISCUSSED IN THE SEP ENTRY, INTEGRALS, DERIVATIVES, ETC.) - \textbf{DONE SEE BELOW}

(5) GOOD AND BAD THINGS ABOUT THESE MODELS, BUT OVERALL THEY SHOW THAT THE PROBABILISTIC FRAMEWORK IS A RICH ANALYTICAL TOOL \textbf{YET TO BE DONE}



\subsection{Minimizing overall errors}

Instead of maximizing expected utility (or minimizing expected costs), 
standards of proof can be analyzed as decision criteria that have 
long term effects on the epistemic performance of the trial system. 
Think about the criminal justice as a whole, making decisions about the guilt and innocence of thousands of defendants facing trial. The system will make a number of 
decisional errors, committing type I and type II errors. Viewing 
standards of proof as probability thresholds helps to 
understand how decisional errors are managed and allocated at this systemic level. 

Consider an idealized model of the criminal trial system. Each defendant is assigned a probability $x$ of criminal liability (or guilt) based on the evidence presented at trial. As is customary, this probability ranges between 0 and 1, or 0\% and 100\%. Since over a period of time many defendants face charges, 
the guilt probability will have its own distribution.  Extreme guilty probabilities set at 0\% or 100\%, presumably, are assigned rarely in trials if ever, while values between 40\% and 80\% are more common. A rigorous way to express this distribution is by means of a probability density function, call it  $f(x)$. The figure below uses a right skewed distribution, for example, $\textsf{beta(18,3)}$. 


\begin{center}
    \includegraphics[width=10cm]{beta(18,3)2.png}
\end{center}
 
 \noindent
 What does the distribution represent? Does it represent the probability of guilt assigned to defendants at the beginning or the end of the trial? There should be a difference between the two---hopefully---or else trial proceedings would be useless. Suppose that the distribution represents the guilt probabilities as they are assigned to defendants at the end of the trial, once all the evidence, counterevidence, arguments and counterarguments have been proferred and weighed appropriately.

The choice of the distribution is for illustrative purposes only. There are no empirical 
data suggesting this is the right distribution to use. But its choice is not 
arbitrary either. The right skew of the distribution reflects the assumption that defendants in criminal cases are prosecuted only if the incriminating evidence against them is strong. It should be no surprise that most defendants are assigned a high probability of guilt. This is plausible in principle. For people should not be prosecuted if the evidence against them is weak. The distribution of the probability of liability in civil cases over a period of time might look quite different, perhaps centered around 50\% or 60\%.

 In the figure above, the threshold for conviction is set at $>80\%$, and the 
area under the curve to the right of the threshold is about $.79$. According to this model, 
79\% of defendants on trial are convicted and 21\% acquitted. These figures are close to the rates 
of conviction and acquittal in many countries (REFERENCES?). 
Since $f(x)$ is a probability density, the total area under the curve 
adds up to 1, encompassing all defendants, both convicted and acquitted defendants. 

If the threshold becomes more stringent---for example, it moves up to 85\%---the rate of 
conviction would decrease. This holds provided the underlying distribution does not change. But, if the threshold is set higher, those who are prosecuted will tend to face comparatively stronger evidence and thus the distribution will become more skewed toward the right---say \textsf{beta(25,3)}. As a consequence, the rate of conviction could still be about 79\% even with a more stringent threshold of 85\%. 

\begin{center}
    \includegraphics[width=10cm]{dbeta(25,3)2.png}
\end{center}


The two graphs above depict the rate of conviction among those who are facing trial, not the rate of conviction in the general population overall. As just shown, the rate of conviction could remain the same even if the probability threhsold is made more stringent. But, the rate of conviction in the general population is likely to diminish so long as higher thresholds, by acting as deterrents against prosecution, make it less likely that people would be prosecuted . 

This formal model does not yet make any distinction between factually guilt and factually innocet defendants. But, presumably, some defendants committed the acts they are accused of and others did not. This is not a clear-cut distinction, however. Some defendants may have committed the acts they are accused of to some extent, but not to the full extent they are accused of, while others may be completely innocent of any crime whatsoever. Leaving this subtlety aside, the formal model can be refined to distinguish between factually innocent and guilty defendants.  

The simplest refinement would create two separate distributions, one distribution for the factually innocent defendants and the other for the factually guilty defendants. The problem with this is that we have little idea about what these distributions should look like in the first place. Hopefully, the innocent distribution will be more left skewed and the guilty distribution more right skewed. Guilty defendants should be assigned, on average, higher guilt probabilities than innocent defendants. The two distributions could still overlap to some extent as some guilt defendants could be assigned as low guilt probabilities as some innocent defendants and conversely some innocent defendants could be assigned as high guilt probabilities as some guilty defendants. This is unfortunate, but also an inevitable consequence of the fallibility of the trial system. 

A more principled way to add two separate distributions to the model, one for guilty and another for innocent defendants, would be to derive them from the overall distribution of defendants. This can be done by following the simple principle that, among those defendants who are assigned a probability of, say, 80%, there should be a corresponding proportion of 80% guilty people and 20% innocent people. These are of coruse expected values, not actual values. Say you are throwing a fair six-faced die. In the long run, you would expect that in 1/6 of the throws the die would land, say on "4".

The expected proportion of guilty and innocent defendants on trial, out of all defendants, can be inferred from the density distribution $f(x)$ under certain assumptions. Suppose each defendant is assigned a guilt probability based on the best and most complete evidence. From the perspective of judges and jurors (or anyone who has access to the evidence and evaluates it the same way), $x\%$ of defendants who are assigned  $x\%$ guilt probability are expected to be guilty and $(1-x)\%$ innocent. For example, 85\% of defendants  who are assigned a 85\% guilt probability are expected to be guilty and 15\% innocent; 90\% of defendants  who are assigned a 90\% guilt probability are expected to be guilty and 10\% innocent; and so on.  

So the expected guilty distribution as a function of $x$ will be $x f(x)$, while the
expected innocent distribution will be $(1-x)f(x)$. In other words, the function $xf(x)$ describes the (expected) assignment  of guilt probabilities for guilty defendants, and similarly, $(1-x)f(x)$  the (expected) assignment of guilt probabilities for innocent defendants. Neither of these functions is a probability density, since %$\int_0^1 \! xf(x) \, \mathrm{d}x=0.86$ and $\int_0^1 \! (1-x)f(x) \, \mathrm{d}x=0.14$. These numbers express the (expected) proportion of guilty and innocent defendants out of all defendants on trial, respectively 86\% and 14\%. 
 
The rates of incorrect decisions---false convictions and false acquittals or more generally false positives and false negatives---can be inferred from this model as a function of the threshold $t$ [@hamer2004; @hamer2014]. The integral $\int_0^t \! xf(x) \, \mathrm{d}x$ equals the expected rate of false acquittals, or in other words, the expected proportion of guilty defendants who fall below threshold $t$ (out of all  defendants), and the  integral $\int_t^1 \! (1-x)f(x) \, \mathrm{d}x$ equals the expected rate of false convictions, or in other words, the expected proportion of innocent defendants who fall above threshold $t$ (out of all defendants).
The rates of correct decisions---true convictions and true acquittals or more generally true positives and true negatives---can be inferred in a similar manner. The integral $\int_t^1 \! xf(x) \, \mathrm{d}x$ equals the expected rate of true convictions and $\int_0^t \! (1-x)f(x) \, \mathrm{d}x$ the expected rate of true acquittals. In the figure below, the regions shaded in gray correspond to false negatives (false acquittals) and false positives (false convictions). The remaining white regions within the solid black curve correspond to true positives (true convictions) and true negatives (true acquittals). Note that the dotted blue curve is the original overall distribution for all defendants. 


 \begin{center}
    \includegraphics[width=10cm]{xfx3.png}
\end{center}



\begin{center}
    \includegraphics[width=10cm]{nxfx3.png}
\end{center}


The size of the grey regions in the figures above---which correspond to false positives and false negatives---is affected by the location of threshold $t$. As $t$ moves upwards, the rate of false positives decreases but the rate of false negatives increases. Conversely, as $t$ moves downwards, the rate of false positives increases but the rate of false negatives decreases. This trade-off is inescapable so long as the underlying distribution is fixed. We have already remarked on the possibility that the distribution would change in shape as a result of changes in the probability threhsold. We will retunr to this point later in the chapter.

Below are both error rates---false positives and false negatives---and their sum plotted against a choice of $t$, while holding fixed the density function $\textsf{binom(18,3)}$. The graph shows that any threshold that is no greater than 50\% would minimize the total error rate %(comprising false positives and false negatives). 
A more stringent threshold, say $>90\%$, would instead  significantly reduce the rate of false positives but also significantly increase the rate of false negatives, es expected. 

\begin{center}
    \includegraphics[width=12cm]{errors.png}
\end{center}
 
In general, the threshold that minimizes the expected rate of incorrect decisions overall, no matter the underlying distribution, lies at $50\%$. The claim that setting threshold at $t=.5$ minimizes the expected error rate holds given the distribution $f(x)=$beta(18,3) as well as any other distribution 
\citep{kaye1982limits, Kaye1999Clarifying-the-, cheng2015}. To show this, 
let $E(t)$ %as a function of threshold $t$ be the sum of  rates of 
false positive and false negative decisions:

\[E(t) = \int_0^t \! x f(x) \, \mathrm{d}x + \int_t^1 \! (1-x) f(x) \, \mathrm{d}x.
\]

 The overall rate of error is minimized when  $E(t)$ is the lowest. 
 To determine the value of $t$ for which $E(t)$ is the lowest, set
the derivative of $E(t)$ %and $R(t)$ 
to zero, that is, $\frac{d}{dt}  E(t)= 0$. 
By calculus,
$t=1/2$.\footnote{Note that $\frac{d}{dt}  E(t)$ is the the sum of the derivatives of $\int_0^t \! x f(x) \, \mathrm{d}x$ 
and 

$\int_t^1 \!(1-x) f(x) \, \mathrm{d}x$
, that is,

\[\frac{d}{dt} E(t) = \frac{d}{dt}  \int_0^t \! x f(x) \, \mathrm{d}x + \frac{d}{dt}  \int_t^1 \! (1-x) f(x) \, \mathrm{d}x.\]

By the fundamental theorem of calculus, 

\[\frac{d}{dt}   \int_0^t \! x f(x) \, \mathrm{d}x = tf(t) \text{ and }
\frac{d}{dt}   \int_t^1 \! (1-x) f(x) \, \mathrm{d}x = -(1-t)f(t). \]

By plugging in the values, 

\[\frac{d}{dt}  E(t) = tf(t)  -(1-t)f(t). \]

Since $\frac{d}{dt}  E(t)= 0$, then $tf(t)  = (1-t)f(t)$
and thus
$t  = 1-t$, so 
$t  = 1/2$ or a $>50\%$ threshold.
} 
This claims holds when the two decisional errors are assigned the same weight, or in other words, the costs of false positives and false negatives are  symmetric. The $>50\%$ threshold therefore should be most suitable for civil trials. In criminal trials, however, false convictions are typically considered significantly more costly than false acquittals, say a cost ratio of 9:1 (but see [@epps2015]). The sum of the two error rates can be weighted by their respective costs:

\[E(t) = \int_0^t \! x f(x) \, \mathrm{d}x + 9\int_t^1 \! (1-x) f(x) \, \mathrm{d}x.
\]

Given a cost ratio of 9:1, the optimal threshold that minimizes the (weighted) overall rate of error is no longer $1/2$, but rather, $t=9/10=90\%$.\footnote{The proof is the same as before. Since $tf(t)  = 9(1-t)f(t)$, it follows that 
$t  = 9/10$.} 

Whenever the decision threshold is more stringent than $>50\%$, the overall (unweighted) error minimization may be sacrificed to pursue other goals, for example, protecting more innocents against mistaken convictions, even at the cost of making a larger number of mistaken trial decisions overall. 

 The standard `proof beyond a reasonable doubt' is often paired with the Blackstone ratio, the principle that it is better that ten guilty defendants go free rather than even just one innocent be convicted. The exact ratio is a matter of controversy [@voloch1997]. It is tempting to think that, say, a 99\% threshold guarantees a 1:99 ratio between false convictions and false acquittals. But this would be hasty for at least two reasons.
First, probabilistic thresholds affect the expected rate of mistaken decisions. The actual rate may deviate from its expected value [@Kaye1999Clarifying-the-]. Second, if the threshold is $99\%$, \textit{at most} 1\% of decision against defendants are expected to be mistaken (false convictions) and \textit{at most} 99\% of the decisions in favor of the defendant are expected to be mistaken (false acquittals). The exact ratio will depend on the probabilities  assigned to defendants and how they are distributed  \citep{allen2014}. The (expected) rate of false positives and false negatives---and thus their ratio---depend on where the threshold is located but also on the distribution of the liability probability  as given by the density function $f(x)$.



## Interval thresholds (Finkelstein)

The prior probability cannot be easily determined [@Friedman2000presumption]. Even if it can be determined, arriving at a posterior probability might be impractical because of lack of adequate quantitative information. Perhaps, decision thresholds should not rely on a unique posterior  probability but on an interval of admissible probabilities given the evidence [@finkelstein1970bayesian].  Perhaps, the assessment of the posterior probability of guilt can be viewed as an idealized process, a regulative ideal which can improve the precision of legal reasoning. (CITE BIEDERMAN TARONI).

-->


# Theoretical challenges - NEW CHAPTER WOULD STATRT HERE



Let's take stock. We briefly examined difficulties in implementation 
for probabilistic standards of proof and set those aside. We then offered a 
few illustrations how probabilistic standards can be used as 
analytical tools to theorize about decision-making at trial.
But even if probabilistic thresholds are used solely as analytical 
tools, legal probabilists are not yet out of the woods. 
Even if the practical problems can be addressed or set aside, theoretical difficulties remain. 
We will focus on three in particular: the problem of priors; naked statistical 
evidence; and the difficulty about conjunction, 
also called the conjunction paradox.
The latter two are difficulties that any theory of 
the standard of proof -- not just a probabilistic theory -- should be able to address. 
The first difficulty is peculiar 
to the probabilistic interpretation 
of standards of proof. We will examine 
each difficulty in turn and 
then examine a promising line of response within 
legal probabilism based on 
likelihood ratios instead of posterior probabilities.







 
## The problem of priors


 
 
## Naked statistical evidence

Suppose one hundred, identically dressed prisoners 
are out in a yard during recreation. Suddenly, ninety-nine of them assault and 
kill the guard on duty. We know that this is what happened 
from a video recording, but we do not know the identity 
of the ninety-nine killers. After the fact, a prisoner is 
picked at random and tried. Since he is one of the prisoners 
who were in the yard, the probability of his guilt would be 99\%. 
But despite the high probability, many have the intuition that this is not enough to establish 
guilt beyond a reasonable doubt. Hypothetical scenarios of this sort suggest that a high probability of guilt, 
while perhaps necessary, is not sufficient to establish guilt
beyond a reasonable doubt.  

Perhpas, the resistance in the prisoner scenario lies in the fact 
that the prisoner was picked at random, and that any prisoner would 
be 99\% likely to be one of the killers. 
Since the statistics cannot single out the one innocent prisoner, they are bad evidence.
But consider this case. Suppose two people enter a department store. 
There are no other customers in the store. After they exit the store, a member of the 
staff finds that an item of merchandise is missing. Since no staff member could be culpable---they are strictly surveilled---the culprit must be one of the customers. One of the customers, John, 
has scored high in a compulsivity test and has been arrested for stealing in department stores several times in the past. The other customer, Rick, has never been arrested  for stealing in a department store and shows 
no sign of high compulsivity. Statistics show that people with a high degree of compulsivity and who have stolen merchandise  in department stores before are more likely than others to steal merchandise if they are unsupervised.  So John is most likely the culprit. Suppose studies show that people like John, when unsupervised, will steal 99 times out of 100 times.  Instead, people like Rick, when unsupervised, will only steal 1 time out of 100 times. 
So John is 99 times more likely than Rick to have stolen the merchandise. Can these statistics 
be enough to convict John? Again, it seems not. There is no evidence against him specifically, say, no merchandise was found on him that could link him to the crime. Many would feel uneasy about convicting John despite the 
fact that, between the two suspects, he is the one who is most likely the culprit.

A similar hypothetical can be constructed for civil cases. 
Suppose a bus company, Blue-Bus, operates 90\% of the buses 
in town on a certain day, while Red-Bus only 10\%. 
That day a bus injures a pedestrian. Although the buses of the two companies can be easily recognized because 
they are respectively painted blue and red, the pedestrian who was injured cannot
remember the color of the bus involved in the accident. No other witness was around. Still, given the statistics about the market shares of the two companies, it is 90\% probable that a Blue-Bus bus was involved in the accident. This is a high probability, well above the 50\% threshold. Yet the 90\% probability that a Blue-Bus bus was involved in the accident would seem---at least intuitively---insufficient for a judgment of liability against Blue-Bus.  This intuition challenges the idea that the 
preponderance standard in civil cases only requires that the plaintiffi establish 
the facts with a probability greater than 50\%. 

Confronted with these hyptheticals, legal probabilists could push back. 
Hypotheticals rely on intuitive judgments, for example, 
that the high probability of the prisoners's guilt in the scenario above does not amount to proof beyond a reasonable doubt. But suppose we changed the numbers and imagined there 
were one thousand prisoners of whom nine hundred  and ninety-nine killed the guard. The guilt probability of a prisoner picked at random would be 99.9\%. Even in this situation, many would insist that guilt has 
not been proven beyond a reasonable doubt despite the extremely high probability of guilt. But others might say that when the guilt probability reaches such extreme values, values as high as 99.9\% or higher, people's intuitive resistance to convicting should subside [@Roth2010]. A more general problem is that intuitions in such hypothetical scenarios are removed from real cases and thus are  potentially unreliable as a guide to theorize about standards of proof [@Lempert1986; @allen2001naturalized; @HeddenColyvan2019legal].

Another reason to be suspicious of these hypotheticals 
is that they seem to amplify biases in human reasoning. 
Say an eyewitness was present during the accident and testified that a Blue-Bus bus was involved. Intuitively, the testimony would be considered enough to rule against Blue-Bus, at least provided the witness survived cross-examination. We exhibit, in other words, an intuitive preference for judgments of liability based 
on testimonial evidence compared to judgments based on statistical evidence. This preference has been experimentally verified [@wells1992naked; @niedermeierEtAl1999; @arkesEtAl2012] and exists outside 
the law [@sykes1999; @friedman2015; @ebert2018]. 
 But testimonial evidence is no less prone to error than 
 statistical evidence. In fact, it may well be more prone to error.
 The unreliability of eyewitness testimony is well-known, 
 especially when the environmental conditions 
 are not optimal [@Loftus1996]. So are we justified in exhibiting an intuitive preference for eyewitness testimony as opposed to statistical evidence, or is this preference a cognitive bias to avoid?


These reservations notwithstanding, 
the puzzles about naked statistical evidence 
cannot be easily dismissed.
Puzzles about statistical evidence in legal proof 
have been around  for a while [@Cohen1977The-probable-an; @Kaye79gate; @Nesson1979Reasonable-doub; @thomson1986liability]. Philosophers and legal scholars have shown a renewed 
interest in both criminal and civil cases [@wasserman1991morality; @Stein05; @redmayne2008exploring; @ho2008philosophy; @Roth2010; @Enoch2012Statistical; @cheng2012reconceptualizing; @pritchard2005epistemic; @BlomeTillmann2017; @nunn2015;  @pundik2017; @moss2018;  @pardo2018; @smith2017; @bolinger2018rational; @diBello2019]. Given the growing interest in the topic, 
legal probabilism cannot be a defensible theoretical position without offering 
a story about naked statistical evidence. 


# Conjuction paradox -- NEW CHAPTER HERE DEVOTED TO PROBABILITY BASED SOLUTIONS


Another theoretical difficulty that any theory of the standard of proof should 
address is the the conjunction paradox or difficulty about conjunction. First formulated by @Cohen1977The-probable-an, the difficulty about conjunction has enjoyed a great deal of scholarly attention every since [@Allen1986A-Reconceptuali; @Stein05; @allen2013; @haack2011legal; @schwartz2017ConjunctionProblemLogic; @AllenPardo2019relative]. 
This difficulty arises when an accusation of wrongdoing, in a civil or criminal proceeding, 
is broken down into its constituent elements. The basic problem is that the probability of a conjuction 
is often lower than the probability of the conjuncts. Thus, even if each conjunct meets the requisite 
probability threshold, the conjunction does not. This chapter examines the difficulty about conjuction 
and how legal probabilists can respond.


## The problem


Suppose that in order to prevail in a 
criminal trial, the prosecution should establish by the required standard, first, that the defendant caused harm to the victim (call it claim $A$), and second, that the defendant had premeditated the harmful act (call it claim $B$). @Cohen1977The-probable-an argues that common law systems subscribe to a conjunction principle, that is, if $A$ and $B$ are established according to the governing standard of proof, so is their conjunction (and vice versa).  If the conjunction principle holds, the following must be equivalent, where $S$ is a placeholder for the standard of proof:

\begin{center}
\begin{tabular}
{@{}ll@{}}
\toprule
\textbf{Separate} &   A is established according to S and B is established according to S\\   
\textbf{Overall}  &   The conjunction $A \et B$ is established according to S  \\ 
\bottomrule
\end{tabular}
\end{center}

\noindent
Let $S[X]$ mean that claim or hypothesis $X$ is established according 
to standard $S$.  Then, in other words, the conjunction principles requires that:
\[S[A \wedge B] \Leftrightarrow S[A] \wedge S[B].\]



The conjunction principle is 
consistent with---perhaps even required by---the case 
law. For example, the United States Supreme Court 
writes that in criminal cases 

\begin{quote}
the accused [is protected] against conviction except upon proof beyond a reasonable doubt of \textit{every fact} necessary to constitute the crime with which he is charged. In re Winship (1970), 397 U.S. 358, 364. 
\end{quote}

\noindent
A plausible way to interpret this quotation is to posit this 
identity: to establish someone's guilt beyond a 
reasonable doubt \textit{just is} to establish each 
element of the crime beyond a reasonable doubt. Thus, 

\begin{align*}\mathsf{BARD}[A_1 \wedge \cdots \wedge A_n] \Leftrightarrow \mathsf{BARD}[A_1] \wedge \cdots \wedge \mathsf{BARD}[A_n],
\end{align*}

\noindent
where the conjunction $A_1 \et \cdots \et A_n$ comprises all the material 
facts that, according to the applicable law, 
constitute the crime with with the accused is charged.

<!--

One could quibble with this intepretation. Perhpas, what the law requires is only the left-to-right direction---that $\text{if }BARD[A \wedge B] \text{ then } BARD[A] \wedge BARD[B]$---not the right-to-left direction---that $\text{if } BARD[A] \wedge BARD[B] \text{ then }BARD[A \wedge B]$. The left-to-right direction is, indeed, the least controversial.\todo{discussion in terms of equation sides is confusing, figure out terms that are more meaningful} If the conjuction is established to the required standard, so should its conjuncts.\todo{This just repeats the claim, I don't know what job it does.} It would be odd if this was not the case. The other direction is more controversial.\todo{Why?} But it is plausible that one would establish the conjunction by establishing each conjunct one by one. As we shall see later, both directions can be questioned. At least, the conjunction principle has some initial plausibility. \todo{Weakened the claim, check}

-->


The problem for the legal probabilist is that the conjunction principle conflicts 
with a threshold-based probabilistic interpretation 
of the standard of proof. For suppose the prosecution presents evidence that establishes claims $A$ and $B$, separately, to the required probability, say about 95\% each. Has the prosecution met the burden of proof? Each claim was established to the requisite probability threshold, and thus it was established to the requisite standard (assuming the threshold-based interpretation of the standard of proof). And if each claim was established to the requisite standard, then (i) guilt as a whole was established to the requisite standard (assuming the conjunction principle). But even though each claim was established to the requisite probability threshold, the probability of their conjunction---assuming the two claims are independent---is only $95\%\times95\%=90.25\%$, below the required 95\% threshold. So (ii) guilt as a whole was \textit{not} established to the requisite standard (assuming a threshold-based  probabilistic interpretation of the standard). 
Hence, we arrive at two contradictory conclusions: (i) that the prosecution met its burden of proof 
and (ii) that it did not meet its burden.

The difficulty about conjunction---the fact that a probabilistic interpretation of the standard of proof conflicts with the conjunction principle---does not subside when the number of constituent claims increases. If anything, the difficulty becomes more apparent. Say the prosecution has established three separate claims to 95\% probability. Their conjunction---again if the claims are independent---would be about 85\% probable, even further below the 95\% threshold.  Nor does the difficulty about conjunction subside if the claims are no longer regarded as independent.
The probabilty of the conjunction $A \et B$, without the assumption of independence, equals $\pr{A | B} \times \pr{B}$. 
But if claims $A$ and $B$, separately, have been established to 95\% probability, enough for 
each to meet the threshold, the probability of $A \et B$ could still be below 
the 95\% threshold unless $\pr{A | B}=100\%$. 
For example, that someone premediated a harmful act against another (claim $B$) makes it more likely that they did cause harm in the end (claim $A$). Since $\pr{A | B} > \pr{A}$, the two claims are not independent. 
Still, premeditation does not always lead to harm, so $\pr{A | B}$ should be below 100\%. Consequently, in this case, the probability of the conjunction $A \et B$ would be below the 95\% threhsold.\todo{False in whole generality, give a counterexample with more specific numbers. M: I changed things a bit. Maybe not it's clear now. The counterexample is basically P(A)=P(B)=0.95, but P(AB)=Pr(A)*P(A|B) and since P(A|B) is below 1, then P(AB) is below 0.95.}

How could a supporter of legal probabilism respond? The conjunction paradox is a difficult problem, as the vast literature on the topic attests. Before we offer our solution to it, we will explore a number of probability-based proposals, and understand why they, albeit promising, ultimately fail. 


## A closer look at the conjunction principle


The conjunction paradox would not arise without 
the conjunction principle. So could legal 
probabilists reject 
this principle and let the paradox disappear? 
<!-- On its face, the conjunction principle is no different from closure principles 
used in epistemic logic. Let $\Box$ the modal operator for belief or knowledge.
The conjunction principle is a special version 
of the closure principle $\Box A \wedge \Box B \leftrightarrow \Box (A \et B)$. (CITE MODAL LOGICS THAT ADOPT THIS PRINCIPLE.) \todo{Ok, I will add a footnote about this.}
--> 
In current discussions in epistemology, an analogous principle about knowledge or justification has been contested because 
it appears to deny the fact that risks of error accumulate (CITE). \todo{Hey, we can quote a paper that's out by Alicja!:) Also, I guess you want me to find the right refs? M: Yes, if you can} If one is justifiably sure about the truth of 
each claim considered seperataly, one should not be equally sure of their conjunction. You have checked 
each page of a book and found no error. So, for each page, you are nearly sure there is no error. Having checked each page and found no error, can you be sure that the book as a whole contains no error? Not really. As the number of pages grow, it becomes virtually certain that there is at least one error in the book you have overlooked, although for each page you are nearly sure there is no error. (ADD CITATION ABOUT PREFACE PARADOX) The same applies to other contexts, say product quality control. You may be sure, for each product you checked, that it is free from defects. But you cannot, on this basis alone, be sure that all products you checked are free from defects. Since the risks of error accumulate, you must have missed at least one defective product. 

Suppose the legal probabilist does away with the conjunction principle. Now what?
How should they define standards of proof? Two immediate options come to mind, but neither is without problems.  One option stipulates that, in order to establish the defendant's guilt beyond a reasonable doubt (or civil liability by preponderance of the evidence), the party making the accusation should establish each claim, separately, to the requisite probability, say at least 95\%, without needing to establish the conjunction to the requisite probability. Call this the \textit{atomistic account}.  On this view, the prosecution could be in a position to establish guilt beyond a reasonable doubt without estalishing the conjunction of different claims with a sufficiently high probability. This account would allow convictions in cases in which the probability of the defendant's guilt, call it $G$, is low, just because $G$ is a conjunction of several independent claims that separately satisfy the standard of proof. This is counterintuitve as it would allow convictions when the defendant is most likely innocent.  

The other option is to require that the prosecution in a criminal case (or the plantiff in a civil case) establish the accusation as a whole---say the cojunction of $A$ and $B$---to the requisite probability. Call this the \textit{holistic account}. This account is not without problems either. The proof of $A\et B$ would impose a higher requirement on the separate probabilities of the conjuncts. If the conjunction $A\et B$ is to be proven with at least 95\% probability, the individual conjuncts should be established with probability higher than the 95\% threshold. So the more conjuncts, the higher their required probability.   Moreover, the standard that applies to one of the conjuncts would depend on what has been achieved for the other conjuncts. For instance, assuming independence, if  $\pr{A}$ is $96\%$, then $\pr{B}$ must be at least $99\%$ so that $\pr{A\et B}$ is above a $95\%$ threshold. But if $\pr{A}$ is $99.99\%$, then $\pr{B}$ must only be greater than $95\%$ to reach the same threshold. Thus, the holistic account would require that the elements of an accusation be proven to different probabilities depeding on how well other claims have been established. 

Denying the conjunction principle, then, is not without difficulties of its own. Legal probabilists should 
still explain how individual claims relate to larger claims in the process of legal proof. 
So it is worth exploring in some detail whether legal probabilists, instead of rejecting it, could in fact 
vidicate on probabilistic grounds the conjunction principle. 


## Aggregating hypotheses and evidence


So far the discussion proceeded without mentioning explicitly 
the evidence proferred in support of the different claims that 
constitute the allegation of wrongdoing. 
This is, however, a simplification. Say evidence $a$ supports claim $A$ and other evidence 
$b$ supports a distinct claim $B$. The question now is whether 
the combination of $a$ and $b$ supports the conjunction $A \et B$, and viceversa. 
<!--The answer to this question can well be affirmative---and without denying 
that the risks of error accumulate. 
No doubt the possible presence of a defeater $D_a$ would undermine 
the support of $a$ in favor of $A$, and another defeater $D_b$ would 
undermine the support of $b$ in favor of $B$. But, absent any defeater $D_a$ or $D_b$, the combination of
items of evidence $a$ and $b$ should jointly support (again, defeasibly or to whatever suitable standard is required) the combined hypothesis $A \wedge B$. \todo{Should we talk more about what we mean by defeaters, perhaps with examples of what the defeaters in this example might be?}
-->
In other words, the question is whether 
the following conjunction principle holds:
\[\text{S[$a, A$] and S[$b, B$] iff S[$a \wedge b, A\wedge B$]},\]

\noindent
where $\text{S}[e, H]$ means that evidence $e$ supports hypothesis 
$H$ by standard $S$.  This conjunction principle 
differs from the earlier one since it mentions explicitly the 
supporting evidence $a$ and $b$.

<!---
This new formulation of the conjunction principle is compatible 
with the fact that risks accumulate.\todo{I am not convinced; the discussion so far indicates that this is so if $S$ represents defeasible support; but I don't see how the claim follows for any sensible standard $S$ that one could formulate.} Hypothesis $A$ and $B$ could be attacked by a defeater $D_a$ or $D_b$. The conjunction $A \wedge B$ could be attacked by a larger set of defeaters, including $D_a$ or $D_b$. So $A \wedge B$ is more susceptible to attack than any individual claim $A$ or $B$. In this sense, risks do accumulate.\todo{Is there another sense in which they don't? How is this related to the probabilistic risk accumulation that the legal probabilists might care more about?} Still, even though \todo{Why do you say "even though"? I am not sure I see a contrast here...} $A \wedge B$ is more susceptible to attack, it requires a larger body of evidence $a \wedge b$ than each of the individual claim $A$ and $B$. So, the larger body of evidence in support of $A \wedge B$ balances off its greater susceptibility to error.
Defeasible logic provides a formal framework to 
make sense of the conjuction principle (REFERENCES?). 
But defeasible logic aside, the challenge for legal 
probabilists, at this point, is to spell out 
a probabilistic version of the conjunction principle 
for the aggregation of evidence and hypotheses. 
Can this challenge be met? \todo{It's at this point unclear how the probabilistic version of support is supposed to be related to the whole defeasibility buisness. In other words, if probabilistic support is what we're after, why would/should things that hold for defeasible support be our guide?}
-->

If we think of evidential support in terms of conditional probability, 
the conjunction principle above would fail in some cases. 
For suppose that $\pr{A | a}>t$ and $\pr{B | b}>t$, 
for a threshold $t$, or in other words, given the 
supporting evidence $a$ and $b$, 
both $A$ and $B$ are sufficiently probable (for a fixed threshold).
It does not generally follow that $A \et B$ is sufficienlty probable given the combined evidence $a\et b$. 
By the probability calculus,
 \begin{align*}
\pr{A \wedge  B | a \wedge b}& =\pr{A |a \wedge b} \times \pr{B | a \wedge b \wedge A}\\
 & = \pr{A |a} \times \pr{B | b}
 \end{align*}

\noindent
The second equality holds assuming certain relationships of independence, 
specifically, the independence of $A$ from $b$ given $a$, 
and of $B$ from $a \wedge A$ given $b$. These relationships of independence do 
not always hold, but they do sometimes. For example, 
in an aggraveted assault case, evidence $a$ could be a witness testimony that the defendant 
physically injured the victim (claim $A$), and $b$ evidence that the defendant 
knew that the victim was a firefighter (claim $B$), for example, another testimony that the 
defendant earlier called the firefighther for help.  Presumably, $\pr{A \vert a}=\pr{A \vert a \wedge b}$
because the fact that the defendant called a firefighter for help ($b$) does not make it more (or less) likely that 
he would physically injure him ($A$). Further, $\pr{B \vert b}=\pr{B \vert a \wedge b \wedge A}$ because the fact that
the defendant injured the victim ($A$) and there is a testimony to that effect ($a$) does not make it more (or less) 
likely that the victim was a firefighter ($B$). 
<!-- the hypotheses $A$ and $B$,  -->
<!-- and the independence of $A$ from $b$, and  -->
<!-- of $B$ from $a$. In other words, the two hypotheses  -->
<!-- are thought to be independent of one another,  -->
<!-- and their supporting evidence is thought  -->
<!-- to be independent of the other hypothesis.  -->
<!---These assumptions are codified in the Bayesian 
network in Figure \ref{network-conjunction}.---> 
Given these assumptions,  if---as is normally the case---neither $\pr{A \vert a}$ 
nor $\pr{B \vert b}$ equal 1, then
\[\pr{A \wedge B \vert a \wedge b}< \pr{A \vert a} \;\ \& \;\ \pr{A \wedge B \vert a \wedge b} < \pr{B \vert b}. \]

\noindent
This is another manifestation of the difficulty about conjuction.  If each piece of 
evidence $a$ and $b$ supports claims $A$ and $B$ with 95\% probability, the combined 
evidence $a\et b$ need not support the conjuction $A\et B$ with 95% probability. 
The conjunction principle fails here. 

Interestingly, even if the independence assumptions are dropped, the difficulty about conjunction still arises in a number of circumstances.  Suppose evidence $a\et b$ establishes claim $A$ and also claim $B$, separately, right above the probability threshold $t$. Since $\pr{A \wedge  B | a \wedge b} =\pr{A |a \wedge b} \times \pr{B | a \wedge b \wedge A}$, it follows that $\pr{A \wedge  B | a \wedge b}$ would be below $t$ so long as $\pr{B | a \wedge b \wedge A}$ is below 100\%, which would often be the case since (i) evidence is fallible and (ii) one hypothesis does not usually entail the other. So even though $A$ and $B$ are established to the required probability, the conjunction is not. 

## Independencies

EXPLAIN RELATIONSHIP OF DEPENDENCE USED HERE:

- BETWEEN CLAIMS

- BETWEEN PIECES OF EVIDENCE

- USE BAYESIAN NETWORK TO EXPLAIN





## Prior probabilities

One of the troubling consequences of the difficulty about conjunction 
is that, the more constituent claims, 
the higher the posterior probability for each claim needed 
to meet the requisite probability threhsold. Assume, for the sake of illustration, 
the independence and equiprobability of the constituent claims. 
If a composite claim constists of $k$ individual 
claims $I_1 \wedge I_2 \dots I_k$, these individual claims will have 
to be established with proabilility 
$>t^{1/k}$, where $t$ is the applicable threhsold. 
For example, if there are ten constituent claims, they will have to be proven with $0.5^{1/10}=0.93$ 
even if the standard of proof is only $>0.5$. If the standard is more stringent, as is appropriate in criminal cases, say $>0.95$, each individual claim will have to be proven with near certainity, which would make the task extremely demanding on the prosecution. For example, if there are ten constituent claims, they will have to be proven with $0.95^{1/10}=0.995$. 
The alternative, of course, would be to allow the prosecution (or the plaintiff in a civil case) to establish the individual claims by the probability threshold applicable to the case as a whole. If so, the reverse problem would arise. The composite claim representing the case as a whole would often be established with a probability below the required threshold. For example, if each constituent claim is established with 95\% probability, the composite claim---assuming, as usual, probabilistic independence between individual claims---would only be established with 59% probability, a far cry from proof beyond a reasonable doubt. So we have a dilemma here: either the standard is too demanding on the prosecution (or the plantiff) or the standard is too lax and thus it is too easy to establish someone's criminal or civil liability. 


```{r, echo=FALSE}
0.5^(1/10)
```

```{r, echo=FALSE}
0.95^(1/10)
```


```{r, echo=FALSE}
0.95^(10)
```


But something has gone unnoticed so far. A composite claim such as $A\wedge B$ will have, 
other things being equal, a much lower prior probability than an individual claim such as $A$ or $B$.
 In general,  a composite claim consists of $k$ individual claims. If the composite claim has a prior probability of $\pi$, each constituent claim, assuming they are independent and equiprobable, will have a prior probability of $\pi^{1/n}$. The prior probability of the individual claims will approach one as the number of constituent claims increases. This observation led  @dawid1987difficulty, in one of the earliest attempts to solve the conjunction paradox from a probabilistic perspective, to write:

\begin{quote}
\dots it is not asking too much of the plaintiff to establish the case as a whole with a posterior probability exceeding one half, even though this means  that the several component issues must be established with much larger posterior probabilities; for the \textit{prior}  probabilities of the components will also be correspondingly larger, compared with that of their conjunction. The overall effect of subdiving a case into more and more component issues is not very large, and, if anything, is to give an advantage to the plaintiff, even though he has to establish each with a high probability. (p. 97)
 \end{quote}

Following Dawid's insight, it is instructive to compare the impact of one item of evidence on the probability of an individual claim, for different level of sensitivity and specificity of the evidence, with the impact of $k$ items of evidence on the probability of a composite claim that consists of $k$ individual claims, again for different levals of specificity and sensitivity of the evidence. Figure \ref{fig:strength-indiv-joint} (top) compares one item of evidence supporting an individual claim and five items of evidence suporting a composite claim consisting of five claims. There is a significant difference in posterior probabilities, as expected, but there is a also a significant difference in prior probabilities. Since the composite claim starts out less likely than any individual clai, it is natural---other things being equal---that the posterior probability of the composite claim will be correspondingly lower. 

\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


postn <- function (x, s1, s2, n){
  ((x)^n)*
    ((
      s1/
        (s1*x+(1-s2)*(1-x))
     )^n)
    
}

x <-seq(0,1,by=0.001)
y0908 <- post(x, 0.9, 0.8)
y0909 <- post(x, 0.99, 0.99)
yy0908 <- postn(x, 0.9, 0.8, 2)
yy0909 <- postn(x, 0.99, 0.99, 2)
yynull <- postn(x, 0.5, 0.5, 2)
y50908 <- postn(x, 0.9, 0.8, 5)
y50909 <- postn(x, 0.99, 0.99, 5)
y5null <- postn(x, 0.5, 0.5, 5)


ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, sens=0.5, spec=0.5"))+
  #geom_line(aes(x = x, y = yynull,color = "2, sens=0.5, spec=0.5")) +
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  #geom_line(aes(x = x, y = yy0908,color = "2, sens=0.9, spec=0.8")) +
  # geom_line(aes(x = x, y = yy0909,color = "2, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y5null, color = "5, sens=0.5, spec=0.5")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.9, 0.2), legend.title = element_blank())
```

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


postnEq <- function (x, s1, s2, n){
  ((x^(1/n))^5)*
    ((
      s1/
        (s1*x^(1/n)+(1-s2)*(1-x^(1/n)))
     )^n)
}

x <-seq(0,1,by=0.001)
y0908 <- post(x, 0.9, 0.8)
y0909 <- post(x, 0.99, 0.99)
y50908 <- postnEq(x, 0.9, 0.8, 5)
y50909 <- postnEq(x, 0.99, 0.99, 5)
y5null <- postnEq(x, 0.5, 0.5, 5)

ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, null"))+
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.8, spec=0.7")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.8, 0.2), legend.title = element_blank())
```

\caption{The comparison here is betwen individual support and joint support. 
Top graph: The null line for joint support ($y=x*x$) is 
much below the null line for individual support ($y=x$).
Bottom graph: the two null lines are equalized and the 
posterior lines adjusted accordingly. The posterior lines for individual 
and joint support get much closer especially for 
high posterior probability values.}
\label{fig:strength-indiv-joint}
\end{figure}


What happens if we make the same comparison between individual and composite claims by equalizing their prior probability so that the role of prior probabilities is factored out? If the claims are independent and equiprobable, let $x$ be the prior probability of an individual claim 9when it is considered in isolation) and let $x^{1/k}$ the prior probability of the same individual claim when it considered as a constituen claim of a composite claim that consosts of $k$ claim, so that the prior probability of the composite claim equals $(x^{1/k})^k=x$ as desired. Figure \ref{fig:strength-indiv-joint} (bottom) shows the result of this process of equalization. We note two differences. First, the difference in posterior probability, albeit still present, is much less apparent. Second, the difference in posterior probability is npw reversed, that is, a composite claim suported by several items of evidence has a higher posterior probability compared to an individual claim supported by one item of evidence.


Has the conjunction paradox disappared, then? To some extent, it has. Arguably, what worried Cohen was that, as the number of constituent claims increases, the prosection or the plaintiff would see their case against the defendant progressively weaken and thus it would become impposisble for them to establish liability. But this worry is an exageration, as we have just seen.  As the number of constituent claims increases, it is not true that the case against teh defedant would automatically weaken. If teh role of prior probabilities is appropruately taken into into, the case against the defene could actually turn stronger, not weaker, if several constituent claims are each supported by an indeoedent line of evidence. 


This strategy for addressing the difficulty about conjuctiom is a reminscent of a familiar point. The support of a piece of evidence $E$---or its evidentila strength---in favor of a hypothesis $H$ should not be understood as a function of the conditional probability $\pr{H | E}$, but rather as a function of thigns like teh Bayes fctor or the likelihood ratio. We discussed this point in earlier chapters (REFER TO EARLIER CHAPTERS) and we should keep it in mind while examining the difficulty about conjunction. In this vein, @dawid1987difficulty observes:
 
 \begin{quote} When the problem is analysed carefully, the `paradox' evaporates: suitably measured, the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents.
 \end{quote}
 
 \noindent

This claim should not be surprising. Under certain conditions, a composite claim supported by several lines of evidence can receive stronger support than an indivudual claim (see Figure \label{fig:strength-indiv-joint} at the bottom). Although Dawid does not provide a general proofof this claim, he seems to be suggesting that posterior probabilities should be replaced with probabilistic measures of evidential support, such as the Bayes factor or the likelihood ratio. Does this strategy yktimately solve the conjunction paradox?  By using the theory of Bayesian networks, we will first verify under what conditions Dawid's claim holds. Next, we will show---perhpas surprisingly---that the difficulty about conjuntion will not go away even after switching from posterior probabilities to measures of evidential support. 


## Bayes factor threshold

A common probabilistic measure of the support of $E$ in 
favor of $H$ is the Bayes factor $\pr{E \vert H}/\pr{E}$. Since by Bayes' theorem

\[\pr{H \vert E} = \frac{\pr{E \vert H}}{\pr{E}}\times \pr{H},\]

\noindent
the Bayes factor measures the extent to which 
a piece of evidence increases the probability 
of a hypothesis. The greater the Bayes factor (for values above one), the stronger 
the support of $E$ in favor of $H$. Putting aside reservations about this measure 
of evidential support (discussed earlier in Chapter CROSSREF), 
the Bayes factor $\pr{E | H}/\pr{E}$, unlike the conditional probability $\pr{H | E}$, offers a potential way to overcome 
the difficulty about conjunction.

Say  $a$ and $b$, separately, support $A$ and $B$
to degree $s_A$ and $s_B$ respectively, that is, $\pr{a | A}/\pr{a}=s_A$ and 
$\pr{b | B}/\pr{b}=s_B$, where both $s_A$ and $s_B$ are greater than one. 
Does the combined evidence $a \wedge b$ provide at least as much support 
in favor of the combined claim $A \wedge B$ as the individual support by $a$ and $b$ 
in favor of $A$ and $B$ considered separately? The combined support should be 
measured by the combined Bayes factor $\pr{a \wedge b| A\wedge B}/\pr{a \wedge b}$.
The latter, under suitable independence assumptions, equals the product of the individual 
supports $s_{A}$ and $s_{B}$.\footnote{By the probability calculus,
 \begin{align*}
\frac{\pr{a \wedge b| A\wedge B}}{\pr{a \wedge b}} & =  \frac{\pr{A \et B| a\wedge b}}{\pr{A \et B}}\\
& =  \frac{\frac{ \pr{A \et B \et a \wedge b }}{\pr{a \et b}}}{\pr{A \et B}} \\ 
& =  \frac{\frac{ \pr{A} \times \pr{B|A} \times \pr{a | A \wedge B} \times \pr{b | A \wedge B \wedge a}}{\pr{a} \times \pr{b | a}}}{\pr{A \et B}} \\ 
& =^*  \frac{\frac{\pr{A} \times \pr{B} \times \pr{a | A} \times \pr{b | B}}{\pr{a} \times \pr{b}}}{\pr{A} \times \pr{B}} \\ 
& =  \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b}} \\
s_{AB}& =  s_{A}\times s_{B} 
 \end{align*}

\noindent
The step marked by the asterisk rests on the 
independence assumptions codified in the Bayesian network 
in Figure \ref{network-conjunction} (top). 
This result can be generalized 
beyond two pieces of evidence and holds generally.} That is, 
 \begin{align*}
\frac{\pr{a \wedge b| A\wedge B}}{\pr{a \wedge b}} & =  \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b}} \\
s_{AB}& =  s_{A}\times s_{B}.
 \end{align*}

 \noindent 
 Thus, the combined support $s_{AB}$ will always be higher 
than the individual support so long as $s_{A}$ and 
$s_{B}$ are greater than one. This vindicates Dawid's claim that  `the support supplied 
by the conjunction of several independent testimonies exceeds 
that supplied by any of its constituents'.

Crucially, if we rely on the Bayes factor as 
 a measure of evidential support, Dawid's claim holds in 
 the class of cases characterized by the relationships of probabilistic 
 independence encoded in the  Bayesian network in Figure \ref{network-conjunction} (top). 
The structure of the network is rather natural because each piece of evidence bears on its hypothesis and is probabilistically independent conditional on one of the hypotheses. This captures the fact that $a$ and $b$ are independent lines of evidence, each supporting their respective hypothesis (SEE DISCUSSION IN EARLIER CHAPTERS).  One might wonder, however, why the arrows go from $A$ and $B$ into the node representing the conjunction $A\wedge B$. This arrangement ensures that $A$ and $B$ are probabilistically independent, an assumption often made in the formulation of the conjunction paradox.\footnote{Another reason why the arrow is incoming into node $A\wedge B$ is that this setting can capture the meaning of the conjunction. The constraint that, for the conjunction to be true, both $A$ and $B$ have to be true, can be defined using conditional probability tables.}



\begin{center}
\begin{figure}[h!]
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,ellipse,text width=1cm,align=center}
]
\node[mynode] (h1) {$A$};
\node[mynode,below right =of h1] (h3) {$A \wedge B$};
\node[mynode,above right =of h3] (h2) {$B$};
\node[mynode,below  =of h1] (e1) {$a$};
\node[mynode,below  =of h2] (e2) {$b$};
\path 
(h1) edge[-latex] (h3)
(h2) edge[-latex] (h3)
(h1) edge[-latex] (e1)
(h2) edge[-latex] (e2); 
\end{tikzpicture}

\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,ellipse,text width=1cm,align=center}
]
\node[mynode] (h1) {$A$};
\node[mynode,below right =of h1] (h3) {$A \wedge B$};
\node[mynode,above right =of h3] (h2) {$B$};
\node[mynode,below  =of h1] (e1) {$a$};
\node[mynode,below  =of h2] (e2) {$b$};
\path 
(h1) edge[-latex] (h2)
(h1) edge[-latex] (h3)
(h2) edge[-latex] (h3)
(h1) edge[-latex] (e1)
(h2) edge[-latex] (e2); 
\end{tikzpicture}
\caption{Two Bayesian networks for two pieces of evidence and a combined hypothesis.}
\label{network-conjunction}
\end{figure}
\end{center}

 But what happens if $A$ and $B$ are not probabilistically independent? 
To drop independence, an arrow can be drawn between the variables representing the two claims. 
The resulting Bayesian network is depicted in Figure \ref{network-conjunction} (bottom). 
Given this network, the following holds:

 \begin{align*}
\frac{\pr{a \wedge b| A\wedge B}}{\pr{a \wedge b}} & =  \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b|a}} \\
s^{'}_{AB}& =  s_{A}\times s^{'}_{B} 
 \end{align*}

\noindent 
Note that factor $s_{B}= \frac{\pr{b |B}}{\pr{b}}$ was replaced by
$s^{'}_{B}=\frac{\pr{b |B}}{\pr{b|a}}$.\footnote{Given the second Bayesian network, 
$b$ need not be probabilistically independent of $a$, and thus there is no guarantee that 
$\pr{b \vert a}=\pr{a}$.} Now, $s^{'}_{B}$ is usually lower than $s_{B}$ 
because $\pr{b | a} > \pr{b}$ (assuming, at least, $a$ and $b$ are convergent pieces 
of evidence; SEE DISCUSSION IN EARLIER CHAPTERS). At the same time, $s^{'}_{B}$ should 
still be greater than one since $b$, by assumption, positively 
supports $B$ even in combination with $a$.\footnote{Note that 
$\frac{\pr{b |B}}{\pr{b|a}}=\frac{\pr{b |B \wedge a}}{\pr{b|a}}$ 
(by the probabilistic independence of $a$ and $b$ given $B$). 
So the claim that $\frac{\pr{b |B}}{\pr{b|a}}>1$ is 
equivalent $\pr{B | b \wedge a}> \pr{B|a}$ since by Bayes' theorem 
$\pr{B | b \wedge a} = \frac{\pr{b |B \wedge a}}{\pr{b|a}} \times \pr{B|a}$.
Presumably, evidence $b$ should still raise the probability of $B$ 
even in cojunction with $a$, or else $b$ would be useless evidence. \textbf{M: THIS CLAIM NEEDS 
TO BE PROVEN MORE RIGOROUSLY BUT I THINK IT'S CORRECT. BASIC INTUITION IS THAT EVIDENCE a 
RAISES THE PROBABILITY OF CLAIM A (OR B) AND THEN EVIDENCE 
b FURTHER RAISES THE PROBABILITY OF CLAIM A (OR B). THIS HAPPENS WHEN WE HAVE CONVERGENT 
EVIDENCE.}} Hence, $s^{'}_{AB}$ should still be greater than one provided $s_A$ and $S_B$ are both 
greater than one, but it might not always exceed the support supplied by $s_A$ and $s_B$ individually. For suppose $s_A=2$ 
and $s_B=3$, but $s^{'}_B=1.2$. Then, $s^{'}_{AB}=2\times 1.2=2.4$, which is below 
the individual support $s_B$, but still above $s_A$.  In general, 
Dawid's claim should be amended as follows. Even though the support supplied by the conjunction of several independent testimonies need not always exceed that supplied by any of its constituents, it is always at least as great as the smallest support supplied by its constituents.\footnote{DO WE NEED A PROOF OF THIS? THIS SHOULD BE CLEAR BY PLOTTING. IN PLITTING THIS, WE SHOULD ENSURE THAT BOTH BF ARE GREATER THAN ONE, THEN THE COMBINED ONE WILL BE GREATER THAN THE SMALLEST EVEN WHEN A AND B ARE DEPENDENT.}


<!-- 
Full derivation:
 \begin{align*}
\frac{\pr{a \wedge b| A\wedge B}}{\pr{a \wedge b}} & =  \frac{\pr{A \et B| a\wedge b}}{\pr{A \et B}}\\
& =  \frac{\frac{ \pr{A \et B \et a \wedge b }}{\pr{a \et b}}}{\pr{A \et B}} \\ 
& =  \frac{\frac{ \pr{A} \times \pr{B|A} \times \pr{a | A \wedge B} \times \pr{b | A \wedge B \wedge a} }{\pr{a} \times \pr{b | a}}}{\pr{A \et B}} \\ 
& =^*  \frac{\frac{\pr{A} \times \pr{B|A} \times \pr{a | A} \times \pr{b | B}}{\pr{a} \times \pr{b |a}}}{\pr{A} \times \pr{B | A}} \\ 
& =  \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b|a}} \\
s^{'}_{AB}& =  s_{A}\times s^{'}_{B} 
 \end{align*}
 -->

If the combined support equals $s_{A}\times s_{B}$ or
$s_{A}\times s^{'}_{B}$, does 
the difficulty about conjunction evaporate, as Dawid thought? 
<!---To some extent 
it does. The crux of the matter here is whether the conjunction of two or more claims 
 satisfies a given standard of proof just in case the individual conjuncts do. 
 --->
 One hurdle here is that
 the standard of proof would no longer be formalized as a posterior probability threshold, but instead as a threshold about the Bayes factor. The threshold would no longer be a probability between 0% and 100%, but rather a number somewhere above 1. The greater this number, the more stringent the standard of proof, for any value above one. In criminal trials, for example, the rule of decision would be: guilt is proven beyond a reasonable doubt if and only if the evidential support in favor of $G$---as measured by the Bayes factor $\frac{\pr{E | G}}{\pr{E}}$---meets a suitably high threshold $t$. The 
obvious question at this point is, how do we identify the appropriate threshold?\todo{good question, will think about it, will need to take a look at "Bayesian Choice", also need to think about a counterexample}\footnote{QUESTION: CAN DECISION THEORY BE APPLIED TO THE BAYESIAN FACTOR OR IS IT ALWAYS ABOUT POSTERIOR PROBABILITIES? IF THERE IS NO DECISION THEORY APPLICABLE HERE, THAT WOULD BE A FURTHER DIFFICULTY.} We will not discuss this question for now. As it turns out, even if this question can be satisfactorily answered, this  approach gives rise to a complication that proves fatal. 


<!--
\noindent
If the conjunction principle holds,\todo{In which direction? Note that at least in one direction the principle still fails. For instance, if your threshold is 15, $14*25$ is 350 but still one of the elements fails to be sufficiently supported.}  the statement above would be equivalent \todo{not sure about the equivalence; at best, this would be a sufficient condition, no? you do talk about this later so putting it this way is a bit misleading} to:
  \begin{quote}
Guilt is proven beyond a reasonable doubt if and only if the evidential support in favor of each constituent claim 
 $C_i$ suported by evidence $E_i$---as measured by the Bayes factor $\frac{\pr{E_i | C_i}}{\pr{E_i}}$---meets a suitably high threshold $t$.
 \end{quote}
 \noindent
-->

At issue here is whether the conjunction principle can be formalized in 
a plausible manner with the Bayes factor. Unfortuantely, the answer is negative. 
To see why, first recall the conjunction principle:
 
 \[\text{S[$a, A$] and S[$b, B$] iff S[$a \wedge b, A\wedge B$]},\]

\noindent
where $\text{S}[E, H]$ means that evidence $E$ supports hypothesis $H$ by standard $S$. If the standard of proof is formalized using the Bayes factor, the conjunction principle would boil down to:

\[  \text{ $\frac{\pr{a | A }}{\pr{a}}>t$ and $\frac{\pr{ b | B}}{\pr{b}}>t$ iff $\frac{\pr{a \et b | A \et B}}{\pr{a \et b}}>t$ } \]

\noindent
The left-to-right direction---call it aggregation---is likely to hold for 
any threhsold $t$ greater than one. As shown earlier, the combined evidential support is greater than 
the individual evidential support 
<!---
because $\frac{\pr{a \et b | A \et B}}{\pr{a \et b}}$ 
equals $\frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b}}$
-->
if A and B are independent, or greater than the smallest individual support 
<!---or $\frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b \vert a}}$ -->
if A and B are dependent. Aggregation could not be justified using posterior probabilities $\pr{A | a}$ and $\pr{B | b}$. So it is an advantage of the Bayes factor that it can justify 
this direction of the conjunction principle. 

However, the right-to-left direction---call it distribution---has now become problematic. 
For suppose the combined evidential support, $\frac{\pr{a \et b | A \et B}}{\pr{a \et b}}$, barely meets 
the threshold. This implies that the individual support, say $\frac{\pr{a |A}}{\pr{a}}$, 
could be be below the threshold unless $\frac{\pr{b |B}}{\pr{b}}=1$
\todo{Not always true; just give a specific numerical counterexample. M: I added 'often'. Is this enough?} (which should not happen if $b$ positively supports $B$). So, curiously, there would be cases in which, even though the conjunction $A\et B$ is established to the desired standard of proof, one of the individual claims  fails to meet the standard. This is odd. More specifically, the following distribution 
principle fails in some cases:

\[\text{If S[$a \wedge b, A\wedge B$], then S[$a, A$] and S[$b, B$].} \tag{DIS1}\]

\noindent
Could this principle be rejected? Perhaps, it is not as essential as we thought at first. Since 
the evidence is not held constant, the support supplied by $a\wedge b$ could be stronger than 
that supplied by $a$ and $b$ individually. So even when the conjunction 
$A \wedge B$ is established to the requisite standard given evidence 
$a\wedge b$, it might still be that $A$ does not meet the requisite 
standard (given $a$) nor does $B$ (given $b$). 

But consider a less controversial version, holding the evidence constant:

 \[\text{If S[$a \wedge b, A\wedge B$], then S[$a \wedge b, A$] and S[$b \wedge b, B$].} \tag{DIS2}\]

\noindent
This principle is harder to deny. That is, one would not want 
to claim that, holding fixed evidence $a\wedge b$, establishing the conjunction 
might not be enough for establishing one of the conjuncts. 
One cannot be willing to assent to the 
conjunction without being willing to assent to one of the conjuncts 
against a fixed body of evidence. Certainly any formalization of the standard of proof should obey 
(DIS2). And yet, it is this very principle that we should deny if we understand 
the standard of proof using the Bayes factor. \footnote{To show that (DIS2) fails, it is enough to show that $\frac{\pr{B \vert a\wedge b \wedge A}}{\pr{B \vert A}}>1$ because $S[a\wedge b, A\wedge B]>S[a\wedge b, A]$ iff $\frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a\wedge b}}>\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b}}$ iff $\frac{\pr{A\wedge B \vert a \wedge b}}{\pr{A\wedge B}}>\frac{\pr{A \vert a \wedge b }}{\pr{A}}$ iff $\frac{\pr{A \vert a \wedge b} \times \pr{B \vert a\wedge b\wedge A}}{\pr{A} \times \pr{B \vert A}}>\frac{\pr{A \vert a \wedge b }}{\pr{A}}$. Now, $\frac{\pr{B \vert a\wedge b \wedge A}}{\pr{B \vert A}}>1$ so long as $a\wedge b$ positively supports $B$ even under the assumption of $A$, unless $A$ entailed $B$. We naturally exclude the situation in which one claim entails the other because otherwise there would be no need to establish the two claims. Establishing one claim alone would suffice. To see why $a\wedge b$ positively supports $B$ (or $A$), note that $S[a, A]=\frac{\pr{a |A}}{\pr{a}} \leq \frac{\pr{a |A} \times \pr{b | A \wedge a}}{\pr{a}\times \pr{b | a}}=\frac{\pr{a \wedge b |A}}{\pr{a\wedge b}}=S[a \wedge b, A]$. The key step here is $\frac{\pr{a |A}}{\pr{a}} \leq \frac{\pr{a |A} \times \pr{b | A \wedge a}}{\pr{a}\times \pr{b | a}}$. The latter holds because $\frac{\pr{b | A \wedge a}}{\pr{b | a}}\geq 1$. It is useful to distinguish two cases. First, if $A$ and $B$ are probabilistically independent, as in the Bayesian network in Figure \ref{network-conjunction} (top), then $\frac{\pr{b | A \wedge a}}{\pr{b | a}}= \frac{\pr{b}}{\pr{b}} = 1$. Second, if $A$ and $B$ are probabilistically dependent, as in the Bayesian network in Figure \ref{network-conjunction} (bottom), evidence $b$ positively supports claim $A$ (even conditional on $a$) so long as $b$ positively supports $B$. The assumption is that claim $A$ and $B$ are positively correlated, and thus, any evidence that supports one of the claims is going to support the other claim, as well. \textbf{SEE EARLIER CHAPTERS FOR A MORE RIGOROUS PROOF OF THIS LAST POINT.}}  The intuitive reason for this---perhaps surprising---result is that $A \wedge B$ has a much lower prior probability than $A$ (or $B$) considered separately. Thus, the same body of evidence is going to have a larger impact on a hypothesis with a lower prior probability, other things being equal.

All in all, using Bayes factor to understad the standard of proof 
has counterintuitive consequences, what we will call the distribution paradox. 
For suppose the prosecution provided evidence for claim $A$, but this evidence still falls short of the threshold $t$ (a certain number above 1). Just by tagging an additional claim $B$ and without doing any further evidentiary work, the prosecution could provide sufficiently strong evidence (which meets the threshold $t$) in favor of claim $A \wedge B$. So, it could well happen that, while the prosecution failed to prove beyond a reasonable doubt that the defendant injured the victim, the prosecution could nevertheless prove beyond a reasonable doubt that the defendant injured the victim and did so intentionally. This is odd. 


<!---

To see why, note that 
given the Bayesian network in Figure \ref{network-conjunction} (top), 
the following equalities hold:

\[ \text{S[$a \wedge b, A$] = S[$a, A$] and  S[$a \wedge b, B$] = S[$b, B$]}. \]

\noindent
If so, (EXT1) and (EXT2) are equivalent, and 
denying one extrapolation principle implies denying the other.\footnote{To show that 
S[$a \wedge b, A$] = S[$a, A$], note that $S[a, A]=\frac{\pr{a |A}}{\pr{a}}=\frac{\pr{a |A} \times \pr{b}}{\pr{a}\times \pr{b}}=\frac{\pr{a |A} \times \pr{b | A \wedge a}}{\pr{a}\times \pr{b | a}}=\frac{\pr{a \wedge b |A}}{\pr{a\wedge b}}=S[a \wedge b, A]$. The key step here is the unconditional probabilistic independence of $a$ and $b$ encoded in the Bayesian network in Figure \ref{network-conjunction} (top). The same reasoning applies for $B$. A related claim is that
$S[a, A] = S[a, A\wedge B]$ because $S[a, A]=\frac{\pr{a |A}}{\pr{a}}=\frac{\pr{a |A \wedge B}}{\pr{a}}=S[a, A \wedge B]$, and similarly for $S[b, B] = S[b, A\wedge B]$.} Thus, if the standard of proof is formalized 
using Bayes factor, the extremely plausible, almost undeniable, extrapolation principle (EXT2) would have to go. 


If we switch to the Bayesian network in Figure \ref{network-conjunction} (bottom), 
the evidential support of $a\wedge b$ 
in favor of $A$ (and the same would apply for $B$) would typically be \textit{stronger} 
than the evidential support of $a$ alone in favor of $A$. So the equalities 
above fail.

-->

EXPLORE WHETHER DIFFERENCE IN BAYESIAN FACTOR IS NUMERICALLY SIGNIFICANT OR NOT. USE GRAPH AND PLOT RESULTS. TRY TO FACTOR OUT THE IMPACT OF PRIORS BUT STRESS THAT BF DOES DEPEND ON PRIORS AND THUS THE DIFFERENCE IN BF IS  ARTFICIAL AND DEPENDS ON PRIOR, SO BF BEHAVES AGAIN LIKE THE POSTERIORS THAT ARE DEPENDENT ON PRIORS. NEED TO FACTOR OUT ROLE OF PRIORS OTEHRWISE IT'S JUST LIKE COMPARING APPLES AND ORANGES. 





## Likelihood ratio threshold

Let's now replace the Bayes factor with the likelihood ratio, another probabilistic 
measure of evidential support. As we shall understand it for now, the likelihood ratio compares
the probability of the evidence on the assumption that 
a hypothesis of interest is true and the probability of the evidence on the assumption that 
the negation of the hypothesis is true, 
that is, $\frac{\pr{E \vert H}}{\pr{E \vert \neg H}}$. The greater the 
likelihood ratio (for values above one), the stronger the evidential support 
in favor of the hypothesis (as contrasted to the its negation). 
We discussed extensively the advatanges and 
limitations of this account in \textbf{REFERENCE TO EARLIER CHAPTER}.
Like the Bayes factor, the likelihood ratio can be used 
to formalize the standard of proof by equating the standard 
to a threshold $t$ above one. The greater the threshold, the more stringent the standard. In criminal trials, for example, the rule of decision would be: guilt is proven beyond a reasonable doubt if and only if the evidential support in favor of $G$---as measured by the likelihood ratio 
$\frac{\pr{E \vert  G}}{\pr{E \vert \neg G}}$---meets a suitably high threshold $t$ above one.
Setting aside the problem of how ot identify 
the appropriate threshold, we will see that this approach 
is able to justify one direction of the conjunction principle---what we 
called aggregation---but still fails to justify the other direction---what we 
called distribution So the distribution 
paradox arises here again. 

Consider aggregation first. Say both individual likelihood 
 ratios $\frac{\pr{a |A}}{\pr{a | \neg A}}$ and $\frac{\pr{b |B}}{\pr{b | \neg B}}$ are above the requisite 
 threshold $t$ for meeting the standard of proof. Will the combined likelihood ratio $\frac{\pr{ a \et b |A \et B}}{\pr{a \et b | \neg (A \et B)}}$ also be above the threshold? The answer is affirmative. To see why, consider numerator and denominator separately. The numerator can be computed easily:
 \begin{align*}
\pr{a \wedge b| A\wedge B} & =  \frac{\pr{A \et B \et a\et b}}{\pr{A \et B}}\\
&= \frac{   \pr{A} \times \pr{B|A} \times \pr{a | A \wedge B} \times \pr{b | A \wedge B \wedge a} }{\pr{A} \times \pr{B \vert A}}\\
& =^* \frac{\pr{A} \times \pr{B | A} \times \pr{a | A} \times \pr{b | B}}{\pr{A}  \times \pr{B | A}} \\
& = \pr{a | A} \times \pr{b | B} 
 \end{align*}
 
 \noindent
 The equality requires the independence assumptions 
 codified in either one of the Bayesian networks in Figure \ref{network-conjunction}. 
 The asterisk marks the step that requires such indepedence assumptions. 
<!-- Let $\pr{a |A}=x$ and $\pr{b |B}=y$, so the numerator simply equals $xy$. -->
 The denominator is more involved:
   \begin{align*}
\pr{a \et b| \neg (A\et B)} & = \frac{\pr{a \et b \et \neg (A\et B)}}{\pr{\neg (A \et B)}} \\
& = \frac{\pr{a \et b \et \neg A\et B} +  \pr{a \et b \et A\et \neg B} + \pr{a \et b \et \neg A\et \neg B}  }{\pr{\neg A \et B} + \pr{A \et \neg B} + \pr{\neg A \et \neg B} } \\
& =^* \frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a | \neg A}\pr{b | B} + \pr{A}\pr{\neg B \vert A} \pr{a | A }\pr{b | \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a | \neg A}\pr{b | \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} }  
 \end{align*}

\noindent 
The same independence assumptions 
invoked before are needed\todo{I am not convinced, we should be more careful here. M: It'd be great to run the program that checks for independencies, though I am pretty sure.} marked by the asterisk in the derivation. 
Combining numerator and denominator yields the formula for the combined likelihood ratio:


  \begin{align*}
\frac{\pr{a \wedge b \vert A\wedge B}}{\pr{a \et b| \neg (A\et B)}} & = \frac{\pr{a | A} \times \pr{b | B}}{\frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a | \neg A}\pr{b | B} + \pr{A}\pr{\neg B \vert A} \pr{a | A }\pr{b | \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a | \neg A}\pr{b | \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} } }
 \end{align*}

\noindent
It is a cumbersome 
formula. Becasue of the many variables at play, it is not easy to show whether the combined evidential support exceeds 
the individual support supplied by $a$ and $b$ towards $A$ and $B$. To circumvent this difficulty,
we provide two arguments. One argument is analytical, but rests on a few simplifying assumptions. The second 
is based on a computer simulation and is more general. 

In the analytical argument, we make three simplifying assumptions. First, the sensitivity of a piece of evidence, say $\pr{a |A}$, is the same as its specificity, $\pr{\neg a | \neg A}$. Let $\pr{a |A}=x$ and $\pr{b |B}=y$. So $\pr{a |\neg A}=1-x$ and $\pr{b | \neg B}=1-y$. Second, claims $A$ and $B$ are independent of one another, a common stipulation in discussions on the difficulty about conjunction as noted before. Finally, the sensitivity (and thus the specificity) of the two pieces of evidence is the same, that is, $\pr{a |A}=x=\pr{b |B}=y$. The combined likelihood ratio therefore reduces to the following, where $\pr{A}=k$ and $\pr{B}=t$:

  \begin{align*}
\frac{\pr{a \wedge b \vert A\wedge B}}{\pr{a \et b| \neg (A\et B)}} & = \frac{xx}{\frac{(1-k)t(1-x)x + k(1-t)x(1-x) + (1-k)(1-t)(1-x)(1-x)}{ \left(1-k\right) t +\left(1-t\right) k+\left(1-k\right) \left(1-t\right)}}
 \end{align*}

\noindent
The graph of the combined likelihood ratio can now be easily plotted against the single likelihood ratios. As Figure \ref{fig:jointLRMarcello} shows, the combined likelihood  ratio varies depeding on the prior probabilities $\pr{A}$ and $\pr{B}$, but always execeeds the individual likelihood ratios whenever
 they are greater than one. 

 
\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)

combined <- function (x,k,t) {
  (x^2)/
(
  ((1-k)*t*(1-x)*x + k*(1-t)*x*(1-x) + (1-k)*(1-t)*(1-x)*(1-x))
  /
    ((1-k)*t +(1-t)*k+(1-k)*(1-t))
)
}

combinedMarcello <- function(x,k,t){
  (x^2)/(k*(1-t)*(x)*(1-x)+(1-k)*t*x*(1-x)+(1-k)*(1-t)*(1-x)*(1-x))
}

x <-seq(0,1,by=0.001)
y0102 <- combined(x,0.1,0.2)
y0608 <- combined(x,0.6,0.8)

ggplot() +
  stat_function(fun=function(x)(x/(1-x)), geom="line", aes(colour="LR(a)=LR(b)"))+
  geom_line(aes(x = x, y = y0102,color = "joint LR for k = 0.1, t=0.2")) +
  geom_line(aes(x = x, y = y0608,color = "joint LR for k = 0.6, t=0.8")) +
        ylim(c(0,5))+
  xlab("x=sensitivity(a)=sensitivity(b)")+ ylab("LR")+theme_tufte()+
  theme(legend.position = c(0.8, 0.2))
```

\caption{Combined likelihood ratios exceeds individual Likelihood ratios. Changes in the prior 
probabilities $t$ and $k$ do not invalidate this result.}
\label{fig:jointLRMarcello}
\end{figure}


<!--

\noindent
The combined likelihood ratio is therefore:
\begin{align}\label{eq:combinedLRMarcello}
\frac{\pr{ a \et b |A \et B}}{\pr{a \et b | \neg (A \et B)}} & = \frac{xy}{\frac{(1-k)t(1-x)y + k(1-t)x(1-y) + (1-k)(1-t)(1-x)(1-y)}{ \left(1-k\right) t +\left(1-t\right) k+\left(1-k\right) \left(1-t\right) }}
 \end{align}


\noindent

  \begin{align*}
\frac{\pr{ a \et b |A \et B}}{\pr{a \et b | \neg (A \et B)}} & = \frac{xx}{\frac{(1-k)t(1-x)x + k(1-t)x(1-x) + (1-k)(1-t)(1-x)(1-x)}{ \left(1-k\right) t +\left(1-t\right) k+\left(1-k\right) \left(1-t\right) }}
 \end{align*}

-->

What happens if we relax the three simplifying assumptions?
Suppose the sensitivity and specificity of the two pieces of 
evidence are not the same. Their likelihood ratios will then 
also be different. In this case, the combined likelihood ratio is not always 
 greater than the individual ratios, but it is always 
greater than the smallest of the two provided the individual likelihood 
 ratios are greater than one. 
\textbf{M: what about dropping other assumptions?}

\todo{M: Need to add simuation results to make this argument 
fully general and drop all the simplifying assumptions.}

<!-- ```{r echo=FALSE} -->
<!-- library(ggplot2) -->
<!-- library("gridExtra") -->
<!-- k <- 0.1 -->
<!-- t <- 0.2 -->
<!-- y <- 0.7 -->
<!-- b <- ggplot(data.frame(x=c(0,1)), aes(x)) +   ylim(0, 30) + -->
<!--   stat_function(fun=function(x)(x/(1-x)), geom="line", aes(colour="item of evidence a, LR-x=(1-x)")) + -->
<!--   stat_function(fun=function(x)(y/(1-y)), geom="line", aes(colour="item of evidence b, LR=y/(1-y)")) + -->
<!--   stat_function(fun=function(x)((x*y)/(k*(1-t)*(x)*(1-y)+(1-k)*t*y*(1-x)+(1-k)*(1-t)*(1-x)*(1-y))), geom="line", aes(colour="two items combined Pr(A)=0.1, Pr(B)=0.2")) + -->
<!--    stat_function(fun=function(x)((x*y)/(k*(1-0.4)*(x)*(1-x)+(1-k)*0.4*x*(1-x)+(1-k)*(1-0.4)*(1-x)*(1-x))), geom="line", aes(colour="two items combined Pr(A)=0.1, Pr(B)=0.4")) + -->
<!--     stat_function(fun=function(x)((x*y)/(0.3*(1-0.4)*(x)*(1-y)+(1-0.3)*0.4*y*(1-x)+(1-0.3)*(1-0.4)*(1-x)*(1-y))), geom="line", aes(colour="two items combined Pr(A)=0.3, Pr(B)=0.4")) + -->
<!--   xlab("x") + ylab("LR") +labs(colour="one versus many") + -->
<!--        ggtitle("Combined v individual support measured by LR with y=0.7") -->


The argument so far verified Dawid's claim that  `the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents'. One caveat here is that, if evidential support is measured by the likelihood ratio, the support supplied by the conjunction of different independent pieces of evidence always exceeds 
the smallest of the support supplied by its consitituents, but there are cases in which it does not exceeds the support supplied by some of its constituents. This detail aside, the likelihood ratio faces another problem, the same problem that plagues the Bayes factor. That is, likelihood ratios still fail to capture the other direction of the conjunction principle, what we called extrapolation. 

For suppose evidence $a \et b$ supports $A \et B$ to 
 the required threshold $t$. If evidential support is measured by the likelihood ratio, the threshold in this case should be  some order of magnitude greater than one. If the combined likelihood ratio meets the threshold $t$, one of the individual likelihood ratios may well be below the threshold. So---if the standard of proof is interpreted using evidential support measured by the likelihood ratio---even though the conjuction $A \et B$ was proven according to the desired standard, one of individual claims might not. The right-to-left direction of the conjunction principle---that is, if $S[a \et b, A \et B]$, then $S[a, A] \et S[b, B]$, what we called earlier the extrapolation principle (EXT1)---fails. 
 
 Consider now the second and less objectionale extrapolation principle discussed earlier. This is the principle that holds 
the evidence fixed throughout, repeated below for convenience:
 
 \[\text{If S[$a \wedge b, A\wedge B$], then S[$a \wedge b, A$] and S[$b \wedge b, B$].} \tag{EXT2}\]

\noindent
Here again, the support of $a\wedge b$ in favor of $A\wedge B$ 
could exceed that of $a\wedge b$ in favor of $A$ alone (or $B$ alone) if evidential support 
is measured using likelihood ratios.\footnote{Note that, assuming either of the Baysian networks 
in Figure \ref{network-conjunction},  $S[a\wedge b, A]=\frac{\pr{a \wedge b \vert A}}{\pr{a\wedge b\vert \neg A}}=\frac{\pr{a \vert A}}{\pr{a \vert \neg A}}\times \frac{\pr{b \vert A}}{\pr{b \vert \neg A}}$, where

\[\frac{\pr{b \vert A}}{\pr{b \vert \neg A}} = \frac{\pr{B \vert A} \times \frac{\pr{b \vert B}}{\pr{b \vert \neg B}} + (1- \pr{B \vert A})}{\pr{B \vert \neg A} \times \frac{\pr{b \vert B}}{\pr{b \vert \neg B}} + (1- \pr{B \vert \neg A})}.\]

\textbf{SEE PROOF IN EARLIER CHAPTERS.}
 If $A$ and $B$ are assumed to be probabilistically independent, the numerator and the denominator will be the same, 
 so $\frac{\pr{b \vert A}}{\pr{b \vert \neg A}}=1$. Thus, 
 $S[a\wedge b, A]=\frac{\pr{a \vert A}}{\pr{a \vert \neg A}}=S[a, A]$. Since $S[a, A]< S[a\wedge b, A\wedge B]$ (see Figure \ref{fig:jointLRMarcello}), it follows $S[a\wedge b, A]<S[a\wedge b, A\wedge B]$. \textbf{What if A and B are dependent? Need simulation data here.}} Even this second, seemingly unobjectionable version of extrapolation fails. The same counterintuitive consequences that arose with Bayes factor manifest themselves here. The extrapolation paradox persists. 

\todo{M: This whole section should be generalized to the case in which A and B are not 
independent using the simulation dara.}

\textbf{M: Might be goot to emphasize how devastaing this finding is for legal probabilists 
who endorsed likelihood ratios as the solution to many problems with legal probabilism.}

 
## The comparative stratgey

Instead of thinking in terms of absolute thresholds---whether relative to posterior probabilities, 
the Bayes factor or the likelihood ratio---the standard of proof can be understood comparatively. This suggestion 
has been advanced by @cheng2012reconceptualizing following the theory 
of relative plausibility by \textbf{REFERENCE TO ALLEN AND PARDO HERE}. Say the prosecutor 
or the plaintiff puts foward a hypothesis $H_p$ about what happened. The defense offers an alternative hypothesis, 
call it $H_d$. On this approach, rather than directly evaluating the 
support of $H_p$ given the evidence and comparing it to a threshold, we compare the 
support that the evidence provides for two competing hypotheses $H_p$ and $H_d$, 
and decide for the one for which the evidence provides better support.

It is controversial whether this is what happens in all trial 
proceedings, especially in criminal trials. The defense may elect to challenge 
the hypothesis put foward by the other party without proposing one of its own. 
For example, in the O.J.\ Simpson trial the defense did not advance 
its own story about what happened, but simply argued that the 
evidence provided by the prosecution, while significant on its face to establish 
OJ's guilt, was riddled with problems and deficencies. This defense strategy 
was enough to secure an acquittal.  So, in order to create a reasonable doubt about 
guilt, the defense does not always provide a full-fledged alternative hypothesis. 
The supporters of the comparative approach, however, will respond that this could happen 
in a small number of cases, even though in general---especially for tactical reasons---the defense 
will provide an alternative hypothesis. After all, not to provide one would usually 
amount to an admission of criminal or civil liability.  

Setting aside this controversy for the time being, 
let's first work out the comparative strategy 
using posterior probabilities. More specifically, given a body of evidence $E$ and two competing hypotheses 
$H_p$ and $H_d$, the probability $\pr{H_p | E}$ should be suitably higher 
than $\pr{H_d | E}$, or in other words, the ratio $\frac{\Pr{H_p | E}}{\Pr{H_d | E}}$ should 
be above a suitable threshold. Presumably, the ratio threshold shoud be higher 
for criminal than civil cases.  In fact, in civil cases it seems enough to require that the 
ratio $\frac{\Pr{H_p | E}}{\Pr{H_d | E}}$ be avove 1, or in other words, 
$\pr{H_p | E}$ should be higher than $\pr{H_d | E}$. Note that $H_p$ and $H_d$ need not be one the negation of the other. 
Whenever two hypotheses are one the negation of the other, $\frac{\pr{H_p | E}}{\pr{H_d | E}}>1$ 
implies that $\pr{H_p | E}>50\%$, the standard probabilistic intepretation of the preponderemca standard. 

One advantage of this approach---as Cheng shows---is that expected utility theory can 
set the appropriate comparative threshold $t$ as a function of the costs and benefits 
of trial decisions. For simplicity, suppose that if the decision is correct, no costs result, 
but incorrect decisions have their price (\textbf{REFERENCE TO EARLIER CHAPTER 
FOR MORE COMPLEX COST STRUCTURE}). The costs of a false positive is $c_{FP}$ and
false negative is $C_{FN}$, both greater than zero. Intuitively, 
the decision rule should minimize the expected costs. That is, a finding against the defendant 
would be acceptable whenever its expected costs---$\pr{H_d \vert E} \times c_{FP}$---are smaller 
than the expected costs of an acquittal---$\pr{H_p \vert E}\times c_{FN}$--- 
or in other words:

\[\frac{\pr{H_p \vert E}}{\pr{H_d \vert E}} > \frac{c_{FP}}{c_{FN}}.\]

\noindent
In civil cases, it is customary to assume the costs ratio of 
false postives to false negatives equals one. So the rule of decision would be: Find against the defedant whenever $\frac{\pr{H_p \vert E}}{\pr{H_d \vert E}} > 1$ or in other words
$\pr{H_p \vert E}$ is greater than $\pr{H_d \vert E}$. In criminal trials, the costs ratio is usually considered higher, since convicting an innocent (false positive) should be more harmful or morally objectionable 
than acquitting a guilty defendant (false negative). Thus, the rule of decision in criminal proceedings would be: 
Convict whenever $\pr{H_p \vert E}$ is significantly greater 
than $\pr{H_d \vert E}$.





<!---
\begin{center}
\begin{tabular}
{@{}llll@{}}
\toprule
& & \multicolumn{2}{c}{Decision}\\
& &  $D_\Delta$ & $D_\Pi$ \\
\cmidrule{3-4}
\multirow{2}{*}{Truth} &  $H_d$    & $0$    & $c_FP$\\
                       &  $H_p$       &  $c_FN$   & $0$ \\ 
\bottomrule
\end{tabular}
\end{center}


Say that given our total evidence $E$ the  relevant conditional probabilities are:
Let us say that  if the defendant is right and we find against them, 
the cost is $c_1$, and if the plaintiff is right and we find against them, the cost is $c_2$:
Intuitively, it seems that we want a  decision rule which minimizes 
the expected cost. Say that given our total evidence $E$ the  relevant conditional probabilities are:

\vspace{-6mm}

\begin{align*}
p_\Delta &= \pr{H_\Delta \vert E} \\
p_\Pi & = \pr{H_\Pi \vert E}
\end{align*}
<!-- \noindent where $\mathtt{P}$ stands for the prior probability (this will be the case throughout our discussion of Cheng). 
\noindent The expected costs for deciding that $H_\Delta$ and $H_\Pi$, respectively, are:
\begin{align*}
E(D_\Delta) & = \pr{H_\Delta \vert E} 0 + p_\Pi c_2 = c_2p_\Pi\\
E(D_\Pi) & = \pr{H_\Pi \vert E} c_1 + p_\Pi 0 = c_1 p_\Delta
\end{align*}
\noindent For this reason, on these assumptions,  we would like to choose $H_\Pi$ just in case $E(D_\Pi) < E(D_\Delta)$. This condition is equivalent to:

\vspace{-6mm}


\begin{align}
\nonumber c_1p_\Delta &< c_2p_\Pi \\
\nonumber c_1 & < \frac{c_2p_\Pi}{p_\Delta}\\
\label{eq:cheng_frac1}\frac{c_1}{c_2} & < \frac{p_\Pi}{p_\Delta}
\end{align}


\noindent @cheng2012reconceptualizing (1261)  notes:
\begin{quote}
At the same time, in a civil trial, the legal system expresses no preference between finding erroneously for the plaintiff (false positives) and finding erroneously for the defendant (false negatives). The costs $c_1$ and $c_2$ are thus equal\dots
\end{quote}
\noindent If we grant this assumption, $c_1=c_2$, \eqref{eq:cheng_frac1} reduces to:

\vspace{-6mm}


\begin{align}
\nonumber 1 &< \frac{p_\Pi}{p_\Delta} \\
\label{eq:cheng_comp1} p_\Pi &> p_\Delta 
\end{align}
\noindent That is, in standard civil litigation we are to find for the plaintiff just in case $H_\Pi$ is more probable given the evidence than $H_\Delta$, which seems plausible.

This instruction is somewhat more general than the usual suggestion of the preponderance standard in civil litigation,  according to which the court should find for the plaintiff just in case $\pr{H_\Pi\vert E} >0.5$. This threshold, however, results from \eqref{eq:cheng_comp1} if it so happens that $H_\Delta$ is $\n H_\Pi$, that is, if the defendant's claim is simply the negation of the plaintiff's thesis.  By no means, Cheng argues, this is always the case. 

-->

Does the comparative strategy just outlined solve 
the difficulty about conjuction? 
We will work through a stylized case used by Cheng himself. 
Suppose, in a civil case, the plaintiff claims that the defendant was speeding ($S$) and that
the crash caused her neck injury ($C$). Thus, the plaintiff's hypothesis $H_p$ is $S\et C$. Given 
the total evidence $E$, the conjuncts, taken separately, meet the decision threshold:
\begin{align}
 \nonumber 
 \frac{\pr{S\vert E}}{\pr{\neg S \vert E}} > 1   & & \frac{\pr{C\vert E}}{\pr{\neg C \vert E}} > 1
\end{align}
\noindent The question is whether $\frac{\pr{S\et C\vert E}}{\pr{H_d \vert E}}>1$. To answer it, we have to decide what the defense hypothesis $H_d$ should. Cheng reasons that there are 
three alternative defense scenarios: $H_{d_1}= S\et \n C$, $H_{d_2}=\n S \et C$, and  $H_{d_3}=\n S \et \n C$. How does the  hypothesis $H_p$ compare to each of them? 
Assuming independence between $C$ and $S$, we have

\begin{align}\label{eq:cheng-multiplication}
\frac{\pr{S\et C\vert E}}{\pr{S\et \n C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{S \vert E}\pr{\n C \vert E}}  =\frac{\pr{C\vert E}}{\pr{\n C \vert E}} > 1 \\
\nonumber
\frac{\pr{S\et C\vert E}}{\pr{\n S\et C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{C\vert E}}  = \frac{\pr{S\vert E}}{\pr{\n S \vert E}} > 1 \\
\nonumber
\frac{\pr{S\et C\vert E}}{\pr{\n S\et \n C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{\n C \vert E}}   > 1 
\end{align}

\noindent So, whatever the defense hypothesis, the plaintiff's hypothesis is more probable. At least in this case, whenever the elements of a plaintiff's claim satisfy the decision threshold, so does their conjunction. The left-to-right direction of the conjunction principle---what we called aggregation---has been vidicated. But what about the opposite direction, what we called extrapolation? Interestingly, if the threshold is just 1---as might be appropriate in civil cases---extrapolation would be satisfied. Even if $\frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{\n C \vert E}}$ might be strictly greater than $\frac{\pr{C\vert E}}{\pr{\n C \vert E}}$ or $\frac{\pr{S\vert E}}{\pr{\n S \vert E}}$, wehenver the former is greater than one the latter must be greater than one. However, suppose the threshold is more stringent than one, as might be appropriate for criminal cases. For some constituent claims $A$ and $B$ in a criminal case, whenever $\frac{\pr{A\vert E}\pr{B\vert E}}{\pr{\n A \vert E}\pr{\n B \vert E}}$ barely meet the threhold $t$, $\frac{\pr{A\vert E}}{\pr{\n A \vert E}}$ or $\frac{\pr{B\vert E}}{\pr{\n B \vert E}}$ could be below $t$. Since the evidence is held fixed throughout, this would be a violation of the extrapolation principle (EXT2). 


Another problem with this approach is that much of the heavy lifting here is done by the strategic splitting of the defense line into multiple scenarios. Now suppose $\pr{H_p\vert E}=0.37$ and the probability of each of the defense lines given $E$ is $0.21$. This means that $H_p$ wins with each of the scenarios, so we should find against the defendant. But should we? Given the evidence, the accusation is very likely to be false, because $\pr{\n H_p \vert E}=0.63$?  The problem generalizes. If, as here, we individualize scenarios by boolean combinations of elements of a case, the more elements there are, into more  scenarios $\n H_p$ needs to be divided. This normally would lead to the probability of each of them being even lower  (because now $\pr{\n H_p}$ needs to be ``split'' between more scenarios). So, if we take this approach seriously, the more elements a case has, the more at disadvantage the defense is. This seems undesirable. 

 
 
 
 REPEAT SAME ARGUMENT USING LR AND COMPARATIVE STRATEGY
 
 
## Which Measure of (Combined) Evidential Support?
 
The discussion so far renews the question about the adequate 
measure of evidential support or evidential strength. Our focus has been on the 
Bayes factor and the likelihood ratio. As we argued earlier in Chapter
(\textbf{REFRENCE TO EARLIER CHAPTER}), the Bayes factor is too sensitive 
to the prior probability of the hypothesis (see the graph below). 

\todo{M: Not sure what's wrong with this R graph...}

\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)

bf <- function (x, s1, s2){
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


x <-seq(0,1,by=0.00001)
bfnull <- bf(x, 0.5, 0.5)
bf0807 <- bf(x, 0.8, 0.7)
bf0909 <- bf(x, 0.999, 0.999)


ggplot() +
  geom_line(aes(x = x, y = bfnull,color = "BF, sensitivity=specificity=0.5")) +
  geom_line(aes(x = x, y = bf0807,color = "BF, sensitivity=0.8, specificity=0.7")) +
  geom_line(aes(x = x, y = bf0909,color = "BF, sensitivity=0.999, specificity=0.999")) +
        ylim(c(1,10))+
  xlab("prior")+ ylab("BF")+theme_tufte()+
  theme(legend.position = c(0.8, 0.5))
```

\label{fig:strength-prior-post}
\end{figure}

Intuitively, the strenght of the evidence should not depend on the prior probability of the hypothesis, but solely on the quality of the evidence itself. The prior probability of the hypothesis seems extrinsic to the quality of the evidence since the latter should solely depend on the sensitivity and specificity of the evidence relative to the hypothesis of interest. Stil, strenght of evidence determines how much the evidence changes, upwards or downwards, the probability of a hypothesis. However, as the prior probability increases, the smaller the impact that the evidence will have on the probability of the hypothesis. If the prior is  close to one, the evidence would have marginal if not null impact. But this does not mean that the evidence weakens as the prior probability of the hypothesis goes up. For consider the same hypothesis which in one context has a very high prior probability and in another has a moderate prior probability (say a disease is common in a  population but rare in another). The outcome of the same diagnostic test (say a positive test result) performed on two people, each drawn from two populations, should not count as stronger evidence in one case than in the other. After all, it is the same test that was performed and thus the quality of the evidence should be the same. (This intuition turns out to be incorrect empirically, however. This is a point we will return to later on.)

To capture the intuition that evidential strength should be independent of the prior, evidential strength can be thought as a general measure of the relationship between prior and posterior. The graph in Figure \ref{fig:strength-prior-post}
below represents to what extent the evidence changes the prior probability of a select hypothesis for any value of the prior. The graph compares the 'base line' (representing no the change from prior to posterior) and the 'posterior line' (representing the posterior as a function of the prior for a given assignment of sensitivity and specificity of the evidence). Roughly, the larger the area between the two base line and the posterior line, the stronger the evidence. Crucially, this area does not depend on the prior probability of the hypothesis, but solely on the sensitivity and specificity of the evidence. As expected, any improvement in sensitivity or specificity will increase the area between the base line and the posterior line. 



\begin{figure}


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}

x <-seq(0,1,by=0.001)
y0906 <- post(x, 0.9, 0.6)
y0609 <- post(x, 0.6, 0.9)
y0909 <- post(x, 0.999, 0.999)

ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="null, sensitivity=specificity=0.5"))+
  geom_line(aes(x = x, y = y0906,color = "sensitivity=0.9, specificity=0.6")) +
  geom_line(aes(x = x, y = y0609,color = "sensitivity=0.6, specificity=0.9")) +
  geom_line(aes(x = x, y = y0909,color = "sensitivity=0.999, specificity=0.999")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.8, 0.2))
```

\caption{Strength of evidence as a relationship between prior and posterior. 
The further away the posterior line from the null line, the stronger the evidence irrespective 
of the prior probability of the hypothesis.}
\label{fig:strength-prior-post}
\end{figure}


The same approch can be used to model the joint evidential strength of two items of evidence, $a \wedge b$, 
relative to the combined hypothesis, $A \wedge B$. For simplicity, assume $a$ and $b$ are independent 
lines of evidence supporting their respective hypothesis $A$ and $B$ 
probabilistically independent of the other, as in the Bayesian network 
in Figure \ref{network-conjunction} (top). The graph in Figure 
\label{fig:strength-indiv-joint} shows how the prior probabilities 
are impacted by one piece of evidence in support of a single hypothesis---say $a$ supports $A$---versus 
two pieces of evidence in support of a joint hypthesis---say $a\wedge b$ supports $A \wedge B$.
The base line is lower in in the latter case than in the former case, simply because the prior probability of 
$A \wedge B$ is lower than the prior probability of $A$. The prior 
of $A$ equals $x$ and the prior of $A\wedge B$ equals $x^2$. The assumption here 
is that $A$ and $B$ have the same prior probability, and as noted before, 
are probabilistically independent of one another. 





\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


postn <- function (x, s1, s2, n){
  ((x)^n)*
    ((
      s1/
        (s1*x+(1-s2)*(1-x))
     )^n)
    
}

x <-seq(0,1,by=0.001)
y0908 <- post(x, 0.9, 0.8)
y0909 <- post(x, 0.99, 0.99)
yy0908 <- postn(x, 0.9, 0.8, 2)
yy0909 <- postn(x, 0.99, 0.99, 2)
yynull <- postn(x, 0.5, 0.5, 2)
y50908 <- postn(x, 0.9, 0.8, 5)
y50909 <- postn(x, 0.99, 0.99, 5)
y5null <- postn(x, 0.5, 0.5, 5)


ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, sens=0.5, spec=0.5"))+
  #geom_line(aes(x = x, y = yynull,color = "2, sens=0.5, spec=0.5")) +
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  #geom_line(aes(x = x, y = yy0908,color = "2, sens=0.9, spec=0.8")) +
  # geom_line(aes(x = x, y = yy0909,color = "2, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y5null, color = "5, sens=0.5, spec=0.5")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.9, 0.2), legend.title = element_blank())
```

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


post2 <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*sqrt(x)+(1-s2)*(1-sqrt(x)))
     )*
    (
      s1/
        (s1*sqrt(x)+(1-s2)*(1-sqrt(x)))
     )
}

x <-seq(0,1,by=0.001)
y0908 <- post(x, 0.9, 0.8)
y0909 <- post(x, 0.99, 0.99)
yy0908 <- post2(x, 0.9, 0.8)
yy0909 <- post2(x, 0.99, 0.99)
yynull <- post2(x, 0.5, 0.5)

ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, null"))+
  geom_line(aes(x = x, y = y0908,color = "1, sensitivity=0.8, specificity=0.7")) +
  geom_line(aes(x = x, y = y0909,color = "1, sensitivity=0.99, specificity=0.99")) +
  geom_line(aes(x = x, y = yy0908,color = "2, sensitivity=0.8, specificity=0.7")) +
  geom_line(aes(x = x, y = yy0909,color = "2, sensitivity=0.99, specificity=0.99")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.8, 0.2))
```

\caption{Strenght of evidence as a general relationship between prior 
and posterior. The comparison here is betwen individual support and joint support. 
Top graph: The null line for joint support ($y=x*x$) is 
much below the null line for individual support ($y=x$).
Bottom graph: the two null lines are equalized and the 
posterior lines adjusted accordingly. The posterior lines for individual 
and joint support get much closer especially for 
high posterior probability values.}
\label{fig:strength-indiv-joint}
\end{figure}


## Specific Narratives [IDEAS OF A SOLUTION]

So far we have assumed the most natural probabilistic interpretation of proof standards, one that 
posits a threshold on the posterior probabilities of a generic hypothesis such as guilt or civil liability. In criminal cases, the requirement is formulated as follows: guilt is proven beyond a reasonable doubt provided  $\Pr(G | E)$ is above a suitable threshold, say 95\%. The threshold is lower in civil trials. 
Civil liability is proven by preponderance provided  $\Pr(L | E)$ is above a suitable threshold, say 50\%. 
The claim that the defendant is guilty or civilly liable can be replaced by a more fine-grained hypothesis, call it $H_p$, the hypothesis put foward by the prosecutor (or the plaintiff in a civil case), for example, the hypothesis 
that the defendant killed the victim with a firearm while bulglarizing the victim's apartment.  $H_p$ can be any hypothesis which, if true, would entail the defendanat is civilly or criminally liable (according to the governing law). Hypothesis $H_p$ is a more precise description of what happened that establishes, if true, the defendant's guilt or civil liability. In defining proof standards, instead of saying -- somewhat generically -- that $\pr{G | E}$ or $\pr{L | E}$ should be above a suitable threshold, a probabilistic interpretation could read: civil or criminal liabbility is proven beyond a reasonable doubt provided $\Pr(H_d | E)$ is above a suitable threshold. 

This variation may appear inconsequential. But we argue -- perhpas surprisingly -- it can address the naked statistical evidence problem and the difficulty about conjunction. Consider the prisoner hypothetical. It is true that the naked statistics make him 99\% likely to be guilty, that is, $\pr{G | E_s}$. It is 99\% likely that he is one the prisoners who attacked and killed the guard. Notice that this a generic claim. It is odd for the prosecution to simply assert that the prisoner was one of those who killed the guard, without saying what he did, how he partook in the killing, what role he played in the attack, etc. If the prosecution offerred a more specific incriminating hypothesis, call it $H_p$, the probability $\pr{H_p | E_{s}}$ of this hypothesis based on the naked statistical evidence $E_s$ would be well below 99\%, even though $\pr{G | E_s}=99\%$. The fact the prisoner on trial is most likely guilty is an artifact of the choice of a generic hypothesis $G$. When this hypthesis is made more specific -- as it should be -- this probability drops significantly. A more detailed defense of this argument is provided in CHAPTER XYZ.

Consider now the difficulty about conjunction, focusing again on criminal cases for the sake of concreteness. 
This difficulty assumes that prosecutors should establish each element of a crime in isolation. If they manage to prove each element to the desired standard, they have meet their burden. This is an artificial 
view of legal proof. Consider a Washington statute about negligent driving:

\begin{quote}
(1)(a) A person is guilty of negligent driving in the first degree if he or she operates a motor vehicle in a manner that is both negligent and endangers or is likely to endanger any person or property, and exhibits the effects of having consumed liquor or marijuana or any drug or exhibits the effects of having inhaled or ingested any chemical, whether or not a legal substance, for its intoxicating or hallucinatory effects. RCW 46.61.5249
\end{quote}

\noindent
In other words, a prosecutor who wishes to establish beyond a reasonable doubt that the 
defendant is guilty of negligent driving should establish:

\begin{quote}
(a) the the defendant operated a vehicle
(b) that, in operating a vehicle, the defendant did so in  negligente manner
(c) that, in operating a vechicle, the defendant did so in a manner likely to endanger a person pr property
(d) that the defendant -- presumably, immediately after the incident -- exihibited the signs of intoxication by liquor or drugs
\end{quote}

\noindent
These four claims form a common narratives about what happenned. NEED TO COMPLETe THIS. BASIC IDEA IS THAT, FIRST, YOU ESTABLISH THE NARTRATIVES, AND, SECOND, THE NARRATIVE IF TRUE PROVES EACH ELEMENT. SO TO PROVE EACH ELEMENT SIMPLY MEANS TO PROVE A NARRARTIVE FROM WHICH ALL ELEMENTS FOLLOW DEDUCTIVELY. THERE IS NO PINT IN THIKING ABOUT WHETHER EACH ELEMENT HAS BEEN PROVEN. 













# The likelihood strategy 

Focusing on 
posterior probabilities is not the only approach that legal probabilists 
can pursue. By Bayes' theorem, the following holds, using $G$ and $I$ 
as competing hypotheses:

\[ \frac{\Pr(G | E)}{\Pr(I | E)} = \frac{\Pr(E | G)}{\Pr(E | I)} \times \frac{\Pr(G)}{\Pr(I)},\]

or using $H_p$ and $H_d$ as competing hypotheses,


\[ \frac{\Pr(H_p | E)}{\Pr(H_d | E)} = \frac{\Pr(E | H_p)}{\Pr(E | H_d)} \times \frac{\Pr(H_p)}{\Pr(H_d)},\]

or in words

\[ \textit{posterior odds} = \textit{likelihood ratio} \times \textit{prior odds}.\]

A difficult problem is to assign numbers to the prior probabiliteis 
such as $\Pr(G)$ or $\Pr(H_p)$, or priors odds such as $\frac{\Pr(G)}{\Pr(I)}$ or $\frac{\Pr(H_p)}{\Pr(H_d)}$. 

DISCUSS DIFFICULTIES ABOUT ASSIGNING PRIORS! WHERE?
  CAN WE USE IMPRECISE PROBABILKITIES T TALK ABOUT PRIORS -- I.E. LOW PRIORS = TOTAL IGNORANCE = VERY IMPRECISE (LARGE INTERVAL) PRIORS? THE PROBLME WITH THIS WOULD BE THAT THERE IS NO UPDSATING POSSIBLE. ALL UPDATING WOULD STILL GET BACK TO THE STARTING POINT. DO YOU HAVE AN ANSWER TO THAT? WOULD BE INTERETSING TO DISCUSS THIS!

Given these difficulties, both practical 
and theoretical, one option is to dispense with 
priors altogether. This is not implausible. Legal disputes in both 
criminal and civil trials should be decided 
on the basis of the evidence presented by the litigants. But 
it is the likelihood ratio -- not the prior ratio -- that offers the best measure 
of the overall strength of the evidece presented. So it is all too natural to focus on likekihood ratios 
and leave the priors out of the picture. If this is the right, the question is, how would a probabilistic interpretation of standards of proof based on the likelihood rato look like? 
At its simplest, this stratgey will look as follows. Recall our discussion 
of expected utility theory:


\[ \text{convict provided}  		 \frac{cost(CI)}{cost(AG)} < \frac{\Pr(H_p | E)}{\Pr(H_d | E )}, \]

which is equivalent to

\[ \text{convict provided}  		 \frac{cost(CI)}{cost(AG)} < \frac{\Pr(E | H_p)}{\Pr(E | H_d)} \times \frac{\Pr(H_p)}{\Pr(H_d)}.\]

By rearraing the terms,

\[ \text{convict provided}  \frac{\Pr(E | H_p)}{\Pr(E | H_d)} >	\frac{\Pr(H_d)}{\Pr(H_p)} \times 	 \frac{cost(CI)}{cost(AG)} .\]

Then, on this intepretation, the likelihood ratio should be above a suitable threshold 
that is a function of the cost ratio and the prior ratio. The outstanding question 
is how this threshold is to be determined. 


## Kaplow  


Quite independently, a similar approach to juridical decisions has been proposed by  @kaplow2014likelihood -- we'll call it \textbf{decision-theoretic legal probabilism (DTLP)}. It turns out that  Cheng's suggestion is  a particular case of this more general approach. Let $LR(E)=\pr{E\vert H_\Pi}/\pr{E\vert H_\Delta}$. In whole generality, DTLP invites us to convict just in case $LR(E)>LR^\star$, where $LR^\star$ is some critical value of the likelihood ratio. 

 Say we want to formulate the usual preponderance rule: convict iff $\pr{H_\Pi\vert E}>0.5$, that is, iff $\frac{\pr{H_\Pi\vert E}}{\pr{H_\Delta\vert E}}>1$. By Bayes' Theorem we have:
 
 \vspace{-6mm}
 
 \begin{align*}
\frac{\pr{H_\Pi\vert E}}{\pr{H_\Delta\vert E}} =  \frac{\pr{H_\Pi}}{\pr{H_\Delta}}\times \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}} &>1 \Leftrightarrow\\
  \Leftrightarrow \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}} &> \frac{\pr{H_\Delta}}{\pr{H_\Pi}} 
 \end{align*}
 \noindent So, as expected, $LR^\star$ is not unique and depends on priors. Analogous reformulations are available for thresholds other than $0.5$. 

Kaplow's  point is not that we can reformulate threshold decision rules in terms of priors-sensitive likelihood ratio thresholds. Rather, he insists, when we make a decision, we should factor in its consequences. Let $G$ represent potential gain from correct conviction, and $L$ stand for the potential loss resulting from mistaken conviction. Taking them into account, Kaplow suggests, we should convict if and only if:

\vspace{-6mm}

 \begin{align}
\label{eq:Kaplow_decision}
\pr{H_\Pi\vert E}\times G > \pr{H_\Delta\vert E}\times L
\end{align}
\noindent Now, \eqref{eq:Kaplow_decision} is equivalent to:

\vspace{-6mm}


\begin{align}
\nonumber
\frac{\pr{H_\Pi \vert E}}{\pr{H_\Delta \vert E}} & > \frac{L}{G}\\
\nonumber
\frac{\pr{H_\Pi}}{\pr{H_\Delta}} \times \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}} &> \frac{L}{G}\\
\nonumber
\frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}}  & > \frac{\pr{H_\Delta}}{\pr{H_\Pi}} \times \frac{L}{G}\\
\label{eq:Kaplow_decision2} LR(E)  & > \frac{\pr{H_\Delta}}{\pr{H_\Pi}} \times \frac{L}{G}
\end{align}

\noindent This is the general format of Kaplow's decision standard. 



## Dawid



Here is a slightly different perspective, due to @dawid1987difficulty, that also suggests that juridical decisions should be likelihood-based. The focus is  on witnesses for the sake of simplicity. Imagine the plaintiff produces two independent witnesses: $W_A$ attesting to $A$, and $W_B$ attesting to $B$.  Say the witnesses are regarded as $70\%$ reliable and $A$ and $B$ are probabilistically independent, so we infer $\pr{A}=\pr{B}=0.7$ and  $\pr{A\et B}=0.7^2=0.49$. 

  But, Dawid argues,  this is misleading, because to reach this result we misrepresented the reliability of the witnesses: $70\%$ reliability of a witness, he continues, does not mean that if the witness testifies that $A$, we should believe that  $\pr{A}=0.7$. To see his point,  consider two potential testimonies:


\begin{center}
\begin{tabular}
{@{}ll@{}}
\toprule
  $A_1$ & The sun rose today. \\
   $A_2$ & The sun moved backwards through the sky today.\\
\bottomrule
\end{tabular}
\end{center}

\noindent     Intuitively, after hearing them, we would still take $\pr{A_1}$ to be close to 1 and $\pr{A_2}$ to be close to 0, because we already have fairly strong convictions about the issues at hand. In general, how we should revise our beliefs  in light of a testimony depends not only on the reliability of the witness, but also on our prior convictions.\footnote{An issue that Dawid does not bring up is the interplay between our priors and our assessment of the reliability of the witnesses. Clearly, our posterior assessment of the credibility of the witness who testified $A_2$ will be lower than that of the other witness.}  And this is as it should be:  as indicated by Bayes' Theorem,  one and the same testimony with different priors might lead to different posterior probabilities.



 So far so good. But how should we represent evidence (or testimony) strength then? Well, one pretty standard way to go is to  focus on how much it contributes to the change in our beliefs in a way independent of any particular choice of prior beliefs. 
 Let $a$ be the event that the  witness testified that $A$.  It is useful to think about the problem in terms of \emph{odds, conditional odds (O)} and \emph{likelihood ratios (LR)}:
 \begin{align*} O(A)  & = \frac{\pr{A}}{\pr{\n A}}\\
 O(A\vert a) &= \frac{\pr{A\vert a}}{\pr{\n A \vert a}}  \\
 LR(a\vert A) &= \frac{\pr{a\vert A}}{\pr{a\vert \n A}}. 
\end{align*}




Suppose our prior beliefs and background knowledge, before hearing a testimony, are captured by the prior probability measure $\prr{\cdot}$, and the only thing that we learn  is $a$. We're interested in what our \emph{posterior} probability measure, $\prp{\cdot}$, and posterior odds should then be. If we're to proceed with Bayesian updating, we should have:


\vspace{-6mm}


 \begin{align*}
 \frac{\prp{A}}{\prp{\n A}} & = \frac{\prr{A\vert a}}{\prr{\n A\vert a}}
 =
 \frac{\prr{a\vert A}}{\prr{a\vert \n A}}
 \times
 \frac{\prr{A}}{\prr{\n A}}
  \end{align*}
 that is,
 
 \vspace{-6mm}
 
 \begin{align}
 \label{bayesodss2}
 O_{posterior}(A)& = O_{prior}(A\vert a) = \!\!\!\!\!  \!\!\!\!\!  \!\! \!\!  \underbrace{LR_{prior}(a\vert A)}_{\mbox{\footnotesize conditional likelihood ratio}}  \!\!\!\!\!   \!\!\!\!\!  \!\! \!\!   \times  O_{prior}(A)
 \end{align}



  The conditional likelihood ratio seems to be a much more direct measure of the value of $a$, independent of our priors regarding $A$ itself.   In general, the posterior probability of an event will equal to the witness's reliability in the sense introduced above only if the prior is $1/2$.\footnote{Dawid gives no general argument, but it is not too hard to  give one. Let $rel(a)=\pr{a\vert A}=\pr{\n a\vert \n A}$. We have in the background $\pr{a\vert \n A}=1-\pr{\n a\vert \n A}=1-rel(a)$.
 We want to find the condition under which $\pr{A\vert a} = \pr{a\vert A}$. Set $\pr{A}=p$ and  start with Bayes' Theorem and the law of total probability, and go from there:
 \begin{align*}
 \pr{A\vert a}& = \pr{a\vert A}\\
 \frac{\pr{a\vert A}p}{\pr{a\vert A}p+\pr{a\vert \n A}(1-p)} &= \pr{a\vert A} \\
 \pr{a\vert A}p & = \pr{a\vert A}[\pr{a\vert A}p+\pr{a\vert \n A}(1-p)]\\
 p & = \pr{a\vert A}p + \pr{a\vert \n A} - \pr{a\vert \n A}p\\
 p &= rel(a) p + 1-rel(a)- (1-rel(a))p\\
 p & = rel(a)p +1 - rel(a) -p +rel(a)p \\
 2p & =  2rel(a)p + 1 - rel(a)  \\
 2p - 2 rel(a)p & = 1-rel(a)\\
 2p(1-rel(a)) &= 1-rel(a)\\
 2p & = 1
 \end{align*}

\noindent  First we multiplied both sides by the denominator. Then we divided both sides by $\pr{a\vert A}$ and multiplied on the right side. Then we used our background notation and information. Next, we manipulated the right-hand side algebraically and  moved  $-p$ to the left-hand side. Move $2rel(a)p$ to the left and manipulate the result algebraically to get to the last line.}




















## Likelihood and DAC

 But how does our preference for the likelihood ratio as a measure of evidence strength relate to DAC? Let's go through Dawid's reasoning. 

 A sensible  way to probabilistically interpret the  $70\%$ reliability of a witness who testifies that $A$  is to take it to consist in the fact that the probability of a positive testimony  if $A$ is the case, just as the probability of a negative testimony  (that is, testimony that $A$ is false) if $A$ isn't the case, is 0.7:\footnote{In general setting, these are called the \emph{sensitivity} and \emph{specificity} of a test (respectively), and they don't have to be equal. For instance, a degenerate test for an illness which always responds positively, diagnoses everyone as ill, and so has sensitivity 1, but specificity 0.} 
  \[\prr{a\vert A}=\prr{\n a\vert\n  A}=0.7.\]
 \noindent   $\prr{a\vert \n A}=1- \prr{\n a\vert \n A}=0.3$, and so the same information is encoded in the appropriate likelihood ratio:
 \[LR_{prior}(a\vert A )=\frac{\prr{a\vert A}}{\prr{a\vert \n A}}= \frac{0.7}{0.3}\] 


 Let's say that $a$ \emph{provides (positive) support} for $A$ in case 
 \[O_{posterior}(A)=O_{prior}(A\vert a)> O_{prior}(A)\]
 \noindent  that is, a testimony $a$ supports $A$ just in case the posterior odds of $A$ given $a$ are greater than the prior odds of $A$ (this happens just in case $\prp{A}>\prr{A}$). By \eqref{bayesodss2}, this will be the case if and only if $LR_{prior}(a\vert A)>1$.




 One question that Dawid addresses is this: assuming reliability of witnesses  $0.7$, and assuming that  $a$ and $b$, taken separately, provide positive support for their respective claims, does it follow that  $a \et b$ provides positive support for $A\et B$?

Assuming the  independence of the witnesses, this will hold  in non-degenerate cases that do not  involve extreme probabilities, on the assumption of independence of $a$ and $b$ conditional on all combinations: $A\et B$, $A\et \n B$, $\n A \et B$ and $\n A \et \n B$.\footnote{Dawid only talks about the independence of witnesses without reference to  conditional independence. Conditional independence does not follow from independence, and it is the former that is needed here (also, four non-equivalent different versions of it).}$^,$~\footnote{In terms of notation and derivation in the optional content that will follow, the claim holds  if and only if $28 > 28 p_{11}-12p_{00}$.  This inequality is not  true for all admissible values of $p_{11}$ and $p_{00}$. If $p_{11}=1$ and $p_{00}=0$, the sides are equal. However, this is a rather degenerate example. Normally, we are  interested in cases where $p_{11}< 1$. And indeed, on this assumption, the inequality holds.}



Let us see why the above claim holds. The calculations are my reconstruction and are not due to Dawid. The reader might be annoyed with me working out the mundane details of Dawid's claims, but it turns out that in the case of Dawid's strategy, the devil is in the details. The independence of witnesses gives us:
\begin{align*}
 \pr{a \et b \vert A\et B}& =0.7^2=0.49\\
 \pr{a \et b \vert A\et \n B}& =  0.7\times 0.3=0.21\\
 \pr{a \et b \vert \n A\et B}& =  0.3\times 0.7=0.21\\
 \pr{a \et b \vert \n A\et \n B}& =  0.3\times 0.3=0.09
 \end{align*}
  Without assuming $A$ and $B$ to be independent, let the probabilities of $A\et B$, $\n A\et B$, $A\et \n B$, $\n A\et \n B$ be $p_{11}, p_{01}, p_{10}, p_{00}$. First, let's see what $\pr{a\et b}$ boils down to.

By the law of total probability we have:
 \begin{align}\label{eq:total_lower}
 \pr{a\et b} & = 
                     \pr{a\et b \vert A \et B}\pr{A\et B} + \\ &  \nonumber
                     +\pr{a\et b \vert A \et \n B}\pr{A\et \n B} \\ &  \nonumber
 + \pr{a\et b \vert \n A \et B}\pr{\n A\et B} + \\ & \nonumber
                     + \pr{a\et b \vert \n A \et \n B}\pr{\n A\et \n B}
 \end{align}
 \noindent which, when we substitute our values and constants, results in:
 \begin{align*}
                     & = 0.49p_{11}+0.21(p_{10}+p_{01})+0.09p_{00}
 \end{align*}
 Now, note that because $p_{ii}$s add up to one, we have $p_{10}+p_{01}=1-p_{00}-p_{11}$. Let us continue.
 \begin{align*}
    & = 0.49p_{11}+0.21(1-p_{00}-p_{11})+0.09p_{00} \\
                     & = 0.21+0.28p_{11}-0.12p_{00}
 \end{align*}
 
 Next, we ask what the posterior of $A\et B$ given $a\et b$ is (in the last line, we also multiply the numerator and the denominator by 100).
 \begin{align*}
 \pr{A\et B\vert a \et b} & =
         \frac{\pr{a\et b \vert A \et B}\pr{A\et B}}
             {\pr{a\et b}}\\
         & =
                     \frac{49p_{11}}
                           {21+28p_{11}-12p_{00}} 
         \end{align*}

 In this particular case, then, our question whether $\pr{A\et B\vert a\et b}>\pr{A\et B}$ boils down to asking whether
 \[\frac{49p_{11}}{21+28p_{11}-12p_{00}}> p_{11}\]
 that is, whether $28 > 28 p_{11}-12p_{00}$ (just divide both sides by $p_{11}$, multiply by the denominator, and manipulate algebraically). 




 Dawid continues working with particular choices of values and provides neither a general statement of the fact that the above considerations instantiate nor a proof of it. In the middle of the paper he says: 
 \begin{quote}
 Even under prior dependence, the combined support is always positive, in the sense that the posterior probability of the case always exceeds its prior probability\dots When the problem is analysed carefully, the `paradox' evaporates [pp. 95-7]\end{quote}
 \noindent where he  still means the case with the particular values that he has given, but he seems to suggest that the claim generalizes to a large array of cases. 


 The paper does not contain a precise statement making the conditions required explicit and, \emph{a fortriori}, does not contain a proof of it.
Given the example above and Dawid's informal reading, let us develop a more precise statement of the claim and  a proof thereof. 


\begin{fact}\label{ther:increase}
Suppose that  $rel(a),rel(b)>0.5$ and witnesses are independent conditional on all Boolean combinations of $A$ and $B$  (in a sense to be specified), and that none of the Boolean combinations of $A$ and $B$ has an extreme probability (of 0 or 1). It follows that  $\pr{A\et B \vert a\et b}>\pr{A\et B}$. (Independence of $A$ and $B$ is not required.)
\end{fact}


Roughly, the theorem says that if independent and reliable witnesses provide positive  support of their separate claims, their joint testimony provides positive support of the conjunction of their claims. 




Let us see why the claim holds. First, we introduce an abbreviation for witness reliability: 
  \begin{align*}\mathbf{a} &=rel(a)=\pr{a\vert A}=\pr{\n a\vert \n A}>0.5\\ 
\mathbf{b} &=rel(b)=\pr{b\vert B}=\pr{\n b\vert \n A}>0.5
\end{align*}
Our independence assumption means:
\begin{align*}
\pr{a\et b \vert A\et B}  &= \mathbf{ab}\\
\pr{a\et b \vert A\et \n B} & = \mathbf{a(1-b)}\\
\pr{a\et b \vert \n A\et B}  & = \mathbf{(1-a)b}\\
\pr{a\et b \vert \n A\et \n  B}  & = \mathbf{(1-a)(1-b)}
\end{align*}

\vspace{-2mm}

Abbreviate the probabilities the way we already did:

\begin{center}
\begin{tabular}{ll}
$\pr{A\et B} = p_{11}$ & $\pr{A\et \n B} = p_{10}$\\
$\pr{\n A \et B} = p_{01}$ & $\pr{\n A \et \n B}=p_{00}$
\end{tabular}
\end{center}
Our assumptions entail $0\neq p_{ij}\neq 1$ for $i,j\in \{0,1\}$ and:
\begin{align}\label{eq:sumupto1}
p_{11}+p_{10}+p_{01}+p_{00}&=1
\end{align}

\noindent So, we can use this with \eqref{eq:total_lower} to get:
\begin{align}\label{eq:aetb}
\pr{a\et b} & =  \mathbf{ab}p_{11} + \mathbf{a(1-b)}p_{10}+\mathbf{(1-a)b}p_{01} + \mathbf{(1-a)(1-b)}p_{00}\\ \nonumber
& = p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})
\end{align}

Let's now work out what the posterior of $A\et B$ will be, starting with an application of the Bayes' Theorem:
\begin{align} \nonumber
\pr{A\et B \vert a\et b} & = \frac{\pr{a\et b \vert A \et B}\pr{A\et B}}{\pr{a\et b}}
\\ \label{eq:boiled}
& = \frac{\mathbf{ab}p_{11}}{p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})}
\end{align}
To answer our question we therefore have to compare the content of \eqref{eq:boiled} to $p_{11}$ and our claim holds just in case:
\begin{align*}
\frac{\mathbf{ab}p_{11}}{p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})} &> p_{11}
\end{align*}
\begin{align*}
 \frac{\mathbf{ab}}{p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})} & > 1\end{align*}
 \begin{align}  
 \label{eq:goal}
p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab}) & < \mathbf{ab}
\end{align}
Proving \eqref{eq:goal} is therefore our goal for now. This is achieved by the following reasoning:\footnote{Thanks to Pawel Pawlowski for working on this proof with me.}



\hspace{-7mm}
\resizebox{13.5cm}{!}{
\begin{tabular}{llr}
 1. & $\mathbf{b}>0.5,\,\,\, \mathbf{a}>0.5$ & \mbox{assumption}\\
 2. & $2\mathbf{b}>1,\,\,\, 2\mathbf{a}> 1$ & \mbox{from 1.}\\
 3. & $2\mathbf{ab}>\mathbf{a},\,\,\, 2\mathbf{ab}>\mathbf{b}$ & \mbox{multiplying by $\mathbf{a}$ and $\mathbf{b}$ respectively}\\
 4.  & $p_{10}2\mathbf{ab}>p_{10}\mathbf{a}$,\,\,\, $p_{01}2\mathbf{ab}>p_{01}\mathbf{b}$ & \mbox{multiplying by $p_{10}$ and $p_{01}$ respectively}\\
 5.  & $p_{10}2\mathbf{ab} +  p_{01}2\mathbf{ab} > p_{10}\mathbf{a} + p_{01}\mathbf{b}$ & \mbox{adding by sides, 3., 4.}\\
 6. & $1- \mathbf{b}- \mathbf{a} <0$ & \mbox{from 1.}\\
 7. & $p_{00}(1-\mathbf{b}-\mathbf{a})<0$ & \mbox{From 6., because $p_{00}>0$}\\
  8.  &  $p_{10}2\mathbf{ab} +  p_{01}2\mathbf{ab} > p_{10}\mathbf{a} + p_{01}\mathbf{b} + p_{00}(1-\mathbf{b}-\mathbf{a})$ & \mbox{from 5. and 7.}\\
  9.  & $p_{10}\mathbf{ab} + p_{10}\mathbf{ab} + p_{01}\mathbf{ab} + p_{01}\mathbf{ab} + p_{00}\mathbf{ab} - p_{00}\mathbf{ab}> p_{10}\mathbf{a} + p_{01}\mathbf{b} + p_{00}(1-\mathbf{b}-\mathbf{a})$ & \mbox{8., rewriting left-hand side}\\
  10.  & $p_{10}\mathbf{ab} + p_{01}\mathbf{ab}  + p_{00}\mathbf{ab} > - p_{10}\mathbf{ab}  -  p_{01}\mathbf{ab} + p_{00}\mathbf{ab} +  p_{10}\mathbf{a} + p_{01}\mathbf{b} + p_{00}(1-\mathbf{b}-\mathbf{a})$ &  \mbox{9., moving from left to right}\\
11. & $\mathbf{ab}(p_{10}+p_{01}+p_{00})> p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{10., algebraic manipulation}\\
12. & $\mathbf{ab}(1-p_{11})> p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{11. and equation \eqref{eq:sumupto1}}\\
13. & $\mathbf{ab}- \mathbf{ab}p_{11}> p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{12., algebraic manipulation}\\
14. & $\mathbf{ab}> \mathbf{ab}p_{11}+ p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{13., moving from left to right}\\
\end{tabular}}
%\end{adjustbox}

\vspace{1mm}

The last line is what we have been after.

\intermezzob





Now that we have as a theorem an explication of what Dawid informally suggested, let's see whether it helps the probabilist handling of DAC. 


## Kaplow

 On RLP, at least in certain cases, the decision rule leads us to \eqref{eq:Cheng:compar2}, which tells us to decide the case based on whether the likelihood ratio is greater than 1.



\footnote{Again, the name of the view is by no means standard, it is  just a term I coined to refer to various types of legal probabilism in a fairly uniform manner.}  While Kaplow did not discuss DAC or the gatecrasher paradox, it is only fair to evaluate Kaplow's proposal from the perspective of these difficulties. 



Add here stuff from Marcello's Mind paper about the prisoner hypothetical. 
Then, discuss Rafal's critique of the likelihood 
ratio threshold and see where we end up. 



# Challenges (again)


## Likelihood ratio and the problem of the priors










## Dawid's likelihood strategy doesn't help

 Recall that DAC was a problem posed for the decision standard proposed by TLP, and the real question is how the information resulting from Fact \ref{ther:increase} can help to avoid that problem.  Dawid does not mention any decision standard, and so addresses quite a different question, and so it is not clear that  ``the `paradox'  evaporates'', as Dawid suggests.

 What Dawid correctly suggests (and we establish in general as Fact \ref{ther:increase})  is that  the support of the conjunction by two witnesses will be positive as soon as their separate support for the conjuncts is positive. That is, that the posterior of the conjunction will be higher that its prior. But  the critic of probabilism never denied that the conjunction of testimonies might raise the probability of the conjunction if the testimonies taken separately support the conjuncts taken separately. Such a critic can still insist that Fact \ref{ther:increase} does nothing to alleviate her concern.  After all, at least \emph{prima facie} it still might be the case that:
\begin{itemize}
\item  the posterior probabilities of the conjuncts are above a given threshold,
\item   the posterior probability of the conjunction is higher than the prior probability of the conjunction,
\item   the posterior probability of the conjunction 
 is still below the threshold.
\end{itemize}
That is, Fact \ref{ther:increase} does not entail that once the conjuncts satisfy a decision standard, so does the conjunction. 



At some point, Dawid makes a  general claim that is somewhat stronger than the one already cited:
 \begin{quote} When the problem is analysed carefully, the `paradox' evaporates: suitably measured, the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents.

  [p. 97]\end{quote}
This is quite a different claim from the content of Fact \ref{ther:increase}, because previously the joint probability was claimed only to increase as compared to the prior, and here it is claimed to increase above the level of the separate increases provided by separate testimonies. Regarding this issue Dawid elaborates (we still use the $p_{ij}$-notation that we've already introduced):
 \begin{quote}
 ``More generally, let $\pr{a\vert A}/\pr{a\vert \n A}=\lambda$, $\pr{b\vert B}/\pr{b\vert \n B}=\mu$, with $\lambda, \mu >0.7$, as might arise, for example, when there are several available testimonies. If the witnesses are
  independent, then \[\pr{A\et B\vert  a\et b} = \lambda \mu p_{11}/(\lambda \mu p_{11} + \lambda p_{10} +\mu p_{01} + p_{00})\] which  increases with
 each of $\lambda$ and $\mu$, and is never less than the larger of $\lambda p_{11}/(1-p_{11}+\lambda p_{11}),
 \mu p_{11} /(1- p_{11} 1 + \mu p_{11})$, the posterior probabilities appropriate to the individual testimonies.'' [p. 95]
 \end{quote}
This claim, however, is false.

\intermezzoa


Let us see why.   The quoted passage is a bit dense. It contains four claims for which no arguments are given in the paper. The first three are listed below as \eqref{eq:lambdamu}, the fourth is that if the conditions in \eqref{eq:lambdamu} hold,  $\pr{A\et B\vert  a\et b}>max(\pr{A\vert a},\pr{B\vert b})$.  Notice that $\lambda=LR(a\vert A)$ and $\mu=LR(b\vert B)$. Suppose the first three claims hold, that is:
 \begin{align}\label{eq:lambdamu}
 \pr{A\et B\vert  a\et b} &= \lambda \mu p_{11}/(\lambda \mu p_{11} + \lambda p_{10} +\mu p_{01} + p_{00})\\
 \pr{A\vert a} & = \frac{\lambda p_{11}}{1-p_{11}+\lambda p_{11}}\nonumber \\
 \pr{B\vert b} & = \frac{\mu p_{11}}{1-p_{11}+\mu p_{11}} \nonumber 
 \end{align}
 \noindent Is it really the case that  $\pr{A\et B\vert  a\et b}>\pr{A\vert a},\pr{B\vert b}$? It does not  seem so. Let $\mathbf{a}=\mathbf{b}=0.6$, $pr =\la p_{11},p_{10},p_{01},p_{00}\ra=\la 0.1, 0.7, 0.1, 0.1 \ra$. Then, $\lambda=\mu=1.5>0.7$ so the assumption is satisfied. 
Then we have $\pr{A}=p_{11}+p_{10}=0.8$, $\pr{B}=p_{11}+p_{01}=0.2$. We can also easily compute $\pr{a}=\mathbf{a}\pr{A}+(1-\mathbf{a})\pr{\n A}=0.56$ and $\pr{b}=\mathbf{b}\pr{B}+(1-\mathbf{b})\pr{\n B}=0.44$. 
 Yet:

 \begin{align*}
 \pr{A\vert a} & = \frac{\pr{a\vert A}\pr{A}}{\pr{a}} = \frac{0.6\times 0.8}{0.6\times 0.8 + 0.4\times 0.2}\approx 0.8571 \\
 \pr{B\vert b} & = \frac{\pr{b\vert B}\pr{B}}{\pr{b}} = \frac{0.6\times 0.2}{0.6\times 0.2 + 0.4\times 0.8}\approx 0.272 \\
 \pr{A\et B \vert a \et b} & = \frac{\pr{a\et b\vert A \et B}\pr{A\et B}}{\splitfrac{\pr{a\et b \vert A\et B}\pr{A\et B}+
   \pr{a\et b\vert A\et \n B}\pr{A\et \n B} +}{+ 
 \pr{a\et b \vert \n A \et B}\pr{\n A \et B} + \pr{a\et b \vert \n A \et \n B}\pr{\n A \et \n B}}} \\
 & = \frac{\mathbf{ab}p_{11}}{
   \mathbf{ab}p_{11} + \mathbf{a}(1-\mathbf{b})p_{10} + (1-\mathbf{a})\mathbf{b}p_{01} + (1-\mathbf{a})(1-\mathbf{b})p_{00}
 }  
    \approx 0.147
 \end{align*}
The posterior probability of $A\et B$ is not only lower than the larger of the individual posteriors, but also lower than any of them! 

So what went wrong in Dawid's calculations in \eqref{eq:lambdamu}? Well, the first formula is correct. However, let us take a look at what the second one says (the problem with the third one is pretty much the same):
\begin{align*}
\pr{A\vert a } & = \frac{\frac{\pr{a\vert A}}{\pr{\n a \vert A}}\times \pr{A\et B}}{\pr{\n (A\et B)}+ \frac{\pr{a\vert A}}{\pr{\n a \vert A}} \times \pr{A\et B}}
\end{align*}
Quite surprisingly, in Dawid's formula for $\pr{A\vert a}$, the probability of $A\et B$ plays a role. To see that it should not take any $B$ that excludes $A$ and the formula will lead to the conclusion that \emph{always} $\pr{A\vert a}$ is undefined. The problem with Dawid's formula is that instead of $p_{11}=\pr{A\et B}$ he should have used $\pr{A}=p_{11}+p_{10}$, in which case the formula would rather say this:
\begin{align*}
\pr{A\vert a } & = \frac{\frac{\pr{a\vert A}}{\pr{\n a \vert A}}\times \pr{A}}{\pr{\n A}+ \frac{\pr{a\vert A}}{\pr{\n a \vert A}} \times \pr{A}}\\
& = \frac{\frac{\pr{a\vert A}\pr{A}}{\pr{\n a \vert A}}}{\frac{\pr{\n a\vert A}\pr{\n A}}{\pr{\n a\vert A}}+ \frac{\pr{a\vert A}\pr{A}}{\pr{\n a \vert A}}}\\
& = \frac{\pr{a\vert A}\pr{A}}{\pr{\n a\vert A}\pr{\n A} + \pr{a\vert A}\pr{A}}
\end{align*}
Now, on the assumption that witness' sensitivity is equal to their specificity, we have $\pr{a\vert \n A}=\pr{\n a \vert A}$ and can substitute this in the denominator:
 \begin{align*} & = \frac{\pr{a\vert A}\pr{A}}{\pr{ a\vert \n A}\pr{\n A} + \pr{a\vert A}\pr{A}}\end{align*}
and this would be a formulation of Bayes' theorem.  And indeed with $\pr{A}=p_{11}+p_{10}$ the formula works (albeit its adequacy rests on the identity of $\pr{a\vert \n A}$ and $\pr{\n a \vert A}$), and yields the result that we already obtained:
\begin{align*}
\pr{A\vert a} &= \frac{\lambda(p_{11}+p_{10})}{1-(p_{11}+p_{10})+\lambda(p_{11}+p_{10})}\\
&= \frac{1.5\times 0.8}{1- 0.8+1.5\times 0.8} \approx 0.8571
\end{align*}



  The situation cannot be much improved by taking $\mathbf{a}$ and $\mathbf{b}$ to be high. For instance, if they're both 0.9 and $pr=\la0.1, 0.7, 0.1, 0.1 \ra$, the posterior of $A$ is $\approx 0.972$, the posterior of $B$ is $\approx 0.692$, and yet the joint posterior of $A\et B$ is $0.525$.

 The situation cannot also be improved by saying that at least if the threshold is 0.5, then as soon as $\mathbf{a}$ and $\mathbf{b}$  are above 0.7 (and, \emph{a fortriori}, so are $\lambda$ and $\mu$), the individual posteriors being above 0.5 entails the joint posterior being above 0.5 as well. For instance, for $\mathbf{a}=0.7$ and $\mathbf{b}=0.9$
 with $pr= \la 0.1, 0.3, 0.5, 0.1\ra$, the individual posteriors of $A$ and $B$ are $\approx 0.608$ and $\approx 0.931$ respectively, while the joint posterior of $A\et B$ is $\approx 0.283$.



\intermezzob

 The situation cannot be improved by saying that what was meant was rather that the joint likelihood is going to be at least as high as the maximum of the individual likelihoods, because quite the opposite is the case: the joint likelihood is going to be lower than any of the individual ones.

 \intermezzoa

Let us make sure this is the case.  We have: 
 \begin{align*}
 LR(a\vert A) & = \frac{\pr{a\vert A}}{\pr{a\vert \n A}}\\
 &= \frac{\pr{a\vert A}}{\pr{\n a\vert  A}} \\
& =  \frac{\mathbf{a}}{\mathbf{1-a}}.
\end{align*}
where the substitution in the denominator is legitimate only because witness' sensitivity is identical to their specificity. 

With the joint likelihood, the reasoning is just a bit more tricky. We will need to know what $\pr{a\et b \vert \n (A\et B)}$ is. There are three disjoint possible conditions in which the condition holds: $A\et \n B, \n A \et B$, and $\n A \et \n B$. The probabilities of $a\et b$ in these three scenarios are respectively $\mathbf{a(1-b),(1-a)b,(1-a)(1-b)}$ (again, the assumption of independence is important), and so on the assumption $\n(A\et B)$ the probability of $a\et b$ is:
\begin{align*}
\pr{a\et b \vert \n (A\et B)} & = 
\mathbf{a}(1-\mathbf{b})+(1-\mathbf{a})\mathbf{b}+(1-\mathbf{a})(1-\mathbf{b})\\ 
& = 
\mathbf{a}(1-\mathbf{b})+(1-\mathbf{a})(\mathbf{b} + 1-\mathbf{b})\\
& = \mathbf{a}(1-\mathbf{b})+(1-\mathbf{a})\\
& = \mathbf{a}-\mathbf{a}\mathbf{b}+1-\mathbf{a} = 1- \mathbf{a}\mathbf{b}
\end{align*}
So, on the assumption of witness independence, we have:
\begin{align*}
LR(a\et b \vert A \et B) & = \frac{\pr{a\et b \vert A \et B}}{\pr{a \et b\vert \n (A \et B)}} \\
& = \frac{\mathbf{ab}}{\mathbf{1-ab}}
\end{align*}

 With $0<\mathbf{a},\mathbf{b}<1$ we have $\mathbf{ab}<\mathbf{a}$, $1-\mathbf{ab}>1-\mathbf{a}$, and consequently:
 \[\frac{\mathbf{ab}}{\mathbf{1-ab}} < \frac{\mathbf{a}}{\mathbf{1-a}}\]
 which means that the joint likelihood is going to be lower than any of the individual ones.




\intermezzob



  Fact \ref{ther:increase} is so far the most optimistic reading of the claim that if witnesses are independent and fairly reliable, their testimonies are going to provide positive support for the conjunction,\footnote{And this is the reading that Dawid in passing suggests: ``the combined support is always positive, in the sense that the posterior probability of the case always exceeds its prior probability.'' [@dawid1987difficulty: 95] and any stronger reading of Dawid's suggestions fails. But Fact \ref{ther:increase} is not too exciting when it comes to answering the original DAC. The original question focused on the adjudication model according to which the deciding agents are to evaluate the posterior probability of the whole case conditional on all evidence, and to convict if it is above a certain threshold. The problem, generally, is that  it might be the case that  the pieces of evidence for particular elements of the claim can have high likelihood and posterior probabilities of particular elements can be above the threshold while the posterior joint probability will still fail to meet the threshold. The fact that the joint posterior will be higher than the joint prior does not  help much. For instance, if $\mathbf{a}=\mathbf{b}=0.7$, $pr=\la 0.1, 0.5, 0.3, 0.1\ra$, the posterior of $A$ is $\approx 0.777$, the posterior of $B$ is $\approx 0.608$ and the joint posterior is $\approx 0.216$ (yes, it is  higher than the joint prior $=0.1$, but this does not help the conjunction to satisfy the decision standard).




 To see the extent to which Dawid's strategy is helpful here, perhaps the following analogy might be useful.  
 Imagine it is winter, the heating does not work in my office and I am quite cold. I pick up the phone and call maintenance. A rather cheerful fellow picks up the phone. I tell him what my problem is, and he reacts:

 \vspace{1mm}
 \begin{tabular}{lp{10cm}}
 --- & Oh, don't worry. \\
 --- & What do you mean? It's cold in here! \\
 --- & No no, everything is fine, don't worry.\\
 --- & It's not fine! I'm cold here! \\
 --- & Look, sir, my notion of it being warm in your office is that the building provides some improvement to what the situation would be if it wasn't there. And you agree that you're definitely warmer than you'd be if your desk was standing outside, don't you? Your, so to speak, posterior warmth is higher than your prior warmth, right? 
 \end{tabular}
 \vspace{1mm}

 Dawid's discussion is in the vein of the above conversation. In response to a problem with the adjudication model under consideration Dawid simply invites us to abandon thinking in terms of it and to abandon requirements crucial for the model.  Instead, he puts forward a fairly weak notion of support (analogous to a fairly weak sense of the building providing improvement), according to which,  assuming witnesses are fairly reliable,  if separate fairly reliable witnesses provide positive support to the conjuncts, then their joint testimony provides positive support for the conjunction. 

 As far as our assessment of the original adjudication model and dealing  with DAC, this leaves us hanging. Yes, if we abandon the model, DAC does not worry us anymore. But should we? And if we do, what should we change it to, if we do not want to be banished from the paradise of probabilistic methods?  




   Having said this, let me emphasize that Dawid's paper is important in the development of the debate, since it shifts focus on the likelihood ratios, which for various reasons are much better measures of evidential support provided by particular pieces of evidence than mere posterior probabilities. 



Before we move to another attempt at a probabilistic formulation of the decision standard, let us introduce the other hero of our story: the gatecrasher paradox. It is against DAC and this paradox that the next model will be judged. 















\intermezzoa

In fact, Cohen replied to Dawid's paper [@cohen1988difficulty]. His reply, however, does not have much to do with the workings of Dawid's strategy, and is rather unusual. Cohen's first point is that the calculations of posteriors require odds about unique events, whose meaning is usually given in terms of potential wagers -- and the key criticism here is that in practice such wagers cannot be decided. This is not a convincing criticism, because the betting-odds interpretations of subjective probability do not require that on each occasion the bet should really be practically decidable. It rather invites one to imagine a possible situation in which the truth could be found out and asks: how much would we bet on a certain claim in such a situation? In some cases, this assumption is false, but there is nothing in principle wrong with thinking about the consequences of false assumptions. 



Second, Cohen says that Dawid's argument works only for testimonial evidence, not for other types thereof. But this claim is simply false -- just because Dawid used testimonial evidence as an example that he worked through it by no means follows that the approach cannot be extended. After all, as long as we can talk about sensitivity and specificity of a given piece of evidence, everything that Dawid said about testimonies can be repeated \emph{mutatis mutandis}.



Third, Cohen complaints that Dawid in his example worked with rather high priors, which according to Cohen would be too high to correspond to the presumption of innocence. This also is not a very successful rejoinder. Cohen picked his priors in the example for the ease of calculations, and the reasoning can be run with lower priors. Moreover, instead of discussing the conjunction problem, Cohen brings in quite a different problem: how to probabilistically model the presumption of innocence, and what priors of guilt should be appropriate? This, indeed, is an important problem; but it does not have much to do with DAC, and should be discussed separately.




## Problem's with Kaplow's stuff



 Kaplow does not discuss the conceptual difficulties that we are concerned with, but this will not stop us from asking whether DTLP can handle them (and answering to the negative). Let us start with DAC.
 
  Say we consider two claims, $A$ and $B$. Is it generally the case that if they separately satisfy the decision rule, then so does $A\et B$? That is, do the assumptions:
 \begin{align*}
 \frac{\pr{E\vert A}}{\pr{E\vert \n A}}  & > \frac{\pr{\n A}}{\pr{A}} \times \frac{L}{G}\\
 \frac{\pr{E\vert B}}{\pr{E\vert \n B}}  & > \frac{\pr{\n B}}{\pr{B}} \times \frac{L}{G}
 \end{align*}
 \noindent entail
 \begin{align*}
 \frac{\pr{E\vert A\et B}}{\pr{E\vert \n (A\et B)}}  & > \frac{\pr{\n (A\et B)}}{
 \pr{A\et B}} \times \frac{L}{G}?
 \end{align*}

Alas, the answer is negative.

\intermezzoa

This can be seen from the following example.  Suppose a random digit from 0-9 is drawn; we do not know the result; we are  told that the result is $<7$ ($E=$`the result is $<7$'), and  we are to decide whether to accept the following claims:
 \begin{center}
 \begin{tabular}{@{}ll@{}}
 \toprule
 $A$ & the result is $<5$. \\
 $B$  & the result is an even number.\\
 $A\et B$ & the result is an even number $<5$. \\
 \bottomrule
 \end{tabular}
 \end{center}
 Suppose that $L=G$ (this is for simplicity only --- nothing hinges on this, counterexamples for when this condition fails are analogous). First, notice that $A$ and $B$ taken separately satisfy \eqref{eq:Kaplow_decision2}. $\pr{A}=\pr{\n A}=0.5$, $\pr{\n A}/\pr{A}=1$ $\pr{E\vert A}=1$, $\pr{E\vert \n A}=0.4$. \eqref{eq:Kaplow_decision2} tells us to check:
 \begin{align*}
 \frac{\pr{E\vert A}}{\pr{E\vert \n A}}&> \frac{L}{G}\times \frac{\pr{\n A}}{\pr{A}}\\
 \frac{1}{0.4} & > 1
 \end{align*}


 \noindent so, following DTLP, we should accept $A$.  
  For analogous reasons, we should also accept $B$. $\pr{B}=\pr{\n B}=0.5$, $\pr{\n B}/\pr{B}=1$ $\pr{E\vert B}=0.8$, $\pr{E\vert \n B}=0.6$, so we need  to check that indeed:
 \begin{align*}
 \frac{\pr{E\vert B}}{\pr{E\vert \n B}}&> \frac{L}{G}\times \frac{\pr{\n B}}{\pr{B}}\\
 \frac{0.8}{0.6} & > 1 
 \end{align*}

 But now, $\pr{A\et B}=0.3$, $\pr{\n (A \et B)}=0.7$, $\pr{\n (A\et B)}/\pr{A\et B}=2\frac{1}{3}$, $\pr{E\vert A \et B}=1$, $\pr{E\vert \n (A\et B)}=4/7$ and it is false that:
  \begin{align*}
 \frac{\pr{E\vert A \et B}}{\pr{E\vert \n (A\et B)}}&> \frac{L}{G}\times \frac{\pr{\n (A \et B)}}{\pr{A \et B}}\\
 \frac{7}{4} & > \frac{7}{3} 
 \end{align*}

The example was easy, but the conjuncts are probabilistically dependent. One might ask: are there counterexamples that  involve claims which are  probabilistically independent?\footnote{Thanks to Alicja Kowalewska for pressing me on this.} 

Consider an experiment in which someone tosses a six-sided die twice. Let the result of the first toss be $X$ and the result of the second one $Y$. Your evidence is that the results of both tosses are greater than one ($E=: X>1 \et Y>1$). Now, let $A$ say that $X<5$ and $B$ say that $Y<5$.

The prior probability of $A$ is $2/3$ and the prior probability of $\n A$ is $1/3$ and so $\frac{\pr{\n A}}{\pr{A}}=0.5$. Further, $\pr{E\vert A}=0.625$, $\pr{E\vert \n A}= 5/6$ and so $\frac{\pr{E\vert A}}{\pr{E\vert \n A}}=0.75$ Clearly, $0.75>0.5$, so $A$ satisfies the decision standard. Since the situation with $B$ is symmetric, so does $B$. 

 Now, $\pr{A\et B}=(2/3)^2=4/9$ and $\pr{\n (A\et B)}=5/9$. So $\frac{\pr{\n(A\et B)}}{\pr{A\et B}}=5/4$. 
 Out of 16 outcomes for which $A\et B$ holds, $E$ holds in 9, so $\pr{E\vert A\et B}=9/16$. Out of 20 remaining outcomes for which $A\et B$ fails, $E$ holds in 16, so
  $\pr{E\vert \n (A\et B)}=4/5$. Thus, $\frac{\pr{E\vert A\et B}}{\pr{E\vert \n (A\et B)}}=45/64 <5/4$, so the conjunction does not satisfy the decision standard.



\intermezzob


Let us turn to the gatecrasher paradox. 


 Suppose $L=G$ and recall our abbreviations: $\pr{E}=e$, $\pr{H_\Pi}=\pi$. DTLP tells us to convict just in case:
 \begin{align*}
 LR(E) &> \frac{1-\pi}{\pi}
 \end{align*}
 \noindent From \eqref{eq:Cheng_lre} we already now that 
 \begin{align*}
 LR(E) & = \frac{0.991-0.991\pi}{0.009\pi}
 \end{align*}
 \noindent so we need to see whether there are any $0<\pi<1$ for which  
 \begin{align*}
  \frac{0.991-0.991\pi}{0.009\pi} &> \frac{1-\pi}{\pi}
 \end{align*}
 \noindent Multiply both sides first by $009\pi$ and then by $\pi$:
 \begin{align*}
 0.991\pi - 0.991\pi^2 &> 0.09\pi - 0.009\pi^2
 \end{align*}
 \noindent Simplify and call the resulting function $f$:
 \begin{align*}
 f(\pi) = - 0.982 \pi^2 + 0.982\pi &>0 
 \end{align*}
 \noindent The above condition is satisfied for any $0<\pi <1$ ($f$ has two zeros: $\pi = 0$ and $\pi = 1$). Here is  a plot of $f$:

 \includegraphics[width=12cm]{f-gate.png}

 Similarly, $LR(E)>1$ for any $0< \pi <1$. Here is a plot of $LR(E)$ against $\pi$:

 \includegraphics[width=12cm]{lre-gate.png}

\noindent Notice that $LR(E)$ does not go below 1. This means that for $L=G$ in the gatecrasher scenario DTLP wold tell us to convict for any prior probability of guilt $\pi\neq 0,1$.

One might ask: is the conclusion very sensitive to the choice of $L$ and $G$? The answer is, not too much.

\intermezzoa


 How sensitive is our analysis to the choice of $L/G$? Well, $LR(E)$ does not change at all, only the threshold moves. For instance, if $L/G=4$, instead of $f$ we end up with \begin{align*}
 f'(\pi) = - 0.955 \pi^2 + 0.955\pi &>0 
 \end{align*}
 and the function still takes positive values on the interval $(0,1)$. In fact, the decision won't change until $L/G$ increases to $\approx 111$. Denote $L/G$ as $\rho$, and let us start with the general decision standard, plugging in our calculations for $LR(E)$:
\begin{align*}
LR(E) &> \frac{\pr{H_\Delta}}{\pr{H_\Pi}} \rho\\
LR(E) &> \frac{1-\pi}{\pi} \rho \\
\frac{0.991-0.991\pi}{0.009\pi} &> \frac{1-\pi}{\pi} \rho\\
\frac{0.991-0.991\pi}{0.009\pi}\frac{\pi}{1-\pi} &>  \rho\\
\frac{0.991\pi-0.991\pi^2}{0.009\pi-0.009\pi^2} &>  \rho\\
\frac{\pi(0.991-0.991\pi)}{\pi(0.009-0.009\pi)} &>  \rho\\
\frac{0.991-0.991\pi}{0.009-0.009\pi} &>  \rho\\
\frac{0.991(1-\pi)}{0.009(1-\pi)} &>  \rho\\
\frac{0.991}{0.009} &>  \rho\\
110.1111 &>  \rho\\
\end{align*}






  

\intermezzob

 So, we conclude, in usual circumstances, DTLP does not handle the gatecrasher paradox. 








# Probabilistic Thresholds Revised

## Likelihood ratios and naked statistical evidence

## Conjunction paradox and Bayesian networks













# Conclusions





Where are we, how did we get here, and where can we go from here?
 We were looking for a probabilistically explicated condition $\Psi$ such that the trier of fact, at least ideally, should accept any relevant claim (including $G$) just in case $\Psi(A,E)$.

From the discussion that transpired it should be clear that we were looking for a $\Psi$ satisfying the following desiderata:

\begin{description}
\item[conjunction closure] If $\Psi(A,E)$ and $\Psi(B,E)$, then $\Psi(A\et B,E)$.
\item[naked statistics] The account should at least make it possible for convictions based on strong, but naked statistical evidence to be unjustified. 
\item[equal treatment] the condition should apply to any relevant claim whatsoever (and not just a selected claim, such as $G$).
\end{description}


Throughout the paper we focused on the first two conditions (formulated in terms of the difficulty about conjunction (DAC), and the gatecrasher paradox), going over various proposals of what $\Psi$ should be like and evaluating how they fare. The results can be summed up in the following table:


\begin{center}
\footnotesize 
 \begin{tabular}{@{}p{3cm}p{2.5cm}p{4cm}p{3cm}@{}}
\toprule
\textbf{View} & \textbf{Convict iff} & \textbf{DAC} & \textbf{Gatecrasher} \\ \midrule
Threshold-based LP (TLP) & Probability of guilt given the evidence is above a certain threshold & fails & fails \\
Dawid's likelihood strategy & No condition given, focus on $\frac{\pr{H\vert E}}{\pr{H\vert \n E}}$ & - If evidence is fairly reliable, the posterior of $A\et B$ will be greater than the prior.

- The posterior of $A\et B$ can still be lower than the posterior of any of $A$ and $B$.

- Joint likelihood, contrary do Dawid's claim, can also be lower than any of the individual likelihoods. & fails  \\
Cheng's relative LP (RLP)
& Posterior of guilt higher than the posterior of any of the defending narrations & The solution assumes equal costs of errors and independence of $A$ and $B$ conditional on $E$. It also relies on there being multiple defending scenarios individualized in terms of  combinations of literals involving $A$ and $B$. & Assumes that the prior odds of guilt are 1, and that the statistics is not sensitive to guilt (which is dubious). If the latter fails, tells to convict as long as the prior of guilt $<0.991$. \\
Kaplow's decision-theoretic LP (DTLP) &
The likelihood of the evidence is higher than the odds of innocence multiplied by the cost of error ratio & fails & convict if cost ratio $<110.1111$
\end{tabular} 
 \end{center} 


Thus, each account either simply fails to satisfy the desiderata, or succeeds on rather unrealistic assumptions.  Does this mean that a probabilistic approach to legal evidence evaluation should be abandoned? No. This only means that if we are to develop a general probabilistic model of legal decision standards, we have to do better. One promising direction is to go back to Cohen's pressure against \textbf{Requirement 1} and push against it. A brief paper suggesting this direction is [@DiBello2019plausibility], where the idea is that the probabilistic standard (be it a threshold or a comparative wrt. defending narrations) should be applied to the whole claim put forward by the plaintiff, and not to its elements. In such a context, DAC does not arise, but \textbf{equal treatment} is violated. Perhaps, there are independent reasons to abandon it, but the issue deserves further discussion. Another strategy might be to go in the direction of employing probabilistic methods to explicate the narration theory of legal decision standards [@urbaniak2018narration], but a discussion of how this approach relates to DAC and the gatecrasher paradox lies beyond the scope of this paper. 






# References








