---
title: "Appendix:  Bayes factor, likelihood ratio, and the difficulty about conjunction"
author: "Marcello Di Bello and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        - Rafal_latex7.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: [../references/referencesMRbook.bib]
csl: [../references/apa-6th-edition.csl]
indent: true
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
library(bnlearn)
library(Rgraphviz)
library(gRain)
library(kableExtra)
library(dagitty)
library(rethinking)
library(ggpubr)
library(tidyverse)
library(ggthemes)
library(plot3D)
library(tidyverse)
```


<!-- \tableofcontents -->


This appendix takes a closer look at the behavior of the Bayes factor and the likelihood ratio 
in the conjunction problem. We do so by building Direct Acyclic Graphs (DAGs) 
to study to what extent these measures of evidential support comply with the conjunction principle. 
We rely on analytic proofs, but when calculations become unmanageable, we rely instead on computer simulations. These simulations are conducted as follows. For each DAG of interest, 100,000 random Bayesian networks were generated whose probability tables had values sampled from the \textsf{Uniform(0,1)} distribution. For each of these networks, we calculated the relevant probabilities, Bayes factors, and likelihood ratios. <!---With the output of such calculations, various questions can be asked and the answers visualized.---> Even for cases in which we derived analytic proofs, the results of a computer simulation sometimes provided additional insights.







Now, for the conceptual development. First, we describe two \textsf{DAG}s at play: one of them represents a conjunction when the hypotheses are independent, and the other one drops this independence assumptions. We then discuss the relation between \textsf{DAG}s and independence, and introduce the independencies in the proofs used later on.  

Then we turn to the Bayes factor. First we prove that for if the stronger independence assumptions hold, the joint Bayes factor is just the result of multiplying the individual Bayes factors.  It follows that aggregation is satisfied in such cases, if individual Bayes factors are greater than one. Once the hypotheses are not independent, a weaker result can be obtained, which entails that the aggregation is satisfied for the Bayes factor, if a certain additional constraint is satisfied.




We investigate simulations based on the first \textsf{DAG}: in general, aggregation fails 25\% of the time if the individual Bayes factors are not constrained to be greater than one, but holds once this constraint is added. Switching to the second \textsf{DAG} does not change the picture, and so the question of whether  the additional constraint needed for the weaker theorem holds in all Bayesian networks based on this \textsf{DAG}. Inspired by this observation,  we show that in fact the additional constraint needed for aggregation to hold falls out of a pair of other independencies entailed by the second \textsf{DAG}. What the simulation reveal is that there is a large class of cases in which individual Bayes factors are above one, aggregation is satisfied, but distribution fails.  

Next, we turn to the likelihood ratio. Since the analytic approach is less feasible here, we approach Bayesian networks based on the simpler \textsf{DAG} analytically, but rely on simulations for cases in which the hypotheses are not assumed to be independent.  Without any constraint on individual likelihood ratios, aggregation fails in 12.5\% cases (twice less often than aggregation for the Bayes factor). Another difference was that while the joint Bayes factor in cases with positive individual Bayes factor was always not less than the larger of the individual factors, now around 70\% of joint likelihood ratios falls between the individual ratios, and is no lower than the smaller of these (if the individual likelihood ratios are assumed to be greater than one). Thus, aggregation is satisfied if individual likelihood ratios equal at least one, but it no longer holds that the joint support is greater than any of the individual support levels. Still, distribution fails in a large class of cases.


Finally, we identify cases in which aggregation can fail even if the individual BFs or LRs are at least one: this can happen if there is a direct dependence between the pieces of evidence. 






The \textsf{\textbf{R}} code we used in the simulations, calculations and visualizations is made available on the book website [LINK TO DOCUMENTATION TO BE ADDED LATER].

## Bayesian networks and probabilistic independence {-}


<!---Bayesian networks represent relationships of conditional probabilistic (in)dependence among variables.---> <!---The edges, intuitively, are meant to capture direct influence between the nodes.---> 
We begin with a refresher of the basic notions. A Bayesian network consists of a graphical part---a directed acyclic graph (DAG)---and a probability measure defined over the nodes (variables) in the graph. Bayesian networks satisfy the Markov condition. That is, any node is conditionally independent of its nondescentants (including ancestors), given its parents. If a probabilistic measure $\pr{}$ that is defined over the nodes (variables) in a graph $\mathsf{G}$ respects the Markov condition, $\pr{}$ is said to be compatible with $\mathsf{G}$. Graph $\mathsf{G}$ and measure $\pr{}$ can then be combined to form a Bayesian network. 

The graphical counterpart of probabilistic independence is \textbf{d-separation}, $\indep_d$.  Two nodes, $X$ and $Y$, are d-separated given a set of nodes $\mathsf{Z}$---$X\indep_d Y \vert \mathsf{Z}$ --- iff for every undirected path from $X$ to $Y$ there is a node $Z'$ on the path such that either (see Figure \ref{fig:connectionsBN}):

\begin{itemize}

\item $Z' \in \mathsf{Z}$ and there is a \textbf{serial} connection, $\rightarrow Z' \rightarrow$, on the path (\textbf{pipe}),
\item  $Z'\in \mathsf{Z}$ and there is a \textbf{diverging} connection, $\leftarrow Z' \rightarrow $, on the path (\textbf{fork}),
\item There is a \textbf{converging} connection $\rightarrow Z' \leftarrow$ on the path (in which case $Z'$ is a \textbf{collider}), and neither $Z'$ nor its descendants are in $\mathsf{Z}$.
\end{itemize}

\vspace{1mm}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
serial <- dagitty("
    dag{
        X -> Z 
        Z -> Y
      }")

diverging <- dagitty("
    dag{
        Z -> X 
        Z -> Y
      }")

converging <- dagitty("
    dag{
        X -> Z 
        Y -> Z
      }")
```



\begin{figure}[H]
\hspace{2mm}\begin{subfigure}[!ht]{0.25\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, fig.width=1, fig.height=2}
coordinates(serial) <- list( x=c(X = 2, Z = 2, Y = 2),
                                            y=c(X = 1, Z = 2, Y = 3) )
drawdag(serial, 
        shapes =  list(X = "c", Z = "c", Y = "c"), radius = 3 )
```
\subcaption{\textsf{Serial: Pipe}}
\end{subfigure} 
\hspace{5mm}\scalebox{1}{\begin{subfigure}[!ht]{0.32\textwidth}
\vspace{1mm}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=1, fig.height=2}
coordinates(diverging) <- list( x=c(X = 1, Z = 2, Y = 3) ,
                                            y=c(X = 2, Z = 1, Y = 2) )
drawdag(diverging, 
        shapes =  list(X = "c", Z = "c", Y = "c"), radius = 3)
```
\subcaption{\textsf{Diverging: Fork}}
\end{subfigure}}
\hspace{5mm}\scalebox{1}{\begin{subfigure}[!ht]{0.32\textwidth}
\vspace{1mm}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=1, fig.height=2}
coordinates(converging) <- list( x=c(X = 1, Z = 2, Y = 3) ,
                                            y=c(X = 1, Z = 2, Y = 1) )
drawdag(converging, 
        shapes =  list(X = "c", Z = "c", Y = "c"), radius = 3)
```
\subcaption{\textsf{Converging:Collider}}
\end{subfigure}}
\normalsize
\caption{Three basic types of connections.}
\label{fig:connectionsBN}
\end{figure}

\noindent Serial, converging and diverging connections represent 
common scenarios. Consider the nodes:

\footnotesize 
\begin{center}
\begin{tabular}{@{}lp{5.3cm}@{}}\toprule
Node & Proposition \\ \midrule 
$G$ & The suspect is guilty. \\
$B$ & The blood stain comes from the suspect.\\
$M$ & The crime scene stain and the suspect's blood share the same DNA profile.\\
\bottomrule
\end{tabular}
\end{center}
\normalsize
\noindent This scenarios is naturally represented by the serial connection $G \rightarrow B \rightarrow M$. If we don't know whether $B$ holds, $G$ has  an indirect impact on the probability of $M$. Yet, once we find out that $B$ is true, we expect the profile match, and whether $G$ holds has no further impact on the probability of $M$.


For converging connections, let  $G$ and $B$ be as above, and let:
\begin{center}
\begin{tabular}{@{}lp{6.3cm}@{}}\toprule
Node & Proposition \\ \midrule 
$O$ & The crime scene stain comes from the offender.\\
\bottomrule
\end{tabular}
\end{center}
\normalsize

\noindent Both $G$ and $O$ influence $B$. If suspect guilty, it is more likely that the blood stain comes from him, and if the blood crime stain comes from the offender it is more likely to come from the suspect (for instance, more so than if it comes from the victim). Moreover, $G$ and $O$ seem independent. Whether the suspect is guilty does not have any bearing on whether the stain comes from the offender. Thus, a converging connection $G\rightarrow B \leftarrow O$ seems appropriate. However, if you do find out that $B$ is true---that the stain comes from the suspect---then whether the crime stain comes from the offender becomes relevant for whether the suspect is guilty. 

Take an example of a diverging connection.  Say you have two coins, one fair, one biased. Conditional on which coin you have chosen, the results of subsequent tosses are independent. But if you don't know which coin you have chosen, the result of previous tosses give you some information about which coin it is, and this has an impact on your estimate of the probability of heads in the next toss. Whether a coin is fair, $F$, or not has an impact on the result of the first toss, $H1$, and on the result of the second toss, $H2$.  So $H1 \leftarrow F \rightarrow H2$ seems to be appropriate. Now, on one hand, so long as you do not know whether $F$, the truth of $H1$ increases the probability of $H2$.  On the other hand, once you know that $F$ is true, $H1$ and $H2$ become independent, and so conditioning on the parent in a fork makes its childern independent (provided there is no other open path between them in the graph).

As a final piece of terminology, two sets of nodes, $\mathsf{X}$ and $\mathsf{Y}$, are d-separated given $\mathsf{Z}$ if every node in $\mathsf{X}$ is d-separated from every node in $\mathsf{Y}$ given $\mathsf{Z}$. Interestingly, it can be proven that if two sets of nodes are d-separated given a third one, they are independent given the third one, for any probabilistic measures compatible with a given DAG. However, lack of d-separation does not necessarily entail dependence for any probabilistic measure compatible with a given DAG. It only allows for it: if nodes are d-separated, there is at least one probabilistic measure fitting the DAG according to which they are dependent. So, at least, no false  independencies can be inferred  from the DAG, and all the dependencies are built into it.


## Independencies in the conjunction problem {-}

Back to the difficulty with conjunction. One assumption often made in the formulation of the problem 
is that hypotheses $A$ and $B$ are probabilistically independent. We will endorse this assumption, but also relax it 
to see whether the problem subsides (it does not). 
<!---
This is not always the case---we have seen that the paradox does subside even if the two claims are dependent. In fact, it will turn out that which independencies hold in a given situation does have an impact on the behavior of the evidential strength measures under consideration. So, first we will provide some background about probabilistic independencies in Bayesian networks, and then move to identifying which independencies hold in which of the plausible candidates for a Bayesian network representing the relations between items of evidence and the hypotheses in the conjunction problem. Once this is done, we will talk about the results.
--->
In particular, we will consider the two Bayesian networks shown in Figure \ref{fig:conjunctionBNs}. They represent 
two hypotheses, $A$ and $B$, their supporting pieces of evidence, $a$ and $b$, and their conjunction $AB$.^[In the  Bayesian networks in this appendix, the conditional probability table for the conjunction node $AB$ mirrors the truth table for the conjunction in propositional logic, as in Table  \ref{tab:CPTconjunction2}.] The difference is that a direct dependence between the hypotheses exists in the second network.


\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
conjunctionDAG <- model2network("[a|A][b|B][AB|A:B][A][B]")
daggityConjunctionDag <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> AB
        B -> AB
      }")


As <- runif(1,0,1)
Bs <- runif(1,0,1)

aifAs <-runif(1,0,1)
aifnAs <- runif(1,0,1)
bifBs <-runif(1,0,1)
bifnBs <- runif(1,0,1)


AProb <-prior.CPT("A","1","0",As)
BProb <- prior.CPT("B","1","0",Bs)
aProb <- single.CPT("a","A","1","0","1","0",aifAs,aifnAs)
bProb <- single.CPT("b","B","1","0","1","0",bifBs,bifnBs)


ABProb <- array(c(1, 0, 0, 1, 0, 1, 0,1), 
                dim = c(2, 2, 2),
                dimnames = list(AB = c("1","0"),
                                B = c("1","0"), 
                                A = c("1","0")))

conjunctionCPT <- list(A = AProb, B = BProb, 
                       a = aProb, b = bProb, AB = ABProb)


conjunctionBN <- custom.fit(conjunctionDAG,conjunctionCPT)

conjunctionDAG2 <- model2network("[a|A][b|B][AB|A:B][A][B|A]")

daggityConjunctionDag2 <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> B
        A -> AB
        B -> AB
      }")

```
\normalsize


\begin{figure}[H]
\hspace{2mm}\scalebox{1}{\begin{subfigure}[!ht]{0.45\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=2.5, fig.height=2}
#graphviz.plot(conjunctionDAG, layout = "dot")
coordinates(daggityConjunctionDag) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                            y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityConjunctionDag, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 3.5)
```
\subcaption{\textsf{DAG1}}
\end{subfigure}} 
\hspace{5mm}\begin{subfigure}[!ht]{0.45\textwidth}
\vspace{1mm}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, fig.width=2.5, fig.height=2}
coordinates(daggityConjunctionDag2) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                            y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityConjunctionDag2, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 3.5)
```
\subcaption{\textsf{DAG2}}
\end{subfigure}
\normalsize
\caption{Two DAGs for the conjunction problem.}
\label{fig:conjunctionBNs}
\end{figure}





\begin{table}[h]
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
CPkable2("conjunctionBN","AB") %>%   
                          kable_styling(latex_options=c("striped","HOLD_position")) 
```
\normalsize
\caption{Conditional probability table for the conjunction node.}
\label{tab:CPTconjunction2}
\end{table}

\newpage 
\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=FALSE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
DAG1Ind <- impliedConditionalIndependencies(daggityConjunctionDag, type  = "all.pairs")

DAG2Ind <- impliedConditionalIndependencies(daggityConjunctionDag2, type  = "all.pairs")
```
\normalsize


Unsurprisingly, the relations of d-separation entailed by the two networks differ. Examples can be found in Table \ref{tab:indepBNS}. In fact, \textsf{DAG1} entails 31 d-separations, while \textsf{DAG2} entails 22 of them.  One warning about the notation. Nodes represent variables, and so each d-separation entails a probabilistic statement about  all combination of the node states (values of variables) involved. For instance, assuming each node is binary with two possible states, 1 and 0, \mbox{$B   \indep_d\,\,  a $}  entails that for any \mbox{$ B_i, a_i \in \{0, 1\}$} we have $\pr{B = B_i} = \pr{B = B_i \vert a = a_i}$. 


\begin{table}[h]
\begin{tabular}{cr}
\toprule
Bayesian network 1  & Bayesian network 2\\
\midrule
\cellcolor{gray!6}{$A   \indep_d\,\, B  $}&\cellcolor{gray!6}{     $ A  \indep_d\,\, b \vert  B  $} \\
$A   \indep_d\,\, b  $& $AB  \indep_d\,\,  a \vert  A$\\
\cellcolor{gray!6}{$\,\,\, AB  \indep_d\,\, a \vert A $}  & \cellcolor{gray!6}{$AB  \indep_d\,\,  b \vert  B $}\\
$\,\,\, AB  \indep_d\,\, b \vert B  $ & $ B  \indep_d\,\,  a \vert  A $\\
\cellcolor{gray!6}{$B   \indep_d\,\, a $}        & \cellcolor{gray!6}{$a  \indep_d\,\,  b \vert  B$ }\\
$\,\, a    \indep_d\,\, b$    & $a  \indep_d\,\,  b \vert  A $ \\
\bottomrule
\end{tabular}
\caption{Some of d-separations entailed by \textsf{DAG1} and \textsf{DAG2}.} 
\label{tab:indepBNS}
\end{table}

<!--One minimal testable implication (with the smallest possible conditioning set) is returned per missing edge of the graph--->

Turning from nodes to states (or events, propositions), Figure \ref{tab:indepBNS-states} lists the independencies between propositions.^[Some caveats. In \eqref{eq:I1} the conditioning on $a$ is not essential, because it's not on the path between the nodes: the key reason why the independence remains upon this conditioning is that there is an unconditioned collider on the path. Still, we need this independence in the proof later on. In \eqref{eq:I3} what we are conditioning on is  $A$ and $B$ jointly. Technically, independence conditional on the conjunction node $AB$ does not fall out of the d-separations present in the network---it follows given that $AB$ and $A,B$ are connected deterministically: fixing $AB$ to true fixes both $A$ and $B$ to true.] It also shows which independencies are entailed by either of the two DAGs.
  
\begin{figure}
\begin{subfigure}[!ht]{0.45\textwidth}
\begin{align} A\indep B  \label{eq:indAB}     &\hspace{2cm}\mbox{\footnotesize DAG1}\\
b \indep a   \label{eq:indab}   & \hspace{2cm}\mbox{\footnotesize DAG1}\\
A \indep b \vert a   \label{eq:I1}    &\hspace{2cm}\mbox{\footnotesize DAG1} \\
B \indep a \et A \vert b \label{eq:I2}&\hspace{2cm}\mbox{\footnotesize DAG1 } \\
a\indep b \vert A\et B \label{eq:I3}  &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\  
a\indep b \vert A \label{eq:I3a}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\ 
a\indep b \vert \n A \label{eq:I3b}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\ 
a\indep b \vert B \label{eq:I3a2}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep b \vert \n B \label{eq:I3b2}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2}
\end{align}
\end{subfigure}
\begin{subfigure}[!ht]{0.5\textwidth}
\begin{align}
a\indep B \vert A \label{eq:I4}    & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep B \vert \n A \label{eq:I4a}    & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep \n B \vert A \label{eq:I4b}   & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep \n B \vert \n A \label{eq:I4c}   & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b\indep A \et a \vert B \label{eq:I5}  & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b\indep \n A \et a \vert B \label{eq:I5a} &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2}  \\
b\indep A \et a \vert \n B \label{eq:I5b} & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b\indep \n A \et a \vert \n B \label{eq:I5c} & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b \indep a \vert B \label{eq:I6} & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} 
\end{align}
\end{subfigure}
\caption{Independencies among propositions according to \textsf{DAG1} and \textsf{DAG2}.} 
\label{tab:indepBNS-states}
\end{figure}


An ambiguity in our notation is worth mentioning. Table \ref{tab:indepBNS} lists independencies between \emph{nodes}.
But Figure \ref{tab:indepBNS-states} is about \textit{states} rather than nodes.
<!---In what follows, however, we will  use a finer level of granularity, being very explicit on what independence assumptions \emph{between propositions} are used in the derivations.---> Each particular instantiation of a node (a state) corresponds to a proposition, for example, $A = 1$ means that the proposition corresponding to $A$ holds, while $A = 0$ means that the negation of $A$ holds. Crucially, an expression such as \mbox{$b\indep A \et a \vert \n B$} should be understood as a claim about states (events, propositions), which  means  the same as  $\pr{b = 1 \vert B = 0}   = \pr{b = 1 \vert A = 1, a = 1, B = 0}$. The distinction betwen nodes and their states (or variables and their values) matters because  independence conditional on $B= 0$ doesn't entail independence given $B=1$. For instance, one's final grade might depend on hard work if the teacher is fair, but this might fail if the teacher is not fair. <!---, and, second, sometimes only some of the independencies entailed by a Bayesian network will be actually required for a given claim to hold, and we want to be explicit about such cases.---> We hope this ambiguity in notation will cause no confusion. Whether we talk about nodes or states (or events, propositions) should be clear from the context.  


## Bayes factor: claims and proofs {-}

We start with the Bayes factor $\pr{E \vert H}/\pr{E}$ as our measure of evidential support.
For ease of reference, we use the following abbreviations:
\begin{align*}
BF_A  & =  \frac{\pr{a \vert A}}{\pr{a}},\\
BF_B & = \frac{\pr{b \vert B}}{\pr{b}},\\\
BF_{AB}  & =  \frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a \wedge b}}
\end{align*}

\noindent
The objective here is to study how the combined support $BF_{AB}$ toward the conjunction $A\wedge B$ 
compares with the individual supports $BF_A$ and $BF_B$ toward the individual claims $A$ and $B$.
We prove the following general theorem:

\begin{theorem}
Given a measure $\pr{}$ compatible with \textsf{DAG1}, if both $BF_A$ and $BF_B$ 
are grater than one, then $BF_{AB}\geq \mathsf{max}(BF_{A},BF_{B})$.
The same holds for a measure compatible with \textsf{DAG2}.
\label{thm:aggregationBf}
\end{theorem}

\noindent
In other words, the combined support $BF_{AB}$ is never 
below the individual supports $BF_A$ and $BF_B$, whether claims $A$ and $B$ 
are independent (\textsf{DAG1}) or not (\textsf{DAG2}).

\begin{proof}
For \textsf{DAG1}, the theorem holds by Fact \ref{fac:BFindep} (and corollary). For 
\textsf{DAG2}, the theorem holds by Fact \ref{fac:BFdep} (and corollary), Lemma \ref{lem:BFLR}, and Fact \ref{fact:BFweaker3}. 
\end{proof}


<!-- \raf{M: ybility, the formulation you use later in FACT 3 is better i.e. "For any probabilistic measure P appropriate for (or compatible with) DAG 1", instead of the numbered assumptions that are hard to keep track. This comment applies to FACT 1 here and FACT 2 below, and all the corollaries.} -->

<!-- \mar{R: I sort of can see your point, but this would weaken the content of the theorems. I see your point about readability, but the proper identification of the exact independencies that are needed and NOTHING more has value. Not all independencies entailed by the DAGs are needed. } -->

\begin{fact} If the independence assumptions \eqref{eq:indAB}, \eqref{eq:indab}, \eqref{eq:I4} and \eqref{eq:I5} hold (all of which are entailed by \textsf{DAG1}), then 
$BF_{AB} = BF_A \times BF_B$. \label{fac:BFindep}
\end{fact}



\begin{proof}

\begin{align*}
\frac{\pr{a \wedge b\vert A\wedge B}}{\pr{a \wedge b}} & = \frac{\pr{A \et B\et a\wedge b}}{\pr{A \et B}} \bigg/ \pr{a \wedge b}
&\mbox{(conditional probability)} \\
&  = \frac{\pr{A} \times \pr {B \vert A}  \times \pr{a \vert A \et B} \times \pr{b \vert A \et B \et a}}{\pr{A \et B}} \bigg/ \pr{a \wedge b}
&\mbox{\,\,\,\,\,\,\, (chain rule)} \\
\end{align*}

\noindent Next, we apply the relevant independence assumptions:

\begin{align*}
&  = \frac{\pr{A} \times \overbrace{\pr {B \vert A}}^{ \pr{B} \mbox{\footnotesize \, by \eqref{eq:indAB}}}  \times
\overbrace{\pr{a \vert A \et B}}^{\pr{a \vert A} \mbox{\footnotesize \, by \eqref{eq:I4}}}
\times \overbrace{\pr{b \vert A \et B \et a}}^{\pr{b \vert B} \mbox{\footnotesize \, by \eqref{eq:I5}}}
}{\underbrace{\pr{A \et B}}_{\pr{A} \times \pr{B} \mbox{\footnotesize \, by \eqref{eq:indAB}}}} \bigg/ \underbrace{\pr{a \wedge b}}_{\pr{a}\times \pr{b} \mbox{ \footnotesize \, by \eqref{eq:indab}}} \\
&  = \frac{\pr{A} \times  \pr{B}   \times \pr{a \vert A}  \times  \pr{b \vert B}}
{\pr{A} \times \pr{B}} \bigg/ \pr{a}\times \pr{b} \\
& = \frac{\pr{a \vert A}  }{\pr{a}}  \times \frac{\pr{b \vert B}}{\pr{b}} \\
& = BF_{A} \times BF_B
\end{align*}

\end{proof}


This fact has the following straightforward consequences. They hold because, if $a = b \times c$ and $b, c>1$, then $a > \mathsf{max}(b,c)$, and if $b, c<1$, then $a < \mathsf{min}(b,c)$.

\begin{corollary} If the independence assumptions \eqref{eq:indAB}, \eqref{eq:indab}, \eqref{eq:I4} and \eqref{eq:I5} hold, and $BF_{A}$ and $BF_{B}$ are both greater than 1, then $BF_{AB}$ is greater than one. In fact,  $BF_{AB}$ is  greater than  $\mathsf{max}(BF_{A},BF_{B})$. \label{cor:BFind2}
\end{corollary}


\begin{corollary} If the independence assumptions \eqref{eq:indAB}, \eqref{eq:indab}, \eqref{eq:I4} and \eqref{eq:I5} hold, and $BF_{A}$ and $BF_{B}$ are both strictly less than 1, then $BF_{AB}$  is less than  $\mathsf{min}(BF_{A}, BF_{B})$. \label{cor:BFind3}
\end{corollary}


This establishes Theorem \ref{thm:aggregationBf} for \textsf{DAG 1}. After dropping the independence assumptions specific to \textsf{DAG1} and shifting to \textsf{DAG 2}, the combined $BF_{AB}$ can no longer be obtained by multiplying the individual ones, although multiplication still provides a decent approximation (see Figure \ref{fig:BFmulti}).  

\begin{figure}[H]
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
conjunctionTable2dep <- readRDS(file = "../datasets/conjunctionTable2Braf.RDS")
ggplot(conjunctionTable2dep, aes(y = BFABs, x = (BFAs * BFBs)))+geom_point(alpha= 0.2, size = .5)+
  xlab(expression(paste(BF[A], "*", BF[B] )))+
  ylab(expression(paste(BF[AB])))+
  ggtitle("Multiplicative claim fails for DAG2")+labs(subtitle = "Pearson's correlation coefficient = .95")+theme_tufte()+xlim(c(0,60))+ylim(c(0,60))
```
\caption{In DAG2, the result of multiplying individual BFs does not equal the joint BF, but often is a good approximation thereof. Axes restricted to $0,60$ (one extreme outlier lying close to the diagonal dropped).}
\label{fig:BFmulti}
\end{figure}


However, if the probabilistic measure fits \textsf{DAG 2}, Theorem \ref{thm:aggregationBf} 
is still satisfied. First, we abbreviate:
\begin{align*}
BF^{'}_{B} & = \frac{\pr{b \vert B}}{\pr{b\vert a}} \\
BF^{'}_{A} & = \frac{\pr{a \vert A}}{\pr{a \vert b}}
\end{align*}
\noindent A claim weaker than Fact \ref{fac:BFindep} can be proven by relying only on the independencies entailed by  \textsf{DAG2}. 
\begin{fact} If \eqref{eq:I4} and \eqref{eq:I5}  hold (and they do in BNs based on \textsf{DAG 2}), then $BF_{AB} =  BF_{A}\times BF^{'}_{B}  = BF_{B} \times BF^{'}_{A}$. \label{fac:BFdep}
\end{fact}


\begin{proof}

We start with the definition of conditional probability and the chain rule, as in the proof of Fact \ref{fac:BFindep}, but now we use fewer independencies (all of them entailed by \textsf{DAG2}). 

 \begin{align*}
\frac{\pr{a \wedge b\vert A\wedge B}}{\pr{a \wedge b}} &
= \frac{\pr{A} \times \pr {B \vert A}  \times
\overbrace{\pr{a \vert A \et B}}^{\pr{a \vert A} \mbox{\footnotesize \, by \eqref{eq:I4}}}
\times \overbrace{\pr{b \vert A \et B \et a}}^{\pr{b \vert B} \mbox{\footnotesize \, by \eqref{eq:I5}}}
}{\underbrace{\pr{A \et B}}_{\pr{A} \times \pr{B\vert A} \mbox{\footnotesize \, by the chain rule}}} \bigg/ \underbrace{\pr{a \wedge b}}_{\pr{a}\times \pr{b\vert a} \mbox{ \footnotesize \, by the chain rule }}\\
& = \frac{\pr{a \vert A}}{\pr{a}} \times \frac{\pr{b\vert B}}{\pr{b \vert a}}\\
& = BF_{A} \times BF^{'}_{B}
\end{align*}
If, instead of obtaining $\pr{a}\pr{b \vert a}$ in the denominator, we deploy the chain rule differently, resulting in $\pr{b}\pr{a \vert b}$, we end up with:
\begin{align*}
& = \frac{\pr{a \vert A}}{\pr{a \vert b}} \times \frac{\pr{b\vert B}}{\pr{b}}\\
& = BF^{'}_{A} \times BF_{B}
\end{align*}

\end{proof}


<!-- Now, to obtain a corollary analogous to Corollary \ref{cor:BFind2}, there are at lest two paths. One starts with an   additional disjunctive assumption that either $\pr{b\vert a} \leq \pr{b}$ or $\pr{a \vert b} \leq \pr{a}$.     -->

<!-- \raf{M: This reasoning here is muddled and does not lead very far. You later show with the simulation that these assumptiosn are false. Unless there is something interesting to learn from the fact they are false, I would remove this corollary.} -->

<!-- \begin{corollary} Suppose \eqref{eq:I4} and \eqref{eq:I5}  (they are entailed by \textsf{DAG 2}) hold and $BF_{BA}, BF_{B} >1$. -->
<!-- Then if either $\pr{b\vert a} \leq \pr{b}$ or \linebreak  $\pr{a \vert b} \leq \pr{a}$, we have $BF_{AB}\geq BF_{A}, BF_{B}$. \label{cor:BFweaker} -->
<!-- \end{corollary} -->



<!-- \begin{proof} -->
<!-- Notice that by Fact \ref{fac:BFdep} we have that on the assumptions of the corollary: -->
<!-- \begin{align*} -->
<!-- BF_{AB} & = BF_{A}\times BF^{'}_{B} \\ -->
<!-- & = BF_{B} \times BF^{'}_{A} -->
<!-- \end{align*} -->
<!-- If we now can show that either $BF^{'}_{B} \geq 1$, or $BF^{'}_{A} \geq 1$, the reasoning we used before applies: the product of two numbers greater than one is not less than either of them.  Consider $BF_{B}$, which we assumed to be greater than 1. If we can show $BF^{'}_{B}\geq BF_{B}$, we're done. But this holds on one of the disjuncts assumed in the corollary, $\pr{b\vert a} \leq \pr{b}$, as then we have: -->
<!-- \begin{align*} -->
<!-- \frac{\pr{b \vert B}}{\pr{b\vert a}} &\geq \frac{\pr{b \vert B}}{\pr{b}} \\ -->
<!-- BF^{'}_{B}  & \geq BF_{B} \\ -->
<!-- BF_{AB} & = BF_{A}\times BF^{'}_{B}   \geq BF_{A} \times BF_{B}\\ -->
<!-- \end{align*} -->
<!-- Similarly, if the other disjunct holds, we run an analogous argument, this time focusing on $BF^{'}_{A}$. -->
<!-- \end{proof} -->

<!-- \mar{R: added this paragraph and another collorary, check} -->
<!-- \raf{M: I would remove the paragraph above (se comments above), so I would remove this paragraph, as well.} -->
<!-- However, the problem with this assumption is that if it holds, this means that one item of evidence makes the other item of evidence at least as surprising as it originally was. This, at least intuitively, might not happen in a legal context. If $A$ and $B$ are elements of a crime, say that the driver was drunk and that they caused harm by erratic driving, evidence for one hypothesis, say the blood alcohol level test, in fact makes the evidence for the other hypothesis---witnesses attesting to the erratic driving, the presence of the harm, and so on,  more likely. -->

<!-- Another approach uses a conjunction of assumptions. -->

<!-- \raf{M: I think the key claim is FACT 3 which you prove nicely later. Fact 3,  combined with this corollary and Fact 2, gives the result with want. I wonder if the argument can be consolidated into just one FACT more succintly.} -->

<!-- \mar{R: I wouldn't do that. This way we have a claim about the relationship between BF and LR, and this formulation allows us to notice and focus on cases in which aggregation fails. I think it's nice to give this in these installments.} -->

<!---And here is how we get to aggregation in this setting.--->

\begin{corollary} Suppose \eqref{eq:I4} and \eqref{eq:I5}  hold (they are entailed by \textsf{DAG 2}), and $BF_{BA}, BF_{B} >1$. Then if both $\pr{a\vert b} \leq \pr{a \vert A}$ and  $\pr{b \vert a} \leq \pr{b\vert B}$, we have $BF_{AB}\geq BF_{A}, BF_{B}$. \label{cor:BFweaker2}
\end{corollary}

\begin{proof}
Assume the first conjunct holds. Then $\frac{\pr{a\vert A}}{\pr{a\vert b}} \geq 1$ and so:
\begin{align*}
BF_{AB} &= BF^{'}_{A} \times BF_{B} \geq BF_{B}
\end{align*}
\noindent The argument for the other comparison is analogous.
\end{proof}

The proof of Theorem \ref{thm:aggregationBf} for  \textsf{DAG 2} is not complete yet. This corollary relies on the additional assumptions $\pr{a\vert b} \leq \pr{a \vert A}$ and  $\pr{b \vert a} \leq \pr{b\vert B}$. They seem plausible. If, say, $a$ is used as evidence for $A$, we often expect $A$ and $a$ to be fairly strongly connected, that is, we expect $\pr{a\vert A}$ to be rather high, while the connection between different pieces of evidence for different hypotheses, intuitively, is not  expected to be as strong.  We provide a proof of these assumptions below. 

<!---##  Assumptions of Corollary \ref{cor:BFweaker2} and \textsf{DAG 2} {-}--->

<!-- \ali{R: this subsection is new, check statements and proofs in detail.} -->

We start with the following lemma.

\begin{lemma} For any probabilistic measure $\mathsf{P}$, if $BF_A >1$, then $LR_A>1$.\label{lem:BFLR}
\end{lemma}


\begin{proof} We start with our assumption.


\begin{align*}
1 & \leq \frac{\pr{a \vert A}}{\pr{a}} & &  \,\, (BF_A \geq 1) \\
\pr{A} & \leq  \frac{\pr{a \vert A}}{\pr{a}} \pr{A} & & (\mbox{algebraic manipulation}) \\
\pr{A} & \leq \pr{A \vert a} & &  (\mbox{Bayes' theorem})  \\
- \pr{A} &\geq - \pr{A \vert a} &  &   (\mbox{algebraic manipulation}) \\
1- \pr{A} & \geq 1 - \pr{A \vert a} & & (\mbox{algebraic manipulation})\\
1- \pr{A}  & \geq \pr{\n A \vert a} & & (\mbox{algebraic manipulation})\\
\pr{a}\left( 1 - \pr{A}\right) & \geq \pr{a}\pr{\n A \vert a}  & & (\mbox{algebraic manipulation})\\
\pr{a} & \geq \frac{\pr{a} \pr{\n A \vert a}}{\pr{\n A}} &  & (\mbox{algebraic manipulation, negation}) \\
\pr{a} & \geq \pr{a \vert \n A}  & &   (\mbox{conditional probability}) \\
\end{align*}
From this and our assumption  that $\pr{a \vert A} \geq \pr{a}$ it follows that $\pr{a \vert A}\geq \pr{a \vert \n A}$, that is, that \mbox{$LR_A \geq 1$}.
\end{proof}

Now the main claim. 

\begin{fact}
For any probabilistic measure $\mathsf{P}$ appropriate for \textsf{DAG 2}, if $BF_A >1$, then $\pr{a \vert A} \geq \pr{a \vert b}$ and 
$\pr{b \vert B} \geq \pr{b \vert a}$.
\label{fact:BFweaker3}
\end{fact}


\begin{proof}

Let's focus on the first conjunct. First, we have:
\begin{align*}
\pr{a \vert b} & = \pr{a \et A \vert b} + \pr{a \et \n A \vert b} & &   (\mbox{total probability}) \\
& = \underbrace{\pr{a \vert b \et A}}_{\pr{a \vert A}  \mbox{\footnotesize \, by \eqref{eq:I3a}}} \pr{A \vert b} +
\underbrace{\pr{a \vert b \et \n A}}_{\pr{a \vert \n A} \mbox{\footnotesize \, by \eqref{eq:I3b}}}\pr{\n A \vert b}  & &   (\mbox{chain rule}) \\
\end{align*}

Now let's introduce some abbreviations:
\begin{align*}
& = \underbrace{\pr{a\vert A}}_k \underbrace{\pr{A \vert b}}_x + \underbrace{\pr{a \vert \n A}}_t \underbrace{\pr{\n A \vert b}}_{(1- x)}
\end{align*}

\noindent Note that the assumption that $BF_A\geq 1$ entails, by Lemma \ref{lem:BFLR}, that $k \geq t$, and so $k-t \geq 0$. Also, since $x$ is a probability, we know $0 \leq x \leq 1$. This allows us to reason algebraically as follows:

\begin{align*}
k & \geq k  \\
k & \geq t + (k - t) \\
k & \geq t + (k -t)x \\
k & \geq kx + t  - tx \\
\pr{a \vert A} = k & \geq kx + t(1-x) = \pr{a \vert b}
\end{align*}

For the second conjunct, notice that we have a similar reasoning, albeit it relies on a different pair of independencies (which nevertheless holds in \textsf{DAG1} and \textsf{DAG 2}).
\begin{align*}
\pr{b \vert a} & = \pr{b \et B \vert a} + \pr{b \et \n B \vert a} & &   (\mbox{total probability}) \\
& = \underbrace{\pr{b \vert a \et B}}_{\pr{b \vert B}  \mbox{\footnotesize \, by \eqref{eq:I3a2}}} \pr{B \vert a} +
\underbrace{\pr{b \vert a \et \n B}}_{\pr{b \vert \n B} \mbox{\footnotesize \, by \eqref{eq:I3b2}}}\pr{\n B \vert a}  & &   (\mbox{chain rule}) \\
\end{align*}

\noindent The rest of the reasoning for this case is algebraically the same as the one used above.
\end{proof}










## Bayes factor: simulations {-}

First, let's look at the distributions of the distances of the joint posterior from the minimum of the individual posteriors (Figure \ref{fig:posteriorFailure}). 



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning =- FALSE, message = FALSE}
conjunctionTable2 <- readRDS(file = "../datasets/conjunctionTable2.RDS")

conjunctionTable2$minIndividual <- pmin(conjunctionTable2$Aifas, conjunctionTable2$Bifbs)
conjunctionTable2$diffIndividualMin <- conjunctionTable2$ABifabs - conjunctionTable2$minIndividual 

positive <- conjunctionTable2[conjunctionTable2$ABifabs > conjunctionTable2$ABs, ]

plotABindBelow2a <-  ggplot(positive, aes(x = diffIndividualMin))+geom_histogram(aes( y = ..density..), bins = 60)+
xlab("P(AB|ab) -  min(P(A|a), P(B|b))")+ggtitle("DAG 1, failure rate ca. 68%")+
labs(subtitle = expression(paste("Assuming  P(AB|ab) > P(AB)")))+xlim(c(-.3,1))+theme_tufte()

conjunctionTable2dep <- readRDS(file = "../datasets/conjunctionTableDepAli2.RDS")

conjunctionTable2dep$minIndividual <- pmin(conjunctionTable2dep$Aifas, conjunctionTable2dep$Bifbs)
conjunctionTable2dep$diffIndividualMin <- conjunctionTable2dep$ABifabs - conjunctionTable2dep$minIndividual 

positiveDEP <-  conjunctionTable2dep[ conjunctionTable2dep$ABifabs > conjunctionTable2dep$ABs,]

plotABindBelow2DEP <-  ggplot(positiveDEP, aes(x = diffIndividualMin))+geom_histogram(aes( y = ..density..), bins = 60)+
  xlab("P(AB|ab) -  min(P(A|a), P(B|b))")+ggtitle("DAG 2, Failure rate ca. 60%")+
  labs(subtitle = expression(paste("Assuming  P(AB|ab) > P(AB)")))+xlim(c(-.7,1))+theme_tufte()

plotABfaila <- ggarrange(plotABindBelow2a, plotABindBelow2DEP, ncol = 2)

```


\begin{figure}[H]

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", message = FALSE, warning = FALSE}
annotate_figure(plotABfaila, top = text_grob("Distances of the joint posterior from the lower individual posterior",size = 13))
```
\caption{Even assuming the joint support is positive, the joint posterior quite often is lower than individual posteriors.}
\label{fig:posteriorFailure}
\end{figure}



Now, let's look at simulations based on \textsf{DAG1} (Figure \ref{fig:DAG1BF}). We  can illustrate the distribution of Bayes factors in the two separate scenarios used as assumptions of the above corollaries. 


\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
old <- theme_set(theme_tufte())
conjunctionTable <-  readRDS(file = "../datasets/conjunctionTable.RDS")
conjunctionTable$maxBF <- pmax(conjunctionTable$BFAs, conjunctionTable$BFBs)
conjunctionTable$minBF <- pmin(conjunctionTable$BFAs, conjunctionTable$BFBs)
conjunctionTable$BFdifsMax <- conjunctionTable$BFABs - conjunctionTable$maxBF
conjunctionTable$BFdifsMin <- conjunctionTable$BFABs - conjunctionTable$minBF

conjunctionTable$maxLR <- pmax(conjunctionTable$LRAs, conjunctionTable$LRBs)
conjunctionTable$minLR <- pmin(conjunctionTable$LRAs, conjunctionTable$LRBs)
conjunctionTable$LRdifsMax <- conjunctionTable$LRABs - conjunctionTable$maxLR 
conjunctionTable$LRdifsMin <- conjunctionTable$LRABs - conjunctionTable$minLR 

plotBFindBelow <- conjunctionTable %>% filter(BFAs < 1 & BFBs < 1) %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("Compared to the the minimum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] < 1)))+xlim(c(-.3,.05))
plotBFindAbove <- conjunctionTable %>% filter(BFAs > 1 & BFBs > 1) %>% ggplot( aes(x = BFdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - max,"(",BF[A], ", ", BF[B],")")))+ggtitle("Compared to  the maximum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(0,3))

plotBFind <- ggarrange(plotBFindBelow, plotBFindAbove, ncol = 2)
```
\normalsize



\begin{figure}
```{r BFind,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
annotate_figure(plotBFind, top = text_grob("Distances of the joint BF from the individual BFs (DAG1)", 
                size = 13)) 
```
\caption{Distances of the joint Bayes factor from maxima and minima of individual Bayes factors, depending on whether the individual support levels are both positive or both negative. Simulation based on 100k Bayesian networks build over the DAG of \textsf{DAG1}.}
\label{fig:DAG1BF}
\end{figure}


For the DAG corresponding to \textsf{DAG1}, the simulated frequency of cases in which $BF_{AB} < BF_{A}, BF_{B}$ is 25\% (which is twice higher than for the likelihood ratio), and the structure of such cases is visualized in Figure \ref{fig:BFfails}.   The picture doesn't change when we move to \textsf{DAG2} (Figure \ref{fig:BFind2}). 

\begin{figure}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
BFfails <- conjunctionTable %>% filter(BFAs > BFABs & BFBs > BFABs ) 
scatter3D(BFfails$BFAs,BFfails$BFBs,BFfails$BFABs,pch=3,cex=0.1, alpha = .5, colvar = NULL, box = TRUE, grid = TRUE,theta=50, phi = 10, axis.ticks = TRUE, ticktype= "detailed",xlab= "BF(A)", ylab="BF(B)",zlab="BF(AB)",main="Cases in which BF(AB) < BF(A), BF(B) (frequency=.25)", cex.lab = .6, cex.axis = .4, cex.main = .8)
```
\caption{Ca. 25k cases (out of simulated 100k) in which the joint BF is below each of the individual BFs.}
\label{fig:BFfails}
\end{figure}



\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
old <- theme_set(theme_tufte())
conjunctionTable2 <-  readRDS(file = "../datasets/conjunctionTable2raf.RDS")
conjunctionTable2$maxBF <- pmax(conjunctionTable2$BFAs, conjunctionTable2$BFBs)
conjunctionTable2$minBF <- pmin(conjunctionTable2$BFAs, conjunctionTable2$BFBs)
conjunctionTable2$BFdifsMax <- conjunctionTable2$BFABs - conjunctionTable2$maxBF
conjunctionTable2$BFdifsMin <- conjunctionTable2$BFABs - conjunctionTable2$minBF

conjunctionTable2$maxLR <- pmax(conjunctionTable2$LRAs, conjunctionTable2$LRBs)
conjunctionTable2$minLR <- pmin(conjunctionTable2$LRAs, conjunctionTable2$LRBs)
conjunctionTable2$LRdifsMax <- conjunctionTable2$LRABs - conjunctionTable2$maxLR 
conjunctionTable2$LRdifsMin <- conjunctionTable2$LRABs - conjunctionTable2$minLR 


plotBFindBelow2 <- conjunctionTable2 %>% filter(BFAs < 1 & BFBs < 1) %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("Compared to the the minimum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] < 1)))+xlim(c(-.3,.05))
plotBFindAbove2 <- conjunctionTable2 %>% filter(BFAs > 1 & BFBs > 1) %>% ggplot( aes(x = BFdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - max,"(",BF[A], ", ", BF[B],")")))+ggtitle("Compared to  the maximum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(0,3))

plotBFind2 <- ggarrange(plotBFindBelow2, plotBFindAbove2, ncol = 2)
```
\normalsize




\begin{figure}
```{r BFind2,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
annotate_figure(plotBFind2, top = text_grob("Distances of the joint BF from the individual BFs (DAG2)", 
                 size = 13)) 
```

\caption{Distances of the joint Bayes factor from maxima and minima of individual Bayes factors, depending on whether the individual support levels are both positive or both negative. Simulation based on 100k Bayesian networks build over the DAG of \textsf{DAG2}.}
\label{fig:BFind2}
\end{figure}



While Corollary \ref{cor:BFweaker2} required additional assumptions to prove that the joint \textsf{BF} will be at least as high as the individual \textsf{BFs}, this assumption is not \emph{explicitly} build into the simulation, and nevertheless the consequent comes out satisfied. In fact, even more can be said. In the simulation $\pr{b \vert a}> \pr{b}$ around 50\% of the time, and the same holds for $\pr{b \vert a}> \pr{b}$, so the assumptions of Corollary \ref{cor:BFweaker} in general fail. However,  it falls out of the \textsf{DAG 2} setup and the assumption that the individual Bayes factor is greater than 1 (this assumption is essential, the claim fails without it) that  $\pr{a \vert b} \leq \pr{a \vert A}$ and $\pr{b \vert a} \leq \pr{b \vert B}$, and so, still both $BF^{'}_{A}$ and $BF^{'}_{B}$ are above one if the individual Bayes factors are above one, even though the former might be lower the latter (respectively).  We will turn to proving this very soon.
 
 
















How about distribution? Note that in principle a failure of distribution can occur whenever the joint \textsf{BF} is strictly greater than at least one of the individual $\textsf{BFs}$. The distribution of such cases for both DAGs can be inspected in Figure  \ref{fig:BFdistr}.

\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
plotBFDistr <- conjunctionTable %>% filter(BFAs > 1 & BFBs > 1) %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 60)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("DAG 1")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(-3,15))

plotBFDistr2 <- conjunctionTable2 %>% filter(BFAs > 1 & BFBs > 1) %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 60)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("DAG 2")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(-.3,15))

plotBFDistrJoint <- ggarrange(plotBFDistr, plotBFDistr2, ncol = 2)
```
\normalsize

\begin{figure}
```{r plotBFDistr,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
annotate_figure(plotBFDistrJoint, top = text_grob("Distribution failures for BF", 
                 size = 13)) 
```
\caption{Distribution failure for the Bayes factor, \textsf{DAG 1}. The $x$ axis restricted to $(-3,15)$ for visibility.}
\label{fig:BFdistr}
\end{figure}





<!-- \raf{M: For readability, it might help to plot the visual relationship between P(a|b) and P(a|A), just as a further clarification about the algebraic claim.} -->
<!-- \mar{R: I am not convinced; P(a|b) is a function of P(a|A) and two other variables, and visualisation of functions of three arguments aren't very transparent. I could try out various simplifying assumptions or particular cases, but the point of the general claim is not to use them, no?} -->



<!-- : it falls out of the \textsf{DAG 2} setup and the assumption that the individual Bayes factor is greater than 1 (this assumption is essential, the claim fails without it) that  $\pr{a \vert b} \leq \pr{a \vert A}$ and $\pr{b \vert a} \leq \pr{b \vert B}$, and so, still both $BF^{'}_{A}$ and $BF^{'}_{B}$ -->













## Likelihood ratio: claims and proofs {-}












Now, let's turn to the likelihood understood as:

\begin{align*}
\frac{\pr{E \vert H}}{\pr{E \vert \neg H}} & =
\frac{\textit{sensitivity}}{\textit{1- specificity}}\end{align*}

\noindent Let's introduce the following abbreviations:
\begin{align*}
LR_{AB} &= \frac{\pr{a\wedge b \vert a\wedge B}}{\pr{a \wedge b \vert \neg (A\wedge B)}}\\
LR_A & = \frac{\pr{a \vert A}}{\pr{a \vert \n A}} \\
LR_B & = \frac{\pr{b \vert B}}{\pr{b \vert \n B}}.
\end{align*}


\begin{fact} If independence conditions  \eqref{eq:I4}, \eqref{eq:I4a}, \eqref{eq:I4b},   \eqref{eq:I4c},  \eqref{eq:I5},   \eqref{eq:I5a},    \eqref{eq:I5b}, and   \eqref{eq:I5c}    hold, then:
\begin{align*}
LR_{AB} & =  \frac{\pr{a \vert A} \times \pr{b \vert B}}
 {\frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} }}
\end{align*}
\end{fact}

\noindent Note that these independence assumptions are entailed not only in \textsf{DAG1}, but also in \textsf{DAG2}.

\begin{proof}
Let's first compute the numerator of $LR_{AB}$:

\begin{align*}
\pr{a \wedge b\vert A\wedge B} & =  \frac{\pr{A \et B \et a\et b}}{\pr{A \et B}}
&\mbox{(conditional probability)}
\\
&= \frac{   \pr{A} \times \pr{B\vert A} \times \pr{a \vert A \wedge B} \times \pr{b \vert A \wedge B \wedge a} }{\pr{A} \times \pr{B \vert A}}
&\mbox{(chain rule)}
\end{align*}

We deploy the relevant independencies as follows:
\begin{align*}
\mbox{      } &= \frac{   \pr{A} \times \pr{B\vert A} \times \overbrace{\pr{a \vert A \wedge B}}^{\pr{a \vert A} \mbox{ \footnotesize \, by \eqref{eq:I4} } } \times \overbrace{\pr{b \vert A \wedge B \wedge a}}^{\pr{b \vert B} \mbox{ \footnotesize \, by \eqref{eq:I5} }} }{\pr{A} \times \pr{B \vert A}}
&\mbox{}\\
 & = \pr{a \vert A} \times \pr{b \vert B} 
 &\mbox{(algebraic manipulation)} 
\end{align*}





\noindent The denominator of $LR_{AB}$ is more complicated, mostly because of the conditioning on  $\neg (A \wedge B)$.

\scalebox{.85}{\parbox{1\linewidth}{
\begin{align*}
\pr{a \et b\vert \neg (A\et B)} & = \frac{\pr{a \et b \et \neg (A\et B)}}{\pr{\neg (A \et B)}} 
&\mbox{ (conditional probability)}\\
& = \frac{\pr{a \et b \et \neg A\et B} +  \pr{a \et b \et A\et \neg B} + \pr{a \et b \et \neg A\et \neg B}  }{\pr{\neg A \et B} + \pr{A \et \neg B} + \pr{\neg A \et \neg B} } 
&\mbox{ (logic \& additivity)}
\end{align*}
}}



\noindent Now consider the first summand from the numerator:
\begin{align*}
\pr{a \et b \et \neg A\et B} & = \pr{\n A} \pr{B \vert \n A} \pr{a \vert \n A \et B} \pr{b\vert a \et \n A \et B} &\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (chain rule)} \\ & = 
\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B}
&\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (independencies \eqref{eq:I4a} and \eqref{eq:I5a})} \\
\end{align*}

The simplification of the other two summanda is analogous (albeit with slightly different independence assumptions---\eqref{eq:I4b} and \eqref{eq:I5b} for the second one and \eqref{eq:I4c} and \eqref{eq:I5c} for the third. Once we plug these into the denominator formula we get:

\scalebox{.8}{\parbox{1\linewidth}{
\begin{align*}
\pr{a \et b\vert \neg (A\et B)} & = \frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} } \\
 & = \frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} }  
 \end{align*}}}

\end{proof}



## Likelihod ratio: simulations {-}


While the analytic approach turns out to be cumbersome, let's inspect the problem using simulations. First of all, there are cases in which the joint likelihood ratio are lower than each of the individual likelihood ratios. Their frequency is twice lower than the corresponding frequency for the Bayes factor (recall Figure \ref{fig:BFfails}). For the DAG corresponding to both \textsf{DAG}s, the simulated frequency of cases in which $LR_{AB} < LR_{A}, LR_{B}$ is 12-12.5\% and the distribution of such cases, somewhat of a different shape, is visualized in Figure \ref{fig:LRfails} (the picture for \textsf{DAG2} is very similar).

\begin{figure}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
LRfails <- conjunctionTable %>% filter(LRAs > LRABs & LRBs > LRABs ) 
scatter3D(LRfails$LRAs,LRfails$LRBs,LRfails$LRABs,pch=3,cex=0.1, alpha = .5, colvar = NULL, box = TRUE, grid = TRUE,theta=50, phi = 10, axis.ticks = TRUE, ticktype= "detailed",xlab= "LR(A)", ylab="LR(B)",zlab="LR(AB)",main="Cases in which LR(AB) < LR(A), LR(B) (frequency=.125 (DAG1))", cex.lab = .6, cex.axis = .4, cex.main = .8)
# 
# LRfails2 <- conjunctionTable2 %>% filter(LRAs > LRABs & LRBs > LRABs ) 
# scatter3D(LRfails2$LRAs,LRfails2$LRBs,LRfails2$LRABs,pch=3,cex=0.1, alpha = .5, colvar = NULL, box = TRUE, grid = TRUE,theta=50, phi = 10, axis.ticks = TRUE, ticktype= "detailed",xlab= "LR(A)", ylab="LR(B)",zlab="LR(AB)",main="Cases in which LR(AB) < LR(A), LR(B) (frequency=.12) (DAG2)", cex.lab = .6, cex.axis = .4, cex.main = .8)
```
\caption{Ca. 25k cases (out of simulated 100k) in which the joint BF is below each of the individual BFs.}
\label{fig:LRfails}
\end{figure}


Interestingly, even if the individual likelihood ratios are $<1$, the joint likelihood ratio can be higher than their minimum, but is never higher than their maximum (Figures \ref{fig:LRlowerPlot} and \ref{fig:LRlowerPlot2}). 

\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
plotLRindBelow <- conjunctionTable %>% filter(LRAs < 1 & LRBs < 1) %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to the the minimum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] < 1)))

plotLRindBelow2 <- conjunctionTable %>% filter(LRAs < 1 & LRBs < 1) %>% ggplot( aes(x = LRdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - max,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to the the maximum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] < 1)))



#belowLR <- conjunctionTable %>% filter(LRAs < 1 & LRBs < 1)
#mean(belowLR$LRABs > belowLR$minLR & belowLR$LRABs < belowLR$maxLR)


LRbelowPlot <- ggarrange(plotLRindBelow,plotLRindBelow2, ncol = 2)



plotLRindAbove <- conjunctionTable %>% filter(LRAs > 1 & LRBs > 1) %>% ggplot( aes(x = LRdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - max,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to  the maximum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] > 1)))+xlim(c(-10,3))


plotLRindAbove2 <- conjunctionTable %>% filter(LRAs > 1 & LRBs > 1) %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to  the minimum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] > 1)))+xlim(c(-1,15))

#aboveLR <-  conjunctionTable %>% filter(LRAs > 1 & LRBs > 1)
#mean(aboveLR$LRABs > aboveLR$minLR & aboveLR$LRABs < aboveLR$maxLR)



LRabovePlot <- ggarrange(plotLRindAbove,plotLRindAbove2, ncol = 2)
```
\normalsize

\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} 
#LRbelowPlot
annotate_figure(LRbelowPlot, top = text_grob("Distances of the joint LR from the individual LRs (negative support) \n50% joint LRs between the individual ones (DAG1)" , size = 13))
```

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are below 1, DAG used in \textsf{DAG1}.} 
\label{fig:LRlowerPlot}
\end{figure}






\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}

plotLRindBelowDep <- conjunctionTable2 %>% filter(LRAs < 1 & LRBs < 1) %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to the the minimum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] < 1)))

plotLRindBelow2Dep <- conjunctionTable2 %>% filter(LRAs < 1 & LRBs < 1) %>% ggplot( aes(x = LRdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - max,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to the the maximum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] < 1)))



# belowLRDep <- conjunctionTable2 %>% filter(LRAs < 1 & LRBs < 1)
# mean(belowLRDep$LRABs > belowLRDep$minLR & belowLRDep$LRABs < belowLRDep$maxLR)


LRbelowPlotDep <- ggarrange(plotLRindBelowDep,plotLRindBelow2Dep, ncol = 2)


#never lower than the mininum
#LRAboveDep <-  conjunctionTable2 %>% filter(LRAs > 1 & LRBs > 1)
#mean(LRAboveDep$LRABs < LRAboveDep$minLR)
#sum(LRAboveDep$LRABs < LRAboveDep$minLR)
#mean(LRAboveDep$LRABs > LRAboveDep$minLR & LRAboveDep$LRABs < LRAboveDep$maxLR)


plotLRindAboveDep <- conjunctionTable2 %>% filter(LRAs > 1 & LRBs > 1) %>% ggplot( aes(x = LRdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - max,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to  the maximum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] > 1)))+xlim(c(-10,3))


plotLRindAbove2Dep <- conjunctionTable2 %>% filter(LRAs > 1 & LRBs > 1) %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to  the minimum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] > 1)))+xlim(c(-1,15))

#aboveLR <-  conjunctionTable %>% filter(LRAs > 1 & LRBs > 1)
#mean(aboveLR$LRABs > aboveLR$minLR & aboveLR$LRABs < aboveLR$maxLR)



LRabovePlotDep <- ggarrange(plotLRindAboveDep,plotLRindAbove2Dep, ncol = 2)
```
\normalsize

\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} 
#LRbelowPlot
annotate_figure(LRbelowPlotDep, top = text_grob("Distances of the joint LR from the individual LRs (negative support) \nStill, 50% joint LRs between the individual ones (DAG2)" , size = 13))
```

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are below 1, DAG used in \textsf{DAG2}.} 
\label{fig:LRlowerPlot2}
\end{figure}





























Once the individual likelihood ratios are above 1, the joint likelihood ratio can be lower than the maximum, but is not lower than the minimum of the individual likelihood ratios (Figures \ref{fig:LRabovePlot} and \ref{fig:LRabovePlotDep}).


\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} 
#LRbelowPlot
annotate_figure(LRabovePlot, top = text_grob("Distances of the joint LR from the individual LRs (positive support) \n70%  of joint LRs between the individual ones (DAG1)",
                 size = 13, hjust = .5))
```

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are above 1, DAG used in \textsf{DAG1}.}
\label{fig:LRabovePlot}
\end{figure}





\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} 
#LRbelowPlot
annotate_figure(LRabovePlotDep, top = text_grob("Distances of the joint LR from the individual LRs (positive support) \nStill, 70%  of joint LRs between the individual ones (DAG2)",
                 size = 13, hjust = .5))
```

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are above 1, DAG used in \textsf{DAG2}.}
\label{fig:LRabovePlotDep}
\end{figure}

An example of one of many \textsf{BNs} in which an individual likelihood exceeds the joint likelihood can be inspected in Figure \ref{tab:CPTconjunctionBNL}.

<!-- \raf{M: Very interesting. This is intuitive. If the two items of evidence are not independent lines of evidence, they might not always strengthen one another. This happens in 14 per cent of the cases, not too often. Is there a pattern? Could be explored in chapter on corroborationor or cross-examination.} -->

<!-- \mar{R: Maybe, we'll keep this in mind.} -->

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
conjunctionDAG <- model2network("[a|A][b|B][AB|A:B][A][B|A]")
# added missing values BifAs and BifnAs to the final table and run the simulation again -> with same seed 
conjunctionTable2 <- readRDS(file = "../datasets/conjunctionTableAdditional.RDS")

# LRAs > LRABs
newdata <- subset(conjunctionTable2, LRAs > LRABs & BFAs > 1 & BFBs > 1 & BFABs > BFAs)
one_row <- head(newdata, 1)


As = one_row[1, "As"] 
BifAs = one_row[1, "BifAs"]
BifnAs = one_row[1, "BifnAs"]
aifAs = one_row[1, "aifAs"]
aifnAs = one_row[1, "aifnAs"]
bifBs = one_row[1, "bifBs"]
bifnBs = one_row[1, "bifnBs"]

AProb <-prior.CPT("A","1","0",As)
BProb <-  single.CPT("B","A","1","0","1","0",BifAs,BifnAs)
aProb <- single.CPT("a","A","1","0","1","0",aifAs,aifnAs)
bProb <- single.CPT("b","B","1","0","1","0",bifBs,bifnBs)


ABProb <- array(c(1, 0, 0, 1, 0, 1, 0,1), 
                dim = c(2, 2, 2),
                dimnames = list(AB = c("1","0"),
                                B = c("1","0"), 
                                A = c("1","0")))


conjunctionCPT <- list(A = AProb, B = BProb, 
                       a = aProb, b = bProb, AB = ABProb)


conjunctionBNL <- custom.fit(conjunctionDAG,conjunctionCPT)

```




\begin{figure}
\begin{subfigure}[!ht]{0.45\textwidth}

\footnotesize 
\begin{tabular}{lr}
\toprule
A & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.892}\\
0 & 0.108\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{B} & \multicolumn{2}{c}{A} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.551} & \cellcolor{gray!6}{0.457}\\
0 & 0.449 & 0.543\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{a} & \multicolumn{2}{c}{A} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.957} & \cellcolor{gray!6}{0.453}\\
0 & 0.043 & 0.547\\
\bottomrule
\end{tabular}


\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{b} & \multicolumn{2}{c}{B} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.678} & \cellcolor{gray!6}{0.573}\\
0 & 0.322 & 0.427\\
\bottomrule
\end{tabular}

\vspace{2mm}

\normalsize
\end{subfigure} \begin{subfigure}[!ht]{0.45\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=2.5, fig.height=2}
graphviz.chart(conjunctionBNL, type = "barprob")
```
\end{subfigure}
\caption{A counterexample to distribution. $LR_A  \approx 2.11$, $LR_B \approx 1.183$,  $LR_{AB} \approx 1.319$. \newline  $BF_A \approx  1.06, BF_B \approx	1.076, BF_{AB}\approx	1.14$.}
\label{tab:CPTconjunctionBNL}
\end{figure}






Note that the result for joint Bayes factors for  \textsf{DAG 2} depended on two independencies that fell out of the dag: that $a$ and $b$ are independent both conditional on $A$ and conditional on $\n A$. A natural question is: does  aggregation hold once this independence is dropped? To investigate, we run a simulation based on \textsf{DAG 3}, illustrated in Figure \ref{fig:dag3}.

\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
doubleDepDAG <- model2network("[a|A:b][b|B][AB|A:B][A][B|A]")

daggityDoubleDepDAG <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> B
        A -> AB
        B -> AB
        b -> a
      }")

```
\normalsize

\begin{figure}[H]

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=2.5, fig.height=2}
#graphviz.plot(conjunctionDAG, layout = "dot")
coordinates(daggityDoubleDepDAG) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                          y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityDoubleDepDAG, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 3.5)
```
\caption{\textsf{DAG 3} with direct dependence between the pieces of evidence.}
\label{fig:dag3}
\end{figure}

In fact, it turns out that simultaneously the joint likelihood ratio is lower than both individual likelihood ratios and the joint Bayes factor is lower than each individual Bayes factor in 14\% cases in which the individual Bayes factor (and therefore also the individual likelihood ratios) are greater than one. One particular counterexample is illustrated in Figure \ref{fig:CPTDoubleL}.



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
doubleDepDAG <- model2network("[a|A:b][b|B][AB|A:B][A][B|A]")
# added missing values BifAs and BifnAs to the final table and run the simulation again -> with same seed 
doubleDepTable <- readRDS(file = "../datasets/doubleDependencyTable.RDS")


newdata <- subset(doubleDepTable, LRAs > LRABs & LRBs > LRABs & BFAs > 1 & BFBs > 1 &
                    BFABs < BFAs & BFABs < BFBs)
one_rowB <- head(newdata, 1)


As = one_rowB[1, "As"] 
BifAs = one_rowB[1, "BifAs"]
BifnAs = one_rowB[1, "BifnAs"]
aifAs = one_rowB[1, "aifAs"]
aifnAs = one_rowB[1, "aifnAs"]
bifBs = one_rowB[1, "bifBs"]
bifnBs = one_rowB[1, "bifnBs"]
aifAbs <- one_rowB[1, "aifAbs"]
aifnAnbs <- one_rowB[1, "aifnAnbs"]
aifnAbs <- one_rowB[1, "aifnAbs"]
aifAnbs <- one_rowB[1, "aifAnbs"]



AProb <-prior.CPT("A","1","0",As)
BProb <-  single.CPT("B","A","1","0","1","0",BifAs,BifnAs)
#aProb <- single.CPT("a","A","1","0","1","0",aifAs,aifnAs)
bProb <- single.CPT("b","B","1","0","1","0",bifBs,bifnBs)


aProb <- array(c(one_rowB[1,"aifAbs"], 1-one_rowB[1,"aifAbs"],
                 one_rowB[1,"aifAnbs"], 1 - one_rowB[1,"aifAnbs"], 
                 one_rowB[1,"aifnAbs"], 1- one_rowB[1,"aifnAbs"],
                 one_rowB[1,"aifnAnbs"], 1 - one_rowB[1,"aifnAnbs"]), 
               dim = c(2, 2, 2),
               dimnames = list(a = c("1","0"),
                               b = c("1","0"), 
                               A = c("1","0")))

ABProb <- array(c(1, 0, 0, 1, 0, 1, 0,1), 
                dim = c(2, 2, 2),
                dimnames = list(AB = c("1","0"),
                                B = c("1","0"), 
                                A = c("1","0")))


conjunctionCPT <- list(A = AProb, B = BProb, 
                       a = aProb, b = bProb, AB = ABProb)


conjunctionDoubleBNL <- custom.fit(doubleDepDAG,conjunctionCPT)

 
```







\begin{figure}
\hspace{2mm}\begin{subfigure}[ht!]{0.45\textwidth}
\footnotesize


\begin{tabular}{lr}
\toprule
A & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.521}\\
0 & 0.479\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{B} & \multicolumn{2}{c}{A} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.66} & \cellcolor{gray!6}{0.729}\\
0 & 0.34 & 0.271\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lllr}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{A} & \multicolumn{1}{c}{b} & \multicolumn{1}{c}{} \\
a &  &  & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.3989398}\\
0 & 1 & 1 & 0.6010602\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.9673984}\\
0 & 0 & 1 & 0.0326016\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0.9693564}\\
0 & 1 & 0 & 0.0306436\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0.7267025}\\
0 & 0 & 0 & 0.2732975\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{b} & \multicolumn{2}{c}{B} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.995} & \cellcolor{gray!6}{0.108}\\
0 & 0.005 & 0.892\\
\bottomrule
\end{tabular}

\normalsize 


\subcaption{Conditional probabilities for the counterexample (the one for \textsf{AB} does not change).}
\end{subfigure} 
\hspace{5mm}\begin{subfigure}{0.45\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=2.5, fig.height=2}
graphviz.chart(conjunctionDoubleBNL, type = "barprob")
```
\subcaption{Marginal probabilities. }
\end{subfigure} 
\caption{A counterexample based on \textsf{DAG 3}, with independence between the items of evidence dropped.   $LR_A  \approx 1.063$, $LR_B \approx 1.159742$,  $LR_{AB} \approx 0.651$. $BF_A \approx  1.022, BF_B \approx	1.079, BF_{AB}\approx	0.699$.}
\label{fig:CPTDoubleL}
\end{figure}








```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
doubleTable <-  readRDS(file = "../datasets/doubleDependencyTable.RDS")

doubleTable$maxBF <- pmax(doubleTable$BFAs, doubleTable$BFBs)
doubleTable$minBF <- pmin(doubleTable$BFAs, doubleTable$BFBs)
doubleTable$BFdifsMax <- doubleTable$BFABs - doubleTable$maxBF
doubleTable$BFdifsMin <- doubleTable$BFABs - doubleTable$minBF

doubleTable$maxLR <- pmax(doubleTable$LRAs, doubleTable$LRBs)
doubleTable$minLR <- pmin(doubleTable$LRAs, doubleTable$LRBs)
doubleTable$LRdifsMax <- doubleTable$LRABs - doubleTable$maxLR 
doubleTable$LRdifsMin <- doubleTable$LRABs - doubleTable$minLR

positiveBF <- doubleTable %>% filter (BFAs >1 & BFBs > 1 )


plotBFfailure <- positiveBF %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("Joint BFs compared to  the minimum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(-3,3))

plotLRfailure <- positiveBF %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Joint LRs compared to  the minimum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(-3,3))

aggregationFailure <- ggarrange(plotBFfailure, plotLRfailure)

aggregationFailurePlot <- annotate_figure(aggregationFailure, top = text_grob("Aggregation failure with DAG3" , size = 13))
```


\begin{figure}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
aggregationFailurePlot
```
\caption{Aggregation failure.} 
\label{fig:aggregationFails}
\end{figure}

\mar{R: Two new passages on fixed joint evidence, check.}
Finally, you might recall that in the chapter we distinguished two variants of the distribution requirement, (DIS1) and (DIS2). The former  has been in the focus so far, and the latter differs in keeping the evidence fixed at $a \et b$, even when calculating individual LRs. So, sticking to LR, distribution in this variant  pertains to the joint LR and the following two ratios: $\nicefrac{\pr{a\et b \vert A}}{\pr{a \et b \vert \n A}}$ and  $\nicefrac{\pr{a\et b \vert B}}{\pr{a \et b \vert \n B}}$. To make sure that switching to (DIS2) doesn't make things easier for legal probabilists, we ran 30k simulations (10k for each BN type), and inspected the status of aggregation and distribution for these ratios. 

Here are the results. If no assumption about the direction of support is made, 
 around 12.7\% of the time  (around twice less often than if the usual 
 individual LRs are used),  the individual LRs with fixed evidence are both greater than the joint LR---this is for \textsf{DAG1}, the frequency goes slightly up to around 13\% if we switch to \textsf{DAG2} and is around the same value if additionally we allow for the dependency between the items of evidence (\textsf{DAG3}). Assuming individual LRs are above one, around 70\% of joint likelihoods (75\% for BN2, 72\% for BN3) are between the individual ones, no joint LR is below the minimum of the individual ratios for \textsf{DAG1}, but is so around 2\% times  for both \textsf{DAG2} and  \textsf{DAG3}. This is one important difference: even aggregation can fail if dependencies are present, if we keep joint evidence fixed in all the ratios. As for distribution, there are no major surprises:  around 30\% of the time, the joint LR is strictly greater than both of the individual LRs with evidence fixed for \textsf{DAG1},  22\% for \textsf{DAG2}, and 26\% for \textsf{DAG3}. In short, keeping the joint evidence fixed across 
 all ratios makes things even harder, when it comes to preserving aggregation and distribution.

 
 
 
.   
 
 
 
 
 
 
