---
title: "Appendix:  Bayes factor, likelihood ratio, and the difficulty about conjunction"
author: "Marcello Di Bello and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        - Rafal_latex7.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: [../references/referencesMRbook.bib]
csl: [../references/apa-6th-edition.csl]
indent: true
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
library(bnlearn)
library(Rgraphviz)
library(gRain)
library(kableExtra)
library(dagitty)
library(rethinking)
library(ggpubr)
library(tidyverse)
library(ggthemes)
library(plot3D)
library(tidyverse)
```


<!-- \tableofcontents -->


In this appendix we take a closer look at the behavior of the Bayes factor and the likelihood ratio with respect to the conjunction problem. We do so by building alternative Direct Acyclic Graphs (DAGs) to represent the problem and study the relevant properties of probabilistic measures appropriate for those DAGS. We do so in two manners: by standard proofs, and by simulations. The former method is well-known, but let us say a few words about the latter.

In what follows, we will provide theorems where we managed to obtain them. However, in some cases analytic calculations are somewhat unmanageable, and so we decided to inspect such issues by means of a \textbf{simulation}.  For each DAG under consideration, we generated 100k random Bayesian networks (with probabilities based on values sampled from the \textsf{Uniform(0,1)} distribution) which share that DAG. For each of these networks, we calculated all the relevant probabilities, Bayes factors, and likelihood ratios. With the output of such calculations, various questions can be asked and the answers visualized. Even for cases in which we obtained the relevant proofs, the results of a simulation provide further insights, for instance, in terms of relative frequencies of various facts.

Now, for the conceptual development. First, we describe two \textsf{DAG}s at play: one of them represents a conjunction when the hypotheses are independent, and the other one drops this independence assumptions. We then discuss the relation between \textsf{DAG}s and independence, and introduce the independencies in the proofs used later on.  

Then we turn to the Bayes factor. First we prove that for if the stronger independence assumptions hold, the joint Bayes factor is just the result of multiplying the individual Bayes factors.  It follows that aggregation is satisfied in such cases, if individual Bayes factors are greater than one. Once the hypotheses are not independent, a weaker result can be obtained, which entails that the aggregation is satisfied for the Bayes factor, if a certain additional constraint is satisfied.




We investigate simulations based on the first \textsf{DAG}: in general, aggregation fails 25\% of the time if the individual Bayes factors are not constrained to be greater than one, but holds once this constraint is added. Switching to the second \textsf{DAG} does not change the picture, and so the question of whether  the additional constraint needed for the weaker theorem holds in all Bayesian networks based on this \textsf{DAG}. Inspired by this observation,  we show that in fact the additional constraint needed for aggregation to hold falls out of a pair of other independencies entailed by the second \textsf{DAG}. What the simulation reveal is that there is a large class of cases in which individual Bayes factors are above one, aggregation is satisfied, but distribution fails.  

Next, we turn to the likelihood ratio. Since the analytic approach is less feasible here, we approach Bayesian networks based on the simpler \textsf{DAG} analytically, but rely on simulations for cases in which the hypotheses are not assumed to be independent.  Without any constraint on individual likelihood ratios, aggregation fails in 12.5\% cases (twice less often than aggregation for the Bayes factor). Another difference was that while the joint Bayes factor in cases with positive individual Bayes factor was always not less than the larger of the individual factors, now around 70\% of joint likelihood ratios falls between the individual ratios, and is no lower than the smaller of these (if the individual likelihood ratios are assumed to be greater than one). Thus, aggregation is satisfied if individual likelihood ratios equal at least one, but it no longer holds that the joint support is greater than any of the individual support levels. Still, distribution fails in a large class of cases.


Finally, we identify cases in which aggregation can fail even if the individual BFs or LRs are at least one: this can happen if there is a direct dependence between the pieces of evidence. 






The \textsf{\textbf{R}} code we used in the simulations, calculations and visualizations is made available on the book website [LINK TO DOCUMENTATION TO BE ADDED LATER].

## Bayesian networks and probabilistic independence {-}

One assumption often made in the formulation 
of the conjunction paradox is that claims $A$ and $B$ are probabilistically independent. This is not always the case---we have seen that the paradox does subside even if the two claims are dependent. In fact, it will turn out that which independencies hold in a given situation does have an impact on the behavior of the evidential strength measures under consideration. So, first we will provide some background about probabilistic independencies in Bayesian networks, and then move to identifying which independencies hold in which of the plausible candidates for a Bayesian network representing the relations between items of evidence and the hypotheses in the conjunction problem. Once this is done, we will talk about the results.





Directed Acyclic Graphs (DAGs) are useful for representing graphically these relationships of independence. 
The edges, intuitively, are meant to capture direct influence between the nodes. The role that such direct influence plays is that in a Bayesian network built over a DAG any node is conditionally independent of its nondescentants (including ancestors), given its parents. If this is the case for a given probabilistic measure $\pr{}$ and a given DAG, we say that $\pr{}$ is compatible with $\mathsf{G}$, and they can be put together to constitute a Bayesian network. 



The graphical counterpart of probabilistic independence is the so-called \textbf{d-separation}, $\indep_d$.  We say that two nodes, $X$ and $Y$, are d-separated given a set of nodes $\mathsf{Z}$---$X\indep_d Y \vert \mathsf{Z}$ --- iff for every undirected path from $X$ to $Y$ there is a node $Z'$ on the path such that either:

\begin{itemize}

\item $Z' \in \mathsf{Z}$ and there is a \textbf{serial} connection, $\rightarrow Z' \rightarrow$, on the path (\textbf{pipe}),
\item  $Z'\in \mathsf{Z}$ and there is a \textbf{diverging} connection, $\leftarrow Z' \rightarrow $, on the path (\textbf{fork}),
\item There is a \textbf{converging} connection $\rightarrow Z' \leftarrow$ on the path (in which case $Z'$ is a \textbf{collider}), and neither $Z'$ nor its descendants are in $\mathsf{Z}$.
\end{itemize}

\vspace{1mm}

\noindent These types of connections are visualized in Figure \ref{fig:connectionsBN}.

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
serial <- dagitty("
    dag{
        X -> Z 
        Z -> Y
      }")

diverging <- dagitty("
    dag{
        Z -> X 
        Z -> Y
      }")

converging <- dagitty("
    dag{
        X -> Z 
        Y -> Z
      }")
```



\begin{figure}[H]
\hspace{2mm}\begin{subfigure}[!ht]{0.25\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, fig.width=1, fig.height=2}
coordinates(serial) <- list( x=c(X = 2, Z = 2, Y = 2),
                                            y=c(X = 1, Z = 2, Y = 3) )
drawdag(serial, 
        shapes =  list(X = "c", Z = "c", Y = "c"), radius = 3 )
```
\subcaption{\textsf{Pipe}}
\end{subfigure} 
\hspace{5mm}\scalebox{1}{\begin{subfigure}[!ht]{0.32\textwidth}
\vspace{1mm}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=1, fig.height=2}
coordinates(diverging) <- list( x=c(X = 1, Z = 2, Y = 3) ,
                                            y=c(X = 2, Z = 1, Y = 2) )
drawdag(diverging, 
        shapes =  list(X = "c", Z = "c", Y = "c"), radius = 3)
```
\subcaption{\textsf{Fork}}
\end{subfigure}}
\hspace{5mm}\scalebox{1}{\begin{subfigure}[!ht]{0.32\textwidth}
\vspace{1mm}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=1, fig.height=2}
coordinates(converging) <- list( x=c(X = 1, Z = 2, Y = 3) ,
                                            y=c(X = 1, Z = 2, Y = 1) )
drawdag(converging, 
        shapes =  list(X = "c", Z = "c", Y = "c"), radius = 3)
```
\subcaption{\textsf{Collider}}
\end{subfigure}}
\normalsize
\caption{Three basic types of connections.}
\label{fig:connectionsBN}
\end{figure}





\noindent Finally, two sets of nodes, $\mathsf{X}$ and $\mathsf{Y}$, are d-separated given $\mathsf{Z}$ if every node in $\mathsf{X}$ is d-separated from every node in $\mathsf{Y}$ given $\mathsf{Z}$. With serial connection, for instance, if:

\footnotesize 
\begin{center}
\begin{tabular}{@{}lp{4.3cm}@{}}\toprule
Node & Proposition \\ \midrule 
$G$ & The suspect is guilty. \\
$B$ & The blood stain comes from the suspect.\\
$M$ & The crime scene stain and the suspect's blood share their DNA profile.\\
\bottomrule
\end{tabular}
\end{center}
\normalsize
\noindent We naturally would like to have the connection $G \rightarrow B \rightarrow M$. If we don't know whether $B$ holds, $G$ has  an indirect impact on the probability of $M$. Yet, once we find out that $B$ is true, we expect the profile match, and whether $G$ holds has no further impact on the probability of $M$.


Take an example of a diverging connections.  Say you have two coins, one fair, one biased. Conditional on which coin you have chosen, the results of subsequent tosses are independent. But if you don't know which coin you have chosen, the result of previous tosses give you some information about which coin it is, and this has an impact on your estimate of the probability of heads in the next toss. Whether a coin is fair, $F$ or not has an impact on the result of the first toss, $H1$, and on the result of the second toss, $H2$.  So $H1 \leftarrow F \rightarrow H2$ seems to be appropriate. Now, on one hand, as long as we don't know whether $F$, $H1$ increases the probability of $H2$.  On the other, once we know that $F$, though, $H1$ and $H2$ become independent, and so conditioning on the parent in a fork makes its childern independent (provided there is no other open path between them in the graph).



For converging connections, let  $G$ and $B$ be as above, and let:
\begin{center}
\begin{tabular}{@{}lp{4.3cm}@{}}\toprule
Node & Proposition \\ \midrule 
$O$ & The crime scene stain comes from the offender.\\
\bottomrule
\end{tabular}
\end{center}
\normalsize

\noindent Both $G$ and $O$ influence $B$. If suspect guilty, it's more likely that the blood stain comes from him, and if the blood crime stain comes from the offender it is more likely to come from the suspect (for instance, more so than if it comes from the victim). Moreover, $G$ and $O$ seem independent -- whether the suspect is guilty doesn't have any bearing on whether the stain comes from the offender. Thus, a converging connection $G\rightarrow B \leftarrow O$ seem appropriate. However, if you do find out that $B$ is true, that the stain comes from the suspect, whether the crime stain comes from the offender becomes relevant for whether the suspect is guilty. 

One important reason why d-separation matters is that it can be proven that if two sets of nodes are d-separated given a third one, then they are independent given the third one, for any probabilistic measures compatible with a given DAG. Interestingly, lack of d-separation doesn't entail dependence for any probabilistic measure compatible with a given DAG. Rather, it only allows for it: if nodes are d-separated, there is at least one probabilistic measure fitting the DAG according to which they are independent.   So, at least, no false  independence can be inferred  from the DAG, and  all the dependencies are built into it.


## Independencies involved in the conjunction problem {-}



We will be considering the conjunction of two hypotheses, $A$ and $B$, their respective pieces of evidence $a$ and $b$, and their conjunction $AB$, in two set-ups, illustrated by the Bayesian networks
shown in Figure \ref{fig:conjunctionBNs} (later on we will also talk about a third set-up, in which aggregation fails). The key difference here is that we allow for a direct dependence between the hypotheses in the second network. In all the  Bayesian networks that will be discussed in this appendix, the CPT for the conjunction trivially mirrors the one for conjunction, as in Table  \ref{tab:CPTconjunction2}.


\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
conjunctionDAG <- model2network("[a|A][b|B][AB|A:B][A][B]")
daggityConjunctionDag <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> AB
        B -> AB
      }")


As <- runif(1,0,1)
Bs <- runif(1,0,1)

aifAs <-runif(1,0,1)
aifnAs <- runif(1,0,1)
bifBs <-runif(1,0,1)
bifnBs <- runif(1,0,1)


AProb <-prior.CPT("A","1","0",As)
BProb <- prior.CPT("B","1","0",Bs)
aProb <- single.CPT("a","A","1","0","1","0",aifAs,aifnAs)
bProb <- single.CPT("b","B","1","0","1","0",bifBs,bifnBs)


ABProb <- array(c(1, 0, 0, 1, 0, 1, 0,1), 
                dim = c(2, 2, 2),
                dimnames = list(AB = c("1","0"),
                                B = c("1","0"), 
                                A = c("1","0")))

conjunctionCPT <- list(A = AProb, B = BProb, 
                       a = aProb, b = bProb, AB = ABProb)


conjunctionBN <- custom.fit(conjunctionDAG,conjunctionCPT)

conjunctionDAG2 <- model2network("[a|A][b|B][AB|A:B][A][B|A]")

daggityConjunctionDag2 <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> B
        A -> AB
        B -> AB
      }")

```
\normalsize


\begin{figure}[H]
\hspace{2mm}\scalebox{1}{\begin{subfigure}[!ht]{0.45\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=2.5, fig.height=2}
#graphviz.plot(conjunctionDAG, layout = "dot")
coordinates(daggityConjunctionDag) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                            y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityConjunctionDag, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 3.5)
```
\subcaption{\textsf{DAG1}}
\end{subfigure}} 
\hspace{5mm}\begin{subfigure}[!ht]{0.45\textwidth}
\vspace{1mm}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300, fig.width=2.5, fig.height=2}
coordinates(daggityConjunctionDag2) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                            y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityConjunctionDag2, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 3.5)
```
\subcaption{\textsf{DAG2}}
\end{subfigure}
\normalsize
\caption{Two DAGs for the conjunction problem.}
\label{fig:conjunctionBNs}
\end{figure}





\begin{table}[h]
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
CPkable2("conjunctionBN","AB") %>%   
                          kable_styling(latex_options=c("striped","HOLD_position")) 
```
\normalsize
\caption{Conditional probability table for the conjunction node.}
\label{tab:CPTconjunction2}
\end{table}

\newpage 
\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=FALSE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
DAG1Ind <- impliedConditionalIndependencies(daggityConjunctionDag, type  = "all.pairs")

DAG2Ind <- impliedConditionalIndependencies(daggityConjunctionDag2, type  = "all.pairs")
```
\normalsize





The   d-separations entailed by these networks differ (examples can be found in Table \ref{tab:indepBNS})---in fact, \textsf{DAG1} entails 31 d-separations, while \textsf{DAG2} entails 22 of them.  Attention should be paid to the notation. Whe we talk about DAGs, variables represent nodes, and so each d-separation  entails a   probabilistic statement about  all combination of the  node states involved. For instance, assuming each node is binary with two possible states, 1 and 0, \mbox{$B   \indep_d\,\,  a $}  entails that for any \mbox{$ B_i, a_i \in \{0, 1\}$} we have $\pr{B = B_i} = \pr{B = B_i \vert a = a_i}$. 






\begin{table}[h]
\begin{tabular}{cr}
\toprule
Bayesian network 1  & Bayesian network 2\\
\midrule
\cellcolor{gray!6}{$A   \indep_d\,\, B  $}&\cellcolor{gray!6}{     $ A  \indep_d\,\, b \vert  B  $} \\
$A   \indep_d\,\, b  $& $AB  \indep_d\,\,  a \vert  A$\\
\cellcolor{gray!6}{$\,\,\, AB  \indep_d\,\, a \vert A $}  & \cellcolor{gray!6}{$AB  \indep_d\,\,  b \vert  B $}\\
$\,\,\, AB  \indep_d\,\, b \vert B  $ & $ B  \indep_d\,\,  a \vert  A $\\
\cellcolor{gray!6}{$B   \indep_d\,\, a $}        & \cellcolor{gray!6}{$a  \indep_d\,\,  b \vert  B$ }\\
$\,\, a    \indep_d\,\, b$    & $a  \indep_d\,\,  b \vert  A $ \\
\bottomrule
\end{tabular}
\caption{Some of d-separations entailed by \textsf{DAG1} and \textsf{DAG2} in the conjunction problem. One minimal testable implication (with the smallest possible conditioning set) is returned per missing edge of the graph.} 
\label{tab:indepBNS}
\end{table}

Crucially, Table \ref{tab:indepBNS} lists some independencies between the \emph{nodes}.
In what follows, however, we will  use a finer level of granularity, being very explicit on what independence assumptions \emph{between propositions} are used in the derivations. Each particular instantiation of a node, in our setup, corresponds to a new proposition (e.g. $A = 1$ means that the proposition corresponding to $A$ is claimed to be true, while $A = 0$ means that it's the negation of $A$ that is claimed to hold). In our discussion we will often be talking about states rather than nodes, and so when we present a derivation, \mbox{$b\indep A \et a \vert \n B$} is a claim about events (or propositions), which  means  the same as  $\pr{b = 1 \vert B = 0}   = \pr{b = 1 \vert A = 1, a = 1, B = 0}$. This distinction matters, as, first,  independence conditional on $B= 0$ doesn't entail independence given $B=1$ (for instance, your final grade might depend on how hard you work if the teacher is fair, but this might fail if the teacher is not fair), and, second, sometimes only some of the independencies entailed by a Bayesian network will be actually required for a given claim to hold, and we want to be explicit about such cases. We hope this slight ambiguity in notation will cause no confusion, as whether we talk about nodes or events will be clear from the context.  


So, moving from nodes to events, here is a list of independencies between propositions  used in the arguments that follow.  We also marked whether they are entailed by a either of the two DAGs.
  

\begin{figure}
\begin{subfigure}[!ht]{0.45\textwidth}
\begin{align} A\indep B  \label{eq:indAB}     &\hspace{2cm}\mbox{\footnotesize DAG1}\\
b \indep a   \label{eq:indab}   & \hspace{2cm}\mbox{\footnotesize DAG1}\\
A \indep b \vert a   \label{eq:I1}    &\hspace{2cm}\mbox{\footnotesize DAG1} \\
B \indep a \et A \vert b \label{eq:I2}&\hspace{2cm}\mbox{\footnotesize DAG1 } \\
a\indep b \vert A\et B \label{eq:I3}  &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\  
a\indep b \vert A \label{eq:I3a}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\ 
a\indep b \vert \n A \label{eq:I3b}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\ 
a\indep b \vert B \label{eq:I3a2}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep b \vert \n B \label{eq:I3b2}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2}
\end{align}
\end{subfigure}
\begin{subfigure}[!ht]{0.5\textwidth}
\begin{align}
a\indep B \vert A \label{eq:I4}    & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep B \vert \n A \label{eq:I4a}    & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep \n B \vert A \label{eq:I4b}   & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep \n B \vert \n A \label{eq:I4c}   & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b\indep A \et a \vert B \label{eq:I5}  & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b\indep \n A \et a \vert B \label{eq:I5a} &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2}  \\
b\indep A \et a \vert \n B \label{eq:I5b} & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b\indep \n A \et a \vert \n B \label{eq:I5c} & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b \indep a \vert B \label{eq:I6} & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} 
\end{align}
\end{subfigure}
\end{figure}

Some caveats. In \eqref{eq:I1} the conditioning on $a$ is not essential, because it's not on the path between the nodes: the key reason why the independence remains upon this conditioning is that there is an unconditioned collider on the path. Still, we need this independence in the proof later on. In \eqref{eq:I3} what we are conditioning on is  $A$ and $B$ jointly. Technically, independence conditional on the conjunction node $AB$ does not fall out of the d-separations present in the network---it follows given that $AB$ and $A,B$ are connected deterministically: fixing $AB$ to true fixes both $A$ and $B$ to true.













## Bayes factor: claims and proofs {-}

We will start with the Bayes factor $\pr{E \vert H}/\pr{E}$ and its relation to conjunction. Let's abbreviate:
\begin{align*}
BF_A  & =  \frac{\pr{a \vert A}}{\pr{a}},\\
BF_B & = \frac{\pr{b \vert B}}{\pr{b}}
\end{align*}


<!-- \raf{M: ybility, the formulation you use later in FACT 3 is better i.e. "For any probabilistic measure P appropriate for (or compatible with) DAG 1", instead of the numbered assumptions that are hard to keep track. This comment applies to FACT 1 here and FACT 2 below, and all the corollaries.} -->

<!-- \mar{R: I sort of can see your point, but this would weaken the content of the theorems. I see your point about readability, but the proper identification of the exact independencies that are needed and NOTHING more has value. Not all independencies entailed by the DAGs are needed. } -->







\begin{fact} If the independence assumptions \eqref{eq:indAB}, \eqref{eq:indab}, \eqref{eq:I4} and \eqref{eq:I5} hold (all of which are entailed by \textsf{DAG1}), then 
$BF_{AB} = BF_A \times BF_B$. \label{fac:BFindep}
\end{fact}



\begin{proof}

\begin{align*}
\frac{\pr{a \wedge b\vert A\wedge B}}{\pr{a \wedge b}} & = \frac{\pr{A \et B\et a\wedge b}}{\pr{A \et B}} \bigg/ \pr{a \wedge b}
&\mbox{(conditional probability)} \\
&  = \frac{\pr{A} \times \pr {B \vert A}  \times \pr{a \vert A \et B} \times \pr{b \vert A \et B \et a}}{\pr{A \et B}} \bigg/ \pr{a \wedge b}
&\mbox{\,\,\,\,\,\,\, (chain rule)} \\
\end{align*}

\noindent Now, let's deploy the respective independence assumptions, as follows:

\begin{align*}
&  = \frac{\pr{A} \times \overbrace{\pr {B \vert A}}^{ \pr{B} \mbox{\footnotesize \, by \eqref{eq:indAB}}}  \times
\overbrace{\pr{a \vert A \et B}}^{\pr{a \vert A} \mbox{\footnotesize \, by \eqref{eq:I4}}}
\times \overbrace{\pr{b \vert A \et B \et a}}^{\pr{b \vert B} \mbox{\footnotesize \, by \eqref{eq:I5}}}
}{\underbrace{\pr{A \et B}}_{\pr{A} \times \pr{B} \mbox{\footnotesize \, by \eqref{eq:indAB}}}} \bigg/ \underbrace{\pr{a \wedge b}}_{\pr{a}\times \pr{b} \mbox{ \footnotesize \, by \eqref{eq:indab}}} \\
&  = \frac{\pr{A} \times  \pr{B}   \times \pr{a \vert A}  \times  \pr{b \vert B}}
{\pr{A} \times \pr{B}} \bigg/ \pr{a}\times \pr{b} \\
& = \frac{\pr{a \vert A}  }{\pr{a}}  \times \frac{\pr{b \vert B}}{\pr{b}} \\
& = BF_{A} \times BF_B
\end{align*}

\end{proof}



This fact has the following straigthforward consequences which simply follow from the simple fact that if $a = b \times c$ and $b, c>1$, then $a > \mathsf{max}(b,c)$ and that if $b, c<1$, then $a < \mathsf{min}(b,c)$.

\begin{corollary} If the independence assumptions \eqref{eq:indAB}, \eqref{eq:indab}, \eqref{eq:I4} and \eqref{eq:I5} hold, and $BF_{A}$ and $BF_{B}$ are both greater than 1, then $BF_{AB}$ is greater than one. In fact,  $BF_{AB}$ is then greater than  $\mathsf{max}(BF_{A},BF_{B})$. \label{cor:BFind2}
\end{corollary}



\begin{corollary} If the independence assumptions \eqref{eq:indAB}, \eqref{eq:indab}, \eqref{eq:I4} and \eqref{eq:I5} hold, and $BF_{A}$ and $BF_{B}$ are both strictly less than 1, then $BF_{AB}$  is less than  $\mathsf{min}(BF_{A}, BF_{B})$. \label{cor:BFind3}
\end{corollary}


Now, does a similar claim if we drop the independence assumption specific to \textsf{DAG1}? For one thing, if we shift to \textsf{DAG 2}, the joint \textsf{BF} can no longer be obtained by multiplying the individual ones, although multiplication often provides a decent approximation (Figure \ref{fig:BFmulti}).  


\begin{figure}[H]
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
conjunctionTable2dep <- readRDS(file = "../datasets/conjunctionTable2Braf.RDS")
ggplot(conjunctionTable2dep, aes(y = BFABs, x = (BFAs * BFBs)))+geom_point(alpha= 0.2, size = .5)+
  xlab(expression(paste(BF[A], "*", BF[B] )))+
  ylab(expression(paste(BF[AB])))+
  ggtitle("Multiplicative claim fails for DAG2")+labs(subtitle = "Pearson's correlation coefficient = .95")+theme_tufte()+xlim(c(0,60))+ylim(c(0,60))
```
\caption{In DAG2, the result of multiplying individual BFs does not equal the joint BF, but often is a good approximation thereof. Axes restricted to $0,60$ (one extreme outlier lying close to the diagonal dropped).}
\label{fig:BFmulti}
\end{figure}




However, as long as the probabilistic measure fits \textsf{DAG 2}, aggregation for \textsf{BF}s is still satisfied.  First, let's abbreviate:
\begin{align*}
BF^{'}_{B} & = \frac{\pr{b \vert B}}{\pr{b\vert a}} \\
BF^{'}_{A} & = \frac{\pr{a \vert A}}{\pr{a \vert b}}
\end{align*}
\noindent Now, a claim somewhat weaker than Fact \ref{fac:BFindep} can be proven without employing independencies entailed by \textsf{DAG1}, relying only on some independencies entailed by  \textsf{DAG2}. 
\begin{fact} If \eqref{eq:I4} and \eqref{eq:I5}  hold (and they do in BNs based on \textsf{DAG 2}), then $BF_{AB} =  BF_{A}\times BF^{'}_{B}  = BF_{B} \times BF^{'}_{A}$. \label{fac:BFdep}
\end{fact}


\begin{proof}

We start with the definition of conditional probability and the chain rule, as in the proof of Fact \ref{fac:BFindep}, but now we use fewer  independencies (all of them entailed by \textsf{DAG2}), as we use the chain rule instead in a few cases. 

 \begin{align*}
\frac{\pr{a \wedge b\vert A\wedge B}}{\pr{a \wedge b}} &
= \frac{\pr{A} \times \pr {B \vert A}  \times
\overbrace{\pr{a \vert A \et B}}^{\pr{a \vert A} \mbox{\footnotesize \, by \eqref{eq:I4}}}
\times \overbrace{\pr{b \vert A \et B \et a}}^{\pr{b \vert B} \mbox{\footnotesize \, by \eqref{eq:I5}}}
}{\underbrace{\pr{A \et B}}_{\pr{A} \times \pr{B\vert A} \mbox{\footnotesize \, by the chain rule}}} \bigg/ \underbrace{\pr{a \wedge b}}_{\pr{a}\times \pr{b\vert a} \mbox{ \footnotesize \, by the chain rule }}\\
& = \frac{\pr{a \vert A}}{\pr{a}} \times \frac{\pr{b\vert B}}{\pr{b \vert a}}\\
& = BF_{A} \times BF^{'}_{B}
\end{align*}
If instead of obtaining $\pr{a}\pr{b \vert a}$ in the denominator we deploy the chain rule differently, resulting in $\pr{b}\pr{a \vert b}$, we end up with:
\begin{align*}
& = \frac{\pr{a \vert A}}{\pr{a \vert b}} \times \frac{\pr{b\vert B}}{\pr{b}}\\
& = BF^{'}_{A} \times BF_{B}
\end{align*}

\end{proof}


<!-- Now, to obtain a corollary analogous to Corollary \ref{cor:BFind2}, there are at lest two paths. One starts with an   additional disjunctive assumption that either $\pr{b\vert a} \leq \pr{b}$ or $\pr{a \vert b} \leq \pr{a}$.     -->

<!-- \raf{M: This reasoning here is muddled and does not lead very far. You later show with the simulation that these assumptiosn are false. Unless there is something interesting to learn from the fact they are false, I would remove this corollary.} -->

<!-- \begin{corollary} Suppose \eqref{eq:I4} and \eqref{eq:I5}  (they are entailed by \textsf{DAG 2}) hold and $BF_{BA}, BF_{B} >1$. -->
<!-- Then if either $\pr{b\vert a} \leq \pr{b}$ or \linebreak  $\pr{a \vert b} \leq \pr{a}$, we have $BF_{AB}\geq BF_{A}, BF_{B}$. \label{cor:BFweaker} -->
<!-- \end{corollary} -->



<!-- \begin{proof} -->
<!-- Notice that by Fact \ref{fac:BFdep} we have that on the assumptions of the corollary: -->
<!-- \begin{align*} -->
<!-- BF_{AB} & = BF_{A}\times BF^{'}_{B} \\ -->
<!-- & = BF_{B} \times BF^{'}_{A} -->
<!-- \end{align*} -->
<!-- If we now can show that either $BF^{'}_{B} \geq 1$, or $BF^{'}_{A} \geq 1$, the reasoning we used before applies: the product of two numbers greater than one is not less than either of them.  Consider $BF_{B}$, which we assumed to be greater than 1. If we can show $BF^{'}_{B}\geq BF_{B}$, we're done. But this holds on one of the disjuncts assumed in the corollary, $\pr{b\vert a} \leq \pr{b}$, as then we have: -->
<!-- \begin{align*} -->
<!-- \frac{\pr{b \vert B}}{\pr{b\vert a}} &\geq \frac{\pr{b \vert B}}{\pr{b}} \\ -->
<!-- BF^{'}_{B}  & \geq BF_{B} \\ -->
<!-- BF_{AB} & = BF_{A}\times BF^{'}_{B}   \geq BF_{A} \times BF_{B}\\ -->
<!-- \end{align*} -->
<!-- Similarly, if the other disjunct holds, we run an analogous argument, this time focusing on $BF^{'}_{A}$. -->
<!-- \end{proof} -->

<!-- \mar{R: added this paragraph and another collorary, check} -->
<!-- \raf{M: I would remove the paragraph above (se comments above), so I would remove this paragraph, as well.} -->
<!-- However, the problem with this assumption is that if it holds, this means that one item of evidence makes the other item of evidence at least as surprising as it originally was. This, at least intuitively, might not happen in a legal context. If $A$ and $B$ are elements of a crime, say that the driver was drunk and that they caused harm by erratic driving, evidence for one hypothesis, say the blood alcohol level test, in fact makes the evidence for the other hypothesis---witnesses attesting to the erratic driving, the presence of the harm, and so on,  more likely. -->

<!-- Another approach uses a conjunction of assumptions. -->

<!-- \raf{M: I think the key claim is FACT 3 which you prove nicely later. Fact 3,  combined with this corollary and Fact 2, gives the result with want. I wonder if the argument can be consolidated into just one FACT more succintly.} -->

<!-- \mar{R: I wouldn't do that. This way we have a claim about the relationship between BF and LR, and this formulation allows us to notice and focus on cases in which aggregation fails. I think it's nice to give this in these installments.} -->

And here is how we get to aggregation in this setting.

\begin{corollary} Suppose \eqref{eq:I4} and \eqref{eq:I5}  hold (they are entailed by \textsf{DAG 2}), and $BF_{BA}, BF_{B} >1$. 
Then if both $\pr{a\vert b} \leq \pr{a \vert A}$ and \linebreak  $\pr{b \vert a} \leq \pr{b\vert B}$, we have $BF_{AB}\geq BF_{A}, BF_{B}$. \label{cor:BFweaker2}
\end{corollary}


\begin{proof}
Assume the first conjunct holds. Then $\frac{\pr{a\vert A}}{\pr{a\vert b}} \geq 1$ and so:
\begin{align*}
BF_{AB} &= BF^{'}_{A} \times BF_{B} \geq BF_{B}
\end{align*}
\noindent The argument for the other comparison is analogous.
\end{proof}


Notice the claim relies on some additional assumptions, which at least seem plausible in usual legal contexts. If, say, $a$ is used as  evidence for $A$, we often expect $A$ and $a$ to be fairly strongly connected, that is, we expect $\pr{a\vert A}$ to be rather high, while the connection between different pieces of evidence for different hypotheses, intuitively, is not  expected to be as strong. 










## Bayes factor: simulations {-}

First, let's look at the distributions of the distances of the joint posterior from the minimum of the individual posteriors (Figure \ref{fig:posteriorFailure}). 



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning =- FALSE, message = FALSE}
conjunctionTable2 <- readRDS(file = "../datasets/conjunctionTable2.RDS")

conjunctionTable2$minIndividual <- pmin(conjunctionTable2$Aifas, conjunctionTable2$Bifbs)
conjunctionTable2$diffIndividualMin <- conjunctionTable2$ABifabs - conjunctionTable2$minIndividual 

positive <- conjunctionTable2[conjunctionTable2$ABifabs > conjunctionTable2$ABs, ]

plotABindBelow2a <-  ggplot(positive, aes(x = diffIndividualMin))+geom_histogram(aes( y = ..density..), bins = 60)+
xlab("P(AB|ab) -  min(P(A|a), P(B|b))")+ggtitle("DAG 1, failure rate ca. 68%")+
labs(subtitle = expression(paste("Assuming  P(AB|ab) > P(AB)")))+xlim(c(-.3,1))+theme_tufte()

conjunctionTable2dep <- readRDS(file = "../datasets/conjunctionTableDepAli2.RDS")

conjunctionTable2dep$minIndividual <- pmin(conjunctionTable2dep$Aifas, conjunctionTable2dep$Bifbs)
conjunctionTable2dep$diffIndividualMin <- conjunctionTable2dep$ABifabs - conjunctionTable2dep$minIndividual 

positiveDEP <-  conjunctionTable2dep[ conjunctionTable2dep$ABifabs > conjunctionTable2dep$ABs,]

plotABindBelow2DEP <-  ggplot(positiveDEP, aes(x = diffIndividualMin))+geom_histogram(aes( y = ..density..), bins = 60)+
  xlab("P(AB|ab) -  min(P(A|a), P(B|b))")+ggtitle("DAG 2, Failure rate ca. 60%")+
  labs(subtitle = expression(paste("Assuming  P(AB|ab) > P(AB)")))+xlim(c(-.7,1))+theme_tufte()

plotABfaila <- ggarrange(plotABindBelow2a, plotABindBelow2DEP, ncol = 2)

```


\begin{figure}[H]

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", message = FALSE, warning = FALSE}
annotate_figure(plotABfaila, top = text_grob("Distances of the joint posterior from the lower individual posterior",size = 13))
```
\caption{Even assuming the joint support is positive, the joint posterior quite often is lower than individual posteriors.}
\label{fig:posteriorFailure}
\end{figure}



Now, let's look at simulations based on \textsf{DAG1} (Figure \ref{fig:DAG1BF}). We  can illustrate the distribution of Bayes factors in the two separate scenarios used as assumptions of the above corollaries. 


\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
old <- theme_set(theme_tufte())
conjunctionTable <-  readRDS(file = "../datasets/conjunctionTable.RDS")
conjunctionTable$maxBF <- pmax(conjunctionTable$BFAs, conjunctionTable$BFBs)
conjunctionTable$minBF <- pmin(conjunctionTable$BFAs, conjunctionTable$BFBs)
conjunctionTable$BFdifsMax <- conjunctionTable$BFABs - conjunctionTable$maxBF
conjunctionTable$BFdifsMin <- conjunctionTable$BFABs - conjunctionTable$minBF

conjunctionTable$maxLR <- pmax(conjunctionTable$LRAs, conjunctionTable$LRBs)
conjunctionTable$minLR <- pmin(conjunctionTable$LRAs, conjunctionTable$LRBs)
conjunctionTable$LRdifsMax <- conjunctionTable$LRABs - conjunctionTable$maxLR 
conjunctionTable$LRdifsMin <- conjunctionTable$LRABs - conjunctionTable$minLR 

plotBFindBelow <- conjunctionTable %>% filter(BFAs < 1 & BFBs < 1) %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("Compared to the the minimum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] < 1)))+xlim(c(-.3,.05))
plotBFindAbove <- conjunctionTable %>% filter(BFAs > 1 & BFBs > 1) %>% ggplot( aes(x = BFdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - max,"(",BF[A], ", ", BF[B],")")))+ggtitle("Compared to  the maximum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(0,3))

plotBFind <- ggarrange(plotBFindBelow, plotBFindAbove, ncol = 2)
```
\normalsize



\begin{figure}
```{r BFind,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
annotate_figure(plotBFind, top = text_grob("Distances of the joint BF from the individual BFs (DAG1)", 
                size = 13)) 
```
\caption{Distances of the joint Bayes factor from maxima and minima of individual Bayes factors, depending on whether the individual support levels are both positive or both negative. Simulation based on 100k Bayesian networks build over the DAG of \textsf{DAG1}.}
\label{fig:DAG1BF}
\end{figure}


For the DAG corresponding to \textsf{DAG1}, the simulated frequency of cases in which $BF_{AB} < BF_{A}, BF_{B}$ is 25\% (which is twice higher than for the likelihood ratio), and the structure of such cases is visualized in Figure \ref{fig:BFfails}.   The picture doesn't change when we move to \textsf{DAG2} (Figure \ref{fig:BFind2}). 

\begin{figure}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
BFfails <- conjunctionTable %>% filter(BFAs > BFABs & BFBs > BFABs ) 
scatter3D(BFfails$BFAs,BFfails$BFBs,BFfails$BFABs,pch=3,cex=0.1, alpha = .5, colvar = NULL, box = TRUE, grid = TRUE,theta=50, phi = 10, axis.ticks = TRUE, ticktype= "detailed",xlab= "BF(A)", ylab="BF(B)",zlab="BF(AB)",main="Cases in which BF(AB) < BF(A), BF(B) (frequency=.25)", cex.lab = .6, cex.axis = .4, cex.main = .8)
```
\caption{Ca. 25k cases (out of simulated 100k) in which the joint BF is below each of the individual BFs.}
\label{fig:BFfails}
\end{figure}



\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
old <- theme_set(theme_tufte())
conjunctionTable2 <-  readRDS(file = "../datasets/conjunctionTable2raf.RDS")
conjunctionTable2$maxBF <- pmax(conjunctionTable2$BFAs, conjunctionTable2$BFBs)
conjunctionTable2$minBF <- pmin(conjunctionTable2$BFAs, conjunctionTable2$BFBs)
conjunctionTable2$BFdifsMax <- conjunctionTable2$BFABs - conjunctionTable2$maxBF
conjunctionTable2$BFdifsMin <- conjunctionTable2$BFABs - conjunctionTable2$minBF

conjunctionTable2$maxLR <- pmax(conjunctionTable2$LRAs, conjunctionTable2$LRBs)
conjunctionTable2$minLR <- pmin(conjunctionTable2$LRAs, conjunctionTable2$LRBs)
conjunctionTable2$LRdifsMax <- conjunctionTable2$LRABs - conjunctionTable2$maxLR 
conjunctionTable2$LRdifsMin <- conjunctionTable2$LRABs - conjunctionTable2$minLR 


plotBFindBelow2 <- conjunctionTable2 %>% filter(BFAs < 1 & BFBs < 1) %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("Compared to the the minimum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] < 1)))+xlim(c(-.3,.05))
plotBFindAbove2 <- conjunctionTable2 %>% filter(BFAs > 1 & BFBs > 1) %>% ggplot( aes(x = BFdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - max,"(",BF[A], ", ", BF[B],")")))+ggtitle("Compared to  the maximum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(0,3))

plotBFind2 <- ggarrange(plotBFindBelow2, plotBFindAbove2, ncol = 2)
```
\normalsize




\begin{figure}
```{r BFind2,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
annotate_figure(plotBFind2, top = text_grob("Distances of the joint BF from the individual BFs (DAG2)", 
                 size = 13)) 
```

\caption{Distances of the joint Bayes factor from maxima and minima of individual Bayes factors, depending on whether the individual support levels are both positive or both negative. Simulation based on 100k Bayesian networks build over the DAG of \textsf{DAG2}.}
\label{fig:BFind2}
\end{figure}



While Corollary \ref{cor:BFweaker2} required additional assumptions to prove that the joint \textsf{BF} will be at least as high as the individual \textsf{BFs}, this assumption is not \emph{explicitly} build into the simulation, and nevertheless the consequent comes out satisfied. In fact, even more can be said. In the simulation $\pr{b \vert a}> \pr{b}$ around 50\% of the time, and the same holds for $\pr{b \vert a}> \pr{b}$, so the assumptions of Corollary \ref{cor:BFweaker} in general fail. However,  it falls out of the \textsf{DAG 2} setup and the assumption that the individual Bayes factor is greater than 1 (this assumption is essential, the claim fails without it) that  $\pr{a \vert b} \leq \pr{a \vert A}$ and $\pr{b \vert a} \leq \pr{b \vert B}$, and so, still both $BF^{'}_{A}$ and $BF^{'}_{B}$ are above one if the individual Bayes factors are above one, even though the former might be lower the latter (respectively).  We will turn to proving this very soon.
 
 
















How about distribution? Note that in principle a failure of distribution can occur whenever the joint \textsf{BF} is strictly greater than at least one of the individual $\textsf{BFs}$. The distribution of such cases for both DAGs can be inspected in Figure  \ref{fig:BFdistr}.

\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
plotBFDistr <- conjunctionTable %>% filter(BFAs > 1 & BFBs > 1) %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 60)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("DAG 1")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(-3,15))

plotBFDistr2 <- conjunctionTable2 %>% filter(BFAs > 1 & BFBs > 1) %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 60)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("DAG 2")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(-.3,15))

plotBFDistrJoint <- ggarrange(plotBFDistr, plotBFDistr2, ncol = 2)
```
\normalsize

\begin{figure}
```{r plotBFDistr,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE, results = FALSE}
annotate_figure(plotBFDistrJoint, top = text_grob("Distribution failures for BF", 
                 size = 13)) 
```
\caption{Distribution failure for the Bayes factor, \textsf{DAG 1}. The $x$ axis restricted to $(-3,15)$ for visibility.}
\label{fig:BFdistr}
\end{figure}



##  Assumptions of Corollary \ref{cor:BFweaker2} and \textsf{DAG 2} {-}

<!-- \ali{R: this subsection is new, check statements and proofs in detail.} -->



Let us start with the following lemma.

\begin{lemma} For any probabilistic measure $\mathsf{P}$, if $BF_A >1$, then $LR_A>1$.\label{lem:BFLR}
\end{lemma}


\begin{proof} We start with our assumption.


\begin{align*}
1 & \leq \frac{\pr{a \vert A}}{\pr{a}} & &  \,\, (BF_A \geq 1) \\
\pr{A} & \leq  \frac{\pr{a \vert A}}{\pr{a}} \pr{A} & & (\mbox{algebraic manipulation}) \\
\pr{A} & \leq \pr{A \vert a} & &  (\mbox{Bayes' theorem})  \\
- \pr{A} &\geq - \pr{A \vert a} &  &   (\mbox{algebraic manipulation}) \\
1- \pr{A} & \geq 1 - \pr{A \vert a} & & (\mbox{algebraic manipulation})\\
1- \pr{A}  & \geq \pr{\n A \vert a} & & (\mbox{algebraic manipulation})\\
\pr{a}\left( 1 - \pr{A}\right) & \geq \pr{a}\pr{\n A \vert a}  & & (\mbox{algebraic manipulation})\\
\pr{a} & \geq \frac{\pr{a} \pr{\n A \vert a}}{\pr{\n A}} &  & (\mbox{algebraic manipulation, negation}) \\
\pr{a} & \geq \pr{a \vert \n A}  & &   (\mbox{conditional probability}) \\
\end{align*}
From this and our assumption  that $\pr{a \vert A} \geq \pr{a}$ it follows that $\pr{a \vert A}\geq \pr{a \vert \n A}$, that is, that \mbox{$LR_A \geq 1$}.
\end{proof}



Now the main claim. 

\begin{fact}
For any probabilistic measure $\mathsf{P}$ appropriate for \textsf{DAG 2}, if $BF_A >1$, then $\pr{a \vert A} \geq \pr{a \vert b}$ and 
$\pr{b \vert B} \geq \pr{b \vert a}$.
\end{fact}


\begin{proof}

Let's focus on the first conjunct. First, we have:
\begin{align*}
\pr{a \vert b} & = \pr{a \et A \vert b} + \pr{a \et \n A \vert b} & &   (\mbox{total probability}) \\
& = \underbrace{\pr{a \vert b \et A}}_{\pr{a \vert A}  \mbox{\footnotesize \, by \eqref{eq:I3a}}} \pr{A \vert b} +
\underbrace{\pr{a \vert b \et \n A}}_{\pr{a \vert \n A} \mbox{\footnotesize \, by \eqref{eq:I3b}}}\pr{\n A \vert b}  & &   (\mbox{chain rule}) \\
\end{align*}

Now let's introduce some abbreviations:
\begin{align*}
& = \underbrace{\pr{a\vert A}}_k \underbrace{\pr{A \vert b}}_x + \underbrace{\pr{a \vert \n A}}_t \underbrace{\pr{\n A \vert b}}_{(1- x)}
\end{align*}

\noindent Note that the assumption that $BF_A\geq 1$ entails, by Lemma \ref{lem:BFLR}, that $k \geq t$, and so $k-t \geq 0$. Also, since $x$ is a probability, we know $0 \leq x \leq 1$. This allows us to reason algebraically as follows:

\begin{align*}
k & \geq k  \\
k & \geq t + (k - t) \\
k & \geq t + (k -t)x \\
k & \geq kx + t  - tx \\
\pr{a \vert A} = k & \geq kx + t(1-x) = \pr{a \vert b}
\end{align*}

For the second conjunct, notice that we have a similar reasoning, albeit it relies on a different pair of independencies (which nevertheless holds in \textsf{DAG1} and \textsf{DAG 2}).
\begin{align*}
\pr{b \vert a} & = \pr{b \et B \vert a} + \pr{b \et \n B \vert a} & &   (\mbox{total probability}) \\
& = \underbrace{\pr{b \vert a \et B}}_{\pr{b \vert B}  \mbox{\footnotesize \, by \eqref{eq:I3a2}}} \pr{B \vert a} +
\underbrace{\pr{b \vert a \et \n B}}_{\pr{b \vert \n B} \mbox{\footnotesize \, by \eqref{eq:I3b2}}}\pr{\n B \vert a}  & &   (\mbox{chain rule}) \\
\end{align*}

\noindent The rest of the reasoning for this case is algebraically the same as the one used above. This completes the proof. 
\end{proof}

<!-- \raf{M: For readability, it might help to plot the visual relationship between P(a|b) and P(a|A), just as a further clarification about the algebraic claim.} -->
<!-- \mar{R: I am not convinced; P(a|b) is a function of P(a|A) and two other variables, and visualisation of functions of three arguments aren't very transparent. I could try out various simplifying assumptions or particular cases, but the point of the general claim is not to use them, no?} -->



<!-- : it falls out of the \textsf{DAG 2} setup and the assumption that the individual Bayes factor is greater than 1 (this assumption is essential, the claim fails without it) that  $\pr{a \vert b} \leq \pr{a \vert A}$ and $\pr{b \vert a} \leq \pr{b \vert B}$, and so, still both $BF^{'}_{A}$ and $BF^{'}_{B}$ -->













## Likelihood ratio: claims and proofs {-}












Now, let's turn to the likelihood understood as:

\begin{align*}
\frac{\pr{E \vert H}}{\pr{E \vert \neg H}} & =
\frac{\textit{sensitivity}}{\textit{1- specificity}}\end{align*}

\noindent Let's introduce the following abbreviations:
\begin{align*}
LR_{AB} &= \frac{\pr{a\wedge b \vert a\wedge B}}{\pr{a \wedge b \vert \neg (A\wedge B)}}\\
LR_A & = \frac{\pr{a \vert A}}{\pr{a \vert \n A}} \\
LR_B & = \frac{\pr{b \vert B}}{\pr{b \vert \n B}}.
\end{align*}


\begin{fact} If independence conditions  \eqref{eq:I4}, \eqref{eq:I4a}, \eqref{eq:I4b},   \eqref{eq:I4c},  \eqref{eq:I5},   \eqref{eq:I5a},    \eqref{eq:I5b}, and   \eqref{eq:I5c}    hold, then:
\begin{align*}
LR_{AB} & =  \frac{\pr{a \vert A} \times \pr{b \vert B}}
 {\frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} }}
\end{align*}
\end{fact}

\noindent Note that these independence assumptions are entailed not only in \textsf{DAG1}, but also in \textsf{DAG2}.

\begin{proof}
Let's first compute the numerator of $LR_{AB}$:

\begin{align*}
\pr{a \wedge b\vert A\wedge B} & =  \frac{\pr{A \et B \et a\et b}}{\pr{A \et B}}
&\mbox{(conditional probability)}
\\
&= \frac{   \pr{A} \times \pr{B\vert A} \times \pr{a \vert A \wedge B} \times \pr{b \vert A \wedge B \wedge a} }{\pr{A} \times \pr{B \vert A}}
&\mbox{(chain rule)}
\end{align*}

We deploy the relevant independencies as follows:
\begin{align*}
\mbox{      } &= \frac{   \pr{A} \times \pr{B\vert A} \times \overbrace{\pr{a \vert A \wedge B}}^{\pr{a \vert A} \mbox{ \footnotesize \, by \eqref{eq:I4} } } \times \overbrace{\pr{b \vert A \wedge B \wedge a}}^{\pr{b \vert B} \mbox{ \footnotesize \, by \eqref{eq:I5} }} }{\pr{A} \times \pr{B \vert A}}
&\mbox{}\\
 & = \pr{a \vert A} \times \pr{b \vert B} 
 &\mbox{(algebraic manipulation)} 
\end{align*}





\noindent The denominator of $LR_{AB}$ is more complicated, mostly because of the conditioning on  $\neg (A \wedge B)$.

\scalebox{.85}{\parbox{1\linewidth}{
\begin{align*}
\pr{a \et b\vert \neg (A\et B)} & = \frac{\pr{a \et b \et \neg (A\et B)}}{\pr{\neg (A \et B)}} 
&\mbox{ (conditional probability)}\\
& = \frac{\pr{a \et b \et \neg A\et B} +  \pr{a \et b \et A\et \neg B} + \pr{a \et b \et \neg A\et \neg B}  }{\pr{\neg A \et B} + \pr{A \et \neg B} + \pr{\neg A \et \neg B} } 
&\mbox{ (logic \& additivity)}
\end{align*}
}}



\noindent Now consider the first summand from the numerator:
\begin{align*}
\pr{a \et b \et \neg A\et B} & = \pr{\n A} \pr{B \vert \n A} \pr{a \vert \n A \et B} \pr{b\vert a \et \n A \et B} &\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (chain rule)} \\ & = 
\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B}
&\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (independencies \eqref{eq:I4a} and \eqref{eq:I5a})} \\
\end{align*}

The simplification of the other two summanda is analogous (albeit with slightly different independence assumptions---\eqref{eq:I4b} and \eqref{eq:I5b} for the second one and \eqref{eq:I4c} and \eqref{eq:I5c} for the third. Once we plug these into the denominator formula we get:

\scalebox{.8}{\parbox{1\linewidth}{
\begin{align*}
\pr{a \et b\vert \neg (A\et B)} & = \frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} } \\
 & = \frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} }  
 \end{align*}}}

\end{proof}



## Likelihod ratio: simulations {-}


While the analytic approach turns out to be cumbersome, let's inspect the problem using simulations. First of all, there are cases in which the joint likelihood ratio are lower than each of the individual likelihood ratios. Their frequency is twice lower than the corresponding frequency for the Bayes factor (recall Figure \ref{fig:BFfails}). For the DAG corresponding to both \textsf{DAG}s, the simulated frequency of cases in which $LR_{AB} < LR_{A}, LR_{B}$ is 12-12.5\% and the distribution of such cases, somewhat of a different shape, is visualized in Figure \ref{fig:LRfails} (the picture for \textsf{DAG2} is very similar).

\begin{figure}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
LRfails <- conjunctionTable %>% filter(LRAs > LRABs & LRBs > LRABs ) 
scatter3D(LRfails$LRAs,LRfails$LRBs,LRfails$LRABs,pch=3,cex=0.1, alpha = .5, colvar = NULL, box = TRUE, grid = TRUE,theta=50, phi = 10, axis.ticks = TRUE, ticktype= "detailed",xlab= "LR(A)", ylab="LR(B)",zlab="LR(AB)",main="Cases in which LR(AB) < LR(A), LR(B) (frequency=.125 (DAG1))", cex.lab = .6, cex.axis = .4, cex.main = .8)
# 
# LRfails2 <- conjunctionTable2 %>% filter(LRAs > LRABs & LRBs > LRABs ) 
# scatter3D(LRfails2$LRAs,LRfails2$LRBs,LRfails2$LRABs,pch=3,cex=0.1, alpha = .5, colvar = NULL, box = TRUE, grid = TRUE,theta=50, phi = 10, axis.ticks = TRUE, ticktype= "detailed",xlab= "LR(A)", ylab="LR(B)",zlab="LR(AB)",main="Cases in which LR(AB) < LR(A), LR(B) (frequency=.12) (DAG2)", cex.lab = .6, cex.axis = .4, cex.main = .8)
```
\caption{Ca. 25k cases (out of simulated 100k) in which the joint BF is below each of the individual BFs.}
\label{fig:LRfails}
\end{figure}


Interestingly, even if the individual likelihood ratios are $<1$, the joint likelihood ratio can be higher than their minimum, but is never higher than their maximum (Figures \ref{fig:LRlowerPlot} and \ref{fig:LRlowerPlot2}). 

\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
plotLRindBelow <- conjunctionTable %>% filter(LRAs < 1 & LRBs < 1) %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to the the minimum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] < 1)))

plotLRindBelow2 <- conjunctionTable %>% filter(LRAs < 1 & LRBs < 1) %>% ggplot( aes(x = LRdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - max,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to the the maximum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] < 1)))



#belowLR <- conjunctionTable %>% filter(LRAs < 1 & LRBs < 1)
#mean(belowLR$LRABs > belowLR$minLR & belowLR$LRABs < belowLR$maxLR)


LRbelowPlot <- ggarrange(plotLRindBelow,plotLRindBelow2, ncol = 2)



plotLRindAbove <- conjunctionTable %>% filter(LRAs > 1 & LRBs > 1) %>% ggplot( aes(x = LRdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - max,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to  the maximum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] > 1)))+xlim(c(-10,3))


plotLRindAbove2 <- conjunctionTable %>% filter(LRAs > 1 & LRBs > 1) %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to  the minimum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] > 1)))+xlim(c(-1,15))

#aboveLR <-  conjunctionTable %>% filter(LRAs > 1 & LRBs > 1)
#mean(aboveLR$LRABs > aboveLR$minLR & aboveLR$LRABs < aboveLR$maxLR)



LRabovePlot <- ggarrange(plotLRindAbove,plotLRindAbove2, ncol = 2)
```
\normalsize

\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} 
#LRbelowPlot
annotate_figure(LRbelowPlot, top = text_grob("Distances of the joint LR from the individual LRs (negative support) \n50% joint LRs between the individual ones (DAG1)" , size = 13))
```

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are below 1, DAG used in \textsf{DAG1}.} 
\label{fig:LRlowerPlot}
\end{figure}






\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}

plotLRindBelowDep <- conjunctionTable2 %>% filter(LRAs < 1 & LRBs < 1) %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to the the minimum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] < 1)))

plotLRindBelow2Dep <- conjunctionTable2 %>% filter(LRAs < 1 & LRBs < 1) %>% ggplot( aes(x = LRdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - max,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to the the maximum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] < 1)))



# belowLRDep <- conjunctionTable2 %>% filter(LRAs < 1 & LRBs < 1)
# mean(belowLRDep$LRABs > belowLRDep$minLR & belowLRDep$LRABs < belowLRDep$maxLR)


LRbelowPlotDep <- ggarrange(plotLRindBelowDep,plotLRindBelow2Dep, ncol = 2)


#never lower than the mininum
#LRAboveDep <-  conjunctionTable2 %>% filter(LRAs > 1 & LRBs > 1)
#mean(LRAboveDep$LRABs < LRAboveDep$minLR)
#sum(LRAboveDep$LRABs < LRAboveDep$minLR)
#mean(LRAboveDep$LRABs > LRAboveDep$minLR & LRAboveDep$LRABs < LRAboveDep$maxLR)


plotLRindAboveDep <- conjunctionTable2 %>% filter(LRAs > 1 & LRBs > 1) %>% ggplot( aes(x = LRdifsMax))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - max,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to  the maximum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] > 1)))+xlim(c(-10,3))


plotLRindAbove2Dep <- conjunctionTable2 %>% filter(LRAs > 1 & LRBs > 1) %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Compared to  the minimum")+
  labs(subtitle = expression(paste("Assuming ",  LR[A], ", ",   LR[B] > 1)))+xlim(c(-1,15))

#aboveLR <-  conjunctionTable %>% filter(LRAs > 1 & LRBs > 1)
#mean(aboveLR$LRABs > aboveLR$minLR & aboveLR$LRABs < aboveLR$maxLR)



LRabovePlotDep <- ggarrange(plotLRindAboveDep,plotLRindAbove2Dep, ncol = 2)
```
\normalsize

\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} 
#LRbelowPlot
annotate_figure(LRbelowPlotDep, top = text_grob("Distances of the joint LR from the individual LRs (negative support) \nStill, 50% joint LRs between the individual ones (DAG2)" , size = 13))
```

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are below 1, DAG used in \textsf{DAG2}.} 
\label{fig:LRlowerPlot2}
\end{figure}





























Once the individual likelihood ratios are above 1, the joint likelihood ratio can be lower than the maximum, but is not lower than the minimum of the individual likelihood ratios (Figures \ref{fig:LRabovePlot} and \ref{fig:LRabovePlotDep}).


\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} 
#LRbelowPlot
annotate_figure(LRabovePlot, top = text_grob("Distances of the joint LR from the individual LRs (positive support) \n70%  of joint LRs between the individual ones (DAG1)",
                 size = 13, hjust = .5))
```

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are above 1, DAG used in \textsf{DAG1}.}
\label{fig:LRabovePlot}
\end{figure}





\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} 
#LRbelowPlot
annotate_figure(LRabovePlotDep, top = text_grob("Distances of the joint LR from the individual LRs (positive support) \nStill, 70%  of joint LRs between the individual ones (DAG2)",
                 size = 13, hjust = .5))
```

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are above 1, DAG used in \textsf{DAG2}.}
\label{fig:LRabovePlotDep}
\end{figure}

An example of one of many \textsf{BNs} in which an individual likelihood exceeds the joint likelihood can be inspected in Figure \ref{tab:CPTconjunctionBNL}.

<!-- \raf{M: Very interesting. This is intuitive. If the two items of evidence are not independent lines of evidence, they might not always strengthen one another. This happens in 14 per cent of the cases, not too often. Is there a pattern? Could be explored in chapter on corroborationor or cross-examination.} -->

<!-- \mar{R: Maybe, we'll keep this in mind.} -->

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
conjunctionDAG <- model2network("[a|A][b|B][AB|A:B][A][B|A]")
# added missing values BifAs and BifnAs to the final table and run the simulation again -> with same seed 
conjunctionTable2 <- readRDS(file = "../datasets/conjunctionTableAdditional.RDS")

# LRAs > LRABs
newdata <- subset(conjunctionTable2, LRAs > LRABs & BFAs > 1 & BFBs > 1 & BFABs > BFAs)
one_row <- head(newdata, 1)


As = one_row[1, "As"] 
BifAs = one_row[1, "BifAs"]
BifnAs = one_row[1, "BifnAs"]
aifAs = one_row[1, "aifAs"]
aifnAs = one_row[1, "aifnAs"]
bifBs = one_row[1, "bifBs"]
bifnBs = one_row[1, "bifnBs"]

AProb <-prior.CPT("A","1","0",As)
BProb <-  single.CPT("B","A","1","0","1","0",BifAs,BifnAs)
aProb <- single.CPT("a","A","1","0","1","0",aifAs,aifnAs)
bProb <- single.CPT("b","B","1","0","1","0",bifBs,bifnBs)


ABProb <- array(c(1, 0, 0, 1, 0, 1, 0,1), 
                dim = c(2, 2, 2),
                dimnames = list(AB = c("1","0"),
                                B = c("1","0"), 
                                A = c("1","0")))


conjunctionCPT <- list(A = AProb, B = BProb, 
                       a = aProb, b = bProb, AB = ABProb)


conjunctionBNL <- custom.fit(conjunctionDAG,conjunctionCPT)

```




\begin{figure}
\begin{subfigure}[!ht]{0.45\textwidth}

\footnotesize 
\begin{tabular}{lr}
\toprule
A & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.892}\\
0 & 0.108\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{B} & \multicolumn{2}{c}{A} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.551} & \cellcolor{gray!6}{0.457}\\
0 & 0.449 & 0.543\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{a} & \multicolumn{2}{c}{A} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.957} & \cellcolor{gray!6}{0.453}\\
0 & 0.043 & 0.547\\
\bottomrule
\end{tabular}


\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{b} & \multicolumn{2}{c}{B} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.678} & \cellcolor{gray!6}{0.573}\\
0 & 0.322 & 0.427\\
\bottomrule
\end{tabular}

\vspace{2mm}

\normalsize
\end{subfigure} \begin{subfigure}[!ht]{0.45\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=2.5, fig.height=2}
graphviz.chart(conjunctionBNL, type = "barprob")
```
\end{subfigure}
\caption{A counterexample to distribution. $LR_A  \approx 2.11$, $LR_B \approx 1.183$,  $LR_{AB} \approx 1.319$. \newline  $BF_A \approx  1.06, BF_B \approx	1.076, BF_{AB}\approx	1.14$.}
\label{tab:CPTconjunctionBNL}
\end{figure}






Note that the result for joint Bayes factors for  \textsf{DAG 2} depended on two independencies that fell out of the dag: that $a$ and $b$ are independent both conditional on $A$ and conditional on $\n A$. A natural question is: does  aggregation hold once this independence is dropped? To investigate, we run a simulation based on \textsf{DAG 3}, illustrated in Figure \ref{fig:dag3}.

\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
doubleDepDAG <- model2network("[a|A:b][b|B][AB|A:B][A][B|A]")

daggityDoubleDepDAG <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> B
        A -> AB
        B -> AB
        b -> a
      }")

```
\normalsize

\begin{figure}[H]

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=2.5, fig.height=2}
#graphviz.plot(conjunctionDAG, layout = "dot")
coordinates(daggityDoubleDepDAG) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                          y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityDoubleDepDAG, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 3.5)
```
\caption{\textsf{DAG 3} with direct dependence between the pieces of evidence.}
\label{fig:dag3}
\end{figure}

In fact, it turns out that simultaneously the joint likelihood ratio is lower than both individual likelihood ratios and the joint Bayes factor is lower than each individual Bayes factor in 14\% cases in which the individual Bayes factor (and therefore also the individual likelihood ratios) are greater than one. One particular counterexample is illustrated in Figure \ref{fig:CPTDoubleL}.



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
doubleDepDAG <- model2network("[a|A:b][b|B][AB|A:B][A][B|A]")
# added missing values BifAs and BifnAs to the final table and run the simulation again -> with same seed 
doubleDepTable <- readRDS(file = "../datasets/doubleDependencyTable.RDS")


newdata <- subset(doubleDepTable, LRAs > LRABs & LRBs > LRABs & BFAs > 1 & BFBs > 1 &
                    BFABs < BFAs & BFABs < BFBs)
one_rowB <- head(newdata, 1)


As = one_rowB[1, "As"] 
BifAs = one_rowB[1, "BifAs"]
BifnAs = one_rowB[1, "BifnAs"]
aifAs = one_rowB[1, "aifAs"]
aifnAs = one_rowB[1, "aifnAs"]
bifBs = one_rowB[1, "bifBs"]
bifnBs = one_rowB[1, "bifnBs"]
aifAbs <- one_rowB[1, "aifAbs"]
aifnAnbs <- one_rowB[1, "aifnAnbs"]
aifnAbs <- one_rowB[1, "aifnAbs"]
aifAnbs <- one_rowB[1, "aifAnbs"]



AProb <-prior.CPT("A","1","0",As)
BProb <-  single.CPT("B","A","1","0","1","0",BifAs,BifnAs)
#aProb <- single.CPT("a","A","1","0","1","0",aifAs,aifnAs)
bProb <- single.CPT("b","B","1","0","1","0",bifBs,bifnBs)


aProb <- array(c(one_rowB[1,"aifAbs"], 1-one_rowB[1,"aifAbs"],
                 one_rowB[1,"aifAnbs"], 1 - one_rowB[1,"aifAnbs"], 
                 one_rowB[1,"aifnAbs"], 1- one_rowB[1,"aifnAbs"],
                 one_rowB[1,"aifnAnbs"], 1 - one_rowB[1,"aifnAnbs"]), 
               dim = c(2, 2, 2),
               dimnames = list(a = c("1","0"),
                               b = c("1","0"), 
                               A = c("1","0")))

ABProb <- array(c(1, 0, 0, 1, 0, 1, 0,1), 
                dim = c(2, 2, 2),
                dimnames = list(AB = c("1","0"),
                                B = c("1","0"), 
                                A = c("1","0")))


conjunctionCPT <- list(A = AProb, B = BProb, 
                       a = aProb, b = bProb, AB = ABProb)


conjunctionDoubleBNL <- custom.fit(doubleDepDAG,conjunctionCPT)

 
```







\begin{figure}
\hspace{2mm}\begin{subfigure}[ht!]{0.45\textwidth}
\footnotesize


\begin{tabular}{lr}
\toprule
A & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.521}\\
0 & 0.479\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{B} & \multicolumn{2}{c}{A} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.66} & \cellcolor{gray!6}{0.729}\\
0 & 0.34 & 0.271\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lllr}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{A} & \multicolumn{1}{c}{b} & \multicolumn{1}{c}{} \\
a &  &  & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.3989398}\\
0 & 1 & 1 & 0.6010602\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.9673984}\\
0 & 0 & 1 & 0.0326016\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0.9693564}\\
0 & 1 & 0 & 0.0306436\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0.7267025}\\
0 & 0 & 0 & 0.2732975\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{b} & \multicolumn{2}{c}{B} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.995} & \cellcolor{gray!6}{0.108}\\
0 & 0.005 & 0.892\\
\bottomrule
\end{tabular}

\normalsize 


\subcaption{Conditional probabilities for the counterexample (the one for \textsf{AB} does not change).}
\end{subfigure} 
\hspace{5mm}\begin{subfigure}{0.45\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300,fig.width=2.5, fig.height=2}
graphviz.chart(conjunctionDoubleBNL, type = "barprob")
```
\subcaption{Marginal probabilities. }
\end{subfigure} 
\caption{A counterexample based on \textsf{DAG 3}, with independence between the items of evidence dropped.   $LR_A  \approx 1.063$, $LR_B \approx 1.159742$,  $LR_{AB} \approx 0.651$. $BF_A \approx  1.022, BF_B \approx	1.079, BF_{AB}\approx	0.699$.}
\label{fig:CPTDoubleL}
\end{figure}








```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
doubleTable <-  readRDS(file = "../datasets/doubleDependencyTable.RDS")

doubleTable$maxBF <- pmax(doubleTable$BFAs, doubleTable$BFBs)
doubleTable$minBF <- pmin(doubleTable$BFAs, doubleTable$BFBs)
doubleTable$BFdifsMax <- doubleTable$BFABs - doubleTable$maxBF
doubleTable$BFdifsMin <- doubleTable$BFABs - doubleTable$minBF

doubleTable$maxLR <- pmax(doubleTable$LRAs, doubleTable$LRBs)
doubleTable$minLR <- pmin(doubleTable$LRAs, doubleTable$LRBs)
doubleTable$LRdifsMax <- doubleTable$LRABs - doubleTable$maxLR 
doubleTable$LRdifsMin <- doubleTable$LRABs - doubleTable$minLR

positiveBF <- doubleTable %>% filter (BFAs >1 & BFBs > 1 )


plotBFfailure <- positiveBF %>% ggplot( aes(x = BFdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(BF[AB] - min,"(",BF[A], ", ", BF[B],")")))+ggtitle("Joint BFs compared to  the minimum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(-3,3))

plotLRfailure <- positiveBF %>% ggplot( aes(x = LRdifsMin))+geom_histogram(aes( y = ..density..), bins = 40)+
  xlab(expression(paste(LR[AB] - min,"(",LR[A], ", ", LR[B],")")))+ggtitle("Joint LRs compared to  the minimum")+
  labs(subtitle = expression(paste("Assuming ",  BF[A], ", ",   BF[B] > 1)))+xlim(c(-3,3))

aggregationFailure <- ggarrange(plotBFfailure, plotLRfailure)

aggregationFailurePlot <- annotate_figure(aggregationFailure, top = text_grob("Aggregation failure with DAG3" , size = 13))
```


\begin{figure}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
aggregationFailurePlot
```
\caption{Aggregation failure.} 
\label{fig:aggregationFails}
\end{figure}

\mar{R: Two new passages on fixed joint evidence, check.}
Finally, you might recall that in the chapter we distinguished two variants of the distribution requirement, (DIS1) and (DIS2). The former  has been in the focus so far, and the latter differs in keeping the evidence fixed at $a \et b$, even when calculating individual LRs. So, sticking to LR, distribution in this variant  pertains to the joint LR and the following two ratios: $\nicefrac{\pr{a\et b \vert A}}{\pr{a \et b \vert \n A}}$ and  $\nicefrac{\pr{a\et b \vert B}}{\pr{a \et b \vert \n B}}$. To make sure that switching to (DIS2) doesn't make things easier for legal probabilists, we ran 30k simulations (10k for each BN type), and inspected the status of aggregation and distribution for these ratios. 

Here are the results. If no assumption about the direction of support is made, 
 around 12.7\% of the time  (around twice less often than if the usual 
 individual LRs are used),  the individual LRs with fixed evidence are both greater than the joint LR---this is for \textsf{DAG1}, the frequency goes slightly up to around 13\% if we switch to \textsf{DAG2} and is around the same value if additionally we allow for the dependency between the items of evidence (\textsf{DAG3}). Assuming individual LRs are above one, around 70\% of joint likelihoods (75\% for BN2, 72\% for BN3) are between the individual ones, no joint LR is below the minimum of the individual ratios for \textsf{DAG1}, but is so around 2\% times  for both \textsf{DAG2} and  \textsf{DAG3}. This is one important difference: even aggregation can fail if dependencies are present, if we keep joint evidence fixed in all the ratios. As for distribution, there are no major surprises:  around 30\% of the time, the joint LR is strictly greater than both of the individual LRs with evidence fixed for \textsf{DAG1},  22\% for \textsf{DAG2}, and 26\% for \textsf{DAG3}. In short, keeping the joint evidence fixed across 
 all ratios makes things even harder, when it comes to preserving aggregation and distribution.

 
 
 
.   
 
 
 
 
 
 
