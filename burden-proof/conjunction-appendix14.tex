% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  10pt,
  dvipsnames,enabledeprecatedfontcommands]{scrartcl}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Appendix: Bayes factor, likelihood ratio, and the difficulty about conjunction},
  pdfauthor={Marcello Di Bello and Rafal Urbaniak},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\documentclass{article}

% %packages
\usepackage{booktabs}
%\usepackage[left]{showlabels}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{ragged2e}
\usepackage{etex}
%\usepackage{yfonts}
\usepackage{marvosym}
\usepackage[notextcomp]{kpfonts}
\usepackage{nicefrac}
\newcommand*{\QED}{\hfill \footnotesize {\sc Q.e.d.}}
\usepackage{floatrow}
\usepackage{multicol}

\usepackage[textsize=footnotesize]{todonotes}
\newcommand{\ali}[1]{\todo[color=gray!40]{\textbf{Alicja:} #1}}
\newcommand{\mar}[1]{\todo[color=blue!40]{#1}}
\newcommand{\raf}[1]{\todo[color=olive!40]{#1}}

%\linespread{1.5}
\newcommand{\indep}{\!\perp \!\!\! \perp\!}


\setlength{\parindent}{10pt}
\setlength{\parskip}{1pt}


%language
%\usepackage{times}
\usepackage{mathptmx}
\usepackage[scaled=0.86]{helvet}
\usepackage{t1enc}
%\usepackage[utf8x]{inputenc}
%\usepackage[polish]{babel}
%\usepackage{polski}




%AMS
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{geometry}
 \geometry{a4paper,left=35mm,top=20mm,}


%environments
\newtheorem{fact}{Fact}



%abbreviations
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}
\newcommand{\n}{\neg}
\newcommand{\et}{\wedge}
\newcommand{\jt}{\rightarrow}
\newcommand{\ko}[1]{\forall  #1\,}
\newcommand{\ro}{\leftrightarrow}
\newcommand{\exi}[1]{\exists\, {_{#1}}}
\newcommand{\pr}[1]{\ensuremath{\mathsf{P}(#1)}}
\newcommand{\cost}{\mathsf{cost}}
\newcommand{\benefit}{\mathsf{benefit}}
\newcommand{\ut}{\mathsf{ut}}

\newcommand{\odds}{\mathsf{Odds}}
\newcommand{\ind}{\mathsf{Ind}}
\newcommand{\nf}[2]{\nicefrac{#1\,}{#2}}
\newcommand{\R}[1]{\texttt{#1}}
\newcommand{\prr}[1]{\mbox{$\mathtt{P}_{prior}(#1)$}}
\newcommand{\prp}[1]{\mbox{$\mathtt{P}_{posterior}(#1)$}}



\newtheorem{q}{\color{blue}Question}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[fact]


%technical intermezzo
%---------------------

\newcommand{\intermezzoa}{
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}



	\tiny{\sc Optional Content Starts}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}\nopagebreak 
	}


\newcommand{\intermezzob}{\nopagebreak 
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}

	\tiny{\sc Optional Content Ends}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}
	}
	
	
%--------------------






















\newtheorem*{reply*}{Reply}
\usepackage{enumitem}
\newcommand{\question}[1]{\begin{enumerate}[resume,leftmargin=0cm,labelsep=0cm,align=left]
\item #1
\end{enumerate}}

\usepackage{float}

% \setbeamertemplate{blocks}[rounded][shadow=true]
% \setbeamertemplate{itemize items}[ball]
% \AtBeginPart{}
% \AtBeginSection{}
% \AtBeginSubsection{}
% \AtBeginSubsubsection{}
% \setlength{\emergencystretch}{0em}
% \setlength{\parskip}{0pt}






\usepackage[authoryear]{natbib}

%\bibliographystyle{apalike}



\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Appendix: Bayes factor, likelihood ratio, and the difficulty
about conjunction}
\author{Marcello Di Bello and Rafal Urbaniak}
\date{}

\begin{document}
\maketitle

This appendix takes a closer look at the behavior of the Bayes factor
and the likelihood ratio in the conjunction problem. We do so by
building Direct Acyclic Graphs (DAGs) to study to what extent these
measures of evidential support comply with the conjunction principle. We
rely on analytic proofs, but when calculations become unmanageable, we
rely instead on computer simulations. These simulations are conducted as
follows. For each DAG of interest, 100,000 random Bayesian networks were
generated whose probability tables had values sampled from the
\textsf{Uniform(0,1)} distribution. For each of these networks, we
calculated the relevant probabilities, Bayes factors, and likelihood
ratios. Even for cases in which we derived analytic proofs, the results
of a computer simulation sometimes provided additional insights.

Now, for the conceptual development. First, we describe two
\textsf{DAG}s at play: one of them represents a conjunction when the
hypotheses are independent, and the other one drops this independence
assumptions. We then discuss the relation between \textsf{DAG}s and
independence, and introduce the independencies in the proofs used later
on.

Then we turn to the Bayes factor. First we prove that for if the
stronger independence assumptions hold, the joint Bayes factor is just
the result of multiplying the individual Bayes factors. It follows that
aggregation is satisfied in such cases, if individual Bayes factors are
greater than one. Once the hypotheses are not independent, a weaker
result can be obtained, which entails that the aggregation is satisfied
for the Bayes factor, if a certain additional constraint is satisfied.

We investigate simulations based on the first \textsf{DAG}: in general,
aggregation fails 25\% of the time if the individual Bayes factors are
not constrained to be greater than one, but holds once this constraint
is added. Switching to the second \textsf{DAG} does not change the
picture, and so the question of whether the additional constraint needed
for the weaker theorem holds in all Bayesian networks based on this
\textsf{DAG}. Inspired by this observation, we show that in fact the
additional constraint needed for aggregation to hold falls out of a pair
of other independencies entailed by the second \textsf{DAG}. What the
simulation reveal is that there is a large class of cases in which
individual Bayes factors are above one, aggregation is satisfied, but
distribution fails.

Next, we turn to the likelihood ratio. Since the analytic approach is
less feasible here, we approach Bayesian networks based on the simpler
\textsf{DAG} analytically, but rely on simulations for cases in which
the hypotheses are not assumed to be independent. Without any constraint
on individual likelihood ratios, aggregation fails in 12.5\% cases
(twice less often than aggregation for the Bayes factor). Another
difference was that while the joint Bayes factor in cases with positive
individual Bayes factor was always not less than the larger of the
individual factors, now around 70\% of joint likelihood ratios falls
between the individual ratios, and is no lower than the smaller of these
(if the individual likelihood ratios are assumed to be greater than
one). Thus, aggregation is satisfied if individual likelihood ratios
equal at least one, but it no longer holds that the joint support is
greater than any of the individual support levels. Still, distribution
fails in a large class of cases.

Finally, we identify cases in which aggregation can fail even if the
individual BFs or LRs are at least one: this can happen if there is a
direct dependence between the pieces of evidence.

The \textsf{\textbf{R}} code we used in the simulations, calculations
and visualizations is made available on the book website {[}LINK TO
DOCUMENTATION TO BE ADDED LATER{]}.

\hypertarget{bayesian-networks-and-probabilistic-independence}{%
\subsection*{Bayesian networks and probabilistic
independence}\label{bayesian-networks-and-probabilistic-independence}}
\addcontentsline{toc}{subsection}{Bayesian networks and probabilistic
independence}

We begin with a refresher of the basic notions. A Bayesian network
consists of a graphical part---a directed acyclic graph (DAG)---and a
probability measure defined over the nodes (variables) in the graph.
Bayesian networks satisfy the Markov condition. That is, any node is
conditionally independent of its nondescentants (including ancestors),
given its parents. If a probabilistic measure \(\pr{}\) that is defined
over the nodes (variables) in a graph \(\mathsf{G}\) respects the Markov
condition, \(\pr{}\) is said to be compatible with \(\mathsf{G}\). Graph
\(\mathsf{G}\) and measure \(\pr{}\) can then be combined to form a
Bayesian network.

The graphical counterpart of probabilistic independence is
\textbf{d-separation}, \(\indep_d\). Two nodes, \(X\) and \(Y\), are
d-separated given a set of nodes
\(\mathsf{Z}\)---\(X\indep_d Y \vert \mathsf{Z}\) --- iff for every
undirected path from \(X\) to \(Y\) there is a node \(Z'\) on the path
such that either (see Figure \ref{fig:connectionsBN}):

\begin{itemize}

\item $Z' \in \mathsf{Z}$ and there is a \textbf{serial} connection, $\rightarrow Z' \rightarrow$, on the path (\textbf{pipe}),
\item  $Z'\in \mathsf{Z}$ and there is a \textbf{diverging} connection, $\leftarrow Z' \rightarrow $, on the path (\textbf{fork}),
\item There is a \textbf{converging} connection $\rightarrow Z' \leftarrow$ on the path (in which case $Z'$ is a \textbf{collider}), and neither $Z'$ nor its descendants are in $\mathsf{Z}$.
\end{itemize}

\vspace{1mm}

\begin{figure}[H]
\hspace{2mm}\begin{subfigure}[!ht]{0.25\textwidth}

\begin{center}\includegraphics[width=0.75\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-2-1} \end{center}
\subcaption{\textsf{Serial: Pipe}}
\end{subfigure} 
\hspace{5mm}\scalebox{1}{\begin{subfigure}[!ht]{0.32\textwidth}
\vspace{1mm}

\begin{center}\includegraphics[width=0.75\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-3-1} \end{center}
\subcaption{\textsf{Diverging: Fork}}
\end{subfigure}}
\hspace{5mm}\scalebox{1}{\begin{subfigure}[!ht]{0.32\textwidth}
\vspace{1mm}

\begin{center}\includegraphics[width=0.75\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-4-1} \end{center}
\subcaption{\textsf{Converging:Collider}}
\end{subfigure}}
\normalsize
\caption{Three basic types of connections.}
\label{fig:connectionsBN}
\end{figure}

\noindent Serial, converging and diverging connections represent common
scenarios. Consider the nodes:

\footnotesize 
\begin{center}
\begin{tabular}{@{}lp{5.3cm}@{}}\toprule
Node & Proposition \\ \midrule 
$G$ & The suspect is guilty. \\
$B$ & The blood stain comes from the suspect.\\
$M$ & The crime scene stain and the suspect's blood share the same DNA profile.\\
\bottomrule
\end{tabular}
\end{center}
\normalsize

\noindent This scenarios is naturally represented by the serial
connection \(G \rightarrow B \rightarrow M\). If we don't know whether
\(B\) holds, \(G\) has an indirect impact on the probability of \(M\).
Yet, once we find out that \(B\) is true, we expect the profile match,
and whether \(G\) holds has no further impact on the probability of
\(M\).

For converging connections, let \(G\) and \(B\) be as above, and let:

\begin{center}
\begin{tabular}{@{}lp{6.3cm}@{}}\toprule
Node & Proposition \\ \midrule 
$O$ & The crime scene stain comes from the offender.\\
\bottomrule
\end{tabular}
\end{center}
\normalsize

\noindent Both \(G\) and \(O\) influence \(B\). If suspect guilty, it is
more likely that the blood stain comes from him, and if the blood crime
stain comes from the offender it is more likely to come from the suspect
(for instance, more so than if it comes from the victim). Moreover,
\(G\) and \(O\) seem independent. Whether the suspect is guilty does not
have any bearing on whether the stain comes from the offender. Thus, a
converging connection \(G\rightarrow B \leftarrow O\) seems appropriate.
However, if you do find out that \(B\) is true---that the stain comes
from the suspect---then whether the crime stain comes from the offender
becomes relevant for whether the suspect is guilty.

Take an example of a diverging connection. Say you have two coins, one
fair, one biased. Conditional on which coin you have chosen, the results
of subsequent tosses are independent. But if you don't know which coin
you have chosen, the result of previous tosses give you some information
about which coin it is, and this has an impact on your estimate of the
probability of heads in the next toss. Whether a coin is fair, \(F\), or
not has an impact on the result of the first toss, \(H1\), and on the
result of the second toss, \(H2\). So \(H1 \leftarrow F \rightarrow H2\)
seems to be appropriate. Now, on one hand, so long as you do not know
whether \(F\), the truth of \(H1\) increases the probability of \(H2\).
On the other hand, once you know that \(F\) is true, \(H1\) and \(H2\)
become independent, and so conditioning on the parent in a fork makes
its childern independent (provided there is no other open path between
them in the graph).

As a final piece of terminology, two sets of nodes, \(\mathsf{X}\) and
\(\mathsf{Y}\), are d-separated given \(\mathsf{Z}\) if every node in
\(\mathsf{X}\) is d-separated from every node in \(\mathsf{Y}\) given
\(\mathsf{Z}\). Interestingly, it can be proven that if two sets of
nodes are d-separated given a third one, they are independent given the
third one, for any probabilistic measures compatible with a given DAG.
However, lack of d-separation does not necessarily entail dependence for
any probabilistic measure compatible with a given DAG. It only allows
for it: if nodes are d-separated, there is at least one probabilistic
measure fitting the DAG according to which they are dependent. So, at
least, no false independencies can be inferred from the DAG, and all the
dependencies are built into it.

\hypertarget{independencies-in-the-conjunction-problem}{%
\subsection*{Independencies in the conjunction
problem}\label{independencies-in-the-conjunction-problem}}
\addcontentsline{toc}{subsection}{Independencies in the conjunction
problem}

Back to the difficulty with conjunction. One assumption often made in
the formulation of the problem is that hypotheses \(A\) and \(B\) are
probabilistically independent. We will endorse this assumption, but also
relax it to see whether the problem subsides (it does not). In
particular, we will consider the two Bayesian networks shown in Figure
\ref{fig:conjunctionBNs}. They represent two hypotheses, \(A\) and
\(B\), their supporting pieces of evidence, \(a\) and \(b\), and their
conjunction \(AB\).\footnote{In the Bayesian networks in this appendix,
  the conditional probability table for the conjunction node \(AB\)
  mirrors the truth table for the conjunction in propositional logic, as
  in Table \ref{tab:CPTconjunction2}.} The difference is that a direct
dependence between the hypotheses exists in the second network.

\vspace{1mm}
\footnotesize

\normalsize

\begin{figure}[H]
\hspace{2mm}\scalebox{1}{\begin{subfigure}[!ht]{0.45\textwidth}

\begin{center}\includegraphics[width=0.75\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-6-1} \end{center}
\subcaption{\textsf{DAG1}}
\end{subfigure}} 
\hspace{5mm}\begin{subfigure}[!ht]{0.45\textwidth}
\vspace{1mm}

\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-7-1} \end{center}
\subcaption{\textsf{DAG2}}
\end{subfigure}
\normalsize
\caption{Two DAGs for the conjunction problem.}
\label{fig:conjunctionBNs}
\end{figure}

\begin{table}[h]
\begin{table}[H]
\centering
\begin{tabular}{lllr}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{A} & \multicolumn{1}{c}{B} & \multicolumn{1}{c}{} \\
AB &  &  & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{1}\\
0 & 1 & 1 & 0\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0}\\
0 & 0 & 1 & 1\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0}\\
0 & 1 & 0 & 1\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0}\\
0 & 0 & 0 & 1\\
\bottomrule
\end{tabular}
\end{table}
\normalsize
\caption{Conditional probability table for the conjunction node.}
\label{tab:CPTconjunction2}
\end{table}

\newpage 
\vspace{1mm}
\footnotesize

\normalsize

Unsurprisingly, the relations of d-separation entailed by the two
networks differ. Examples can be found in Table \ref{tab:indepBNS}. In
fact, \textsf{DAG1} entails 31 d-separations, while \textsf{DAG2}
entails 22 of them. One warning about the notation. Nodes represent
variables, and so each d-separation entails a probabilistic statement
about all combination of the node states (values of variables) involved.
For instance, assuming each node is binary with two possible states, 1
and 0, \mbox{$B   \indep_d\,\,  a $} entails that for any
\mbox{$ B_i, a_i \in \{0, 1\}$} we have
\(\pr{B = B_i} = \pr{B = B_i \vert a = a_i}\).

\begin{table}[h]
\begin{tabular}{cr}
\toprule
Bayesian network 1  & Bayesian network 2\\
\midrule
\cellcolor{gray!6}{$A   \indep_d\,\, B  $}&\cellcolor{gray!6}{     $ A  \indep_d\,\, b \vert  B  $} \\
$A   \indep_d\,\, b  $& $AB  \indep_d\,\,  a \vert  A$\\
\cellcolor{gray!6}{$\,\,\, AB  \indep_d\,\, a \vert A $}  & \cellcolor{gray!6}{$AB  \indep_d\,\,  b \vert  B $}\\
$\,\,\, AB  \indep_d\,\, b \vert B  $ & $ B  \indep_d\,\,  a \vert  A $\\
\cellcolor{gray!6}{$B   \indep_d\,\, a $}        & \cellcolor{gray!6}{$a  \indep_d\,\,  b \vert  B$ }\\
$\,\, a    \indep_d\,\, b$    & $a  \indep_d\,\,  b \vert  A $ \\
\bottomrule
\end{tabular}
\caption{Some of d-separations entailed by \textsf{DAG1} and \textsf{DAG2}.} 
\label{tab:indepBNS}
\end{table}

Turning from nodes to states (or events, propositions), Figure
\ref{tab:indepBNS-states} lists the independencies between
propositions.\footnote{Some caveats. In \eqref{eq:I1} the conditioning
  on \(a\) is not essential, because it's not on the path between the
  nodes: the key reason why the independence remains upon this
  conditioning is that there is an unconditioned collider on the path.
  Still, we need this independence in the proof later on. In
  \eqref{eq:I3} what we are conditioning on is \(A\) and \(B\) jointly.
  Technically, independence conditional on the conjunction node \(AB\)
  does not fall out of the d-separations present in the network---it
  follows given that \(AB\) and \(A,B\) are connected deterministically:
  fixing \(AB\) to true fixes both \(A\) and \(B\) to true.} It also
shows which independencies are entailed by either of the two DAGs.

\begin{figure}
\begin{subfigure}[!ht]{0.45\textwidth}
\begin{align} A\indep B  \label{eq:indAB}     &\hspace{2cm}\mbox{\footnotesize DAG1}\\
b \indep a   \label{eq:indab}   & \hspace{2cm}\mbox{\footnotesize DAG1}\\
A \indep b \vert a   \label{eq:I1}    &\hspace{2cm}\mbox{\footnotesize DAG1} \\
B \indep a \et A \vert b \label{eq:I2}&\hspace{2cm}\mbox{\footnotesize DAG1 } \\
a\indep b \vert A\et B \label{eq:I3}  &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\  
a\indep b \vert A \label{eq:I3a}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\ 
a\indep b \vert \n A \label{eq:I3b}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\ 
a\indep b \vert B \label{eq:I3a2}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep b \vert \n B \label{eq:I3b2}   &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2}
\end{align}
\end{subfigure}
\begin{subfigure}[!ht]{0.5\textwidth}
\begin{align}
a\indep B \vert A \label{eq:I4}    & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep B \vert \n A \label{eq:I4a}    & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep \n B \vert A \label{eq:I4b}   & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
a\indep \n B \vert \n A \label{eq:I4c}   & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b\indep A \et a \vert B \label{eq:I5}  & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b\indep \n A \et a \vert B \label{eq:I5a} &\hspace{2cm}\mbox{\footnotesize DAG1 , DAG2}  \\
b\indep A \et a \vert \n B \label{eq:I5b} & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b\indep \n A \et a \vert \n B \label{eq:I5c} & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} \\
b \indep a \vert B \label{eq:I6} & \hspace{2cm}\mbox{\footnotesize DAG1 , DAG2} 
\end{align}
\end{subfigure}
\caption{Independencies among propositions according to \textsf{DAG1} and \textsf{DAG2}.} 
\label{tab:indepBNS-states}
\end{figure}

An ambiguity in our notation is worth mentioning. Table
\ref{tab:indepBNS} lists independencies between \emph{nodes}. But Figure
\ref{tab:indepBNS-states} is about \textit{states} rather than nodes.
Each particular instantiation of a node (a state) corresponds to a
proposition, for example, \(A = 1\) means that the proposition
corresponding to \(A\) holds, while \(A = 0\) means that the negation of
\(A\) holds. Crucially, an expression such as
\mbox{$b\indep A \et a \vert \n B$} should be understood as a claim
about states (events, propositions), which means the same as
\(\pr{b = 1 \vert B = 0} = \pr{b = 1 \vert A = 1, a = 1, B = 0}\). The
distinction betwen nodes and their states (or variables and their
values) matters because independence conditional on \(B= 0\) doesn't
entail independence given \(B=1\). For instance, one's final grade might
depend on hard work if the teacher is fair, but this might fail if the
teacher is not fair. We hope this ambiguity in notation will cause no
confusion. Whether we talk about nodes or states (or events,
propositions) should be clear from the context.

\hypertarget{posterior-probabilities}{%
\subsection*{Posterior probabilities}\label{posterior-probabilities}}
\addcontentsline{toc}{subsection}{Posterior probabilities}

We first examine how posterior probabilities behave in the conjunction
problem. The joint posterior \(\pr{A\wedge B \vert a\wedge b}\) is often
lower than the individual posterior \(\pr{A \vert a}\) and
\(\pr{B \vert b}\), whether the hypotheses \(A\) and \(b\) are
independent or not. We establish this fact via a computer simulation. We
simulated 10,000 random Bayesian networks based on \textsf{DAG1}
(independent hypotheses) and \textsf{DAG2} (dependent hypotheses). If
any such network has an equal probability of occurring, the joint
posterior is lower than both individual posteriors 68\% of the time for
\textsf{DAG1}, and around 60\% for \textsf{DAG2}. Figure
\ref{fig:posteriorFailure} displays the distributions of the distances
of the joint posterior from the lowest of the individual posteriors.

\begin{figure}[H]


\begin{center}\includegraphics[width=0.75\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-11-1} \end{center}
\caption{Even assuming the joint support is positive, the joint posterior often is lower than individual posteriors.}
\label{fig:posteriorFailure}
\end{figure}

\hypertarget{bayes-factor-proofs}{%
\subsection*{Bayes factor: proofs}\label{bayes-factor-proofs}}
\addcontentsline{toc}{subsection}{Bayes factor: proofs}

Next, we turn to the Bayes factor as our measure of evidential support.
For ease of reference, we use the following abbreviations:
\begin{align*}
BF_A  & =  \frac{\pr{a \vert A}}{\pr{a}},\\
BF_B & = \frac{\pr{b \vert B}}{\pr{b}},\\\
BF_{AB}  & =  \frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a \wedge b}}
\end{align*}

\noindent The objective here is to study how the combined support
\(BF_{AB}\) compares to the individual supports \(BF_A\) and \(BF_B\).
We prove the following general theorem:

\begin{theorem}
Given a measure $\pr{}$ compatible with \textsf{DAG1}, if both $BF_A$ and $BF_B$ 
are grater than one, then $BF_{AB}\geq \mathsf{max}(BF_{A},BF_{B})$.
The same holds for a measure compatible with \textsf{DAG2}.
\label{thm:aggregationBf}
\end{theorem}

\noindent In other words, the combined support \(BF_{AB}\) is never
below the individual supports \(BF_A\) and \(BF_B\), whether claims
\(A\) and \(B\) are independent (\textsf{DAG1}) or not (\textsf{DAG2}).

\begin{proof}
For \textsf{DAG1}, the theorem holds by Fact \ref{fac:BFindep} (and corollary). For 
\textsf{DAG2}, the theorem holds by Fact \ref{fac:BFdep} (and corollary), Lemma \ref{lem:BFLR}, and Fact \ref{fact:BFweaker3}. 
\end{proof}

\begin{fact} If the independence assumptions \eqref{eq:indAB}, \eqref{eq:indab}, \eqref{eq:I4} and \eqref{eq:I5} hold (all of which are entailed by \textsf{DAG1}), then 
$BF_{AB} = BF_A \times BF_B$. \label{fac:BFindep}
\end{fact}

\begin{proof}

\begin{align*}
\frac{\pr{a \wedge b\vert A\wedge B}}{\pr{a \wedge b}} & = \frac{\pr{A \et B\et a\wedge b}}{\pr{A \et B}} \bigg/ \pr{a \wedge b}
&\mbox{(conditional probability)} \\
&  = \frac{\pr{A} \times \pr {B \vert A}  \times \pr{a \vert A \et B} \times \pr{b \vert A \et B \et a}}{\pr{A \et B}} \bigg/ \pr{a \wedge b}
&\mbox{\,\,\,\,\,\,\, (chain rule)} \\
\end{align*}

\noindent Next, we apply the relevant independence assumptions:

\begin{align*}
&  = \frac{\pr{A} \times \overbrace{\pr {B \vert A}}^{ \pr{B} \mbox{\footnotesize \, by \eqref{eq:indAB}}}  \times
\overbrace{\pr{a \vert A \et B}}^{\pr{a \vert A} \mbox{\footnotesize \, by \eqref{eq:I4}}}
\times \overbrace{\pr{b \vert A \et B \et a}}^{\pr{b \vert B} \mbox{\footnotesize \, by \eqref{eq:I5}}}
}{\underbrace{\pr{A \et B}}_{\pr{A} \times \pr{B} \mbox{\footnotesize \, by \eqref{eq:indAB}}}} \bigg/ \underbrace{\pr{a \wedge b}}_{\pr{a}\times \pr{b} \mbox{ \footnotesize \, by \eqref{eq:indab}}} \\
&  = \frac{\pr{A} \times  \pr{B}   \times \pr{a \vert A}  \times  \pr{b \vert B}}
{\pr{A} \times \pr{B}} \bigg/ \pr{a}\times \pr{b} \\
& = \frac{\pr{a \vert A}  }{\pr{a}}  \times \frac{\pr{b \vert B}}{\pr{b}} \\
& = BF_{A} \times BF_B
\end{align*}

\end{proof}

This fact has the following straightforward consequences. They hold
because, if \(a = b \times c\) and \(b, c>1\), then
\(a > \mathsf{max}(b,c)\), and if \(b, c<1\), then
\(a < \mathsf{min}(b,c)\).

\begin{corollary} If the independence assumptions \eqref{eq:indAB}, \eqref{eq:indab}, \eqref{eq:I4} and \eqref{eq:I5} hold, and $BF_{A}$ and $BF_{B}$ are both greater than 1, then $BF_{AB}$ is greater than one. In fact,  $BF_{AB}$ is  greater than  $\mathsf{max}(BF_{A},BF_{B})$. \label{cor:BFind2}
\end{corollary}

\begin{corollary} If the independence assumptions \eqref{eq:indAB}, \eqref{eq:indab}, \eqref{eq:I4} and \eqref{eq:I5} hold, and $BF_{A}$ and $BF_{B}$ are both strictly less than 1, then $BF_{AB}$  is less than  $\mathsf{min}(BF_{A}, BF_{B})$. \label{cor:BFind3}
\end{corollary}

This establishes Theorem \ref{thm:aggregationBf} for \textsf{DAG 1}.
After dropping the independence assumptions specific to \textsf{DAG1}
and shifting to \textsf{DAG 2}, the combined \(BF_{AB}\) can no longer
be obtained by multiplying the individual ones, although multiplication
still provides a decent approximation (see Figure \ref{fig:BFmulti}).

\begin{figure}[H]

\begin{center}\includegraphics[width=0.75\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-12-1} \end{center}
\caption{In DAG2, the result of multiplying individual BFs does not equal the joint BF, but often is a good approximation thereof. Axes restricted to $0,60$ (one extreme outlier lying close to the diagonal dropped).}
\label{fig:BFmulti}
\end{figure}

However, if the probabilistic measure fits \textsf{DAG 2}, Theorem
\ref{thm:aggregationBf} is still satisfied. First, we abbreviate:
\begin{align*}
BF^{'}_{B} & = \frac{\pr{b \vert B}}{\pr{b\vert a}} \\
BF^{'}_{A} & = \frac{\pr{a \vert A}}{\pr{a \vert b}}
\end{align*} \noindent A claim weaker than Fact \ref{fac:BFindep} can be
proven by relying only on the independencies entailed by \textsf{DAG2}.

\begin{fact} If \eqref{eq:I4} and \eqref{eq:I5}  hold (and they do in BNs based on \textsf{DAG 2}), then $BF_{AB} =  BF_{A}\times BF^{'}_{B}  = BF_{B} \times BF^{'}_{A}$. \label{fac:BFdep}
\end{fact}

\begin{proof}

We start with the definition of conditional probability and the chain rule, as in the proof of Fact \ref{fac:BFindep}, but now we use fewer independencies (all of them entailed by \textsf{DAG2}). 

 \begin{align*}
\frac{\pr{a \wedge b\vert A\wedge B}}{\pr{a \wedge b}} &
= \frac{\pr{A} \times \pr {B \vert A}  \times
\overbrace{\pr{a \vert A \et B}}^{\pr{a \vert A} \mbox{\footnotesize \, by \eqref{eq:I4}}}
\times \overbrace{\pr{b \vert A \et B \et a}}^{\pr{b \vert B} \mbox{\footnotesize \, by \eqref{eq:I5}}}
}{\underbrace{\pr{A \et B}}_{\pr{A} \times \pr{B\vert A} \mbox{\footnotesize \, by the chain rule}}} \bigg/ \underbrace{\pr{a \wedge b}}_{\pr{a}\times \pr{b\vert a} \mbox{ \footnotesize \, by the chain rule }}\\
& = \frac{\pr{a \vert A}}{\pr{a}} \times \frac{\pr{b\vert B}}{\pr{b \vert a}}\\
& = BF_{A} \times BF^{'}_{B}
\end{align*}
If, instead of obtaining $\pr{a}\pr{b \vert a}$ in the denominator, we deploy the chain rule differently, resulting in $\pr{b}\pr{a \vert b}$, we end up with:
\begin{align*}
& = \frac{\pr{a \vert A}}{\pr{a \vert b}} \times \frac{\pr{b\vert B}}{\pr{b}}\\
& = BF^{'}_{A} \times BF_{B}
\end{align*}

\end{proof}

\begin{corollary} Suppose \eqref{eq:I4} and \eqref{eq:I5}  hold (they are entailed by \textsf{DAG 2}), and $BF_{BA}, BF_{B} >1$. Then if both $\pr{a\vert b} \leq \pr{a \vert A}$ and  $\pr{b \vert a} \leq \pr{b\vert B}$, we have $BF_{AB}\geq BF_{A}, BF_{B}$. \label{cor:BFweaker2}
\end{corollary}

\begin{proof}
Assume the first conjunct holds. Then $\frac{\pr{a\vert A}}{\pr{a\vert b}} \geq 1$ and so:
\begin{align*}
BF_{AB} &= BF^{'}_{A} \times BF_{B} \geq BF_{B}
\end{align*}
\noindent The argument for the other comparison is analogous.
\end{proof}

The proof of Theorem \ref{thm:aggregationBf} for \textsf{DAG 2} is not
complete yet. This corollary relies on the additional assumptions
\(\pr{a\vert b} \leq \pr{a \vert A}\) and
\(\pr{b \vert a} \leq \pr{b\vert B}\). They seem plausible. If, say,
\(a\) is used as evidence for \(A\), we often expect \(A\) and \(a\) to
be fairly strongly connected, that is, we expect \(\pr{a\vert A}\) to be
rather high, while the connection between different pieces of evidence
for different hypotheses, intuitively, is not expected to be as strong.
We provide a proof of these assumptions below.

We start with the following lemma.

\begin{lemma} For any probabilistic measure $\mathsf{P}$, if $BF_A >1$, then $LR_A>1$.\label{lem:BFLR}
\end{lemma}

\begin{proof} We start with our assumption.


\begin{align*}
1 & \leq \frac{\pr{a \vert A}}{\pr{a}} & &  \,\, (BF_A \geq 1) \\
\pr{A} & \leq  \frac{\pr{a \vert A}}{\pr{a}} \pr{A} & & (\mbox{algebraic manipulation}) \\
\pr{A} & \leq \pr{A \vert a} & &  (\mbox{Bayes' theorem})  \\
- \pr{A} &\geq - \pr{A \vert a} &  &   (\mbox{algebraic manipulation}) \\
1- \pr{A} & \geq 1 - \pr{A \vert a} & & (\mbox{algebraic manipulation})\\
1- \pr{A}  & \geq \pr{\n A \vert a} & & (\mbox{algebraic manipulation})\\
\pr{a}\left( 1 - \pr{A}\right) & \geq \pr{a}\pr{\n A \vert a}  & & (\mbox{algebraic manipulation})\\
\pr{a} & \geq \frac{\pr{a} \pr{\n A \vert a}}{\pr{\n A}} &  & (\mbox{algebraic manipulation, negation}) \\
\pr{a} & \geq \pr{a \vert \n A}  & &   (\mbox{conditional probability}) \\
\end{align*}
From this and our assumption  that $\pr{a \vert A} \geq \pr{a}$ it follows that $\pr{a \vert A}\geq \pr{a \vert \n A}$, that is, that \mbox{$LR_A \geq 1$}.
\end{proof}

Now the main claim.

\begin{fact}
For any probabilistic measure $\mathsf{P}$ appropriate for \textsf{DAG 2}, if $BF_A >1$, then $\pr{a \vert A} \geq \pr{a \vert b}$ and 
$\pr{b \vert B} \geq \pr{b \vert a}$.
\label{fact:BFweaker3}
\end{fact}

\begin{proof}

Let's focus on the first conjunct. First, we have:
\begin{align*}
\pr{a \vert b} & = \pr{a \et A \vert b} + \pr{a \et \n A \vert b} & &   (\mbox{total probability}) \\
& = \underbrace{\pr{a \vert b \et A}}_{\pr{a \vert A}  \mbox{\footnotesize \, by \eqref{eq:I3a}}} \pr{A \vert b} +
\underbrace{\pr{a \vert b \et \n A}}_{\pr{a \vert \n A} \mbox{\footnotesize \, by \eqref{eq:I3b}}}\pr{\n A \vert b}  & &   (\mbox{chain rule}) \\
\end{align*}

Now let's introduce some abbreviations:
\begin{align*}
& = \underbrace{\pr{a\vert A}}_k \underbrace{\pr{A \vert b}}_x + \underbrace{\pr{a \vert \n A}}_t \underbrace{\pr{\n A \vert b}}_{(1- x)}
\end{align*}

\noindent Note that the assumption that $BF_A\geq 1$ entails, by Lemma \ref{lem:BFLR}, that $k \geq t$, and so $k-t \geq 0$. Also, since $x$ is a probability, we know $0 \leq x \leq 1$. This allows us to reason algebraically as follows:

\begin{align*}
k & \geq k  \\
k & \geq t + (k - t) \\
k & \geq t + (k -t)x \\
k & \geq kx + t  - tx \\
\pr{a \vert A} = k & \geq kx + t(1-x) = \pr{a \vert b}
\end{align*}

For the second conjunct, notice that we have a similar reasoning, albeit it relies on a different pair of independencies (which nevertheless holds in \textsf{DAG1} and \textsf{DAG 2}).
\begin{align*}
\pr{b \vert a} & = \pr{b \et B \vert a} + \pr{b \et \n B \vert a} & &   (\mbox{total probability}) \\
& = \underbrace{\pr{b \vert a \et B}}_{\pr{b \vert B}  \mbox{\footnotesize \, by \eqref{eq:I3a2}}} \pr{B \vert a} +
\underbrace{\pr{b \vert a \et \n B}}_{\pr{b \vert \n B} \mbox{\footnotesize \, by \eqref{eq:I3b2}}}\pr{\n B \vert a}  & &   (\mbox{chain rule}) \\
\end{align*}

\noindent The rest of the reasoning for this case is algebraically the same as the one used above.
\end{proof}

\hypertarget{bayes-factor-simulations}{%
\subsection*{Bayes factor: simulations}\label{bayes-factor-simulations}}
\addcontentsline{toc}{subsection}{Bayes factor: simulations}

Computer simulations provide additional insights. The joint \(BF_AB\)
may be lower than the individual \(BF_A\) and \(BF_B\). Simulated cases
in which \(BF_{AB} < BF_{A}, BF_{B}\) are about 25\% of the total (which
is twice higher than for the likelihood ratio; more o this later). The
structure of such cases is visualized in Figure \ref{fig:BFfails}.

\vspace{1mm}
\footnotesize

\normalsize

\vspace{1mm}
\footnotesize

\normalsize

\begin{figure}

\begin{center}\includegraphics[width=0.8\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-15-1} \end{center}
\caption{25k cases (out of simulated 100k) in which the joint BF is below each of the individual BFs.}
\label{fig:BFfails}
\end{figure}

\noindent This result should not be surprising. It happens when \(BF_A\)
and \(BF_B\) are lower than one. When they are greater than one, the
joint \(BF_{AB}\) exceeds the individual ones. The distribution of Bayes
factors based on \textsf{DAG1} are displayed in Figure \ref{fig:DAG1BF}.
Interestingly, the distribution is unchanged under \textsf{DAG2} (Figure
\ref{fig:BFind2}).

These simulations and the earlier theorem demonstrate that, whenever
individual \(BF_A\) and \(BF_B\) are above a fixed threshold, so is the
combined \(BF_{AB}\). This fact justifies the principle of aggregation,
as explained in the main text of the chapter. However, the converse does
not hold. Whenever \(BF_{AB}\) is above a threshold, \(BF_A\) or
\(BF_B\) may be below the threshold. Such cases for \textsf{DAG1} and
\textsf{DAG2} are displayed in Figure \ref{fig:BFdistr}. Hence, the
converse of aggregation, the principle of distribution, fail in some
cases.

\begin{figure}

\begin{center}\includegraphics[width=0.8\linewidth]{conjunction-appendix14_files/figure-latex/BFind-1} \end{center}
\caption{Distances of the joint Bayes factor from maxima and minima of individual Bayes factors, depending on whether the individual support levels are both positive or both negative. Simulation based on 100k Bayesian networks build over the DAG of \textsf{DAG1}.}
\label{fig:DAG1BF}
\end{figure}

\begin{figure}

\begin{center}\includegraphics[width=0.75\linewidth]{conjunction-appendix14_files/figure-latex/BFind2-1} \end{center}

\caption{Distances of the joint Bayes factor from maxima and minima of individual Bayes factors, depending on whether the individual support levels are both positive or both negative. Simulation based on 100k Bayesian networks build over \textsf{DAG2}.}
\label{fig:BFind2}
\end{figure}

\vspace{1mm}
\footnotesize

\normalsize

\begin{figure}

\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/plotBFDistr-1} \end{center}
\caption{Distribution failure for the Bayes factor, \textsf{DAG 1}. The $x$ axis restricted to $(-3,15)$ for visibility.}
\label{fig:BFdistr}
\end{figure}

\hypertarget{likelihood-ratio-proofs}{%
\subsection*{Likelihood ratio: proofs}\label{likelihood-ratio-proofs}}
\addcontentsline{toc}{subsection}{Likelihood ratio: proofs}

We now turn to the likelihood ratio. For ease of reference, we use the
following abbreviations:

\begin{align*}
LR_{AB} &= \frac{\pr{a\wedge b \vert a\wedge B}}{\pr{a \wedge b \vert \neg (A\wedge B)}}\\
LR_A & = \frac{\pr{a \vert A}}{\pr{a \vert \n A}} \\
LR_B & = \frac{\pr{b \vert B}}{\pr{b \vert \n B}}.
\end{align*}

\begin{fact} If independence conditions  \eqref{eq:I4}, \eqref{eq:I4a}, \eqref{eq:I4b},   \eqref{eq:I4c},  \eqref{eq:I5},   \eqref{eq:I5a},    \eqref{eq:I5b}, and   \eqref{eq:I5c}    hold, then:
\begin{align*}
LR_{AB} & =  \frac{\pr{a \vert A} \times \pr{b \vert B}}
 {\frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} }}
\end{align*}
\end{fact}

\noindent Note that these independence assumptions are entailed not only
in \textsf{DAG1}, but also in \textsf{DAG2}.

\begin{proof}
Let's first compute the numerator of $LR_{AB}$:

\begin{align*}
\pr{a \wedge b\vert A\wedge B} & =  \frac{\pr{A \et B \et a\et b}}{\pr{A \et B}}
&\mbox{(conditional probability)}
\\
&= \frac{   \pr{A} \times \pr{B\vert A} \times \pr{a \vert A \wedge B} \times \pr{b \vert A \wedge B \wedge a} }{\pr{A} \times \pr{B \vert A}}
&\mbox{(chain rule)}
\end{align*}

We deploy the relevant independencies as follows:
\begin{align*}
\mbox{      } &= \frac{   \pr{A} \times \pr{B\vert A} \times \overbrace{\pr{a \vert A \wedge B}}^{\pr{a \vert A} \mbox{ \footnotesize \, by \eqref{eq:I4} } } \times \overbrace{\pr{b \vert A \wedge B \wedge a}}^{\pr{b \vert B} \mbox{ \footnotesize \, by \eqref{eq:I5} }} }{\pr{A} \times \pr{B \vert A}}
&\mbox{}\\
 & = \pr{a \vert A} \times \pr{b \vert B} 
 &\mbox{(algebraic manipulation)} 
\end{align*}


\noindent The denominator of $LR_{AB}$ is more complicated, mostly because of the conditioning on  $\neg (A \wedge B)$.

\scalebox{.85}{\parbox{1\linewidth}{
\begin{align*}
\pr{a \et b\vert \neg (A\et B)} & = \frac{\pr{a \et b \et \neg (A\et B)}}{\pr{\neg (A \et B)}} 
&\mbox{ (conditional probability)}\\
& = \frac{\pr{a \et b \et \neg A\et B} +  \pr{a \et b \et A\et \neg B} + \pr{a \et b \et \neg A\et \neg B}  }{\pr{\neg A \et B} + \pr{A \et \neg B} + \pr{\neg A \et \neg B} } 
&\mbox{ (logic \& additivity)}
\end{align*}
}}



\noindent Now consider the first summand from the numerator:
\begin{align*}
\pr{a \et b \et \neg A\et B} & = \pr{\n A} \pr{B \vert \n A} \pr{a \vert \n A \et B} \pr{b\vert a \et \n A \et B} &\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (chain rule)} \\ & = 
\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B}
&\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (independencies \eqref{eq:I4a} and \eqref{eq:I5a})} \\
\end{align*}

The simplification of the other two summanda is analogous (albeit with slightly different independence assumptions---\eqref{eq:I4b} and \eqref{eq:I5b} for the second one and \eqref{eq:I4c} and \eqref{eq:I5c} for the third. Once we plug these into the denominator formula we get:

\scalebox{.8}{\parbox{1\linewidth}{
\begin{align*}
\pr{a \et b\vert \neg (A\et B)} & = \frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} } \\
 & = \frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} }  
 \end{align*}}}

\end{proof}

\hypertarget{likelihod-ratio-simulations}{%
\subsection*{Likelihod ratio:
simulations}\label{likelihod-ratio-simulations}}
\addcontentsline{toc}{subsection}{Likelihod ratio: simulations}

In the case of the likelihood ratio, the analytic approach is
cumbersome. Instead, we most rely on computer simulations. First of all,
the joint \(LR_{AB}\) can be lower than any of the individual \(LR_A\)
and \(LR_B\). Based on \textsf{DAG1} and \textsf{DAG2}, the frequency of
such cases in which \(LR_{AB} < LR_{A}, LR_{B}\) is about 12.5\%, half
the frequency for the Bayes factor. The distribution of these cases is
displayed in Figure \ref{fig:LRfails}.

\begin{figure}

\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-17-1} \end{center}
\caption{12.5k cases (out of simulated 100k) in which the joint BF is below each of the individual BFs.  The picture for \textsf{DAG2} is very similar.}
\label{fig:LRfails}
\end{figure}

Consider now cases in which both individual likelihood ratios are above
one. Interestingly, one of the individual likelihood ratios, \(LR_A\) or
\(LR_B\), may be greater than the joint \(LR{AB}\) (see example in
Figure \ref{tab:CPTconjunctionBNL}). This does not happen with the Bayes
factor. But even though the joint likelihood ratio can be lower than the
maximum, it is never lower than the minimum of the individual likelihood
ratios (Figures \ref{fig:LRabovePlot} and \ref{fig:LRabovePlotDep}).
Conversely, if both both individual likelihood ratios are below one, the
joint likelihood ratio can be higher than their minimum, but is never
higher than their maximum (Figures \ref{fig:LRlowerPlot} and
\ref{fig:LRlowerPlot2}).

\vspace{1mm}
\footnotesize

\normalsize

\vspace{1mm}
\footnotesize

\normalsize

\begin{figure}
\begin{subfigure}[!ht]{0.45\textwidth}

\footnotesize 
\begin{tabular}{lr}
\toprule
A & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.892}\\
0 & 0.108\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{B} & \multicolumn{2}{c}{A} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.551} & \cellcolor{gray!6}{0.457}\\
0 & 0.449 & 0.543\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{a} & \multicolumn{2}{c}{A} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.957} & \cellcolor{gray!6}{0.453}\\
0 & 0.043 & 0.547\\
\bottomrule
\end{tabular}


\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{b} & \multicolumn{2}{c}{B} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.678} & \cellcolor{gray!6}{0.573}\\
0 & 0.322 & 0.427\\
\bottomrule
\end{tabular}

\vspace{2mm}

\normalsize
\end{subfigure} \begin{subfigure}[!ht]{0.45\textwidth}

\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-21-1} \end{center}
\end{subfigure}
\caption{$LR_A  \approx 2.11$, $LR_B \approx 1.183$,  $LR_{AB} \approx 1.319$. \newline  $BF_A \approx  1.06, BF_B \approx  1.076, BF_{AB}\approx   1.14$.}
\label{tab:CPTconjunctionBNL}
\end{figure}

\begin{figure}


\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-22-1} \end{center}

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are above 1, DAG used in \textsf{DAG1}.}
\label{fig:LRabovePlot}
\end{figure}

\begin{figure}


\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-23-1} \end{center}

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are above 1, DAG used in \textsf{DAG2}.}
\label{fig:LRabovePlotDep}
\end{figure}

\begin{figure}


\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-24-1} \end{center}

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are below 1, DAG used in \textsf{DAG1}.} 
\label{fig:LRlowerPlot}
\end{figure}

\begin{figure}


\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-25-1} \end{center}

\caption{Distances of joint likelihood ratios for the minima and the maxima of the individual likelihood ratios if the individual likelihood ratios are below 1, DAG used in \textsf{DAG2}.} 
\label{fig:LRlowerPlot2}
\end{figure}

\hypertarget{keeping-evidence-fixed}{%
\subsection{Keeping evidence fixed}\label{keeping-evidence-fixed}}

\hypertarget{dependent-evidence}{%
\subsection*{Dependent evidence}\label{dependent-evidence}}
\addcontentsline{toc}{subsection}{Dependent evidence}

Both \textsf{DAG 1} and \textsf{DAG 2} ensures that the items of
evidence are conditionally independent on their respective hypothesis
(specifically, that \(a\) is independent both conditional on \(A\) and
conditional on \(\n A\), and the same for \(b\) and \(B\).).\footnote{In
  fact, \textsf{DAG 1} ensure that they are also unconditionally
  independent.} The results established so far, then, rests of this
assumption. What happens if this independence is dropped? To investigate
this question, we run a simulation based on \textsf{DAG 3}, illustrated
in Figure \ref{fig:dag3}.

\vspace{1mm}
\footnotesize

\normalsize

\begin{figure}[H]


\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-27-1} \end{center}
\caption{\textsf{DAG 3} with direct dependence between the pieces of evidence.}
\label{fig:dag3}
\end{figure}

In fact, it turns out that simultaneously the joint likelihood ratio is
lower than both individual likelihood ratios and the joint Bayes factor
is lower than each individual Bayes factor in 14\% cases in which the
individual Bayes factor (and therefore also the individual likelihood
ratios) are greater than one. One particular counterexample is
illustrated in Figure \ref{fig:CPTDoubleL}.

\begin{figure}
\hspace{2mm}\begin{subfigure}[ht!]{0.45\textwidth}
\footnotesize


\begin{tabular}{lr}
\toprule
A & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.521}\\
0 & 0.479\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{B} & \multicolumn{2}{c}{A} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.66} & \cellcolor{gray!6}{0.729}\\
0 & 0.34 & 0.271\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lllr}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{A} & \multicolumn{1}{c}{b} & \multicolumn{1}{c}{} \\
a &  &  & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.3989398}\\
0 & 1 & 1 & 0.6010602\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.9673984}\\
0 & 0 & 1 & 0.0326016\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0.9693564}\\
0 & 1 & 0 & 0.0306436\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0.7267025}\\
0 & 0 & 0 & 0.2732975\\
\bottomrule
\end{tabular}

\vspace{2mm}

\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{b} & \multicolumn{2}{c}{B} \\
  & 1 & 0\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0.995} & \cellcolor{gray!6}{0.108}\\
0 & 0.005 & 0.892\\
\bottomrule
\end{tabular}

\normalsize 


\subcaption{Conditional probabilities for the counterexample (the one for \textsf{AB} does not change).}
\end{subfigure} 
\hspace{5mm}\begin{subfigure}{0.45\textwidth}

\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-29-1} \end{center}
\subcaption{Marginal probabilities. }
\end{subfigure} 
\caption{A counterexample based on \textsf{DAG 3}, with independence between the items of evidence dropped.   $LR_A  \approx 1.063$, $LR_B \approx 1.159742$,  $LR_{AB} \approx 0.651$. $BF_A \approx  1.022, BF_B \approx  1.079, BF_{AB}\approx   0.699$.}
\label{fig:CPTDoubleL}
\end{figure}

\begin{figure}

\begin{center}\includegraphics[width=1\linewidth]{conjunction-appendix14_files/figure-latex/unnamed-chunk-31-1} \end{center}
\caption{Aggregation failure.} 
\label{fig:aggregationFails}
\end{figure}

\mar{R: Two new passages on fixed joint evidence, check.}

Finally, you might recall that in the chapter we distinguished two
variants of the distribution requirement, (DIS1) and (DIS2). The former
has been in the focus so far, and the latter differs in keeping the
evidence fixed at \(a \et b\), even when calculating individual LRs. So,
sticking to LR, distribution in this variant pertains to the joint LR
and the following two ratios:
\(\nicefrac{\pr{a\et b \vert A}}{\pr{a \et b \vert \n A}}\) and
\(\nicefrac{\pr{a\et b \vert B}}{\pr{a \et b \vert \n B}}\). To make
sure that switching to (DIS2) doesn't make things easier for legal
probabilists, we ran 30k simulations (10k for each BN type), and
inspected the status of aggregation and distribution for these ratios.

Here are the results. If no assumption about the direction of support is
made, around 12.7\% of the time (around twice less often than if the
usual individual LRs are used), the individual LRs with fixed evidence
are both greater than the joint LR---this is for \textsf{DAG1}, the
frequency goes slightly up to around 13\% if we switch to \textsf{DAG2}
and is around the same value if additionally we allow for the dependency
between the items of evidence (\textsf{DAG3}). Assuming individual LRs
are above one, around 70\% of joint likelihoods (75\% for BN2, 72\% for
BN3) are between the individual ones, no joint LR is below the minimum
of the individual ratios for \textsf{DAG1}, but is so around 2\% times
for both \textsf{DAG2} and \textsf{DAG3}. This is one important
difference: even aggregation can fail if dependencies are present, if we
keep joint evidence fixed in all the ratios. As for distribution, there
are no major surprises: around 30\% of the time, the joint LR is
strictly greater than both of the individual LRs with evidence fixed for
\textsf{DAG1}, 22\% for \textsf{DAG2}, and 26\% for \textsf{DAG3}. In
short, keeping the joint evidence fixed across all ratios makes things
even harder, when it comes to preserving aggregation and distribution.

.

\end{document}
