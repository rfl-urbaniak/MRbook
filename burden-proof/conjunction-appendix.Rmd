---
title: "Appendix: confirmation measures and the difficulty about conjunction"
author: "Marcello Di Bello and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        - Rafal_latex7.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: [../references/referencesMRbook.bib]
csl: [../references/apa-6th-edition.csl]
indent: true
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(bnlearn)
library(Rgraphviz)
library(gRain)
library(kableExtra)
library(dagitty)
library(rethinking)
```



One assumption often made in the formulation 
of the conjunction paradox is that claims $A$ and $B$ are probabilistically independent. This is not always the case---we have seen that the paradox does subside even if the two claims are dependent. However, two fairly natural set-ups for conjunctive hypotheses and evidence supporting them  (Figure \ref{fig:conjunctionBNs}) indeed do have some independencies built in, and so it is also natural what can be said about the conjunction problem given these independencies. Moreover, in some context, we will be freely using the independence assumptions, as a counterexample to a general claim still remains one even if it satisfies an additional requirement, that is, independence conditions.





We will be considering the conjunction of two hypotheses, $A$ and $B$, their respective pieces of evidence $a$ and $b$, and their conjunction $AB$, in two set-ups, illustrated by the Bayesian networks
shown in Figure \ref{fig:conjunctionBNs}. The key difference here is that we allow direct dependence between the hypotheses in the second network. In both Bayesian networks, the CPT for the conjunction trivially mirrors the one for conjunction, as in Table  \ref{tab:CPTconjunction2}.




\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
conjunctionDAG <- model2network("[a|A][b|B][AB|A:B][A][B]")
daggityConjunctionDag <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> AB
        B -> AB
      }")


As <- runif(1,0,1)
Bs <- runif(1,0,1)

aifAs <-runif(1,0,1)
aifnAs <- runif(1,0,1)
bifBs <-runif(1,0,1)
bifnBs <- runif(1,0,1)


AProb <-prior.CPT("A","1","0",As)
BProb <- prior.CPT("B","1","0",Bs)
aProb <- single.CPT("a","A","1","0","1","0",aifAs,aifnAs)
bProb <- single.CPT("b","B","1","0","1","0",bifBs,bifnBs)


ABProb <- array(c(1, 0, 0, 1, 0, 1, 0,1), 
                dim = c(2, 2, 2),
                dimnames = list(AB = c("1","0"),
                                B = c("1","0"), 
                                A = c("1","0")))

conjunctionCPT <- list(A = AProb, B = BProb, 
                       a = aProb, b = bProb, AB = ABProb)


conjunctionBN <- custom.fit(conjunctionDAG,conjunctionCPT)

conjunctionDAG2 <- model2network("[a|A][b|B][AB|A:B][A][B|A]")

daggityConjunctionDag2 <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> B
        A -> AB
        B -> AB
      }")

```
\normalsize


\begin{figure}[H]
\hspace{2mm}\scalebox{1}{\begin{subfigure}[!ht]{0.45\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300}
#graphviz.plot(conjunctionDAG, layout = "dot")
coordinates(daggityConjunctionDag) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                            y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityConjunctionDag, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 5)
```
\subcaption{\textsf{BN1}}
\end{subfigure}} 
\hspace{5mm}\begin{subfigure}[!ht]{0.45\textwidth}
\vspace{1mm}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", dpi = 300}
coordinates(daggityConjunctionDag2) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                            y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityConjunctionDag2, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 5)
```
\subcaption{\textsf{BN2}}
\end{subfigure}
\normalsize
\caption{Two Bayesian networks for the conjunction problem.}
\label{fig:conjunctionBNs}
\end{figure}





\begin{table}[h]
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
CPkable2("conjunctionBN","AB") %>%   
                          kable_styling(latex_options=c("striped","HOLD_position")) 
```
\normalsize
\caption{Conditional probability table for the conjunction node.}
\label{tab:CPTconjunction2}
\end{table}

\newpage 
\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=FALSE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
bn1Ind <- impliedConditionalIndependencies(daggityConjunctionDag, type  = "all.pairs")

bn2Ind <- impliedConditionalIndependencies(daggityConjunctionDag2, type  = "all.pairs")
```
\normalsize




Directed Acyclic Graphs (DAGs) are useful for representing graphically these relationships of independence. 
The edges, intuitively, are meant to capture direct influence between the nodes. The role that such direct influence plays is that in a Bayesian network built over a DAG any node is conditionally independent of its nondescentants (including ancestors), given its parents. If this is the case for a given probabilistic measure $\pr{}$ and a given DAG, we say that $\pr{}$ is compatible with $\mathsf{G}$, and they can be put together to constitute a Bayesian network. 

The graphical counterpart of probabilistic independence is the so-called \textbf{d-separation}, $\indep_d$.  We say that two nodes, $X$ and $Y$, are d-separated given a set of nodes $\mathsf{Z}$---$X\indep_d Y \vert \mathsf{Z}$ --- iff for every undirected path from $X$ to $Y$ there is a node $Z'$ on the path such that either:
\begin{itemize} 
\item $Z' \in \mathsf{Z}$ and there is a \emph{serial} connection, $\rightarrow Z' \rightarrow$, on the path,
\item  $Z'\in \mathsf{Z}$ and there is a diverging connection, $\leftarrow Z' \rightarrow $, on the path,
\item There is a connection $\rightarrow Z' \leftarrow$ on the path, and neither $Z'$ nor its descendants are in $\mathsf{Z}$.
\end{itemize}

\vspace{1mm}

Finally, two sets of nodes, $\mathsf{X}$ and $\mathsf{Y}$, are d-separated given $\mathsf{Z}$ if every node in $\mathsf{X}$ is d-separated from every node in $\mathsf{Y}$ given $\mathsf{Z}$. With serial connection, for instance, if:

\footnotesize 
\begin{center}
\begin{tabular}{@{}lp{4.3cm}@{}}\toprule
Node & Proposition \\ \midrule 
$G$ & The suspect is guilty. \\
$B$ & The blood stain comes from the suspect.\\
$M$ & The crime scene stain and the suspect's blood share their DNA profile.\\
\bottomrule
\end{tabular}
\end{center}
\normalsize
\noindent We naturally would like to have the connection $G \rightarrow B \rightarrow M$. If we don't know whether $B$ holds, $G$ seems to have an indirect impact on the probability of $M$. Yet, once we find out that $B$ is true, we expect the profile match, and whether $G$ holds has no further impact on the probability of $M$.


Take an example of a diverging connections.  Say you have two coins, one fair, one biased. Conditional on which coin you have chosen, the results of subsequent tosses are independent. But if you don't know which coin you have chosen, the result of previous tosses give you some information about which coin it is, and this has impact on your estimate of the probability of heads in the next toss. Whether a coin is fair, $F$ or not has an impact on the result of the first toss, $H1$, and on the result of the second toss, $H2$.  So $H1 \leftarrow F \rightarrow H2$ seems to be appropriate. Now, on one hand, as long as we don't know whether $F$, $H1$ increases the probability of $H2$.  On the other, once we know that $F$, though, $H1$ and $H2$ become independent, and so conditioning on the parent in a fork makes its childern independent (provided there is no other open path between them in the graph).



For converging connections, let  $G$ and $B$ be as above, and let:
\begin{center}
\begin{tabular}{@{}lp{4.3cm}@{}}\toprule
Node & Proposition \\ \midrule 
$O$ & The crime scene stain comes from the offender.\\
\bottomrule
\end{tabular}
\end{center}
\normalsize

\noindent Both $G$ and $O$ influence $B$. If he's guilty, it's more likely that the blood stain comes from him, and if the blood crime stain comes from the offender it is more likely to come from the suspect (for instance, more so than if it comes from the victim). Moreover, $G$ and $O$ seem independent -- whether the suspect is guilty doesn't have any bearing on whether the stain comes from the offender. Thus, a converging connection $G\rightarrow B \leftarrow O$ seem appropriate. However, if you do find out that $B$ is true, that the stain comes from the suspect, whether the crime stain comes from the offender becomes relevant for whether the suspect is guilty. 

One important reason why d-separation matters is that it can be proven that if two sets of nodes are d-separated given a third one, then they are independent given the third one, for any probabilistic measures compatible with a given DAG. Interestingly, lack of d-separation doesn't entail dependence for any probabilistic measure compatible with a given DAG. Rather, it only allows for it: if nodes are d-separated, there is at least one probabilistic measure fitting the DAG according to which they are independent.   So, at least, no false  independence can be inferred  from the DAG, and  all the dependencies are built into it.

Now, getting back to the conjunction problem,  the   d-separations entailed by these networks differ (examples can be found in Table \ref{tab:indepBNS})---in fact, \textsf{BN1} entails 31 d-separations, while \textsf{BN2} entails 22 of them.  Attention should be paid to the notation. In the above, variables represent nodes, and so each d-separation  entails a   probabilistic statement about  all combination of the  node states involved. For instance, assuming each node is binary with two possible states, 1 and 0, \mbox{$B   \indep_d\,\,  a $}  entails that for any \mbox{$ B_i, a_i \in \{0, 1\}$} we have $\pr{B = B_i} = \pr{B = B_i \vert a = a_i}$. 






\begin{table}[h]
\begin{tabular}{cr}
\toprule
Bayesian network 1  & Bayesian network 2\\
\midrule
\cellcolor{gray!6}{$A   \indep_d\,\, B  $}&\cellcolor{gray!6}{     $ A  \indep_d\,\, b \vert  B  $} \\
$A   \indep_d\,\, b  $& $AB  \indep_d\,\,  a \vert  A$\\
\cellcolor{gray!6}{$\,\,\, AB  \indep_d\,\, a \vert A $}  & \cellcolor{gray!6}{$AB  \indep_d\,\,  b \vert  B $}\\
$\,\,\, AB  \indep_d\,\, b \vert B  $ & $ B  \indep_d\,\,  a \vert  A $\\
\cellcolor{gray!6}{$B   \indep_d\,\, a $}        & \cellcolor{gray!6}{$a  \indep_d\,\,  b \vert  B$ }\\
$\,\, a    \indep_d\,\, b$    & $a  \indep_d\,\,  b \vert  A $ \\
\bottomrule
\end{tabular}
\caption{Some of d-separations entailed by \textsf{BN1} and \textsf{BN2} in the conjunction problem. One minimal testable implication (with the smallest possible conditioning set) is returned per missing edge of the graph.} 
\label{tab:indepBNS}
\end{table}

In what follows, however, we will sometimes use a finer level of granularity, being very explicit on what independence assumptions are used in the derivations. In such contexts, we will be talking about states rather than nodes, and so when we present the derivation, \mbox{$b\indep A \et a \vert \n B$} is a claim about events (or propositions) means  the same as  $\pr{b = 1 \vert B = 0}   = \pr{b = 1 \vert A = 1, a = 1, B = 0}$. This distinction matters, as, first,  independence conditional on $B= 0$ doesn't entail independence given $B=1$ (for instance, your final grade might depend on how hard you work if the teacher is fair, but this might fail if the teacher is not fair), and, second, sometimes only some of the independencies entailed by a Bayesian network will be actually required for a given claim to hold, and we want to be explicit about such cases. We hope this slight ambiguity in notation will cause no confusion, as whether we talk about nodes or events will be clear from the context.  So, moving to events, here is a list of independence claims used in the arguments that follow.  We also marked whether they are entailed by the Bayesian networks under consideration.

\begin{align} A\indep B  \label{eq:indAB}     &\hspace{2cm}\mbox{\footnotesize BN1}\\
b \indep a   \label{eq:indab}   & \hspace{2cm}\mbox{\footnotesize BN1}\\
A \indep b \vert a   \label{eq:I1}    &\hspace{2cm}\mbox{\footnotesize BN1} \\
B \indep a \et A \vert b \label{eq:I2}&\hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\
a\indep b \vert A\et B \label{eq:I3}  &\hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\  
a\indep b \vert A \label{eq:I3a}   &\hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\ 
a\indep B \vert A \label{eq:I4}    & \hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\
a\indep B \vert \n A \label{eq:I4a}    & \hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\
a\indep \n B \vert A \label{eq:I4b}   & \hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\
a\indep \n B \vert \n A \label{eq:I4c}   & \hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\
b\indep A \et a \vert B \label{eq:I5}  & \hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\
b\indep \n A \et a \vert B \label{eq:I5a} &\hspace{2cm}\mbox{\footnotesize BN1 , BN2}  \\
b\indep A \et a \vert \n B \label{eq:I5b} & \hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\
b\indep \n A \et a \vert \n B \label{eq:I5c} & \hspace{2cm}\mbox{\footnotesize BN1 , BN2} \\
b \indep a \vert B \label{eq:I6} & \hspace{2cm}\mbox{\footnotesize BN1 , BN2} 
\end{align}

\raf{Double check if we in fact use I3.}


We will start with the Bayes factor $\pr{E \vert H}/\pr{E}$ and its relation to conjunction. First, given CONDITIONS, if separate BFs are greater than 1, then so is the joint BF.

\begin{fact} Suppose   $a$ and $b$ positively support $A$ and $B$, that is:
\begin{align} BF_A  & =  \frac{\pr{a \vert A}}{\pr{a}} >1 \\  
BF_B & = \frac{\pr{b \vert B}}{\pr{b}=s_B} = >1.
\end{align}Then, if
CONDITIONS, it follows that:
\begin{align}BF_{AB} >1 & = \frac{\pr{a \wedge b\vert A\wedge B}}{\pr{a \wedge b}} >1. 
\end{align}
\end{fact}


\begin{proof}
\begin{align*}
\frac{\pr{a \wedge b\vert A\wedge B}}{\pr{a \wedge b}} & =  \frac{\pr{A \et B\vert a\wedge b}}{\pr{A \et B}}
&\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (Bayes's theorem)}
\\
& =  \frac{\frac{ \pr{A \et B \et a \wedge b }}{\pr{a \et b}}}{\pr{A \et B}} 
&\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (definition of conditional probability)}\\ 
& =  \frac{\frac{ \pr{A} \times \pr{B\vert A} \times \pr{a \vert A \wedge B} \times \pr{b \vert A \wedge B \wedge a}}{\pr{a} \times \pr{b \vert a}}}{\pr{A \et B}} 
&\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (chain rule)}
\\ 
& = \frac{\frac{ \pr{A} \times \pr{B\vert A} \times \pr{a \vert A \wedge B} \times \pr{b \vert A \wedge B \wedge a}}{\pr{a} \times \pr{b \vert a}}}{\pr{A} \times \pr{B}}  
&\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (independencies in Figure \ref{fig:conjunctionDAG})} \\
& = \frac{\frac{\pr{A} \times \pr{B} \times \pr{a \vert A} \times \pr{b \vert B}}{\pr{a} \times \pr{b}}}{\pr{A} \times \pr{B}} 
&\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (independencies in Figure \ref{fig:conjunctionDAG})}\\ 
& =  \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b}} 
&\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\, (algebraic manipulation)} \\
BF_{AB}& =  BF_{A}\times BF_{B} 
 \end{align*}






\end{proof}




 The step marked by asterisk rests on the 
independence assumptions codified in Figure \ref{fig:conjunctionDAG}, namely: \eqref{eq:indAB}, \eqref{eq:I4}, and \eqref{eq:I5}.]
\ali{M: Check reference to independence assumptions}
\begin{align*}
\frac{\pr{a \wedge b \vert A \wedge B}}{\pr{a \wedge b}} &= \frac{\pr{a \vert A}}{\pr{a}} \times \frac{\pr{b \vert B}}{\pr{b}}\\
BF_{AB} &= BF_{A} \times BF_{B}\\
\end{align*}






