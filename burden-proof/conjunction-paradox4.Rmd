---
title: "Chapter 13: The Difficulty With Conjunction"
author: "Marcello Di Bello and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        - Rafal_latex7.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: [../references/referencesMRbook.bib]
csl: [../references/apa-6th-edition.csl]
indent: true
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(bnlearn)
library(Rgraphviz)
library(gRain)
library(kableExtra)
library(dagitty)
library(rethinking)
library(tidyverse)
library(ggpubr)
```


<!-- \tableofcontents -->


The standard of proof in criminal and civil trials is a criterion of decision. If the evidence meets the standard of proof, the defendant should be judged criminally or civilly liable, and otherwise the accusation should be dismissed. <!--- But when does the evidence meet the standard of proof and thus warrant a judgement of liability? ---> According to many legal probabilists, a verdict against the defendant is warranted in case the evidence establishes, with a probability above a suitable threshold, that the defendant is criminally or civilly liable [see @Bernoulli1713Ars-conjectandi; @Laplace1814;  @Kaplan1968decision;  @kaye79; @Dekay1996; @laudan2006truth]. In criminal cases, the bar is  high, so the threshold will be, say, 'at least .95 probability'. In civil cases,  the bar is lower, so the threshold will be, say, 'at least .5 probability'.  


This interpretation of the standard of proof is simple and elegant. Chapters 11 \todo{REFER TO EARLIER CHAPTER} showed how the .95 and .5 thresholds can be justified within the framework of statistical decision theory. But, unfortunately,  this interpretation is not without problems. Chapter 12 \todo{REFER TO EARLIER CHAPTER} examined a first theoretical problem: the paradox of naked statistical evidence. The present chapter examines a second theoretical problem:  \textbf{the difficulty with conjunction}, also known as \textbf{the conjunction paradox}.  Section \ref{sec:difficulty} describes in some detail the contours of the problem. Sections \ref{sec:strength} and \ref{sec:comparative} articulate two broad strategies that legal probabilists have pursued as a response to this difficulty. These strategies are promising and worth examining, but we show that they are ultimately unsatisfactory. Sections \ref{sec:reject} and \ref{sec:proposal}
put forward our own proposal. 

Our proposal for addressing the difficulty with conjunction complements our solution to the problem of naked statistical evidence. Both problems underscore an important fact about trial proceedings. Even though, on a general level, the two sides in a trial disagree about whether the defendant is guilty (or civilly liable), their disagreement is more fine-grained than that. The two sides disagree about a case-specific story or account tailored to the individual defendant. Conclusions about the defendant's guilt (or civil liability) are then reached via an assessment of the truth of these more factually specific propositions. This perspective affords a better understanding of how standards of proof operate in a trial. 



# Formulating the difficulty
\label{sec:difficulty}

First formulated  by @Cohen1977The-probable-an, the difficulty with conjunction has enjoyed a great deal of scholarly attention every since [@Allen1986A-Reconceptuali; @dawid1987difficulty; @Stein05;  @haack2011legal; @cheng2012reconceptualizing; @clermont2012aggregation; @allen2013; @spottswood2016; @schwartz2017ConjunctionProblemLogic; @AllenPardo2019relative]. 
This difficulty arises when a claim of wrongdoing, in a civil or criminal proceeding, 
is broken down into its constituent elements. For example, suppose the prosecution should establish two claims: first, that the defendant caused the victim's death; and second, that the defendant's action was  premeditated.   By the probability calculus, whenever two claims $A$ and $B$ are independent, the probability of their conjunction $\pr{A\wedge B}$ is the product of the individual probabilities, $\pr{A}\times \pr{B}$. The probability of the conjunction should therefore be lower than the probability of the independent conjuncts so long as these do not have probability one. So, even when the probability of each conjunct is a above a suitable threshold, the probability of the conjunction might be below the threshold. This is an undisputed fact about probability. But, once the standard of proof is equated to a probability threshold, it follows that establishing each individual conjunct by the required standard of proof need not be enough to establish the overall claim of wrongdoing (the conjunction)  by the required standard. 

@Cohen1977The-probable-an  and others after him believe that this outcome is counter-intuitive and runs contrary to trial practice.  More specifically, they believe that common law systems subscribe to a \textbf{conjunction principle}, according to which if two claims, $A$ and $B$, are established according to the governing standard of proof, so is their conjunction $A\wedge B$ (and \emph{vice versa}).  If the conjunction principle holds, the following must be equivalent: 

\begin{center}
\begin{tabular}
{@{}ll@{}}
\toprule
\textbf{Separate} &   $A$ is established by standard $S$ and $B$ is established by standard $S$\\   
\textbf{Overall}  &   The conjunction $A \et B$ is established by standard $S$  \\ 
\bottomrule
\end{tabular}
\end{center}

\noindent
If we generalize to more than just two constituent claims, the conjunction principle requires that:

\[S[C_1 \wedge C_2  \wedge \dots \wedge  C_k] \Leftrightarrow S[C_1] \wedge S[C_2]  \wedge \dots \wedge  S[C_k],\]

\noindent
where $S[C_i]$ means that claim or hypothesis $C_i$ is established according 
to standard $S$.

The principle goes in both directions. The implication from right to left, call it \textbf{aggregation},  posits that establishing the individual claims by the requisite standard is enough to establish the conjunction by the same standard.^[Aggregation bears some similarity to the multi-premise principle of epistemic closure. This principle reads: if a subject $S$ knows $A$ and knows  $B$, and also knows that $A$ and $B$ entail $C$ (for example, they trivially entail $A\wedge B$), then $S$  knows $C$. The phenomenon of risk accumulation in lottery scenarios is usually deployed to challenge the multi-premise principle of epistemic closure. Risk accumulation also challenges the conjunction principle. On this point, see the discussion later in Section \ref{sec:reject}.]  The implication in the opposite direction,
call it \textbf{distribution}, posits  that establishing the conjunction by the requisite standard is enough to establish the individual claims by the same standard.  The difficulty with conjunction is traditionally concerned with the failure of aggregation, but as we will see, on some probabilistic explications of the standard of proof, distribution can also fail.  


 We will eventually question and then reject the tenability of the conjunction principle (more on this in Section \ref{sec:reject} and \ref{sec:proposal}). <!---\mar{Cite here "The Conjunction Problem and the Logic of Jury Findings" by David S. Schwartz and Elliott Sober. Argues that (a) elements are usually dependent, (b) elements function as checklist and (c) jury instructions usually do not give rise to the conjunction paradox  because they do not follow the conjunction principle.}---> For the time being, however, let us assume the principle correctly captures two desired features of the standard of proof.  The principle has some degree of plausibility and is consistent with the case law. For example, the United States Supreme Court writes  that in criminal cases

\begin{quote}
the accused [is protected] against conviction except upon proof beyond a reasonable doubt of \textit{every fact} necessary to constitute the crime with which he is charged. \linebreak 
(In re Winship, 397 U.S. 358, 364, 1970)
\end{quote}

\noindent
A plausible way to interpret this quotation 
is to assume the following identity: to establish someone's guilt beyond a 
reasonable doubt \textit{just is} to establish each 
element of the crime beyond a reasonable doubt (abbreviated as $\mathsf{BARD}$). Thus, 
\begin{align*}\mathsf{BARD}[C_1 \wedge C_2   \wedge \cdots \wedge C_k] \Leftrightarrow \mathsf{BARD}[C_1] \wedge \mathsf{BARD}[C_2]  \wedge \cdots \wedge \mathsf{BARD}[C_k],
\end{align*}

\noindent
where the conjunction $C_1 \et C_2 \cdots \et C_k$ comprises all the material 
facts that, according to the applicable law, constitute the crime with with the accused is charged.
A similar argument could be run for the standard of proof in civil cases, preponderance 
of the evidence or clear and convincing evidence. 

<!--
One could quibble with this intepretation. Perhpas, what the law requires is only the left-to-right direction---that $\text{if }BARD[A \wedge B] \text{ then } BARD[A] \wedge BARD[B]$---not the right-to-left direction---that $\text{if } BARD[A] \wedge BARD[B] \text{ then }BARD[A \wedge B]$. The left-to-right direction is, indeed, the least controversial.\todo{discussion in terms of equation sides is confusing, figure out terms that are more meaningful} If the conjuction is established to the required standard, so should its conjuncts.\todo{This just repeats the claim, I don't know what job it does.} It would be odd if this was not the case. The other direction is more controversial.\todo{Why?} But it is plausible that one would establish the conjunction by establishing each conjunct one by one. As we shall see later, both directions can be questioned. At least, the conjunction principle has some initial plausibility. \todo{Weakened the claim, check}
-->

The problem for the legal probabilist is that the conjunction principle conflicts with a threshold-based probabilistic interpretation of the standard of proof.  For suppose the prosecution in a criminal case presents evidence that establishes claims $A$ and $B$, separately, by the required probability, say about $.95$ each.  If each claim is established by the requisite probability threshold, each claim is established by the requisite standard (assuming the threshold-based interpretation of the standard of proof). And if each claim is established by the requisite standard, then liability as a whole is established by the requisite standard (assuming the conjunction principle). And yet, if the claims are independent, the probability of their conjunction will be  only $.95\times .95 \approx .9$, below the required $.95$ threshold. So liability as a whole is \textit{not} established by the requisite standard (assuming a threshold-based  probabilistic interpretation of the standard). This contradicts the conjunction principle. Even though aggregation posits that establishing each conjunct by the required standard of proof is enough to establish the conjunction as a whole, the probability of each conjunct, in isolation, can meet the required  probability threshold without the conjunction as a whole meeting the threshold. 

This difficulty is  persistent. It does not subside as the number of constituent claims increases. If anything, the difficulty becomes more apparent. Say the prosecution has established three separate claims by $.95$ probability. Their conjunction---again if the claims are independent---would be about $.85$ probable, even further below the $.95$ probability threshold.  Nor does the difficulty with conjunction generally subside if the claims are no longer regarded as independent. The probability of the conjunction $A \et B$, without the assumption of independence, equals $\pr{A \vert B} \times \pr{B}$ or $\pr{B \vert A} \times \pr{A}$. But if claims $A$ and $B$, separately, are established with $.95$ probability, enough for each to meet the threshold, the probability of $A \et B$ should still be below 
the $.95$ threshold unless $\pr{A \vert B}$ or $\pr{B \vert A}$ equal one.^[For example, that someone premeditated a harmful act against another (call it \textsf{premed}) makes it more likely that they did cause harm in the end (call it \textsf{harm}). Since $\pr{\textsf{harm} \vert \textsf{premed}} > \pr{\textsf{harm}}$, the two claims are not independent. Still, premeditation does not always lead to harm, so $\pr{\textsf{harm} \vert \textsf{premed}}$ will often be below $1$. If both claims are established with $.95$, the probability of the conjunction $\textsf{harm} \et \textsf{premed}$ should still be below the $.95$ threshold so long as $\pr{\textsf{harm} \vert \textsf{premed}}$ is still below $1$.]


## Adding the evidence

So far we proceeded without mentioning the evidence in support of the claims that constitute the wrongdoing. This is a simplification. As we will see, it is crucial to pay attention to the supporting evidence. With this in mind,  the conjunction principle for two claims can be formulated, as follows:
\[\text{S[$a, A$] and S[$b, B$] $\Leftrightarrow$ S[$a \wedge b, A\wedge B$]}.\]

\noindent In the case of more than two claims, the formulation of the principle can be extended accordingly.  Note that  $a$ and $b$ denote the evidence for claims $A$ and $B$ respectively, and $S$ denotes the standard by which the evidence establishes the claim in question. So, for example, the expression S[$a, A$] should be read as 'evidence $a$ supports claim $A$ by standard $S$'. If we adopt the threshold interpretation of the standard of proof, the expression S[$a, A$] should be interpreted as 'evidence $a$ establishes claim $A$ with a probability above a suitable threshold $t$ corresponding to standard $S$' or in symbols $\pr{A \vert a}>t$.  An analogous probabilistic reading applies to the expressions S[$b, B$] and S[$a \wedge b, A\wedge B$]. 

Does a threshold-based probabilistic interpretation of the standard of proof also conflict with this revised version of the conjunction principle? The answer is positive, but seeing why requires a bit  more work.  We should check whether, if  both $\pr{A \vert a}$ and $\pr{B \vert b}$ meet the threshold, say $.95$, then so does $\pr{A\wedge B \vert a\wedge b}$. 
<!---Could the combined evidence $a\wedge B$ provide a boost to $A\wedge B$ so that $\pr{A\wedge B \vert a\wedge b}$ meets the threshold just when $\pr{A \vert a}$ and $\pr{B \vert b}$ do? --->
We are no longer just comparing the probability of $A\wedge B$ to the probability of $A$ and the probability of $B$ as such. Rather, we are comparing the probability of $A \wedge B$ given the combined evidence $a \wedge b$ to the probability of $A$ given evidence $a$ and the probability of $B$ given evidence $b$.
<!-- This comparison requires more laborious computations.  -->

We first start with a stylized example and then generalize to a large class of cases. Consider an aggravated assault case in which the prosecution should establish, first, that the defendant injured the victim, and second, that the defendant knew he was interacting with a public official. A witness testimony (call it \textsf{witness}) is offered in support of the proposition that the defendant injured the victim (call this proposition \textsf{injury}). In addition, the defendant's call to an emergency number (call it \textsf{emergency}) is offered as evidence for the proposition that the defendant knew he was interacting with a firefighter  (call this proposition \textsf{firefighther}). If $\pr{\textsf{injury} \vert \textsf{witness}}$ and $\pr{\textsf{firefighter} \vert \textsf{emergency} }$ both meet the required probability threshold, it does not necessarily follow that $\pr{\textsf{injury} \wedge \textsf{firefighter} \vert \textsf{witness} \wedge \textsf{emergency}}$ also meets the threshold.  To see why that is, we need to make two assumptions. The first is that $\pr{\textsf{injury} \vert \textsf{witness}}=\pr{\textsf{injury} \vert \textsf{witness} \wedge \textsf{emergency}}$. This assumption is plausible: that the defendant called an emergency number should not make it more (or less) likely that the defendant would cause injury to someone. The second assumption is that $\pr{\textsf{firefighther} \vert \textsf{emergency} }=\pr{\textsf{firefighther} \vert \textsf{witness} \wedge \textsf{emergency} \wedge \textsf{injury}}$. This assumption is also plausible: that the defendant injured the victim and there is a testimony to that effect does not make it more (or less) likely that the victim was a firefighter. Admittedly, more is needed to justify these assumptions than an appeal to plausibility, a point to which we will soon return. But, granting for now that the two assumptions hold, it follows that:^[By the chain rule and the independence assumptions  $\pr{A | a}=\pr{A | a \wedge b}$ and $\pr{B | b}=\pr{B | a \wedge b \wedge A}$, the following holds:
 \begin{align*}
\pr{A \wedge  B \vert a \wedge b}& =\pr{A \vert a \wedge b} \times \pr{B \vert  a \wedge b \wedge A}\\
 & = \pr{A \vert a} \times \pr{B \vert  b}
 \end{align*}
 ]
\[\pr{\textsf{injury} \wedge \textsf{firefighther} \vert \textsf{witness} \wedge \textsf{emergency}}= \pr{\textsf{injury} \vert \textsf{witness}} \times \pr{\textsf{firefighther} \vert \textsf{emergency}}. \]

\noindent
If the equality holds, even when $\pr{\textsf{injury} \vert \textsf{witness}}$ and $\pr{\textsf{firefighther} \vert \textsf{emergency} }$ meet the required probability threshold, $\pr{\textsf{injury} \wedge \textsf{firefighther} \vert \textsf{witness} \wedge \textsf{emergency}}$  will not, at least provided both $\pr{\textsf{injury} \vert \textsf{witness}}$ and $\pr{\textsf{firefighther} \vert \textsf{emergency} }$ are below 1 (as is usually the case given that the evidence offered in a trial is fallible). The conclusion is that aggregation is violated.  


So, in at least one case and under seemingly plausible assumptions, the revised conjunction principle fails
if the standard of proof is interpreted as a probability threshold. But can we say something more general?
To address this question, we will now represent by means of Bayesian networks the relationship between  claims $A$, $B$ and the conjunction $A\wedge B$, as well as the supporting evidence $a$, $b$ and the conjunction $a\wedge b$. 



##  Independent hypotheses 

 We already studied Bayesian networks in Chapter 6. \todo{REF TO OTHER CHAPETR} Here we only sketch the essential ideas.  A Bayesian network is a formal model that consists 
 of a graphical part (a directed acyclic graph, \textsf{DAG}) and a numerical part (a probability table).  

The nodes in the graph represent random variables that can take different values.
For ease of exposition, we will  use 'nodes'  and 'variables' interchangeably. 
The nodes are connected by directed edges (arrows). 
  No loops are allowed, hence the name acyclic. 
 

 


\begin{wrapfigure}{r}{0.4\textwidth}
```{r fig:hEDAG,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.widh = "65%", fig.height= 1.5, fig.width= 1}
daggityEH <- dagitty("
    dag{
        H -> E 
    }")

coordinates(daggityEH) <- list( x=c(E = 1, H = 1) ,
                                            y=c(E = 2, H = 1) )
drawdag(daggityEH, shapes =  list(E = "c", H = "c"),radius = 3.5)
#hEDAG <- model2network("[E|H][H]")
#graphviz.plot(hEDAG)
```
\begin{tabular}{c|cc}
& $H$ & $\neg H$ \\
\hline
$E$ & $\pr{E \vert H}$ & $\pr{E\vert \neg H}$ \\
$\neg E$ & $\pr{\neg E\vert H}$ & $\pr{\neg E\vert \neg H}$ \\
\end{tabular}

\caption{DAG of the simplest evidential relation along with a probability table}
\label{fig:hEDAG}
\end{wrapfigure}


The simplest evidential relation, one of 
evidence bearing on a hypothesis, can be represented 
by the directed graph displayed in Figure \ref{fig:hEDAG}.  The arrow need not have a causal interpretation. 
The direction of the arrow indicates which conditional probabilities 
should be supplied in the probability table. Since the arrow goes from $H$ to $E$, we should specify the probabilities of the different values of $E$ conditional on the different 
values of $H$.



Return now to the difficulty with conjunction. We will first examine the case in which 
the two hypotheses $A$ and $B$ are probabilistically independent. We will relax this assumption later.
The two directed graphs visualized in Figure \ref{fig:abBDAG} represent two items of evidence each supporting its own hypothesis: $a$ supports $A$ and $b$ supports $B$.  To represent the conjunction $A\wedge B$, 
a conjunction node $AB$ is added and arrows are drawn from nodes $A$ and $B$ into node $AB$ (Figure \ref{fig:conjunctionDAGchapter}). This arrangement makes it possible to express the meaning of $A\wedge B$ via a probability table that mirrors the truth table for the conjunction in propositional logic (see Table \ref{tab:CPTconjunction}).^[The difference is that  the values $1$ and $0$ stand for two different things depending on where they are in the table. In the columns corresponding to the nodes they represent node states: true and false; in the $\textsf{Pr}$ column they represent the conditional probability of a given state of $AB$ given the states of $A$ and $B$ listed in the same row. For instance, take a look at row two. It says: if $A$ and $B$ are both in states 1, then the probability of $AB$ being in state 0 is 0. In principle we could use 'true' and 'false' instead of 1 and 0 to represent states, but the numeric representation is easier to use in programming, which we do quite a bit in the background, so the reader might as well get used to this harmless ambiguity.]

<!---For binary nodes, we will consistently  use '1' and '0' for the states, it's just probabilities that in this case end up being extreme.] --->




\begin{figure}[h]
\begin{subfigure}[!ht]{0.45\textwidth}
```{r fig:aADAG,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%",fig.height= 1.5, fig.width= 1}
daggityaA <- dagitty("
    dag{
        A -> a 
    }")

coordinates(daggityaA) <- list( x=c(a = 1, A = 1) ,
                                            y=c(a = 2, A = 1) )

drawdag(daggityaA, shapes =  list(a = "c", A = "c"),radius = 3)
```
\end{subfigure}\begin{subfigure}[!ht]{0.45\textwidth}
```{r fig:bBDAG,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%",fig.height= 1.5, fig.width= 1}
daggitybB <- dagitty("
    dag{
        B -> b 
    }")

coordinates(daggitybB) <- list( x=c(b = 1, B = 1) ,
                                            y=c(b = 2, B = 1) )
drawdag(daggitybB, shapes =  list(b = "c", B = "c"),radius = 3)
```
\end{subfigure}
\caption{\textsf{DAG}s of $a$ supporting $A$ and $b$ supporting $B$.}
\label{fig:abBDAG}
\end{figure}


\begin{figure}[H]
```{r fig:conjunctionDAG,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%",fig.width=2.5, fig.height=2}
#conjunctionDAG <- model2network("[a|A][b|B][AB|A:B][A][B]")
#graphviz.plot(conjunctionDAG)
daggityConjunctionDag <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> AB
        B -> AB
      }")
coordinates(daggityConjunctionDag) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                            y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityConjunctionDag, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 3.5)
```

\caption{\textsf{DAG} of the conjunction set-up, with the usual independence assumptions built in (\textsf{DAG 1}).}
\label{fig:conjunctionDAGchapter}
\end{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
source("../scripts/CptCreate.R")
source("../scripts/kableCPTs.R")
conjunctionDAG <- model2network("[a|A][b|B][AB|A:B][A][B]")
As <- runif(1,0,1)
Bs <- runif(1,0,1)

aifAs <-runif(1,0,1)
aifnAs <- runif(1,0,1)
bifBs <-runif(1,0,1)
bifnBs <- runif(1,0,1)


AProb <-prior.CPT("A","1","0",As)
BProb <- prior.CPT("B","1","0",Bs)
aProb <- single.CPT("a","A","1","0","1","0",aifAs,aifnAs)
bProb <- single.CPT("b","B","1","0","1","0",bifBs,bifnBs)


ABProb <- array(c(1, 0, 0, 1, 0, 1, 0,1), 
                dim = c(2, 2, 2),
                dimnames = list(AB = c("1","0"),
                                B = c("1","0"), 
                                A = c("1","0")))

conjunctionCPT <- list(A = AProb, B = BProb, 
                       a = aProb, b = bProb, AB = ABProb)


conjunctionBN <- custom.fit(conjunctionDAG,conjunctionCPT)

#CPkable2("conjunctionBN","AB")
```


\begin{wraptable}{l}{0.4\textwidth}
\begin{tabular}{lllr}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{A} & \multicolumn{1}{c}{B} & \multicolumn{1}{c}{} \\
AB &  &  & Pr\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{1}\\
0 & 1 & 1 & 0\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0}\\
0 & 0 & 1 & 1\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0}\\
0 & 1 & 0 & 1\\
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0}\\
0 & 0 & 0 & 1\\
\bottomrule
\end{tabular}

\caption{Probability table for the conjunction node. Mind the ambguity in notation:  0's and 1's under AB, A and B are truth values (equivalent to F and T).  0's and 1's under Pr are probabilities.}
\label{tab:CPTconjunction}
\end{wraptable}
The resulting graph, call it  \textsf{DAG 1}, satisfies the desired independence assumptions. 
First, the two claims $A$ and $B$ are probabilistically independent of one another. Their independence is guaranteed by the fact that the conjunction node $AB$ is a collider and thus no information flows through it.^[A formal treatment of this point is provided in Chapter 6.]\todo{REFER TO OTHER CHAPTER} Second,  the supporting items of evidence $a$ and $b$ are also probabilistically independent of one another. The reason is the same: node $AB$ blocks any flow of information between the evidence nodes. Notably, the independence of the items of evidence is not always explicitly stated in the formulation of the conjunction paradox. The good thing is that the Bayesian network makes such assumptions explicit. 

With this set-up in place, the conjunction paradox arises because aggregation is 
violated.  By the theory of Bayesian networks,  \textsf{DAG 1} in Figure \ref{fig:conjunctionDAGchapter}
ensures the following:^[This is because the only path between \textsf{A} and \textsf{B} goes through \textsf{AB}, which is a collider; as long as we do not condition on it, all paths between \textsf{A} and \textsf{B} remain blocked. See Chapter 6 on Bayesian networks for details on this point.] \todo{REFER TO APPROPRIATE CHAPTER}
 \begin{align*}
\pr{A \wedge  B \vert a \wedge b}& =\pr{A \vert a \wedge b} \times \pr{B \vert  a \wedge b \wedge A}\\
 & = \pr{A \vert a} \times \pr{B \vert  b}
 \end{align*}

<!---\noindent 
If, as is normally the case, neither $\pr{A \vert a}$ 
nor $\pr{B \vert b}$ equal 1, then
\[\pr{A \wedge B \vert a \wedge b}< \pr{A \vert a} \;\ \& \;\ \pr{A \wedge B \vert a \wedge b} < \pr{B \vert b}. \]
--->

\noindent
Thus, even when claims $A$ and $B$ are sufficiently probable given their supporting evidence $a$ and $b$ (for a fixed threshold $t$)---in symbols, $\pr{A \vert a}>t$ and $\pr{B \vert b}>t$---it does not follow that $A \et B$  is sufficiently probable given combined evidence $a\et b$ provided (as is normally the case) neither $\pr{A \vert a}$ nor $\pr{B \vert b}$ equal 1. As before, the conjunction principle fails because aggregation fails. 

The argument here goes beyond the specific 
example about aggravated assault in the previous section. The argument only assumes 
that the directed graph in Figure \ref{fig:conjunctionDAGchapter} is an adequate representation of a situation in which two items of evidence, $a$ and $b$, support their own hypothesis, $A$ and $B$. The graph encodes two plausible relations of probabilistic independence: between hypotheses $A$ and $B$ and between items of evidence $a$ and $b$.  The theory of Bayesian networks does the rest of the work.  




## Dependent hypotheses 

Consider now what happens if claims $A$ and $B$ are not regarded as probabilistically independent. 
To represent this, it is enough to draw an arrow between nodes $A$ and $B$. The modified graph is displayed in Figure \ref{fig:conjunctionDAG2chapter}, call it  \textsf{DAG 2}. The open path between nodes $A$ and $B$ no longer guarantees the probabilistic independence of $A$ and $B$ or the independence of evidence nodes $a$ and $b$. <!---Note, however, that there is still no \emph{direct} dependence between the items of evidence.---> But the items of evidence  are still probabilistically independent of one another \textit{conditional} on their respective hypothesis. That is, $\pr{a \vert A}=\pr{a \vert A \wedge b}$ and $\pr{b \vert B}=\pr{b \vert B  \wedge a}$.   So $a$ and $b$ still count as independent lines of evidence despite not being (unconditionally) probabilistically independent.^[Here is an illustration of the idea of independent lines of evidence without unconditional independence. Suppose the same phenomenon (say blood pressure) is measured by two instruments. The reading of the two instruments (say `high' blood pressure) should be \textit{probabilistically dependent} of one another. After all, if the instruments were both infallible and they were measuring the same phenomenon, they should give the exact same reading.  On the other hand, the two instruments measuring the same phenomenon should count as \textit{independent lines of evidence}. This fact is rendered in probabilistic terms by means of probabilistic independence conditional on the hypothesis of interest. These ideas can be worked out more systematically 
in the language of Bayesian networks. Roughly, two variables are probabilistically dependent if there 
is an open path between them. On the other hand, an open path 
can be closed by conditioning on one of the variables along the path. For a more rigorous exposition of the notions of open and closed paths, see Chapter 6.] \todo{CITE EARLIER CHAPTERS} 


\begin{figure}[h]
```{r fig:conjunctionDAG2,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%", fig.width=2.5, fig.height=2}
daggityConjunctionDag2 <- dagitty("
    dag{
        A -> a 
        B -> b
        A -> B
        A -> AB
        B -> AB
      }")

coordinates(daggityConjunctionDag2) <- list( x=c(a = 1, A = 2, AB = 3, B = 4, b = 5) ,
                                            y=c(a = 3, A = 1, AB = 2, B = 1, b = 3) )
drawdag(daggityConjunctionDag2, 
        shapes =  list(A = "c", a = "c", AB = "c", B = "c", b = "c"), radius = 3.5)

```
\caption{\textsf{DAG} of the conjunction set-up, without independence between $A$ and $B$ (\textsf{DAG 2}).}
\label{fig:conjunctionDAG2chapter}
\end{figure}


The difficulty with conjunction arises even when hypotheses $A$ and $B$ are no longer assumed to be independent. Using a simulation we can study how often, in principle, the joint posterior $\pr{A\wedge B \vert a \wedge b}$ is below both the individual posteriors $\pr{A \vert a}$ and $\pr{B \vert b}$. We simulated 10,000 random Bayesian networks based on \textsf{DAG 1} (independent hypotheses) and \textsf{DAG 2} (dependent hypotheses).  Assuming that each possible network has an equal probability of occurring, the joint posterior is lower than both individual posteriors 68\% of the time for \textsf{DAG 1}, and around 60\% for \textsf{DAG 2} (see the appendix for details). This result agrees with @schwartz2017ConjunctionProblemLogic who point out that, if hypotheses $A$ and $B$ are probabilistically dependent on one another, aggregation will fail less often. Still, postulating a dependence between hypotheses does little for solving the difficulty with conjunction since a drop in the failure rate from 68% to 60% is limited. 



\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=FALSE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
bB <- .9
ba <- .55
BA <- .55

(bB/ba)
1/BA
(bB/ba) < 1/BA


#counterexample
bB <- .8

ba <- .51

BA <- .66

(bB/ba)
1/BA
(bB/ba) < 1/BA
```
\normalsize







# Evidential Strength 
\label{sec:strength}

<!---
We do not rehearse these arguments here. Our focus is on \textit{probabilistic} strategies for handling the difficulty with conjunction. 
Probability theory is a well-established theory with applications in several fields. We should not reject it unless strong reasons mandate it. 
Since we have not yet explored any alternative strategies within the probabilistic approach, the failures of aggregation are not strong enough reasons to reject probability theory at this point.  
--->

<!---The failures of the conjunction principles so far are failures 
of aggregation. When the probability of $A$ and the probability of $B$ are both above a given threshold, the probability of the conjunction $A\wedge B$ often is not. This can happen whether or not $A$ and $B$ are probabilistically independent. These failures of aggregation occur if the standard of proof is understood as a posterior probability threshold.
--->

The previous section demonstrated that aggregation fails in a large class of cases if the standard of proof is understood as a posterior probability threshold. Some believe that such failures are strong enough ground to advocate a different conception of probability, along the lines of Baconian probability [@cohen1981can], fuzzy logic or belief functions [@clermont2013paradox], or reject legal probabilism altogether [@allen2013].  But posterior probability thresholds are not the only way of understanding standards of proof 
from a probabilistic perspective. Standards of proof can also be modeled using probabilistic measures of \textit{evidential strength}. This is the approach we explore in this section.  

<!---As we shall see, this alternative way of modeling proof standards succeeds at validating the principle of aggregation (at least, given some additional assumptions). This is promising, but we will see that this approach is ultimately unsatisfactory. --->

Two common probabilistic measures of evidential strength are the Bayes factor and the likelihood ratio. We discussed them earlier in Chapter 5. \todo{REFER TO EARLIER CHAPTERS} As we show below, under plausible assumptions, these measures of evidential strength validate one direction of the conjunction principle: aggregation. If $a$ is sufficiently strong evidence in favor of $A$ and $b$ is sufficiently strong evidence in favor of $B$, then $a\wedge b$ is sufficiently strong evidence in favor of the conjunction $A \wedge B$. In fact, the evidential support for the conjunction will often exceed that for the individual claims, a point already made by @dawid1987difficulty who wrote that 'suitably measured, the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents' (97).^[Dawid's claim holds for the Bayes factor, but does not hold for the likelihood ratio. The joint likelihood ratio may be lower than one of the individual likelihood ratios, although it is not lower than both; more on this later in this section.]
<!---That is, probability theory justifies this claim: if distinct items of evidence $a$ and $b$ constitute sufficiently strong evidence for claims $A$ and $B$, so does the conjunction $a\wedge b$ for the composite claim $A\wedge B$ (although, there are some caveats and extra assumptions for this to hold, and \emph{contra} Dawid, it is false that joint likelihood ratio is always higher than each of the individual likelihood ratios;  more on this later).---> 

Dawid thought that vindicating aggregation was enough for the conjunction paradox to 'evaporate'. 
But, as we show below, this is not quite right. On the evidential strength interpretation of the standard of proof, the other direction of the conjunction principle, distribution, fails. <!---If $a \wedge b$ is sufficiently strong evidence in favor of $A \wedge B$, it does not follow that $a$ is sufficiently strong evidence in favor of $A$ or $b$ sufficiently strong evidence in favor of $B$.---> If $a \wedge b$ is sufficiently strong evidence in favor of $A \wedge B$, it does not follow that $a\wedge b$ is sufficiently strong evidence in favor of $A$ or that $a\wedge b$ is sufficiently strong evidence in favor of $B$. This is  odd.  If we interpret the standard of proof using evidential strength, one could establish beyond a reasonable doubt that $A \wedge B$ (say the defendant killed the victim \textit{and} acted intentionally) while failing to establish one of the conjuncts. 

The prospects for legal probabilism look dim. If the standard of proof is understood as a posterior probability threshold, the conjunction principle fails because aggregation fails (previous section). If, instead, the standard of proof is understood as a threshold relative to evidential strength, the conjunction principle still fails because distribution fails (this section). From a probabilistic perspective, it seems impossible to capture both directions of the conjunction principle. In what follows, we defend more precisely the claim that, on the evidential strength approach, aggregation succeeds but distribution fails.  The argument is laborious.  The reader should arm themselves with patience or take our word for it and jump ahead. The curious reader is welcome to read the appendix for the technical details. 


## Combined support: Bayes factor 

The first part of the argument shows that the combined support supplied 
by multiple pieces of evidence (e.g. $a\wedge b$) for a conjunctive claim (e.g. $A\wedge B$) typically exceeds the individual support supplied by individual pieces of evidence for individual claims. This claim holds for the Bayes factor and to some extent for the likelihood ratio. We start with the Bayes factor $\pr{E \vert H}/\pr{E}$ as our measure of the support of $E$ in 
favor of $H$. As is apparent from Bayes' theorem,

\[\pr{H \vert E} = \frac{\pr{E \vert H}}{\pr{E}}\times \pr{H},\]

\noindent
the Bayes factor measures the extent to which 
a piece of evidence increases the probability 
of a hypothesis, as compared to its prior probability. The greater the Bayes factor (for values above one), the stronger the support of $E$ in favor of $H$. 
<!---Putting aside reservations about this measure 
of evidential support (see Chapter XX \todo{REFER TO EARLIER CHAPTER}), 
the Bayes factor $\pr{E \vert H}/\pr{E}$, unlike the conditional probability 
$\pr{H \vert E}$, offers a potential way to overcome 
the difficulty with conjunction by vindicating aggregation.--->


Suppose Bayes factors $\nicefrac{\pr{a \vert A}}{\pr{a}}$ (abbreviated $BF_A$)  and 
$\nicefrac{\pr{b \vert B}}{\pr{b}}$ (abbreviated $BF_B$) are greater than one. That is, 
items of evidence  $a$ and $b$ positively support $A$ and $B$, separately.
The combined support here is (see the appendix for a proof): 
\begin{align*}
\frac{\pr{a \wedge b \vert A \wedge B}}{\pr{a \wedge b}} &= \frac{\pr{a \vert A}}{\pr{a}} \times \frac{\pr{b \vert B}}{\pr{b}}\\
BF_{AB} &= BF_{A} \times BF_{B}
\end{align*}
\noindent Consequently, the combined support $BF_{AB}$ is always higher than the individual support so long as the individual pieces of evidence positively support their respective hypothesis.  This claim holds assuming hypotheses $A$ and $B$ are independent and items of evidence $a$ and $b$ are independent. These assumptions are validated by  \textsf{DAG 1} in Figure \ref{fig:conjunctionDAGchapter}. 
<!---compares the Bayes factor of one item of evidence, say $\nicefrac{\pr{a \vert A}}{\pr{a}}$ with the combined Bayes factor for five items of evidence, say $\nicefrac{\pr{a_1 \wedge \dots \wedge a_5 \vert A_1 \wedge \dots \wedge A_5}}{\pr{a_1\wedge \dots \wedge a_5}}$, for different values of sensitivity and specificity of the evidence.^[The \textbf{sensitivity} of a piece of evidence $E$ relative to a hypothesis $H$ is $\pr{E \vert H}$, while its \textbf{specificity} is $\pr{\neg E \vert \neg H}$.]---> <!---The combined Bayes factor always exceeds the individual Bayes factors provided, as usual, the individual pieces of evidence positively support the individual hypotheses.---> Under these circumstances,  Dawid's claim that `the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents' (if support is to be measured in terms of Bayes factor) is verified.^[The result holds beyond two pieces of evidence; see Figure \ref{fig:bfconjunction5}. On the other hand, if both items of evidence oppose the individual hypotheses, the joint Bayes factor will be lower than any of the individual ones. Neutral evidence always results in a joint Bayes factor of one.]  


\begin{figure}[h]
```{r bfconjunction5,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%", warning = FALSE, message = FALSE}
library(ggplot2)
library(ggthemes)

bf <- function (x, sensitivity, specificity){
    (
      sensitivity/
        (sensitivity*x+(1-specificity)*(1-x))
     )
}


bfn <- function(x, sensitivity, specificity, n){
  bf(x, sensitivity, specificity)^n
}


x <-seq(0,1,by=0.001)
ynull <- bf(x, 0.5, 0.5)
yynull <- bfn(x, 0.5, 0.5, 2)
y5null <- bfn(x, 0.5, 0.5, 5)
y06055 <- bf(x, 0.6, 0.55)
yy06055 <- bfn(x, 0.6, 0.55,2)
y506055 <- bfn(x, 0.6, 0.55,5)
y04045 <- bf(x, 0.4, 0.45)
yy04045 <- bfn(x, 0.4, 0.45,2)
y504045 <- bfn(x, 0.4, 0.45,5)


conIndDF <- data.frame(x, ynull , yynull, y5null, y06055, yy06055, y506055, y04045, yy04045, y504045)

library(data.table)
conIndDFlong <- melt(conIndDF, id.vars = c("x"), variable.name = "type")


conIndDFlong$items <- ifelse (conIndDFlong$type == "ynull" |conIndDFlong$type == "y06055"| 
                             conIndDFlong$type == "y04045", "one", 
                             ifelse(conIndDFlong$type == "yynull" |conIndDFlong$type == "yy06055"| 
                             conIndDFlong$type == "yy04045", "two", "five")
                             )


conIndDFlong$strength <- ifelse (conIndDFlong$type == "ynull" |conIndDFlong$type == "yynull"| 
                             conIndDFlong$type == "y5null", "Individual BFs equal to one", 
                             ifelse(conIndDFlong$type == "y06055" |conIndDFlong$type == "yy06055"| 
                             conIndDFlong$type == "y506055", "Individual BFs greater than one: Pr(E | H)=.6 and Pr(E | not-H)=.45", "Individual BFs below one: Pr(E | H)=.4 and Pr(E | not-H)=.55")
                             )

conIndDFlong$items <- as.factor(conIndDFlong$items)

levels(conIndDFlong$items) <- c("five", "one", "two")

ggplot(conIndDFlong, aes(x = x, y = value, group = type, lty = items, color = strength)) + xlab("prior")+ ylab("Bayes factor")+theme_tufte()+
  theme(legend.position = c(0.65, 0.8), legend.title = element_blank())+geom_line()

```
\caption{Joint Bayes factors for one, two and five items of evidence, for individual Bayes factors greater than one (positive support), equal to one (neutral support) and below one (negative support). The independence assumptions in Figure \ref{fig:conjunctionDAGchapter}, \textsf{DAG 1}, hold.}
\label{fig:bfconjunction5}
\end{figure}


<!---### Dependent hypotheses --->

Even when $A$ and $B$ are not probabilistically independent, the combined Bayes factor $BF_{AB}$ 
will still be greater than both the individual Bayes factor $BF_{A}$ and $BF_{B}$
if the probabilistic measure fits \textsf{DAG 2} 
in Figure \ref{fig:conjunctionDAG2chapter}. To see why, 
first note that the following holds (see the appendix for a proof):
  \begin{align}
BF_{AB} = \frac{\pr{a \wedge b\vert A\wedge B}}{\pr{a \wedge b}}&= \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b|a}}&=  BF_{A}\times BF^{'}_{B} \nonumber \\
 & = \frac{\pr{a |A}}{\pr{a | b}} \times \frac{\pr{b |B}}{\pr{b}}& =  BF^{'}_{A}\times BF_{B}  \nonumber 
 \end{align}



\noindent The difference from the case of independent hypotheses 
is that $BF_B=\nicefrac{\pr{b \vert B}}{\pr{b}}$ is now replaced by  $BF^{'}_B=\nicefrac{\pr{b \vert B}}{\pr{b\vert a}}$, or alternatively $BF_A=\nicefrac{\pr{a \vert A}}{\pr{a}}$ by  $BF^{'}_A=\nicefrac{\pr{a \vert A}}{\pr{a\vert b}}$.^[Since $b$ need not be probabilistically independent of $a$,  there is no guarantee that $\pr{b \vert a}=\pr{b}$ or $\pr{a \vert b}=\pr{a}$.] Still, if the probabilistic measure fits \textsf{DAG 2},  whenever $BF_B$ is  greater than 1, so is $BF^{'}_B$, and whenever 
$BF_A$ is  greater than 1, so is $BF^{'}_A$ (see the appendix for a proof). Thus, the joint Bayes factor $BF_{AB}$ will be greater than any of the individual Bayes factors.^[In contrast, if the underlying \textsf{DAG} were to contain a direct dependence between the items of evidence, the joint Bayes factor could be lower than either of the individual Bayes factors (see the appendix for a proof).]

<!---
\mar{R: I though hard about including the graphics here, but at this point I don't think this would contribute to clarity, give it a thought.}
--->

## Combined support: likelihood ratio

We now run a similar argument for the likelihood ratio, another probabilistic measure of evidential strength. The likelihood ratio---extensively discussed in Chapter 5\todo{REFER TO EARLIER CHAPTER}---compares the probability of the evidence on the assumption that a hypothesis of interest is true (\textit{sensitivity}) and the probability of the evidence on the assumption that the negation of the hypothesis is true (\textit{1- specificity}). That is,
\[\frac{\pr{E \vert H}}{\pr{E \vert \neg H}}=\frac{\textit{sensitivity}}{\textit{1- specificity}}\]

\noindent
The greater the likelihood ratio (for values above one), the stronger the evidential support 
in favor of the hypothesis (as contrasted to its negation). <!---Unlike the Bayes factor, the likelihood ratio does not vary depending on the prior probability of the hypothesis so long as sensitivity and specificity do not.---> <!---^[Whether the sensitivity and specificity of the evidence depends on the prior probability of the hypothesis is debated in the literature \textbf{CITE}. Further, we will see that specificity does depend on the prior probability in the case of conjunctive hypotheses such as $A \wedge B$.] --->

The question is whether the combined support measured by the joint likelihood ratio
\[\frac{\pr{a\wedge b \vert A\wedge B}}{\pr{a \wedge b \vert \neg (A\wedge B)}}\]

\noindent
exceeds the individual support measured by the individual likelihood ratios $\nicefrac{\pr{a \vert A}}{\pr{a \vert \neg A}}$ and $\nicefrac{\pr{b \vert B}}{\pr{b \vert \neg B}}$. Under suitable assumptions, the answer is positive. So, details aside, Bayes factor and likelihood ratio agree here. The argument for the likelihood ratio, however, is more laborious.  




<!---### Same sensitivity and specificity, and independence --->

We start with the fact that, in a large class of cases (see the appendix for a proof),

\begin{align*}
LR_{AB} & = \frac{\pr{a\wedge b \vert A\wedge B}}{\pr{a \wedge b \vert \neg (A\wedge B)}} = \frac{\pr{a \vert A} \times \pr{b \vert B}}
 {\frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a \vert \neg A}\pr{b \vert B} + \pr{A}\pr{\neg B \vert A} \pr{a \vert A }\pr{b \vert \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a \vert \neg A}\pr{b \vert \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} }}
\end{align*}

\noindent This equality holds whether or 
not hypotheses $A$ and $B$ are probabilistically independent. 
But, given the many unknowns here, it pays to make 
some simplifications if only for illustrative purposes. 

The first temporary simplification we make is to assume that the sensitivity and specificity of the items of evidence  are both equal to the same value $x$. Second, we assume that $A$ and $B$ are probabilistically independent in agreement with \textsf{DAG 1} (Figure \ref{fig:conjunctionDAGchapter}). The combined likelihood ratio can now be  plotted as a function of $x$. <!---^[In this simplified set-up, the combined likelihood ratio reduces to the following, where $\pr{A}=k$ and $\pr{B}=t$:
  \begin{align*}
\frac{\pr{a \wedge b \vert A\wedge B}}{\pr{a \et b\vert \neg (A\et B)}} & = \frac{x^2}{\frac{(1-k)t(1-x)x + k(1-t)x(1-x) + (1-k)(1-t)(1-x)(1-x)}{ \left(1-k\right) t +\left(1-t\right) k+\left(1-k\right) \left(1-t\right)}}
 \end{align*}] ---> Figure \ref{fig:jointLRMarcello} shows that the combined likelihood ratio always exceeds the individual likelihood ratios whenever they are greater than one (or in other words, as is usually assumed, the two pieces of evidence provide positive support for their respective hypotheses).^[Interestingly, the combined likelihood ratio 
varies depending on the prior probabilities $\pr{A}$ and $\pr{B}$.] As with the Bayes factor, the combined likelihood ratio exceeds the individual likelihood ratios.  

\begin{figure}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)

combined <- function (x,k,t) {
  (x^2)/
(
  ((1-k)*t*(1-x)*x + k*(1-t)*x*(1-x) + (1-k)*(1-t)*(1-x)*(1-x))
  /
    ((1-k)*t +k*(1-t)+(1-k)*(1-t))
)
}

# combinedMarcello <- function(x,k,t){
#   (x^2)/(k*(1-t)*(x)*(1-x)+(1-k)*t*x*(1-x)+(1-k)*(1-t)*(1-x)*(1-x))
# }

x <-seq(0,1,by=0.001)
y0102 <- combined(x,0.1,0.2)
y0608 <- combined(x,0.6,0.8)


ggplot() +
  stat_function(fun=function(x)(x/(1-x)), geom="line", aes(colour="LR(a)=LR(b)"))+
   geom_line(aes(x = x, y = y0102,color = "joint LR for P(A)=0.1, P(B)=0.2"))+ylim(c(0,5))+
  xlab("x=sensitivity(a)=sensitivity(b)")+ ylab("LR")+theme_tufte()+
  theme(legend.position = c(0.8, 0.2)) +
  geom_line(aes(x = x, y = y0608,color = "joint LR for P(A)=0.6, P(B)=0.8"))
```

\caption{Combined likelihood ratios exceeds individual Likelihood ratios as soon as sensitivity is above .5. Changes in the prior probabilities $\pr{A}$ and $\pr{B}$ do not invalidate this result.}
\label{fig:jointLRMarcello}
\end{figure}


<!---### Different sensitivity and specificity --->

What happens if the two simplifying assumptions are relaxed? If the items of evidence have different levels of sensitivity and specificity, the combined likelihood ratio never goes below the lower of the two individual likelihood ratios, but can be lower than the higher individual likelihood ratio. We establish this claim by means of a computer simulation (see the appendix for details). This holds if the probabilistic measure fits \textsf{DAG 1} (independent hypotheses) or \textsf{DAG 2} (dependent hypotheses), but fails---unsurprisingly---if there is direct dependence between the pieces of evidence.^[The simplified set-up from before does not contradict this claim but follows from it. In the simplified set-up, both individual likelihood ratios were the same, so whenever the joint likelihood was higher than the minimum of the individual likelihood ratios, it was higher than the both of them.]  In this sense, the joint likelihood ratio behaves differently than the joint Bayes factor since it is greater than the lowest of the individual likelihood ratios, rather than being greater than both of them.  Consequently, Dawid's claim that  `the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents' should be amended. <!---restricted to cases in which there is no direct probabilistic dependence between the items of evidence.---> When it is measured by the likelihood ratio, the evidential support supplied by the conjunction of several independent items of evidence exceeds the support supplied by at least one individual item of evidence but possibly not all. 



##  Vindicating aggregation

We have seen that under suitable assumptions the combined evidential support 
is never below the lowest individual support. 
More precisely, let $\mathsf{str}[E, H]$ stand for the strength of the evidential support of a piece of evidence $E$ toward a hypothesis $H$, measured by the Bayes factor or the likelihood ratio.  We have established the following fact:

\[\mathsf{str}[a \wedge b, A\wedge B] \geq \text{min}(\mathsf{str}[a, A], \mathsf{str}[b, B]),\]
<!---
By the principle above, the following implication holds:

\[STR[a, A]>t_{STR} \wedge STR[b, B]>t_{STR} \Rightarrow  STR[a \wedge b, A\wedge B]>t_{STR},\] 

\noindent
where $t_{STR}$ is a threshold on the strength of evidential support. 
This implication resembles the aggregation principle, repeated below for convenience:

\[S[a, A] \wedge S[b, B] \Rightarrow  S[a \wedge b, A\wedge B],\] 

\noindent 
--->
where the function $\text{min}$ returns the lowest of its arguments. 

This fact can be used to justify aggregation. 
To be sure, aggregation is not directly a principle about evidential strength. 
It is a principle about the standard of proof. That is:

\[S[a, A] \wedge S[b, B] \Rightarrow  S[a \wedge b, A\wedge B],\] 
where the expression $S[a, A]$ should be read as 'evidence $a$ supports claim $A$ by standard $S$', and similarly for $S[b, B]$  and $S[a \wedge b, A\wedge B]$. So, to complete the argument, the standard of proof $S$ should be tied to evidential strength $\mathsf{str}$. How? The rule of decision could be: liability is proven according to the governing standard of proof in case the strength of the evidential support---measured by the Bayes factor or the likelihood ratio---meets a suitably high threshold. More formally:

\[S[E, L] \Leftrightarrow \mathsf{str}[E, L]>t_{str},\]
where $E$ is the total evidence available and $L$ is the claim that the defendant is criminally or civilly liable. On this reading, the threshold $t_{str}$ is not a probability between 0 and 1, but a number somewhere above one. The greater this number, the more stringent the standard of proof, for any value above one.  

The only question left at this point concerns how one should identify the appropriate evidential strength threshold $t_{str}$. The answer isn't obvious.  Below we examine two possible approaches. The first approach consists in fixing the threshold $s_{str}$ to some critical value. For example, the critical value for the likelihood ratio $\nicefrac{\pr{E \vert L}}{\pr{E \vert \neg L}}$ can be fixed at 1 for civil trials and greater than 100 for criminal trials. Similar thresholds can be applied to 
the Bayes factor $\nicefrac{\pr{E \vert L}}{\pr{E}}$. These values are plausible, since the evidence against the defendant in criminal cases should be stronger than in civil cases to justify a verdict of liability. But this approach raises a further question. Why should critical values such as 1 for  civil trials and 100 for criminal trials be chosen? What is the rationale for their choice? Why not, say, 10 and 1,000?
A second,  more roundabout approach is better suited to address these questions. 

The threshold for the Bayes factor and the threshold for the likelihood ratio need not be fixed directly, but can instead be indirectly derived from the threshold for the posterior probability by manipulating two simple equations. Consider first the Bayes factor threshold. Since 
\begin{align*}
\mathsf{ Bayes \: factor }=\frac{\mathsf{posterior }}{\mathsf{ prior}},
\end{align*}

\noindent
the Bayes factor threshold, call it $t_{BF}$, can be defined as follows:
\begin{align*}t_{BF} & = \frac{t}{\textsf{prior}},
\end{align*}
where $t$ is the posterior probability threshold. The value of $t$ can be determined in a decision-theoretic manner by minimizing expected costs (see Chapter 11\todo{REFER TO ERALIER CHAPPER}). Once $t$ is fixed in this way, the value $t_{BF}$ can be easily derived by the equations above. 
The same strategy works for the likelihood ratio threshold, call it $t_{LR}$. 
By the odds version of Bayes' theorem, 
\begin{align*}\textsf{likelihood ratio}=\frac{\textsf{posterior odds}}{\textsf{prior odds}}.
\end{align*}
If the posterior ratio is fixed at, say $t/1-t$, then the value of $t_{LR}$ can be easily obtained as follows:
\begin{align*}t_{LR} & = \frac{\nicefrac{t}{1-t}}{\textsf{prior odds}}.
\end{align*}

\noindent
Note that thresholds $t_{BF}$ and $t_{LR}$ will depend on the prior probability of the hypothesis. The higher the prior probability, the lower the threshold.  Whether this is a desirable property of a decision threshold can be questioned, but a similar point applies to the posterior threshold $t$: the higher the prior probability, the easier it is to meet the threshold.   

This second approach allows for a principled method to identify the critical thresholds, but incurs a major shortcoming: aggregation still fails. There will be cases in which the conjuncts taken separately satisfy the decision standard $t_{BF}$ or $t_{LR}$, while the conjunction does not.<!---To illustrate this point, consider the principle of aggregation formulated in terms of the Bayes factor  threshold:
 \begin{align*} \frac{\pr{a \vert A }}{\pr{a}}>t^A_{BF} &\mbox{ and } 
\frac{\pr{ b \vert B}}{\pr{b}}>t^B_{BF} \Rightarrow  
\frac{\pr{a \et b \vert A \et B}}{\pr{a \et b}}>t^{A\wedge B}_{BF} 
\end{align*}
 \begin{align*}
 \frac{\pr{a \vert A }}{\pr{a \vert \n A}}>t^A_{LR} &\mbox{ and } 
\frac{\pr{ b \vert B}}{\pr{b\vert B}}>t^B_{LR} \mbox{ iff } 
\frac{\pr{a \et b \vert A \et B}}{\pr{a \et b\vert \n (A \et B)}}>t^{A\wedge B}_{LR} \\
\end{align*}
---> To illustrate, consider a posterior probability threshold of $.95$ as might be appropriate in a criminal case. If the individual claims $A$ and $B$ both have a prior probability of, \mbox{say $.1$,} the Bayes factor threshold $t^A_{BF}=t_{BF}^B=\nicefrac{.95}{.1}=9.5$ for $A$ or $B$ individually. Note the superscripts $A$ and $B$. The Bayes factor threshold $t_{BF}$ is indexed to the claim of interest because the threshold is prior-dependent and thus claim-dependent (since different claims have different prior probabilities). 
If, as is often assumed, claims $A$ and $B$ are probabilistically independent, the composite claim $A \wedge B$ will be associated with the Bayes factor threshold $t^{A\wedge B}_{BF}=\nicefrac{.95}{(.1\times .1)}=95$, a much higher value. Suppose each claim barely meet the 9.5 Bayes factor threshold. Given independence, the joint Bayes factor results from multiplying the individual Bayes factors, that is, $9.5 \times 9.5=90.25$. This is not quite enough to meet $t^{A\wedge B}_{BF}=95$. So aggregation fails. An analogous 
point holds for the likelihood ratio threshold.^[Say $A$ and $B$ have  prior probabilities of 
$.2$ and $.3$ respectively. On this approach, the likelihood ratio threshold for $A$ and $B$ will be  $t_{LR}^{A}\approx 76$ and $t_{LR}^{B}a\approx 44$, assuming posterior probability threshold of 0.95. The likelihood ratio threshold for the composite claim $A \wedge B$ will be $t^{A\wedge B}_{LR}\approx 297$.   Suppose the individual likelihood ratios meet their threshold. And, for simplicity, suppose  sensitivity and specificity of the individual items of supporting evidence are the same. In this case, for $t_{LR}^{A}$ to be met, evidence $a$ should have sensitivity (and specificity) of at least 0.988. For $t_{LR}^{B}$ to be met,  evidence $b$ should have  sensitivity (and specificity) of 0.978. Given these values, the combined likelihood ratio equals about 145, far short from the threshold $t^{A\wedge B}_{LR}\approx 297$.] 
The culprit is the fact that $t_{BF}$ and $t_{LR}$  have different values when applied to individual claims $A$ and $B$ as opposed to the composite claim $A \wedge B$. 
<!---
The difference  grows as (i) the prior probability of the individual claims decreases, (ii)  as the posterior threshold $t$  decreases, and  (iii) as the number of constituents claims increases. For two constituents, the combined Bayes factor remains only $5\%$ below the value needed to meet $t_{BF}^{A\wedge B}$. The difference at here is between $t_{BF}^{A\wedge B}=\nicefrac{.95}{p^2}$ and $t_{BF}^{A}*t_{BF}^{B}=(\nicefrac{.5}{p})^{2}$. Note that $\frac{\nicefrac{.95}{p^2} - (\nicefrac{.5}{p})^{2}}{\nicefrac{.95}{p^2}}=.05$, for any value of the prior $p$. Given five constituent claims, $\frac{\nicefrac{.95}{p^5} - (\nicefrac{.95}{p})^{5}}{\nicefrac{.95}{p^5}}=.18$.  Now, say $t=.5$. Even with just two claims, $t^{A\wedge B}_{BF}=.5/(.1*.1)=50$, but $t^A_{BF}*t_{BF}^B=(.5/.1)*(.5/.1)=25$, only half of the required value. 
--->



<!---
Things don't get better with a lower posterior threshold. Say $t= .5$, as might be appropriate 
in a civil case. The likelihood ratio thresholds for $A$ and $B$ will be 
$t_{LR}^{A}\approx 4$ and $t_{LR}^{B}a\approx 2.3$. The likelihood ratio threshold for  the composite claim $A \wedge B$ will be $t^{A\wedge B}_{LR}\approx 15.6$.    To ensure that $t_{LR}^{A}$ is met, 
evidence $a$ should have a sensitivity of at least 0.8, and  to ensure that $t_{LR}^{B}$ is met, 
evidence $b$ should have a  sensitivity of at least 0.7. To ensure that $t_{LR}^{B}$ is met, 
evidence $b$ should have a sensitivity of at least 0.7. With these parameters, the combined likelihood ratio equals about 5, far short that what the threshold $t^{A\wedge B}_{LR}$ requires, namely a likelihood ratio of 15.
--->

<!-- ```{r, echo=FALSE} -->
<!-- (0.5/(1-0.5))/(0.2/(1-0.2)) -->
<!-- (0.82/(1-0.82)) -->

<!-- (0.5/(1-0.5))/(0.3/(1-0.3)) -->
<!-- (0.7/(1-0.7)) -->


<!-- (0.8*0.7)/ -->
<!-- ( -->
<!--   ((1-0.2)*(1-0.8)*0.3*0.7+0.2*0.8*(1-0.3)*(1-0.7)+(1-0.8)*(1-0.2)*(1-0.7)*(1-0.3))/ -->
<!--   ((1-0.2)*0.3+0.2*(1-0.3)+(1-0.2)*(1-0.3)) -->
<!-- ) -->
<!-- ``` -->





<!-- ```{r, echo=FALSE, eval = FALSE} -->
<!-- (0.5/(1-0.5))/(0.2/(1-0.2)) -->
<!-- (0.5/(1-0.5))/(0.3/(1-0.3)) -->
<!-- (0.5/(1-0.5))/((0.3*0.2)/(1-(0.3*0.2))) -->
<!-- ``` -->

<!-- ```{r, echo=FALSE, eval = FALSE} -->
<!-- (0.95/(1-0.95))/(0.2/(1-0.2)) -->
<!-- (0.987/(1-0.987)) -->

<!-- (0.95/(1-0.95))/(0.3/(1-0.3)) -->
<!-- (0.978/(1-0.978)) -->

<!-- (0.987*0.978)/ -->
<!-- ( -->
<!--   ((1-0.2)*(1-0.987)*0.3*0.978+0.2*0.987*(1-0.3)*(1-0.978)+(1-0.987)*(1-0.2)*(1-0.978)*(1-0.3))/ -->
<!--   ((1-0.2)*0.3+0.2*(1-0.3)+(1-0.2)*(1-0.3)) -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r, echo=FALSE, EVAL = FALSE} -->
<!-- (0.95/(1-0.95))/(0.2/(1-0.2)) -->
<!-- (0.95/(1-0.95))/(0.3/(1-0.3)) -->
<!-- (0.95/(1-0.95))/((0.3*0.2)/(1-(0.3*0.2))) -->
<!-- ``` -->









<!-- Consider a posterior threshold $t=0.95$, as might be appropriate in a criminal case.  -->
<!--
\noindent
If the conjunction principle holds,\todo{In which direction? Note that at least in one direction the principle still fails. For instance, if your threshold is 15, $14*25$ is 350 but still one of the elements fails to be sufficiently supported.}  the statement above would be equivalent \todo{not sure about the equivalence; at best, this would be a sufficient condition, no? you do talk about this later so putting it this way is a bit misleading} to:
  \begin{quote}
Guilt is proven beyond a reasonable doubt if and only if the evidential support in favor of each constituent claim 
 $C_i$ suported by evidence $E_i$---as measured by the Bayes factor $\frac{\pr{E_i \vert C_i}}{\pr{E_i}}$---meets a suitably high threshold $t$.
 \end{quote}
 \noindent
-->




<!---
If the standard of proof is formalized using a fixed  threshold for the Bayes factor or the likelihood ratio, the conjunction principle boils down to one of these:
 \begin{align*} \frac{\pr{a \vert A }}{\pr{a}}>t_{BF} &\mbox{ and } 
\frac{\pr{ b \vert B}}{\pr{b}}>t_{BF} \Leftrightarrow 
\frac{\pr{a \et b \vert A \et B}}{\pr{a \et b}}>t_{BF} \\
 \frac{\pr{a \vert A }}{\pr{a \vert \n A}}>t_{LR} &\mbox{ and } 
\frac{\pr{ b \vert B}}{\pr{b\vert B}}>t_{LR} \Leftrightarrow 
\frac{\pr{a \et b \vert A \et B}}{\pr{a \et b\vert \n (A \et B)}}>t_{LR}
\end{align*}
--->

<!-- \[  \text{ $\frac{\pr{a \vert A }}{\pr{a}}>t_{BF}$ and $\frac{\pr{ b \vert B}}{\pr{b}}>t_{BF}$ iff $\frac{\pr{a \et b \vert A \et B}}{\pr{a \et b}}>t_{BF}$ } \] 
-->

<!---
\noindent
The superscripts have been dropped since the threshold is the same for individual and conjunctive claims. 
--->


<!---Perhaps, this is not the path that the proponent of the evidential strength approach would take anyway. The variable threshold for Bayes factor or the likelihood ratio is parasitic on the posterior probability threshold. It should not be surprising that, if there are reasons to reject the posterior probability threshold, these reasons also apply to other thresholds that are parasitic to it. --->

But not all hope is lost. Perhaps it is unsurprising that deriving the evidential strength threshold from the posterior probability threshold did not solve the problem. If there are reasons to reject the posterior probability threshold, these reasons also apply, \emph{mutatis mutandis}, to thresholds that are parasitic to it. Let us return to the first, more straightforward approach. It consists in fixing the evidential strength threshold independently of the posterior probability threshold and regardless of the prior probability of the hypothesis. Criminal cases would still have a higher decision threshold than civil cases, but the threshold would be constant across individual and composite claims. As noted before, however, this approach raises the question of how the critical thresholds should be fixed in the first place.^[Since the threshold is no longer derived from the posterior probability threshold, standard decision theory cannot help here. We do not further examine this question, since this approach is not part of our ultimate solution to the conjunction paradox.]  But the good news is that the fixed threshold approach vindicates aggregation. The argument is easy. As seen previously, the combined evidential support is usually 
greater than at least one, if not all, individual evidential supports, whether measured 
by the Bayes factor or the likelihood ratio. So, for a fixed value of the threshold, 
whenever $BF_A$ and $BF_B$ meet the threshold $t_{BF}$, usually also the combined Bayes factor 
$BF_{AB}$ meets $t_{BF}$. The same applies for the likelihood ratio threshold. Whenever $LR_A$ 
and $LR_B$ meet the threshold $t_{LR}$, then usually also $LR_{AB}$ meets $t_{LR}$. Aggregation is finally vindicated! 

## The failure of distribution 

But it is too soon to declare victory. The trouble is that, if the evidential strength threshold is held fixed across individual and composite claims, the principle of distribution becomes problematic. This is the other direction of the conjunction principle. Distribution posits that establishing the conjunction, by a governing standard of proof, is sufficient for establishing the individual conjuncts:
\begin{align}
S[a \wedge b, A\wedge B] \Rightarrow S[a, A] \wedge S[b, B], \tag{DIS1}
\end{align}
This is a seemingly uncontroversial principle. How can it possibly fail? To be clear, the principle of distribution does not fail if the standard of proof is understood as a posterior probability threshold. After all, the probability of a conjunction cannot be higher than the probability of its conjuncts. But distribution does fail if the standard of proof is understood as an evidential strength threshold. 

For suppose the joint Bayes factor, $\nicefrac{\pr{a \et b \vert A \et B}}{\pr{a \et b}}$, barely meets  the threshold. We have shown that, in a class of cases codified by \textsf{DAG1} and \textsf{DAG2}, the joint Bayes factor exceeds the individual ones. So, the joint Bayes factor can meet the threshold while the individual ones might not. The argument is analogous for the joint likelihood ratio and the individual ones. <!---The individual support, say $\nicefrac{\pr{a \vert A}}{\pr{a}}$, could  still be below the threshold unless $\nicefrac{\pr{b \vert B}}{\pr{b}}=1$ (which should not happen if $b$ positively supports $B$). The problem for likelihood ratio is analogous. ---> Looking at the graphs in Figure \ref{fig:bfconjunction5} and Figure \ref{fig:jointLRMarcello} should be enough to convince oneself that this is the case.  <!---For suppose evidence $a \et b$ supports $A \et B$ to the required threshold $t$. The threshold in this case should be  some order of magnitude greater than one. If the combined likelihood ratio meets the threshold $t_{LR}$, one of the individual likelihood ratios may well be below $t_{LR}$.---> So---if the standard of proof is interpreted as a fixed evidential strength threshold---even though the conjunction $A \et B$ was proven according to the desired standard, one of the individual claims might not.

We have just shown that principle (DS1) fails. But, perhaps, this principle is not as obviously intuitive as one might have thought. Since the evidence is not held constant throughout, the support supplied by $a\wedge b$ could be stronger than that supplied by $a$ and $b$ individually. After all, the individual evidence $a$ or $b$ is weaker evidence than the combined evidence $a\wedge b$. At least one might argue this way.

To accommodate this line of reasoning, here is a weaker distribution principle:
\begin{align}
S[a \wedge b, A\wedge B] \Rightarrow S[a \wedge b, A] \wedge S[a\wedge b, B]. \tag{DIS2}
\end{align}

\noindent
<!---\mar{R: revised this bit in light of the stuff I added in the appendix.}--->
This principle holds the evidence constant throughout and seems less controversial.
Indeed, it would be odd to say: the evidence establishes beyond a reasonable doubt that the defendant caused the victim's death \textit{and} did so intentionally, but the same evidence does not establish beyond a reasonable doubt that the defendant caused the victim's death. <!---One would not want to claim that, while holding fixed evidence $a\wedge b$, establishing the conjunction might not be enough for establishing one of the conjuncts.--->It would also be odd to say: the evidence establishes by preponderance that the defendant was speeding \textit{and} run a red light, but the same evidence does not establish by preponderance that the defendant was speeding. Yet, as explained in the appendix, the distribution principle (DIS2) is in no better position than (DIS1). If the standard of proof is understood as a fixed threshold on evidential strength, measured by the likelihood ratio or the Bayes factor, the principle (DIS2) would fail in a significant number of cases under \textsf{DAG1} (independent hypotheses) as well as  \textsf{DAG2} (dependent hypotheses). This is a very serious difficulty for those who wish to analyze the standard of proof by means of the Bayes factor or the likelihood ratio. 

<!---when it comes to the fate of aggregation and distribution. Distribution fails even without direct dependence between hypothesis, and aggregation starts to fail as soon as such a dependence is allowed.---> <!---But if distribution fails, there would be cases in which, even though the conjunction $A\et B$ is established by the desired standard of proof, one of the individual claims  fails to meet the standard. --->



# The comparative strategy
\label{sec:comparative}

We hit two dead ends.  The posterior probability threshold is subject to the difficulty with conjunction, specifically, the failure of aggregation (Section \ref{sec:difficulty}). The evidential strength threshold is subject to another kind of difficulty, the failure of distribution (Section \ref{sec:strength}). As a third approach, the standard of proof can be understood comparatively. This approach 
has been advanced by @cheng2012reconceptualizing following the theory 
of relative plausibility by @Pardo2008judicial. Say the prosecutor 
or the plaintiff puts forward a hypothesis $H_p$ about what happened. The defense offers an alternative hypothesis, call it $H_d$. On this approach, rather than directly evaluating the 
support of $H_p$ given the evidence and comparing it to a threshold, we compare the 
support that the evidence provides for two competing hypotheses $H_p$ and $H_d$, 
and decide for the one that garners greater evidential support.


<!---\mar{R: revised this bit in light of our discussion}--->
It is controversial whether this is what happens in all trial 
proceedings, especially in criminal trials, if one 
thinks of the defense hypothesis $H_d$ as a substantial account of what has happened. The defense may elect to challenge the hypothesis put forward by the other party without proposing one of its own.<!---It is also quite clear that the defense hypothesis needs not to be proven more plausible than the prosecution hypothesis: it is enough that it is sufficiently plausible to cast reasonable doubt on the prosecution hypothesis, but the required plausibility level does not have to be high enough to warrant belief.  --->
In the O.J.\ Simpson trial, for example, the defense did not advance its own story about what happened, but simply argued that the evidence provided by the prosecution, while significant on its face to establish OJ's guilt, was riddled with problems and deficiencies, and that the alternative explanations of the evidence were at least to some extent plausible. This defense strategy 
was enough to secure an acquittal.  So, in order to create a reasonable doubt about 
guilt, the defense does not always provide a full-fledged alternative hypothesis. 
The supporters of the comparative approach might respond, perhpas correctly, that this  happens 
in a small number of cases, even though in general---especially for tactical reasons---the defense 
will provide an alternative hypothesis. 

<!---And even if the defense does not explicitly put forward can alternative hypothesis, such cases can still be construed as featuring a defense hypothesis, one that is simply equivalent to the negation of the other party's hypothesis.^[If so, however, the comparative approach collapses onto the threshold approach since $\pr{H}$ and $\pr{\neg H}$ must 
add up to one.] 
--->

<!-- After all, not to provide one would usually  -->
<!-- amount to an admission of criminal or civil liability.   -->

## Comparing posteriors 

We set aside these qualms and show that, in any event, the comparative strategy is still unable to satisfactorily address the difficulty with conjunction. To show this, we start by working out the comparative strategy using posterior probabilities. Here, the standard of proof is understood as follows: given a body of evidence $E$ and two competing hypotheses $H_p$ and $H_d$, the probability $\pr{H_p \vert E}$ should be suitably higher than $\pr{H_d \vert E}$, or in other words, the ratio $\nicefrac{\pr{H_p \vert E}}{\pr{H_d \vert E}}$ should be above a suitable threshold. Presumably, the ratio threshold should be higher 
for criminal than civil cases, for example, greater than one for civil cases and substantively above one for criminal cases.\todo{refer to earlier chapter}^[In the comparative approach---as @cheng2012reconceptualizing  shows---expected utility theory can set the appropriate threshold as a function of the costs and benefits of trial decisions. Suppose the costs of a false positive is $c_{FP}$ and that of a false negative is $c_{FN}$, both greater than zero. Intuitively, the decision rule should minimize the expected costs (see Chapter 11). That is, a finding against the defendant would be acceptable whenever its expected costs---$\pr{H_d \vert E} \times c_{FP}$---are smaller than the expected costs of an acquittal---$\pr{H_p \vert E}\times c_{FN}$---or in other words: $\frac{\pr{H_p \vert E}}{\pr{H_d \vert E}} > \frac{c_{FP}}{c_{FN}}$.
In civil cases, it is customary to assume the costs ratio of 
false positives to false negatives equals one. So the rule of the decision would be: find against the defendant whenever $\pr{H_p \vert E}$ is greater than $\pr{H_d \vert E}$. In criminal trials, the costs ratio is considered higher, since convicting an innocent (false positive) should be more harmful or morally objectionable than acquitting a guilty defendant (false negative). So the rule of decision would be: convict whenever $\pr{H_p \vert E}$ is substantively greater than $\pr{H_d \vert E}$.]  Note that $H_p$ and $H_d$ need not be one the negation of the other.^[If two hypotheses are exclusive and exhaustive, $\nicefrac{\pr{H_p \vert E}}{\pr{H_d \vert E}}>1$ implies that $\pr{H_p \vert E}>.5$, the standard probabilistic interpretation of the preponderance standard for civil cases, and $\nicefrac{\pr{H_p \vert E}}{\pr{H_d \vert E}}>19$ implies that $\pr{H_p \vert E}>.95$, a common interpretation of proof beyond a reasonable doubt.] 

<!---Does the comparative strategy just outlined solve  the difficulty with conjunction?--->
To see whether the comparative strategy just outlined solves the difficulty with conjunction,
we will work through a stylized case used by Cheng himself. 
Suppose, in a civil case, the plaintiff claims that the defendant was speeding ($S$) and that
the crash caused her neck injury ($C$). Here the plaintiff's hypothesis is $S\et C$. 
Suppose, given evidence $E$, the conjuncts, taken separately, meet the decision threshold:
\begin{align}
 \nonumber 
 \frac{\pr{S\vert E}}{\pr{\neg S \vert E}} > 1   & & \frac{\pr{C\vert E}}{\pr{\neg C \vert E}} > 1
\end{align}
\noindent To see whether aggregation is satisfied, we check whether $\nicefrac{\pr{S\et C\vert E}}{\pr{H_d \vert E}}>1$. The key is to decide on the defense hypothesis $H_d$. Cheng reasons that there are 
three alternative defense scenarios: $H_{d_1}= S\et \n C$, $H_{d_2}=\n S \et C$, and  $H_{d_3}=\n S \et \n C$. How does $H_p$ compare to each of them? 
Assuming independence between $C$ and $S$, we have:^[We are assuming that $E$ is the conjunction of two items of evidence, $s\et c$, where $s$ supports $S$ and $c$ supports $C$. The inequalities holds on \textsf{DAG 1} in Figure \ref{fig:conjunctionDAGchapter} after replacing $A$ and $a$ by $S$ and $s$, and $B$ and $b$ by $C$ and $c$, respectively.]

\begin{align}\label{eq:cheng-multiplication}
\frac{\pr{S\et C\vert E}}{\pr{S\et \n C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{S \vert E}\pr{\n C \vert E}}  =\frac{\pr{C\vert E}}{\pr{\n C \vert E}} > 1 \\
\nonumber
\frac{\pr{S\et C\vert E}}{\pr{\n S\et C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{C\vert E}}  = \frac{\pr{S\vert E}}{\pr{\n S \vert E}} > 1 \\
\nonumber
\frac{\pr{S\et C\vert E}}{\pr{\n S\et \n C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{\n C \vert E}}   > 1 
\end{align}

\noindent So, whatever the defense hypothesis, the plaintiff's hypothesis is more probable. At least in this case, whenever the elements of a plaintiff's claim satisfy the decision threshold, so does their conjunction. The left-to-right direction of the conjunction principle---what we are calling aggregation---has been vindicated, at least for simple cases involving independence. Success. 

What about the opposite direction, distribution? Distribution is not generally satisfied. Suppose $\nicefrac{\pr{S\et C\vert E}}{\pr{H_d \vert E}}>1$, or in other words, the combined hypothesis $S \et C$ has been established by preponderance of the evidence. The question is whether the individual hypotheses have been established by the same standard, specifically, whether $\frac{\pr{C\vert E}}{\pr{\n C \vert E}} > 1$ and $\frac{\pr{S\vert E}}{\pr{\n S \vert E}} > 1$. If $\nicefrac{\pr{S\et C\vert E}}{\pr{H_d \vert E}}>1$, the combined hypothesis is assumed to be more probable than any of the competing hypotheses, in particular, $\nicefrac{\pr{S\et C\vert E}}{\pr{\neg S \et C \vert E}}>1$, $\nicefrac{\pr{S\et C\vert E}}{\pr{S \et \neg C \vert E}}>1$ and $\nicefrac{\pr{S\et C\vert E}}{\pr{\neg S \et \neg C \vert E}}>1$.  We have:
\begin{align}\label{eq:cheng-multiplication-two}
1 < \frac{\pr{S\et C\vert E}}{\pr{S\et \n C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{S \vert E}\pr{\n C \vert E}}  =\frac{\pr{C\vert E}}{\pr{\n C \vert E}} \\
\nonumber
1 < \frac{\pr{S\et C\vert E}}{\pr{\n S\et C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{C\vert E}}  = \frac{\pr{S\vert E}}{\pr{\n S \vert E}}  \\
\nonumber
1 < \frac{\pr{S\et C\vert E}}{\pr{\n S\et \n C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{\n C \vert E}}   
\end{align}

\noindent
In the first two cases, clearly, if the composite hypothesis meets the threshold, 
so do the individual claims. But consider the third case. $\nicefrac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{\n C \vert E}}$ might be strictly greater than $\nicefrac{\pr{C\vert E}}{\pr{\n C \vert E}}$ or $\nicefrac{\pr{S\vert E}}{\pr{\n S \vert E}}$. It is possible that $\nicefrac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{\n C \vert E}}$ is greater than one, while either$\nicefrac{\pr{C\vert E}}{\pr{\n C \vert E}}$ or $\nicefrac{\pr{S\vert E}}{\pr{\n S \vert E}}$ are not, say when they are 3 and 0.5, respectively. Distribution fails. And the same problem would arise with a more stringent threshold as might be appropriate in criminal cases. 

There is a more general problem with Cheng's comparative approach. Much of the heavy lifting here is done by the strategic splitting of the defense line into multiple scenarios. Suppose, for illustrative purposes, $\pr{H_p\vert E}=0.37$ and the probability of each of the defense lines given $E$ is $0.21$. This means that $H_p$ wins with each of the scenarios. We should then find against the defendant. But should we? Given the evidence, the accusation is likely to be false, because $\pr{\n H_p \vert E}=0.63$.  The problem generalizes. If, as here, we individualize scenarios by Boolean combinations of elements of a case, the more elements, the more alternative scenarios into which $\n H_p$ needs to be divided. This normally would lead to lowering even further the probability of each of them  (because now $\pr{\n H_p}$ needs to be split between more scenarios). If we take this approach seriously, the more elements a case has, the more at a disadvantage the defense is. This seems undesirable. 

## Comparing likelihoods

Instead of posterior probabilities, we could consider comparing likelihoods and their ratios. The standard of proof would then be as follows: the ratio between the likelihoods $\nicefrac{\pr{E \vert H_p}}{\pr{E \vert H_d}}$ should be above a suitable threshold. Note that the posterior ratio $\nicefrac{\pr{H_p \vert E}}{\pr{H_d \vert E}}$ from before was replaced by the likelihood ratio $\nicefrac{\pr{E \vert H_p}}{\pr{E \vert H_d}}$ where $H_d$ and $H_p$, as before, need not be exhaustive hypotheses.  In civil cases, the likelihood ratio should perhaps be just be above 1, meaning that the evidence supports $H_p$ more strongly than it supports $H_d$. In criminal cases, the ratio should be several orders of magnitude above one. This approach runs into the same problem as Cheng's. It cannot justify distribution. 

We do not provide all the details of the argument. The reasoning is analogous. Consider the car crash example from before, where $S$ stands for the defendant's speeding, $C$ stands for the statement that the crash caused neck injury, and $E$ stands for the total evidence. The plaintiff's hypothesis $H_p$ is $S\et C$. Suppose $\nicefrac{\pr{E \vert S\et C}}{\pr{E \vert H_d}}>1$, or in other words, the combined hypothesis $S \et C$ has been established by preponderance of the evidence. The question is whether the individual hypotheses have been established by the same standard, specifically, whether $\frac{\pr{E \vert C}}{\pr{E\vert \neg C}} > 1$ and $\frac{\pr{E \vert S}}{\pr{E\vert \neg S}} > 1$.
Focusing on a specific defense hypothesis, $\n S\et \n C$, the following holds:^[The total evidence $E$ can be split into two items: $s$ (evidence for claim $S$) and $c$ (evidence for claim $C$). The inequality holds in \textsf{DAG1}, by replacing $a$ and $b$ with $s$ and $c$ and $A$ and $B$ with $S$ and $C$, because:
\begin{align*}
 \frac{\pr{E \vert S\et C}}{\pr{E\vert \n S\et \n C}} & = \frac{\pr{s \et c \vert S\et C}}{\pr{ s \et c \vert \n S\et \n C}} \\
& =^*  \frac{\pr{s \vert S}\pr{c \vert C}}{\pr{ s \vert \n S} \pr{c \vert  \n C}} \\
& =  \frac{\pr{s \vert S}\pr{c}\pr{c \vert C}\pr{s}}{\pr{ s \vert \n S}\pr{c} \pr{c \vert  \n C}\pr{s}} \\
& =^*  \frac{\pr{s \vert S}\pr{c \vert S}\pr{c \vert C}\pr{s\vert C}}{\pr{ s \vert \n S}\pr{c\vert \n S} \pr{c \vert  \n C}\pr{s\vert \n C}} \\
& =  \frac{\pr{s \et c \vert S}\pr{ s\et c \vert C}}{\pr{ s \et c \vert \n S} \pr{s\et c \vert  \n C}} \\
& =  \frac{\pr{E \vert S}\pr{E \vert C}}{\pr{E \vert \n S} \pr{E \vert  \n C}}
\end{align*}
The steps marked by $*$ in the derivation holds because of the independencies in \textsf{DAG1}.]\raf{M: I think now this proof works, but it is restricted to DAG1 which is ok. Please double check.}
\begin{align}\label{eq:lr-multiplication-two}
1 < \frac{\pr{E \vert S\et C}}{\pr{E\vert \n S\et \n C}} & = \frac{\pr{E\vert S}\pr{E\vert C}}{\pr{ E \vert \n S}\pr{E \vert \n C}}   
\end{align}

\noindent
Note that $\nicefrac{\pr{E\vert S}\pr{E\vert C}}{\pr{E \vert \n S}\pr{E\vert \n C}}$ might be strictly greater than $\nicefrac{\pr{E\vert C}}{\pr{E\vert \n C}}$ or $\nicefrac{\pr{E\vert S}}{\pr{E\vert \n S}}$. It is possible that $\nicefrac{\pr{E \vert S\et C}}{\pr{E \vert H_d}}$ is greater than one, while either $\frac{\pr{E \vert C}}{\pr{E\vert \neg C}}$ and $\frac{\pr{E \vert S}}{\pr{E\vert \neg S}}$ are not, say when they are 3 and 0.5, respectively. Once again, distribution fails. 

A more general  worry lingers,  related to how this comparative likelihood strategy is sensitive to the choice of the hypotheses. There might be pairs of hypotheses that one wishes to compare, say $H_1$ and $H_2$, such that $\pr{E\vert H_1}$ is much (say, at least a few times)  larger  than $\pr{E\vert H_2}$. And yet, $\pr{E \vert H_1}$ is still smaller than $\pr{E \vert \n H_1}$. In such circumstances, the comparative likelihood strategy would be recommending the acceptance of $H_1$ (because it enjoys  stronger evidential support than $H_2$) even though, in absolute terms, the evidence supports the negation of $H_1$ to a greater extent. 




# Rejecting the conjunction principle?
\label{sec:reject} 

A number of strategies the legal probabilist can pursue to theorize about the standard of proof have proven problematic: posterior probability (Section \ref{sec:difficulty}), evidential strength (Section \ref{sec:strength}), comparing posteriors or likelihoods (Section \ref{sec:comparative}). It seems impossible, on probabilistic grounds, to justify both directions of the conjunction principle.  Of course, these strategies do not exhaust the entire space of possibilities. The legal probabilist could pursue other strategies, but the ones examined so far provide good \textit{prima facie} evidence that perseverance will not pay off. It is time to try a different approach. 

Observe that the difficulty with conjunction would not arise without endorsing the conjunction principle. Should legal probabilists simply reject this principle?  So far we have not challenged it, but it is time to scrutinize it more closely. In this section, we provide an epistemic argument and a legal argument to question the conjunction principle. At the same time, we caution that merely rejecting the conjunction principle will not  automatically dissolve the difficulty with conjunction.  More work needs to be done. We take it on in the final section. 


## The legal argument

Before moving further, it is worth asking what the law says about the conjunction principle. The answer, perhaps unsurprisingly, is that the law does say not very much about it. We have been assuming that the law agrees with the conjunction principle. At least, this is what Cohen thought. Matters, however, are not so clear-cut. Looking at legal practice, the conjunction principle is an uncertain principle at best. 

There will be of course differences across countries. We cannot provide a comprehensive analysis here. We shall be content with just a few examples. The best place to look is how jury instructions are formulated. Do they 
obey the conjunction principle? To some extent, they do. For example, here are 
sample jury instructions about negligence in civil cases:

> A negligence claim has three elements:

> 1. [Defendant] did not use ordinary care;

> 2. [Defendants] failure to use ordinary care caused [Plaintiffs] harm; and

> 3. [Plaintiff] is entitled to damages as compensation for that
harm.

> [Plaintiff] must prove each element by a preponderance of the
evidencethat each element is more likely so than not so. If
[Plaintiff] proves each element, your verdict must be for [Plaintiff]. 
If [Plaintiff] does not prove each element, your verdict must
be for [Defendant].^[Standardized Civil Jury Instructions for the District of Columbia, Sec. 5.01 (Civil Jury Instructions, revised edition 2017).]

\noindent
The elements are explicitly separated and the standard of proof is applied to each element separately. This seems to confirm the conjunction principle. Other jury instructions are more ambiguous:

> In order to find that the plaintiff is entitled to recover, you must
decide it is more likely true than not true that:

> 1. the defendant was negligent;

> 2. the plaintiff was harmed; and

> 3. the defendants negligence was a substantial 
factor in causing the plaintiff's harm.^[Alaska Civil Pattern Jury Instructions, Sec. 3.01 (Civil Pattern Jury Instructions 2017).)]

\noindent
The elements are still separated, but the standard of proof ('more likely than not') applies to the conjunction as a whole, not the individual claims.  This second set of jury instruction is at best ambiguous between an atomistic reading (the standard of proof applies to each claim separately) and a holistic reading (the standard of proof applies to the conjunction). Only the atomistic reading would justify the conjunction principle. 

This quick survey of jury instructions gives us some reassurance that, should we decide to reject the conjunction principle, we would not violate a well-entrenched, indispensable legal principle.^[For a detailed analysis of whether jury instructions obey the conjunction principle, see
[@schwartz2017ConjunctionProblemLogic]. Their review of the empirical evidence shows a variety of formulations, not all compliant with the conjunction principle. ]


## Risk accumulation

Beside legal uncertainty about the tenability of the conjunction principle, there are also independent theoretical reasons to question the principle. In current discussions in epistemology about knowledge or justification, a principle similar to the conjunction principle has been contested, because it appears to deny the fact that risks of error accumulate [@Kowalewska2021conjunction].  If one is reasonably sure about the truth of each claim considered separately, one should not be equally reasonably sure of their conjunction. You have checked each page of a book and found no error. So, for each page, you are reasonably sure there is no error. Having checked each page and found no error, can you be equally reasonably sure that the book as a whole contains no error? Not really. As the number of pages grow, it becomes virtually certain that there is at least one error in the book you have overlooked, although for each page you are reasonably sure there is no error [@Makinson1965-MAKTPO-2]. A reasonable doubt about the existence of an error, in one page or another, creeps up as one considers more and more pages. The same observation applies to other contexts, say product quality control. You may be reasonably sure, for each product you checked, that it is free from defects. But you cannot, on this basis alone, be equally reasonably sure that all products you checked are free from defects. Since the risks of error accumulate, you must have missed at least one defective product.^[The phenomenon of risk accumulation can also be formulated without using an explicitly probabilistic language. Say a claim 
is established if all reasonable defeaters have been ruled out. 
You have checked the quality of one product and it appears free from defects, and you have done the same with many other products. They all appear to be free from defects. In this sense, for each product, the claim 'this product is free from defects' has been established, but the conjunctive claim 'every product examined so far is free from defects' might not. After all, you know for sure you  made at least one mistake. You know that from numerous past experiences. Your track record supplies a reasonable defeater to the claim 'every product examined so far is free from defects' that is not a defeater for the individual claims of the form `this product is free from defects'.]  

Risk accumulation challenges aggregation: even if the probability of several claims, considered individually, is above a threshold $t$, their conjunction need not be above $t$. It does not, however, challenge distribution. If, all risks considered, you have good reasons to accept a conjunction, no further risk is involved in accepting any of the conjuncts separately. This is also mirrored by what happens with probabilities. If the probability of the conjunction of several claims is above $t$, so is the probability of each individual claim.<!---This means that risk accumulation does not help one to save the approaches for which distribution fails. \todo{I added a sentence of how this is related to previous sections, should we say sth more here? What's our take on risk accummulation?}--->
<!---\mar{R: What's our take on risk accummulation?} --->
The standard of proof in criminal or civil cases can be understood as a criterion concerning the degree of risk that judicial decisions should not exceed. If this understanding of the standard of proof is correct, the phenomenon of risk accumulation would invalidate the conjunction principle, specifically, it would invalidate aggregation. It would no longer be correct to assume that, if each element is  proven according to the applicable standard, the case as a whole is proven according to the same standard. And, in turn, if the conjunction principle no longer holds, the conjunction paradox will disappear. Or will it?
As we shall now see, matters are not so straightforward.

## Atomistic and holistic approches

Suppose legal probabilists do away with the conjunction principle. Now what?
How should they define standards of proof? Two immediate options 
come to mind, but neither is without problems.  

Let's stipulate that, in order to establish the defendant's guilt beyond a reasonable doubt (or civil liability by preponderance of the evidence or clear and convincing evidence), the party making the accusation should establish each claim, separately, to the requisite probability, say at least .95 (or .5 in a civil case), without needing to establish the conjunction to the requisite probability. Call this the \textit{atomistic account}.  On this view, the prosecution could be in a position to establish guilt beyond a reasonable doubt without establishing the conjunction of different claims with a sufficiently high probability. This account would allow convictions in cases in which the probability of the defendant's guilt is relatively low, just because guilt is a conjunction of several independent claims that separately satisfy the standard of proof. For example, if each constituent claim is established with .95 probability, a composite claim consisting of five subclaims---assuming, as usual, probabilistic independence between the subclaims---would only be established with  probability equal to .77, a far cry from proof beyond a reasonable doubt. This is counterintuitve, as it would allow convictions when the defendant is not very likely to have committed the crime. A similar argument can be run for the civil standard of proof 'preponderance of the evidence'. Under the atomistic account, the composite claim representing the case as a whole would often be established with a probability below the required threshold. The atomistic approach is a non-starter. 

Another option is to require that the prosecution in a criminal case (or the plaintiff in a civil case) establish the accusation as a whole---say the conjunction of $A$ and $B$---to the requisite probability. Call this the \textit{holistic account}. This account is not without problems either.

The standard that applies to one of the conjuncts would depend on what has been achieved for the other conjuncts. For instance, assuming independence, if  $\pr{A}$ is $.96$, then $\pr{B}$ must be at least $.99$ so that $\pr{A\et B}$ is above a $.95$ threshold. But if $\pr{A}$ is $.9999$, then $\pr{B}$ must only be slightly greater than $.95$ to reach the same threshold. Thus, the holistic account might require that the elements of an accusation be proven to different probabilities---and thus different standards---depending on how well other claims have been established. This result runs counter to the tacit assumption that each element should be established to the same standard of proof [@Urbaniak2019standards2]. 

Fortunately, this challenge can be addressed. It is true that different elements will be established with different probabilities, depending on the probabilities of the other elements. But this follows from the fact that the prosecution or the plaintiff may choose different strategies to argue their case.  They may decide that, since they have strong evidence for one element and weaker evidence for the other, one element should be established with a higher probability than the other. What matters is that the case as a whole meets the required threshold, and this objective can be achieved via different means. What will never happen is that, while the case as a whole meets the threshold, one of the constituent elements does not. As seen earlier, the probability of the conjunction never exceeds the probability of one of the conjunct, or in other words, distribution is never violated.

A more difficult challenge is the observation that the proof of $A\et B$ would impose a higher requirement on the separate probabilities of the conjuncts. If the conjunction $A\et B$ is to be proven with at least .95 probability, the individual conjuncts should be established with probability higher than .95. So the more constituent claims, the higher the posterior probability for each claim needed for the conjunction to meet the requisite probability threshold. 

This challenge is best appreciated by running some numbers. Assume, for the sake of illustration, the independence and equiprobability of the constituent claims. If a composite claim consists of $k$ individual claims, these individual claims will have to be established with probability of at least $t^{1/k}$, where $t$ is the threshold to be applied to the composite claim.\footnote{Let $p$ the probability of each constituent claim. To meet threshold $t$, the probability of the composite claim, $p^k$, should satisfy the constraint $p^k>t$, or in other words, $p>t^{1/k}$.} For example, if there are ten constituent claims, they will have to be proven with $.5^{1/10}=.93$ even if the probability threshold is only $.5$. If the threshold is more stringent, as is appropriate in criminal cases, say $.95$, each individual claim will have to be proven with near certainty. This would make the task extremely demanding on the prosecution, if not downright impossible. If there are ten constituent claims, they will have to be proven with $.95^{1/10}=.995$. So the plaintiff or the prosecution would face the demanding task of establishing each element of the accusation beyond what the standard of proof would seem to require.



<!-- ```{r, echo=FALSE} -->
<!-- 0.5^(1/10) -->
<!-- ``` -->

<!-- ```{r, echo=FALSE} -->
<!-- 0.95^(1/10) -->
<!-- ``` -->

<!-- ```{r, echo=FALSE} -->
<!-- 0.95^(10) -->
<!-- ``` -->



We reached an impasse. Under the atomistic approach, the standard is too lax because it allows for findings of liability when the defendant quite likely committed no wrong. Under the holistic approach, the standard is too demanding on the prosecution (or the plaintiff) because it requires the individual claims to be established with extremely high probabilities.

## Not asking too much

Consider again the holistic approach. It is true that the individual elements (the individual conjuncts) should be established with a higher probability than the case as a whole (the conjunction). This would seem to impose an unreasonably stringent burden of proof on the prosecution or the plaintiff. But the burden might not be as unreasonable as it appears at first.  As @dawid1987difficulty pointed out, in one of the earliest attempts to solve the conjunction paradox from a probabilistic perspective, the prior probabilities of the conjuncts will also be higher than the prior probability of their conjunction:

\begin{quote}
\dots it is not asking too much of the plaintiff to establish the case as a whole with a posterior probability exceeding one half, even though this means  that the several component issues must be established with much larger posterior probabilities; for the \textit{prior}  probabilities of the components will also be correspondingly larger, compared with that of their conjunction~[p.~97].
 \end{quote}
 
Dawid's proposal seems compelling. The prior probabilities of the conjuncts are surely higher than the prior probability of the conjunction. But why, exactly, is it `not asking too much' to establish the individual conjuncts by a higher threshold than the case as a whole? Perhaps, Dawid is pointing out that the \textit{difference} between prior and posterior probabilities of the individual claims which the evidence should bring about will not be unreasonably large.  In other words, Dawid might be recommending---as the rest of his paper suggests---that the standard of proof not be understood solely in terms of posterior probabilities. Measures of how strongly each claim issupported by the evidence, such as the Bayes' factor or the likelihood ratio, account for the difference between prior and posterior probabilities. So, presumably, Dawid is recommending these measures as better suited to formalize the standard of proof. 

Now, as the reader will have realized, we have pursued Dawid's strategy already in Section \ref{sec:strength}. This strategy can justify, on purely probabilistic grounds, one direction of the conjunction principle: aggregation. The evidential support---measured by the Bayes' factor or the likelihood ratio---for the conjunction often exceeds the individual support for (at least one of) the individual claims. This is a success, especially because the failure of aggregation motivated Cohen's formulation of the conjunction paradox. Unfortunately, we have  already seen that this strategy invalidates a previously unchallenged direction of the conjunction principle: distribution. 



# The proposal
\label{sec:proposal}

Here is where we have gotten so far. There might be good reasons to reject the conjunction principle, but rejecting it does not automatically solve the difficulty with conjunction. We still need a theory that explains how individual claims are combined, together with the available evidence, to form more complex claims. The conjunction principle provides a recipe---a very simple one at that---to combine individual claims and form conjunctive claims. If that recipe is not right, a good theory of the standard of proof should  provide an alternative recipe for combining individual claims. 

Our proposal is inspired by the story model of adjudication [@penn1993; @wagenaar1993anchored] and the relative plausibility theory [@Pardo2008judicial; @AllenPardo2019relative]. It posits that prosecutors and plaintiffs should aim to establish a unified narrative of what happened or explanation of the evidence, not establish each individual element of wrongdoing separately. As we shall see, any attempt to proceed in a piecemeal manner implicitly requires, sooner or later, to weave the different elements together into a unified whole. Our argument consists of two key ideas. First, the guilt or civil liability of a defendant cannot be equated with a generic statement of guilt or civil liability as defined in the law. The allegations against the defendant facing trial should always be grounded in specific details. Call this the specificity argument. Second, it is erroneous to think of someone's guilt or civil liability as the mere conjunction of separate claims. The separate claims must be coherently unified, not just added up in a conjunction.  Call this the unity argument. 


## The specificity argument

We start with the specificity argument. The probabilistic interpretation of proof standards usually posits a threshold that applies to the posterior probability of a *generic* hypothesis, such as the defendant is guilty of a crime, call it $G$, or civilly liable, call it $L$. In criminal cases, the requirement is formulated as follows: the evidence $E$ presented at trial establishes guilt beyond a reasonable doubt provided $\pr{G \vert E}$ is above a suitable threshold, say .95. The threshold is lower in civil trials. Civil liability is proven by preponderance provided  $\pr{L \vert E}$ is above a suitable threshold, say .5. 

This formulation conflates two things. The act of wrongdoing as defined in the applicable law is one thing. The way in which the wrongdoing is established in court is another thing. The wrongdoing is defined in the law in a generic manner and its definition is applicable across a class of situations, whereas the way the wrongdoing is established in court is specific to a unique situation and tailored to the individual defendant. A prosecutor in a criminal case does not just establish that the defendant assaulted the victim in one way or another, but rather, that the defendant behaved in such and such a manner in this time and place, and that the behavior in question fulfills the legal definition of assault. The requirement of specificity is a consequence of the fact that defendants have a right to be informed with sufficient detail and be in a position to prepare a defense.^[How the objective of specificity is actually achieved in trial proceedings is a difficult question. Different countries and jurisdictions may use different approaches, say, through the discovery process itself or a request of a bill of particulars. For example, at the end of the 19th century, the Supreme Court of Massachusetts wrote: 'It is always open to the defendant to move the judge before whom the trial is had to order the prosecuting attorney to give a more particular description, in the nature of a specification or bill of particulars, of the acts on which he intends to rely, and to suspend the trial until this can be done; and such an order will be made whenever it appears to be necessary to enable the defendant to meet the charge against him, or to avoid danger of injustice.' (Commonwealth v. Sherman, 95 Mass. 248, 13 Allen 248, 250, 1866).]

If this is right, the probabilistic interpretation of proof standards should be revised. The generic statement that the defendant is guilty or civilly liable should be replaced by a more fine-grained factual hypothesis, call it $H_p$, the hypothesis put forward by the prosecutor (or the plaintiff in a civil case), for example, that the defendant, given reasonably specific circumstances, approached the victim, pushed and kicked the victim to the ground, and then run away. Hypothesis $H_p$ is a more precise description of what happened and entails that the defendant committed the criminal offense or civil wrong of which they are accused. In defining the standard of proof, instead of saying---generically---that $\pr{G \vert E}$ or $\pr{L \vert E}$ should be above a suitable threshold, a probabilistic interpretation should read: civil or criminal liability is proven by the applicable standard provided $\Pr(H_p \vert E)$  is above a suitable threshold, where $H_p$ is a reasonably specific description of what happened according to the prosecution or the plaintiff.

This revision may appear inconsequential, but it is not. It is the revision we invoked to address the puzzles of naked statistical evidence. Here is the gist of the argument. Consider the prisoner hypothetical, a standard example of naked statistical evidence. The naked statistics $E_s$ make the prisoner on trial .99 likely to be guilty, that is, $\pr{G \vert E_s} =.99$. Given the known facts in the prisoner hypothetical, it is $.99$ likely that the prisoner on trial was one of those who attacked and killed the guard. But this is a very generic claim. It merely asserts that the prisoner was---with very high probability---one of those who killed the guard, without specifying what he did, what role he played in the attack, how he killed the guard, etc. If the prosecution offered a more specific incriminating hypothesis $H_p$, the probability $\pr{H_p \vert E_{s}}$ of this hypothesis based on the naked statistical evidence $E_s$ would be well below $.99$, even though $\pr{G \vert E_s}=.99$. That the prisoner on trial is most likely guilty is an artifact of the choice of a generic hypothesis $G$. When this hypothesis is made more specific---as should be---this probability drops significantly. And the puzzle of naked statistical evidence disappears.  For a detailed articulation of this argument, see Chapter 12.^[An earlier version of this argument can be found in @di2013statistics and @DiBello2021Specific.] \todo{REFERENCE TO EARLIER CHAPTER}


<!-- One upshot of this holistic approach is that it highlights a distinction that is often made, between statements of fact and  statements of law. That the defendant was driving on highway 1 and the car was moving erratically left and right is a statement of fact. That such an occurrence counts as `negligent driving' is a statement of law.  The instance 'car moving erratically' is subsumed under the category 'driving erratically'. A defense lawyer could challenge the inference. Perhaps the car was moving erratically because of other reasons, say, malfunctioning of the brakes due to manifacturing defects not attributable to the driver. \textbf{TO BE COMPLETED} -->


<!---
Thus, the prosecutor or plaintiff should not just establish element-specific narratives, but a unified narrative that entails each element of wrongdoing. The options on the table are as follows. The prosecutor should (1) prove each element as defined by law (atomistic approach); (2) the conjunction of each element (holistic approach); well-specified element-specific narratives (element-specific narratives approach); (4) a well-specified unified narrative that entails each element (narrative-based holistic approach). We found (1) and (2) wanting. As noted above, (3) gives rise to a similar problem as (1) if only in narrative form. The last option, call it *holistic approach revised*, outperforms the others. In what follows, we show why (4) is the best---in fact, the only possible---option available.
-->

<!---
For concreteness, consider a Washington statute about negligent driving in the first degree, a criminal offense.^[(1)(a) A person is guilty of negligent driving in the first degree if he or she operates a motor vehicle in a manner that is both negligent and endangers or is likely to endanger any person or property, and exhibits the effects of having consumed liquor or marijuana or any drug or exhibits the effects of having inhaled or ingested any chemical, whether or not a legal substance, for its intoxicating or hallucinatory effects. RCW 46.61.5249] A prosecutor who wishes to establish beyond a reasonable doubt that the defendant engaged in negligent driving should establish:

\begin{quote}
(a) that the defendant operated a vehicle;
(b) that, in operating a vehicle, the defendant did so in a negligent manner;
(c) that, in operating a vehicle, the defendant did so in a manner likely to endanger a person or property; and
(d) that, in operating the vehicle, the defendant exhibited the signs of intoxication by liquor or  drugs.
\end{quote}
\noindent

It would be difficult for the prosecutor to establish each each claim in general. It would also be difficult to establish these claims in isolation. The prosecutor cannot simply establish that (a) the defendant was operating a vehicle at some point in time and -- separate from that -- that (d) at some point in time the defendant exhibited signs of intoxication. The prosecutor should ground both claims within the same spatio-temporal frame. A prosecutor could establish, say, that the defendant was driving on a busy Highway 1 north San Francisco at about 8:30 PM; the car was moving erratically left and right, cutting across other lanes; the defendant was stopped by a police officer who conducted a field sobriety test and used a breathalyzer, both tests showing a higher-than-normal quantity of alcohol. This narrative, if true, establishes each element of the offense. 

Two points are worth noting here. First, the narrative offered by the prosecutor is more specific that the mere conjunction of elements (a) through (d). The narrative exemplifies one way among many others in which the defendant could have engaged in negligent driving. This point underscores that prosecutors establish the elements of a crime by establishing a set of specific facts, typically woven together in a narrative. Second, because of its greater specificity, the narrative as whole will be less probable that the individual claims or even the conjunction of such claims. [DISSOLUTION OF PARADOX. NARRATIVE IS LESS LIKELY THAN INDIVIDUAL CLAIMS. SURE. IT IS MORE SPECIFIC. NO PARADOX. REALLY?]


The *revised holistic approach* -- as we understand it now --- consists in establishing unifying narrative that entails the different elements of wrongdoing as defined by law. The specificity of the narrative far exceed the specificity of the conjunction of each element understood generically. This is different from merely requiring that the conjunction, say, $A \wedge B$ being proven by the required standard. In addition, a narrative ensures that all different elements are part of the same unit or episode of wrongdoing.  Instead, the mere conjunction would still be missing the required connections that ensure that, say, $A$ and $B$ are part of the same unit of wrongdoing.


--->


## The unity argument


The specificity argument addresses the problem of naked statistical evidence, but also provides the necessary background for addressing the difficultly about conjunction. In its formulation, not only are $G$ and $L$ understood as generic claims, but they are also understood as *mere conjunctions* of simpler claims that correspond to the elements of wrongdoing in the applicable law. Since the probability of a conjunction is often lower than the probability of its  conjuncts, the individual claims can be established with a suitably high probability that meets the required threshold even though the conjunction as a whole fails to meet the same threshold.   This mismatch gives rise to the difficulty with conjunction. 

Here lies another conflation. It is one thing to establish that the defendant committed $A$ and $B$, where the wrongdoing in question is defined in the law as comprising two elements, $A$ and $B$. It is another thing to establish that the defendant committed the wrong. <!---Someone's guilt or civil liability cannot be a mere conjunction of the claims corresponding to the elements of wrongdoing as defined in the law.  These events, taken as a whole, can be subsumed under the legal definition that consists of several discrete elements.--> The conjunction paradox---in particular, the conjunction principle---assumes that criminal or civil wrongdoings are  mere collections of separate elements. But the law is more nuanced. <!---Someone's guilt is a state of affairs that is described by a well-specified series of events that possess a coherent, structured unity.--> Legal definitions often impose a structured unity on how the different elements relate to one another. This unity could take many forms. It could be a temporal unity, the unity that exists between a plan and its execution, or the unity between the \textit{actus reus} and the \textit{mens rea} in a criminal offense. 

<!---^[Consider, for example, the allegation of negligent misrepresentation to be established by clear and convincing evidence. As an illustration, we follow the jury instructions of the State of Washington. (EXACT REFERENCE) These are the elements a plaintiff should establish:
(E1) defendant supplied false information to guide plaintiff in their business transaction; 
(E2) defendant knew the false information was supplied to guide plaintiff;
(E3) defendant was negligent in obtaining or communicating the false information;
(E4) plaintiff relied on the false information;
(E5) plaintiff's reliance on the false information was reasonable; and
(E6) the false information proximately caused damages to the plaintiff.
For the plaintiff to prevail in this type of case, they should prove each element by the required standard. What does that actually require? The plaintiff should first establish, at a minimum, that the defendant supplied false information for the guidance of the plaintiff in the course of a business transaction. Say the defendant, a owner of a vacation resort, told plaintiff, a travel agent, that the resort included amenities that were not actually there, and the plaintiff decided to book several clients at the defendant's resort instead of other resorts.
The plaintiff could offer copies of emails communication 
or screenshots from the resort's website. This would take care of the first element. The second element qualifies the first, in that the defendant *knew* they were supplying 
false information to guide the business transaction.  Clearly, establishing the second element presupposes having already established the first since the second element is a qualification of the first. The truth of the second element entails the truth of the first. If the defendant knowingly supplied false information (second element), they did supply false information (first element). The third element requires to show that the defendant was negligent in obtaining or communicating the false information. This is another qualification that applies to the first element and cannot be proven without having already proven the first element. The fourth and fifth element should be understood together. The plaintiff relied on the false information (fourth element) and such reliance was reasonable (fifth element). In turn, establishing the fourth and fifth element presupposes that the plaintiff did supply false information to begin with (first element).  Finally, the sixth element concerns causation of harm. This is again a qualification of the first element and cannot be established without having already established the first element.] 
--->

But what if the law, at least in simple cases, did not impose any structure onto the different elements of wrongdoing? In such cases, one might argue, the conjunction principle---by which the different claims are simply added to one another in a conjunction---would be adequate. But even in simple cases in which the different elements of wrongdoing are not structured in any explicitly way, it would still be a mistake to follow the conjunction principle.  To see why, consider a case in which only two elements must be proven. Element 1: the defendant's conduct caused a bodily injury to the victim. Element 2: the defendant's conduct consisted in reckless driving. This offense is sometimes called 'vehicular assault'.^[See, for example, Arizona Revised Statutes Title 13. Criminal Code \ 131204 ('A person commits aggravated assault if the person commits assault ... under any of the following circumstances... [such as] ... the person causes serious physical injury to another ... [or]... the person uses a deadly weapon or dangerous instrument.').] The two elements each add novel information. It could be that the defendant's driving caused an injury to victim, but the driving was not reckless, or the driving was reckless, but no injury ensued. Neither element is presupposed by the other. Crucially, here the law does not impose any explicit structure between the elements. But---we will now see---the unity argument  still applies at a conceptual level. 

Let's think about how to establish the claim of reckless driving that 
caused injury to the victim. One option is to offer a detailed reconstruction of what happened. The reconstruction could go something like this. The defendant was driving above the speed limit, veering left and right. The defendant's reached a school crosswalk when children were getting out of school. The defendant hit a child on the crosswalk who was then pushed against a light pole on the sidewalk incurring a head injury. Suppose this story is supported by several testimonies by other children, people standing around, police officers, paramedics. There is plenty of supporting evidence as the incident occurred in the middle of the day. Taken at face value, this story does establish both elements: reckless driving and cause of injury. Parts of the story are relevant for element 1 (reckless driving) and other parts are relevant for element 2 (cause of injury). The two cannot be neatly separated, however. Still, what is crucial is that the different parts of the story are part of the same episode, the same unit of wrongdoing. 

Could the prosecutor prove vehicular assault in a piecemeal manner? Suppose the prosecutor attempted to do that, by establishing, first, that the defendant drove recklessly, and second---*separately from the first element*---that the defendant's action caused injury. As noted before in the specificity argument, it is not enough to establish that the defendant drove recklessly at some point in time somewhere.  Nor is it enough to establish that the defendant's action caused injury. The prosecutor should offer a  specific story detailing what happened, a story relevant for the first element and a story relevant for the second element. Say this expectation of specificity is met. Suppose the prosecutor did not just establish generic element 1 and generic element 2, but rather, a reasonably detailed story for ech element. Would that be enough? It wouldn't. Even if each element---that is, each story associated with each element---was established by the required standard, there would still be something missing here. 

The prosecutor should establish that the two elements are part of the same unity of wrongdoing. It must be *this* reckless driving that caused *this* injury. So, under the piecemeal approach, the prosecutor would be tasked with establishing three claims: (a) the defendant, in some well-specified circumstances, was driving reckless; (b) the defendant, in some well-specified circumstances, caused injury to the victim; and (c) the well-specified circumstances in (a) and (b) are part of the same episode. But once (c) is established, the prosecutor would have effectively established the charge by the required standard in accordance with the holistic approach. The prosecutor did not only establish each separate element (two separate stories) but also combine the two elements (the two stories) together. Once the piecemeal approach is pursued to its logical conclusion, it coincides with the holistic approach.^[We should be clear that it is not enough for the prosecutor or plaintiff to provide well-specified narrative in support of their allegations, even when they are well-supported by the evidence. When the two narratives are combined into one narrative, its probability could well be below the threshold. If we only require that each element-specific narrative be proven, a defendant could be found criminally or civilly liable even though it is unlikely that they committed the alleged wrongful act. This counter-intuitive result is similar to the one that arose with the atomistic approach.]


Let's summarize the unity argument in schematic form. If the prosecutor or the plaintiff is expected to establish claim $A$ and $B$ by the required standard, what the law actually requires---even in terms of the piecemeal approach---is (a) to establish $A$; (b) to establish $B$; and (c) to establish $A$ and $B$ are part of the same unit of wrongdoing by the required standard. Item (c) is often implicit, which leaves the impression that the law only requires to establish (a) and (b) separately. Interestingly, (c) entails (a) and (b). In fact, (c) amounts to establishing a unified story, narrative or theory about what happened. Such a narrative should be subsumed under the different elements of wrongdoing as defined in the law. The piecemeal approach and the holistic approach, therefore, converge.

To be sure, not all wrongful acts, in civil or criminal cases, require the prosecutor 
or the plaintiff to establish a unified *spatio-temporal* narrative.  It might not be necessary to show that all elements of an offense occurred at the same point in time or in close succession one after the other. Some wrongful acts may consist of a pattern of acts that stretches for several days, months or even years. There may be temporal and spatial gaps that cannot not be filled.
We consider several of these examples in our discussion of naked statistical evidence in Chapter 12. \todo{SEE PREVIOUS CHAPTER} Be that as it may, an accusation of wrongdoing in a criminal or civil case should still have a degree of cohesive unity. The  acts and occurrences that constitute the wrongdoing should belong to the same wrongful act. It is this unity which the plaintiff and the prosecution must establish when they make their case. One way to establish this unity is by providing a unifying narrative, but this need not be the only way. A unifying 'theory' of what happened or a cohesive 'explanation' of the evidence could all deliver the structured unity that is needed to establish the defendant's liability. 


## The conjuction principle trivialized 

What do the specificity and unity argument tell us about the difficulty with conjunction? The maxim that prosecutors and plaintiffs should aim to establish a well-specified, unified account of the wrongdoing ends up trivializing the conjunction principle and ultimately dissolving the difficulty about conjunction.  How so? Suppose the prosecutor established a narrative $N$ by a very high probability, say above the required threshold for proof beyond a reasonable doubt. Denote the elements of wrongdoing by $EL_1, EL_2, \dots$. Then, 
\[\text{ $\pr{EL_1\wedge EL_1 \wedge \dots \wedge EL_k \vert N}=\pr{EL_i \vert N} = 1$ for any $i=\{1, 2, ..., k\}$}.\]
\noindent

Both directions of the conjunction principle, aggregation and distribution, are now trivially 
satisfied. By conditioning on the narrative $N$, each individual claim has a probability of one 
and thus their conjunction also has a probability of one.  The conjunction principle is  reduced to a deductive check that the elements of the wrongdoing follow from the narrative put forward. The narrative, however, has a probability short of one, up to whatever value is required to meet the governing standard of proof. The standard applies to the narrative as a whole, and only indirectly---via a deductive check---to the individual elements. This trivialization of the conjunction principle is perhaps unsurprising. If anything, it mirrors the fact that no lawyer has ever been concerned with the reliability of inference rules such conjunction elimination and conjunction introduction.

Admittedly, this trivialization does not vindicate the conjunction principle as stated at the beginning of this chapter in any way. But---we have argued---the conjunction principle is not the right principle to combine simple, individual claims into more complex claims. Mere conjunctive addition does not get us very far in complex legal cases.  There is a key distinction between the narrative $N$ (or theory, explanation) and the mere conjunction $EL_1\wedge EL_1 \wedge \dots \wedge EL_k$  of the elements of wrongdoing. The narrative describes one way among many of instantiating the conjunction. The claims that constitute a narrative (or unified theory, explanation) need not map neatly onto the elements of the wrongdoing. The narrative will comprise claims about the evidence itself and how the evidence supports other claims in the narrative, say that witnesses were standing around when the defendant's car hit the child. The narrative (or theory, explanation) will not only comprise a description of what happened but also of how we know it is what happened. 

If more structure is needed than the mere conjunctive addition of the elements of wrongdoing, the conjunction principle should be rejected and replaced by a more nuanced method to aggregate evidence and construct complex claims. We have gestured at what this method should look like---roughly, it relies on Bayesian networks. Here is the general idea. The individual claims or hypotheses to be established and the supporting pieces of evidence are represented as nodes in a Bayesian network. Some nodes will count as 'evidence nodes' and others as 'claim nodes' or `hypothesis nodes'. A narrative (or theory, explanation) is then a suitable collection of evidence nodes and hypothesis nodes that are connected to one another by relationships of conditional probabilistic dependence (represented by arrows in the Bayesian network), meant to capture causal or evidential relationships. This web of dependencies affords the narrative its unity and coherence. More details on this point are in Part III of the book devoted to the evaluation and assessment of evidence, especially Chapter 7 and 8 on Bayesian networks.\todo{REF TO OTHER CHAPTERS}


## Probability, specificity and completeness

We conclude this chapter with a few general observations. To start, the distinction between narrative and the mere conjunction of elements matters for how we should understand the standard of proof. Other things being equal, the conjunction is more probable on the evidence than the narrative, and each conjunct even more probable. But this does not mean that the mere conjunction is established by a higher standard of proof than the narrative. As we argued in Chapter 12 on naked statistical evidence, \todo{REFERENCE TO EARLIER CHAPTERS} a highly probable narrative that nevertheless lacks the desired degree of specificity will fail to meet the standard of proof. By contrast, a more specific narrative that is otherwise less probable than the mere conjunction might well meet the standard. On this account, the standard of proof is sensitive to two variables: (1) the posterior probability of the proposed narrative (or theory, explanation) given the evidence presented at trial; and (2) the degree of specificity and unity of the narrative (or theory, explanation). 

But a unified narrative (theory, or explanation) may still fall short if it contains gaps---that is, if there are propositions about which the narrative should make a commitment but instead remains neutral about them. The narrative may also have incomplete evidence if it contains evidential gaps---that it, if there are evidence-bearing propositions whose probability is non-negligible (given what is assumed to be true in the narrative), but are nevertheless not included in the evidence nodes. So another variable worth adding to posterior probability and specificity is (3) the completeness of the evidence presented at trial. Could the probability of someone's guilt be extremely high just because the evidence presented is one-sided and missing crucial pieces of information? It surely can. If the probability of liability is high because the evidence is partial, liability was not proven beyond a reasonable doubt (and perhaps not even by a lower standard such as preponderance of the evidence).^[It is a matter of dispute whether knowledge about missing or partial evidence should affect the posterior probability. After all, if we know that some evidence is missing, shouldn't we revise the assessment of the posterior probability of the hypothesis? The problem is that the content of the missing evidence is unknown. The missing evidence might increase or decrease this probability. We cannot know that without knowing  the content of the evidence. If we knew how the missing evidence would affect our judgment about the defendant's guilt, the evidence would no longer be---strictly speaking---missing. That is why we prefer to add the completeness of the evidence as a third variable to consider. For a recent case in which missing evidence was alleged as a reason for reversing a verdict of guilt, see Johnson v.\ Premo, 315 Oregon Appeal 1 (2021).]

On our proposal, the standard of proof is informed by three maxims:

1. The probability of the defendant's liability should be sufficiently (or reasonably) high;

2. The narrative (or theory, explanation) of the defendant's liability should be sufficiently (or reasonably) specific; and  

3. The supporting evidence should be sufficiently (or reasonably) complete.

\noindent 
At this point, the reader might wonder: are we ultimately giving up on legal probabilism? We are only giving up on *traditional* legal probabilism. Even though ideas such as specificity, unity and completeness of the evidence cannot be formalized in the language of posterior probability alone, they can be formalized as properties of Bayesian networks.^[A more detailed articulation of the ideas of narrative unity and coherence, factual specificity, and evidential gaps is carried out in Chapter 10. An early development of this approach can be found in @urbaniak2018narration.] \todo{REF TO OTHER CHAPTERS}

<!---
Once we represent the relevant claims and pieces of evidence and their interaction as a Bayesian network (or a set thereof), some of the binary nodes corresponding to various propositions are marked as evidence nodes, and some qualify as narration nodes---these are the nodes that various sides or proposed scenarios disagree about. Within this setup various intuitively needed notions can be explicated. For instance, an accusing narration should 'make sense' of evidence in the following sense: any item of evidence
presented, should have sufficiently high posterior probability in the network updated with the total evidence obtained. In contrast, a defending narration is supposed to explain evidence in a different sense: if the defense story is rather minimal and mostly constitutes in rebutting the accusations, it isnt reasonable to expect the defense to explain all pieces of evidence, and it is not reasonable to expect the defense to
provide a story explaining how each piece of evidence came into existence. Rather, the defense should argue that the probability of the evidence being as it is while the defenses narration is true isn't below a rejection threshold. As another example, a piece of evidence is missing if there is an evidential proposition such that the probability of it being instantiated (given what is known about cases of a given type and about the particularities of a given case) is non-negligible, but it is not included in the evidence. Yet another example: a narration contains gaps just in case  there are claims that the narration should choose from (nodes that a narration should consider instantiated), but it does not do so.  This, of course, is very hand-wavy at this stage, but at this point we just want to leave the reader with the impression that more can be explicated with Bayesian networks than one might initially expect, leaving detailed development to a different chapter. \todo{REF TO APPROPRIATE CHAPTER}
--->

Probability theory plays also a second-order role in our argument. Specifically, our analysis of the standard of proof---which combines three ingredients: posterior probability, specificity and completeness---can be evaluated at the meta-level using concepts from probability theory. Suppose we wish to compare a trial system that convicts defendants on the basis of claims that are generic but highly probable, as opposed to a trial system that convicts defendants on the basis of  claims that are more specific but less probable. Which trial system will make fewer mistakes---fewer false convictions and false acquittals---in the long run? The answer is not obvious. But the question can be made precise in the language of probability. The question concerns the diagnostic properties of the two trial systems, such as  their rate of false positives and false negatives. We examine this question in Chapter 16. We conduct a simulation study of the impact of properties of narratives (as defined in Bayesian networks) on their expected accuracy. \todo{REFERENCE TO LATER CHAPTER} To anticipate, we argue that more specific claims are liable to more extensive adversarial scrutiny than generic claims. The more specific someone's claim, the more liable to be attacked. At the same time, if a specific claim resists adversarial scrutiny, it becomes more firmly established than a less specific claim that was not scrutinized. So specificity plays an accuracy-conducive role even though more specific claims are, other things being equal, less probable than more generic claims.^[The argument in this chapter is sympathetic with a point made by Karl @Popper2002 about science: 'Science does not aim, primarily, at high probabilities. It aims at a high informative content, well backed by experience. But a hypothesis may be very probable simply because it tells us nothing, or very little. A high degree of probability is therefore not an indication of goodness' (416, Appendix *IX).]
<!---That is why specificity should be an important ingredient in any theory of the standard of proof.^[A similar argument can be run for the notion of completeness of the evidence.] --->

<!---
\mar{R: do you still want to add these?}
THIGNS TO ADD:
1. ROLE OF COMPLETENESS OF EVIDENCE, SEE OREGON DNA CASE, MEANT TO SHOW THAT HOLISTIC AOPPROACH IS NOT AD HOC, WEIGHT, RESILIENCE IMPORTANT FOR ASESSING STRENGHT OF EVIDENCE AND STANDART OF PROOF, GUILT COULD BE HIGHLY PROBABLE GIVEN AVAILABLE EVIDEMCE, BUT THIS NEED NOT BE ENOUGH IF EVIDENCE THAT SHOULD BE THERE IS MISSING
--->









<!-- \noindent\makebox[\linewidth]{\rule{\paperwidth}{1pt}} -->

<!-- CHAPTER ENDS HERE -->

<!-- \noindent\makebox[\linewidth]{\rule{\paperwidth}{1pt}} -->


<!-- # Extra unstructured materials -->




<!-- ## Notes on the argument  -->

<!-- - Need to insists on risk accumulation. Many reasons why probability of conjunction is less than probability of conjunct. This cannot be denied. Examples. Illustrations.  -->

<!-- - What is the role of defeaters? Say A has defeater Da (undercutter) and not-A (rebutter) and B has defeater Db (undercutter) and not-B (rebutter). If we can rule out Da and Db and not-A and not-B, could we say that the we have ruled out all defeaters for AB? Not necessarily. The rebutter not-AB need not be a rebutter for A or B, and there could be an undercutter Dab that does not undercut A or B. Need to give examples.  -->

<!-- - Key move. We do not establish claims A and B in a vacuum. They need to be part of a narrative, explanation, story. The story gives the broader context, but also explains how the evidence support the claim and how challenges are addressed. Call this Na and Nb. The task then is to put together Na and Nb.  -->

<!-- - Could whole narrative N be more coherent (and thus more probable) than Na and Nb? Boost of probability due to coherence.  -->

<!-- - Elements of an accusation are structured in different ways. Often we see an "accumulation" structure, where each elements is intended to provide an even more specific description of the crime. So the structure is something like this. Element 1: A. Element 2: A + B. Element 3: (A + B) + C. Etc. I such cases the conjunction paradox does not apply. -->


<!-- - Suppose we isolate parts of the story that are relevant for Element 1, call it S1, and parts of the story that are relevant for Element 2, call it S2. Since both sub-stories are indexed through a time and space, it is clear that they form the same unity. Now S1 and S2 will be more likely than S. There is no doubt about that. This is a probabilistic fact. And  fact about the world. But S1 and S2 will also be much less detailed than S. There would be several question that S1 and S2 do not address. S1 will not address whether the driver hit anyone or got on the sidewalk, while S2 presumably would not address what the car was driving before hitting the child. Now specificity is part of the standard of proof. The standard of proof require adequate probability and adequate specificity. These two direction cannot be directly compared. Sp, in a sense S1 and S2 are more firmly established because they are more likely, but S is more firmly established because it is more specific. The standard of proof includes probability and specificity. -->

<!-- - Does that mean that specificity is not a probabilstic notion? Cite Popper. How does the notion of specificity relate to coherence? -->

<!-- - Another dimension of the standard of proof is however resistance to challenges, or else one could just come up with a very specific story that one has completely made up. A more specific story is more liable to be proven wrong. This is basically why it is less probable, other things being equal. There are any more ways it could be false compared to a less specific story. But now compare a very specific story -- that ahs survived all reasonable challenges directed at the story -- to a less specific story that -- that has survived all reasonable challenges directed at the story. Which one should we be more impressed by? It seems taht the more specific story that has survived all the challenges should be at least as well (if not more) established than the less specific story that has survived all the challenges? Here is an analogy. Suppose you have gone out in a mountaineering expedition, while your friend has just gone out to stroll. You both comeback home safe (you survived the challenges), but which one is more indicative of strong survival skill? Clearly, the fact that your survived a mountaineer expedition, not just a stroll. The initial probability of survival was lower in the former than in the latter. -->

<!-- - But the problem recurs. What does it mean to survive challenges? Can this be explicated in probabilistic terms? -->

<!-- - Suppose  we are understanding the standard of proof as consisting of: probability, specificity, resistance to challenge, completeness of evidence. The objection here might be that this criteria could violate accuracy maximization. Perhaps we need a simulation to show that following these four criteria is actually better for accuracy than just probability. Probability does not capture all sources of uncertainty at play and thus it is an inadequate criterion for decision. -->

<!-- ## What is our argument? [M:sketch, not intended to be part of chapter] -->

<!-- OPTION 1. Given risk accumulation, we reject the conjunction  -->
<!-- principle (specifically, we reject aggregation). But if we reject aggregation,  -->
<!-- we are still left with two problems: (1) individual claims will have to be proven with extremely  -->
<!-- high probability, which seems to make the task too demanding for prosecutors and (2) -->
<!-- claims will have to be proven with different standard of proof not equally,  -->
<!-- not standard would not apply uniformly across claims. Problem (1) can be addressed by noting that individual claims will  have higher prior probabilities, so this would not make the task too demanding on prosecutors. We need here to develop Dawid's argument in the direction of factoring out the priors. Problem (2). Not sure.  -->

<!-- CHALLENGE We need to come up with a formal way to factor out priors. See some attempts below. But since measures of evidential support one way or another depend on priors, it's not clear how this could be done properly.  -->
<!-- So basically, we would need to come up with a formal measure of "how hard it is prove something" and this formal measures should capture the "burden of proof" and should return the intuitive results we expect.  -->

<!-- OPTION 2. We switch to narratives. Prosecutor establishes a narrative to the required probability, $\pr{N \vert E}$. The narratives entails all the separate claims that needs to be proven. Conditional on the narrative, the conjunction principle is trivially satisfied. -->

<!-- PROS. The switch to narratives captures the complexity of proof, different structured claims in relationships of dependence or independence. -->

<!-- PROBLEM. Not clear what this move actually accomplishes. Seems it changes the topic.  -->

<!-- OPTION 3. We attack the conjunction principle upfront and show there are two different principles, each captured by probability theory -- i.e. posterior probability captures distribution, while strength of evidence captures aggregation. The conjunction principle runs these two direction together as though they must be part of the same things. Perhaps not. Perhaps the standard of proof can be understood as a mixture of these two approaches.  -->


<!-- OPTION 4. The standard of proof cannot just be based on a single criterion, say a threshold on posterior probability or a threshold on evidential strength. While these are important, most likely necessary conditions to be met, they are not sufficient. What else? The trial is an adversarial process, in which the evidence and the story or explanations proposed by one of the two parties or both are scrutinized, tested, challenged. Rebutting and undercutting defeaters must be ruled out. The most complete body of evidence, for and against the defendant, must be considered. No evidence should be left out. It is true that the posterior probability of a conjunction will tend to be lower than posterior probability of individual conjuncts. But a conjunction -- the case as a whole -- will typically also have a much greater degree of coherence than individual conjuncts; it will have survived more challenges as it is liable to more challenges, and  the completeness of the evidence is more easily ascertained relative to the overall case. So given this multidimensional theory of the burden of proof, it is not obvious that, other things being equal, the conjunction receives a weaker support than the the conjuncts or that it should receive a stronger support than the conjuct. This will depend on a variety of factors.  -->

<!-- See Spottswood paper, Unraveling the Conjunction Paradox -->

<!-- OPTION 5. p-values, what about the conjucntion of p-values? -->













<!-- ## Prior Probabilities -->

<!-- It is worth examining the holistic account more closely,  -->
<!-- focusing in particular on the role of prior  -->
<!-- probabilities, an aspect that has gone unnoticed so far.  -->
<!-- The main problem with the holistic approach is that it would require, especially in  -->
<!-- criminal cases, individual claims to  -->
<!-- be established with a very high probability, often making the task unsurmountable  -->
<!-- for the prosecution. Or so it would seem. But a composite claim such as $A\wedge B$ will have,  -->
<!-- other things being equal, a lower prior probability than any individual claim $A$ or $B$. -->
<!-- Say a composite claim consists of $k$ individual claims. If its prior probability is $\pi$, each constituent claim, assuming they are independent and equiprobable, will have a prior probability of $\pi^{1/k}$. The prior probability of the individual claims will approach one as the number of constituent claims increases.   -->

<!--
Compare the impact of one item of evidence on the probability of an individual claim, for different level of sensitivity and specificity of the evidence, with the impact of $k$ items of evidence on the probability of a composite claim that consists of $k$ individual claims, again for different levels of specificity and sensitivity of the evidence. More specifically, let $c_i$ be a constituent claim and $e_i$ its supporting evidence. The comparison is between prior probabilities $\pr{C_i}$ and
$\pr{C_1 \wedge C_2 \wedge \dots \wedge C_k}$, contrasted with the posterior probabilities $\pr{C_i \vert E_i}$ and
$\pr{C_1 \wedge C_2 \wedge \dots \wedge C_k \vert E_1 \wedge E_2 \wedge \dots \wedge E_k}$. Figure \ref{fig:post-indiv-joint-first} (top) compares one item of evidence supporting an individual claim and five items of evidence supporting a composite claim consisting of five claims. As is customary, the items of evidence and constituent claims are independent. In addition, for the sake of simplicity, the prior probabilities of the constituent claims are assumed to be the same. There is a significant difference in posterior probabilities, as expected, but there is a also a significant difference in prior probabilities. Since the composite claim starts out less likely than any individual claim, it is natural---other things being equal---that its posterior probability would be correspondingly lower. 
 
 \begin{figure}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


postn <- function (x, s1, s2, n){
  ((x)^n)*
    ((
      s1/
        (s1*x+(1-s2)*(1-x))
     )^n)
    
}

x <-seq(0,1,by=0.001)
y0908 <- post(x, 0.9, 0.8)
y0909 <- post(x, 0.99, 0.99)
yy0908 <- postn(x, 0.9, 0.8, 2)
yy0909 <- postn(x, 0.99, 0.99, 2)
yynull <- postn(x, 0.5, 0.5, 2)
y50908 <- postn(x, 0.9, 0.8, 5)
y50909 <- postn(x, 0.99, 0.99, 5)
y5null <- postn(x, 0.5, 0.5, 5)


ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, sens=0.5, spec=0.5"))+
  #geom_line(aes(x = x, y = yynull,color = "2, sens=0.5, spec=0.5")) +
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  #geom_line(aes(x = x, y = yy0908,color = "2, sens=0.9, spec=0.8")) +
  # geom_line(aes(x = x, y = yy0909,color = "2, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y5null, color = "5, sens=0.5, spec=0.5")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.9, 0.2), legend.title = element_blank())
```
\caption{The comparison here is betwen individual support and joint support. 
The null line for joint support ($y=x*x$) is 
much below the null line for individual support ($y=x$).}
\label{fig:post-indiv-joint-first}
\end{figure}
 -->

<!-- Cohen worried that, as the number of constituent claims increases, the prosection or the plaintiff -->
<!-- would see their case against the defendant become progressively weaker and it would  -->
<!-- become impossisble for them to establish liability. But this worry is an  -->
<!-- exaggeration. The paradox, as is commonly formulated, starts by assuming that the constituent claims are -->
<!-- established by the required probability threshold and then shows that the probability of the conjuction may fall below the threshold. However, following the holistic approach, the order of presentation can be reversed. Start by assuming that the composite claim is established by the required probability threshold. No doubt the individual claims will have to be established with a higher probability, a violation of the conjuction principle. Yet, this violation is not as counterintuitive as it might first appear for two reasons. First, since risks aggregate, it is natural that the probability of a conjunction would be lower than the probability of the conjuncts. Second, the prior probabilities of the conjuncts will be higher than the prior probability of the conjunction. Thus, establishing the conjuncts with a higher probability will not be exceedingly demanding. -->

<!-- Along this lines, @dawid1987difficulty, in one of the earliest attempts to solve the conjunction  -->
<!--  paradox from a probabilistic perspective, wrote: -->

<!-- \begin{quote} -->
<!-- \dots it is not asking too much of the plaintiff to establish the case as a whole with a posterior probability exceeding one half, even though this means  that the several component issues must be established with much larger posterior probabilities; for the \textit{prior}  probabilities of the components will also be correspondingly larger, compared with that of their conjunction (p. 97). -->
<!--  \end{quote} -->

<!--  <!--- The overall effect of subdiving a case into more and more component issues ... [gives] an advantage to the plaintiff, even though he has to establish each with a high probability. --> 

<!-- \noindent  -->
<!-- The price of this strategy is the denial of the conjunction principle, specifically aggregation, the very motivation behind the conjunction paradox. Cohen could insist that this solution  -->
<!-- amounts to denying the paradox. To address the paradox, legal probabilists  -->
<!-- should offer a justification of the conjunction principle in probabilistic terms,  -->
<!-- something Cohen maintains cannot be done. Or can it be done?  -->




<!-- ## The Conjunction Principle Is False -->


<!-- Neither the Bayes factor nor the likelihood ratio  -->
<!-- managed to fully justify both directions  -->
<!-- of the conjunction principle. One direction, aggregation, was justified. -->
<!-- So the original concern that was driving Cohen's formulation of the  -->
<!-- conjuction paradox was addressed. But the  -->
<!-- other direction, distribution, failed.  The failure of distribution creates a paradox  -->
<!-- of its own, what we called distribution paradox. It is odd that one could have sufficiently  -->
<!-- strong evidence in support of $A\wedge B$, while not having sufficiently strong evidence for $A$ or $B$.  -->
<!-- This occurs even when $A$ and $B$ are probabilistically independent. If they were dependent of one another---say $A$ and $B$ were mutually reinforcing---it is possible the evidence would strongly support the conjunction, but not one of the conjuncts in isolation (becuase the additional support from the other claim, $A$ or $B$, would be missing). But the failure of distribution manifests itself even when $A$ and $B$ are independent. What should we make of this? -->
<!-- This problem exists for both the Bayes factor and the likelihood ratio.  -->

<!-- ## Factoring Out Prior Probabilities -->

<!-- Let us return to the role of prior probabilities and their effect on measures of evidential strength. -->
<!-- Dawid observed that the prior probabilities of the conjuncts are correspondingly higher  -->
<!-- than the prior probability of the conjunction. The conjunction principle, instead,  -->
<!-- ignores the role of prior probabilities and treat the conjuncts and the conjuction only in relation to the  -->
<!-- evidence, irrespective of the prior probabilities. So, in order to capture the conjunction principle, legal probabilists  -->
<!-- should rely on probabilistic measures that are not heavily depend on prior probabilities. But, as we have seen, neither the Bayes factor nor the likelihood ratio are such measures.  -->


<!-- We have seen that the joint Bayes factor $\pr{a \wedge b\vert A\wedge B}/\pr{a \wedge b}$,  -->
<!-- under suitable independence assumptions, is greter than the individual Bayes factors -->
<!-- $\pr{a \vert A}/\pr{a}$ and $\pr{b|B}/\pr{b}$. This inequality holds even  -->
<!-- if the evidence is held constant. The joint Bayes factor -->
<!-- $\pr{a \wedge b\vert A\wedge B}/\pr{a \wedge b}$,  -->
<!-- under suitable independence assumptions, is still greter than the individual Bayes factors  -->
<!-- (for composite evidence) $\pr{a \wedge b \vert A}/\pr{a\wedge b}$ and $\pr{a\wedge b|B}/\pr{a\wedge b}$. -->
<!-- But the larger Bayes factor associated with the  -->
<!-- composite claim, holding the evidence fixed, need not be a sign of  -->
<!-- stronger evidence, but merely an artifact  -->
<!-- of the lower prior probability of the composite claim. The same can be said for the combined likelihood ratio.  -->
<!-- We have seen that, holding fixed the sensitivty and specificity of $a$ and $b$, the combined likelihood  -->
<!-- ratio can be changed by varying the priors of $A$ and $B$. The lower the priors, the stronger  -->
<!-- the likelihood ratio.  Perhpas, the same body of evidence may strongly suport the composite  -->
<!-- claim $A\wedge B$, while failing to strongly support $A$ or $B$ simply because $A\wedge B$  -->
<!-- has a lower prior probability and this lower prior probability, everything else being equal,  -->
<!-- inflates the likelihood ratio or the Bayes factor \textit{qua} measures of evidential strength.  -->

<!-- Intuitively, the strength of the evidence should not depend on the prior probability of the hypothesis, but solely on the quality of the evidence itself. We will later see that this intuition is not completely correct, but it has a great deal of plausibility, so it is worth taking it seriously. The prior probability of the hypothesis seems extrinsic to the quality of the evidence since the latter should solely depend on the sensitivity and specificity of the evidence relative to the hypothesis of interest. Strength of evidence determines how much the evidence changes, upwards or downwards, the probability of a hypothesis. However, as the prior probability increases, the smaller the impact that the evidence will have on the probability of the hypothesis. If the prior is  close to one, the evidence would have marginal if not null impact. But this does not mean that the evidence weakens as the prior probability of the hypothesis goes up. For consider the same hypothesis which in one context has a very high prior probability and in another has a moderate prior probability (say a disease is common in a  population but rare in another). The outcome of the same diagnostic test (say a positive test result) performed on two people, each drawn from two populations, should not count as stronger evidence in one case than in the other. After all, it is the same test that was performed and thus the quality of the evidence should be the same.  For just one  -->
<!-- item of evidence, Bayes factor does not capture this intuition, but the likelihood ratio does, which can be considered an argument in favor of the latter and against the former measure of evidenetial support. However, we have seen that, for more than one item of evidence, the Bayes factor as well as the likelihood ratio are prior dependent. -->

<!-- To circumvent the phenomenon of prior dependency, evidential strength can be thought as a relationship between prior and posterior probabilities. The graph in Figure \ref{fig:strength-prior-post} -->
<!-- below represents to what extent the evidence changes the probability of a select hypothesis for any value of the prior probability of the hypothesis. The graph compares the 'base line' (representing no change in probability) and the 'posterior line' (representing the posterior probability of the hypothesis as a function of the prior for a given assignment of sensitivity and specificity of the evidence). Roughly, the larger the area between the base line and the posterior line, the stronger the evidence. Crucially, this area does not depend on the prior probability of the hypothesis, but solely on the sensitivity and specificity of the evidence. As expected, any improvement in sensitivity or specificity will increase the area between the base line and the posterior line. To be sure, what matters is the ratio of sensitivity to 1-specificity, not their absolute value. So evidence with sensitivity and specificity of 0.9 and 0.9 would be equally strong as evidence with sensitivity and specificity at 0.09 and 0.09 becaue $0.9/(1-0.9) = 0.09/(1-0.09)$. -->


<!-- \begin{figure} -->

<!-- ```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"} -->
<!-- library(ggplot2) -->
<!-- library(ggthemes) -->


<!-- post <- function (x, s1, s2){ -->
<!--   (x)* -->
<!--     ( -->
<!--       s1/ -->
<!--         (s1*x+(1-s2)*(1-x)) -->
<!--      ) -->
<!-- } -->

<!-- x <-seq(0,1,by=0.001) -->
<!-- y0906 <- post(x, 0.9, 0.6) -->
<!-- y0609 <- post(x, 0.6, 0.9) -->
<!-- y0909 <- post(x, 0.999, 0.999) -->

<!-- ggplot() + -->
<!--   stat_function(fun=function(x)(x), geom="line", aes(colour="null, sensitivity=specificity=0.5"))+ -->
<!--   geom_line(aes(x = x, y = y0906,color = "sensitivity=0.9, specificity=0.6")) + -->
<!--   geom_line(aes(x = x, y = y0609,color = "sensitivity=0.6, specificity=0.9")) + -->
<!--   geom_line(aes(x = x, y = y0909,color = "sensitivity=0.999, specificity=0.999")) + -->
<!--         ylim(c(0,1))+ -->
<!--   xlab("prior")+ ylab("posterior")+theme_tufte()+ -->
<!--   theme(legend.position = c(0.8, 0.2)) -->
<!-- ``` -->

<!-- \caption{The further away the posterior line from the base  -->
<!-- line, the stronger the evidence irrespective  -->
<!-- of the prior probability of the hypothesis.} -->
<!-- \label{fig:strength-prior-post} -->
<!-- \end{figure} -->


<!-- \begin{figure} -->

<!-- ```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"} -->
<!-- library(ggplot2) -->
<!-- library(ggthemes) -->


<!-- # function for plotting difference between prior and posterior -->

<!-- diff <- function (x, s1, s2){ -->
<!--   post(x, s1, s2) - x -->
<!-- } -->

<!-- x <-seq(0,1,by=0.001) -->
<!-- y0906 <- diff(x, 0.9, 0.6) -->
<!-- y0609 <- diff(x, 0.6, 0.9) -->
<!-- y0909 <- diff(x, 0.999, 0.999) -->

<!-- ggplot() + -->
<!--   # stat_function(fun=function(x)(x), geom="line", aes(colour="null, sensitivity=specificity=0.5"))+ -->
<!--   geom_line(aes(x = x, y = y0906,color = "sensitivity=0.9, specificity=0.6")) + -->
<!--   geom_line(aes(x = x, y = y0609,color = "sensitivity=0.6, specificity=0.9")) + -->
<!--   geom_line(aes(x = x, y = y0909,color = "sensitivity=0.999, specificity=0.999")) + -->
<!--         ylim(c(0,1))+ -->
<!--   xlab("prior")+ ylab("posterior")+theme_tufte()+ -->
<!--   theme(legend.position = c(0.8, 0.8)) -->
<!-- ``` -->

<!-- \caption{Difference between priors and posteriors as a measure of te strength of evidence.} -->
<!-- \label{fig:strength-difference} -->

<!-- \end{figure} -->


<!-- The same approach can model the joint evidential strength of two items of evidence, $a \wedge b$,  -->
<!-- relative to the combined hypothesis, $A \wedge B$. For simplicity, assume $a$ and $b$ are independent  -->
<!-- lines of evidence supporting their respective hypothesis $A$ and $B$. Further, assume  -->
<!-- $A$ and $B$ are probabilistically independent of the other, as in the Bayesian network  -->
<!-- in Figure \ref{network-conjunction} (top). The graph in Figure  -->
<!-- \ref{fig:post-indiv-joint} (top) shows how the prior probabilities  -->
<!-- are impacted by evidence in support of a single hypothesis---say $a\wedge b$ supports $A$\footnote{Given the assumptions of independence we are working with, the strength of $a$ in support of $A$ is the same the strength of  -->
<!-- $a\wedge b$ in support of $A$ since $\frac{\pr{a\wedge b \vert A}}{\pr{a \wedge b}}$ $=\frac{\pr{a \vert A}\pr{b \vert A}}{\pr{a}\pr{b}}=\frac{\pr{a \vert A}}{\pr{a}}$.}---versus  -->
<!-- evidence in support of a joint hypothesis---say $a\wedge b$ supports $A \wedge B$. -->
<!-- The base line is lower in the latter than in the former case because the prior probability of  -->
<!-- $A \wedge B$ is lower than the prior probability of $A$. The prior  -->
<!-- of $A$ equals $x$ and the prior of $A\wedge B$ equals $x^2$ (assuming  -->
<!-- $A$ and $B$ have the same prior probability, and as noted before,  -->
<!-- are probabilistically independent of one another). -->


<!-- \begin{figure} -->

<!-- ```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"} -->
<!-- library(ggplot2) -->
<!-- library(ggthemes) -->


<!-- post <- function (x, s1, s2){ -->
<!--   (x)* -->
<!--     ( -->
<!--       s1/ -->
<!--         (s1*x+(1-s2)*(1-x)) -->
<!--      ) -->
<!-- } -->


<!-- postn <- function (x, s1, s2, n){ -->
<!--   ((x)^n)* -->
<!--     (( -->
<!--       s1/ -->
<!--         (s1*x+(1-s2)*(1-x)) -->
<!--      )^n) -->

<!-- } -->

<!-- x <-seq(0,1,by=0.001) -->
<!-- y0908 <- post(x, 0.9, 0.8) -->
<!-- y0909 <- post(x, 0.99, 0.99) -->
<!-- yy0908 <- postn(x, 0.9, 0.8, 2) -->
<!-- yy0909 <- postn(x, 0.99, 0.99, 2) -->
<!-- yynull <- postn(x, 0.5, 0.5, 2) -->
<!-- y50908 <- postn(x, 0.9, 0.8, 5) -->
<!-- y50909 <- postn(x, 0.99, 0.99, 5) -->
<!-- y5null <- postn(x, 0.5, 0.5, 5) -->
<!-- y100908 <- postn(x, 0.9, 0.8, 10) -->
<!-- y100909 <- postn(x, 0.99, 0.99, 10) -->
<!-- y10null <- postn(x, 0.5, 0.5, 10) -->


<!-- ggplot() + -->
<!--   stat_function(fun=function(x)(x), geom="line", aes(colour="1, sens=0.5, spec=0.5"))+ -->
<!-- #geom_line(aes(x = x, y = yynull,color = "2, sens=0.5, spec=0.5")) + -->
<!--   geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) + -->
<!--   geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) + -->
<!-- #geom_line(aes(x = x, y = yy0908,color = "2, sens=0.9, spec=0.8")) + -->
<!-- # geom_line(aes(x = x, y = yy0909,color = "2, sens=0.99, spec=0.99")) + -->
<!--   geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) + -->
<!--   geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) + -->
<!--   geom_line(aes(x = x, y = y5null, color = "5, sens=0.5, spec=0.5")) + -->
<!--   geom_line(aes(x = x, y = y100908,color = "10, sens=0.9, spec=0.8")) + -->
<!-- # geom_line(aes(x = x, y = y100909,color = "10, sens=0.99, spec=0.99")) + -->
<!--   geom_line(aes(x = x, y = y10null, color = "10, sens=0.5, spec=0.5")) + -->
<!--         ylim(c(0,1))+ -->
<!--   xlab("prior")+ ylab("posterior")+theme_tufte()+ -->
<!--   theme(legend.position = c(0.9, 0.2), legend.title = element_blank()) -->
<!-- ``` -->



<!-- ```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"} -->
<!-- library(ggplot2) -->
<!-- library(ggthemes) -->


<!-- post <- function (x, s1, s2){ -->
<!--   (x)* -->
<!--     ( -->
<!--       s1/ -->
<!--         (s1*x+(1-s2)*(1-x)) -->
<!--      ) -->
<!-- } -->


<!-- post2 <- function (x, s1, s2){ -->
<!--   (x)* -->
<!--     ( -->
<!--       s1/ -->
<!--         (s1*sqrt(x)+(1-s2)*(1-sqrt(x))) -->
<!--      )* -->
<!--     ( -->
<!--       s1/ -->
<!--         (s1*sqrt(x)+(1-s2)*(1-sqrt(x))) -->
<!--      ) -->
<!-- } -->



<!-- postn <- function (x, s1, s2, n){ -->
<!--   (x)* -->
<!--     ( -->
<!--       s1/ -->
<!--         (s1*x^(1/n)+(1-s2)*(1-x^(1/n))) -->
<!--      )^n -->
<!-- } -->



<!-- x <-seq(0,1,by=0.001) -->
<!-- y0908 <- post(x, 0.9, 0.8) -->
<!-- y0909 <- post(x, 0.99, 0.99) -->
<!-- yy0908 <- post2(x, 0.9, 0.8) -->
<!-- yy0909 <- post2(x, 0.99, 0.99) -->
<!-- yynull <- post2(x, 0.5, 0.5) -->
<!-- y40_0909 <- postn(x, 0.99, 0.99, 40) -->

<!-- ggplot() + -->
<!--   stat_function(fun=function(x)(x), geom="line", aes(colour="1, null"))+ -->
<!--   geom_line(aes(x = x, y = y0908,color = "1, sensitivity=0.8, specificity=0.7")) + -->
<!--   geom_line(aes(x = x, y = y0909,color = "1, sensitivity=0.99, specificity=0.99")) + -->
<!--   geom_line(aes(x = x, y = yy0908,color = "2, sensitivity=0.8, specificity=0.7")) + -->
<!--   geom_line(aes(x = x, y = yy0909,color = "2, sensitivity=0.99, specificity=0.99")) + -->
<!--   geom_line(aes(x = x, y = y40_0909,color = "40, sensitivity=0.99, specificity=0.99")) + -->
<!--         ylim(c(0,1))+ -->
<!--   xlab("prior")+ ylab("posterior")+theme_tufte()+ -->
<!--   theme(legend.position = c(0.8, 0.2)) -->
<!-- ``` -->

<!-- \caption{The comparison is between individual support (marked by 1, for one individual  -->
<!-- hypothesis) and joint support (marked by 2, for a two-claim composite claim).  -->
<!-- Top graph: The base line for joint support ($y=x*x$) is  -->
<!-- below the base line for individual support ($y=x$). -->
<!-- Bottom graph: the two base lines are equalized and the  -->
<!-- posterior lines adjusted accordingly. The posterior  -->
<!-- lines for individual and joint support  -->
<!-- get closer especially for high posterior probability values.} -->
<!-- \label{fig:post-indiv-joint} -->
<!-- \end{figure} -->



<!-- ```{r} -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 1)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 2)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 5)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 10)$val -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 20)$val -->

<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 1)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 2)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 5)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 10)$val -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 20)$val  -->

<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 1)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 1)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 2)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 2)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 5)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 5)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 10)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 10)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 20)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 20)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 25)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 25)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99, n = 40)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 40)$val  -->

<!-- ```  -->


<!-- ```{r} -->
<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 1)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 2)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 5)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 10)$val -->
<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 20)$val -->

<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 1)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 2)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 5)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 10)$val -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 20)$val  -->

<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 1)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 1)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 2)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 2)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 5)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 5)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 10)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 10)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.6 , s2 =0.7, n = 20)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 20)$val  -->
<!-- ```  -->

<!-- ```{r} -->
<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 1)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 2)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 5)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 10)$val -->
<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 20)$val -->

<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 1)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 2)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 5)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 10)$val -->
<!-- integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 20)$val  -->

<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 1)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 1)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 2)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 2)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 5)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 5)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 10)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 10)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.9 , s2 =0.8, n = 20)$val - integrate(postn, 0, 1, s1 =0.5 , s2 =0.5, n = 20)$val  -->
<!-- ```  -->

<!-- ```{r}  -->
<!-- integrate(post, 0, 1, s1 =0.99 , s2 =0.99)$val  -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99 , n = 2)$val -->
<!-- integrate(postn, 0, 1, s1 =0.99 , s2 =0.99 , n = 5)$val -->
<!-- ``` -->




<!-- \begin{figure} -->

<!-- ```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"} -->
<!-- library(ggplot2) -->
<!-- library(ggthemes) -->


<!-- diff <- function (x, s1, s2, n){ -->
<!--   postn(x, s2, s2, n) - x^n  } -->

<!-- x <-seq(0,1,by=0.001) -->
<!-- y0908 <- diff(x, 0.9, 0.8, 1) -->
<!-- y0909 <- diff(x, 0.99, 0.99, 1) -->
<!-- yy0908 <- diff(x, 0.9, 0.8, 2) -->
<!-- yy0909 <- diff(x, 0.99, 0.99, 2) -->
<!-- yynull <- diff(x, 0.5, 0.5, 2) -->
<!-- y50908 <- diff(x, 0.9, 0.8, 5) -->
<!-- y50909 <- diff(x, 0.99, 0.99, 5) -->
<!-- y5null <- diff(x, 0.5, 0.5, 5) -->
<!-- y100908 <- diff(x, 0.9, 0.8, 10) -->
<!-- y100909 <- diff(x, 0.99, 0.99, 10) -->


<!-- ggplot() + -->
<!-- #stat_function(fun=function(x)(x), geom="line", aes(colour="1, sens=0.5, spec=0.5"))+ -->
<!-- #geom_line(aes(x = x, y = yynull,color = "2, sens=0.5, spec=0.5")) + -->
<!--   geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) + -->
<!-- # geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) + -->
<!--   geom_line(aes(x = x, y = yy0908,color = "2, sens=0.9, spec=0.8")) + -->
<!-- # geom_line(aes(x = x, y = yy0909,color = "2, sens=0.99, spec=0.99")) + -->
<!--   geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) + -->
<!-- # geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) + -->
<!-- #  geom_line(aes(x = x, y = y5null, color = "5, sens=0.5, spec=0.5")) + -->
<!--   geom_line(aes(x = x, y = y100908,color = "10, sens=0.9, spec=0.8")) + -->
<!-- # geom_line(aes(x = x, y = y100909,color = "10, sens=0.99, spec=0.99")) +   -->
<!--         ylim(c(0,1))+ -->
<!--   xlab("prior")+ ylab("difference posterior - prior")+theme_tufte()+ -->
<!--   theme(legend.position = c(0.9, 0.8), legend.title = element_blank()) -->
<!-- ``` -->


<!-- \end{figure} -->

<!-- <!--- -->
<!--  integrate(diff, 0, 1, s1 =0.99 , s2 =0.99 , n = 1) -->
<!--  integrate(diff, 0, 1, s1 =0.99 , s2 =0.99 , n = 2) -->
<!--  integrate(diff, 0, 1, s1 =0.99 , s2 =0.99 , n = 5) -->
<!-- --> 


<!-- ```{r} -->
<!-- prior_a <- 0.6 -->
<!-- prior_b <- 0.4 -->
<!-- sen_a <- 0.85  # Pr(a given A) -->
<!-- spe_a <- 0.75  #  Pr(not-a given not-A) -->
<!-- sen_b <- 0.7 -->
<!-- spe_b <- 0.75 -->

<!-- prior_ab <- prior_a*prior_b  -->

<!-- bf_a <- sen_a/((sen_a*prior_a)+((1-spe_a)*(1-prior_a))) -->
<!-- bf_b <- sen_b/((sen_b*prior_b)+((1-spe_b)*(1-prior_b))) -->
<!-- bf_ab <- bf_a*bf_b -->

<!-- post_a <- bf_a*prior_a -->
<!-- post_b <- bf_b*prior_b -->
<!-- post_ab <- bf_ab*prior_ab -->

<!-- prior_a -->
<!-- prior_b -->
<!-- prior_ab -->

<!-- post_a -->
<!-- post_b -->
<!-- post_ab -->

<!-- prior_a <- 0.24 -->
<!-- prior_b <- 0.24 -->

<!-- bf_a <- sen_a/((sen_a*prior_a)+((1-spe_a)*(1-prior_a))) -->
<!-- bf_b <- sen_b/((sen_b*prior_b)+((1-spe_b)*(1-prior_b))) -->
<!-- bf_ab <- bf_a*bf_b -->

<!-- post_a <- bf_a*prior_a -->
<!-- post_b <- bf_b*prior_b -->
<!-- post_a -->
<!-- post_b -->
<!-- ``` -->


<!-- What happens if we make the same comparison between individual and composite claims by equalizing their prior probability? If the claims are independent and equiprobable, let $x$ be the prior probability of an individual claim (when it is considered in isolation) and let $x^{1/k}$ the prior probability of the same individual claim when it is part of a composite claim that consists of $k$ claims. In this way---and again, assuming independence and equiprobability of the hypotheses---the prior probability of the composite claim equals the prior probability of the individual claim since $(x^{1/k})^k=x$, as desired. These different claims are then plotted having the same priors. Here we are explicitly factoring out the role of prior probabilities. Figure \ref{fig:post-indiv-joint} (bottom) shows the result of this process of equalization.  -->

<!-- We observe two things. First, the difference in posterior probability, though still present, is less significant, especially for values above the 75\% threshold and even more clearly above the 95\% threshold. Second, whatever remaining difference in posterior probability is now reversed, that is, a composite claim supported by several items of evidence has a higher posterior probability compared to an individual claim supported by one item of evidence. This second observation  -->
<!-- agrees with the analysis based on the Bayes factor and the likelihood ratio in the earlier section. That analysis showed that the support for a composite claim by a joint body of evidence often exceeds the support for an individual claim.  -->

<!-- These two observations establish that, by factorig out prior probabilities and under certain independence assumptions, whenever the individual claims meet the applicable posterior threhsold, so does the composite claim. This verifies aggregation. Conversely, whenever the composite claim, say $A \wedge B$, meets the applicable posterior threshold, so do the individual claims insofar as the threshold is about 75\% or higher. This verifies---to some approximation and in a limited class of cases---the other direction of the conjunction principle, what we called distribution.  -->


<!---

The same equalization can be done with the Bayes factor, arriving at similar results. Compare the difference betwen teh graph (top) in which teh Bayes factor has not been equalized relative to the priors and the graph (bottom) in which it has. In the latter case, individual Bayes factor and joint Bayes factor tend to converge especially for relatively high values of sensitivity and specificity SEE FIGURE BELOW. 


\begin{figure}


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


bf <- function (x, s1, s2){
  
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


bfn <- function (x, s1, s2, n){

    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )^n
}

x <-seq(0,1,by=0.001)
y0908 <- bf(x, 0.9, 0.8)
y0909 <- bf(x, 0.99, 0.99)
y50908 <- bfn(x, 0.9, 0.8, 5)
y50909 <- bfn(x, 0.99, 0.99, 5)
y5null <- bfn(x, 0.5, 0.5, 5)

ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, null"))+
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
        ylim(c(0,10))+
  xlab("prior")+ ylab("Bayes Factor")+theme_tufte()+
  theme(legend.position = c(0.8, 0.7), legend.title = element_blank())
```

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


bf <- function (x, s1, s2){
  
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


bfnEq <- function (x, s1, s2, n){

    (
      s1/
        (s1*x^(1/n)+(1-s2)*(1-x^(1/n)))
     )^n
}

x <-seq(0,1,by=0.001)
y0908 <- bf(x, 0.9, 0.8)
y0909 <- bf(x, 0.99, 0.99)
y50908 <- bfnEq(x, 0.9, 0.8, 5)
y50909 <- bfnEq(x, 0.99, 0.99, 5)
y5null <- bfnEq(x, 0.5, 0.5, 5)

ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, null"))+
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
        ylim(c(0,10))+
  xlab("prior")+ ylab("Bayes Factor Prior Equalized")+theme_tufte()+
  theme(legend.position = c(0.8, 0.7), legend.title = element_blank())
```

\label{fig:bf-indiv-joint}
\end{figure}


-->

<!-- Has the distribution paradox been eliminated then? The approach we have just  -->
<!-- described---equalizing the prior probabilities across individual and composite claims---does  -->
<!-- not entirely eliminate the paradox. There are still cases in which a composite hypothesis, say $A \wedge B$, receives stronger support than an individual hypothesis, given the same body of evidence. Sensitivity to priors seems to play a role.  -->
<!-- But it cannot be the only factor at play, or else the equalization of the prior probabilities would have eliminated the paradox entirely. So what else is going on? -->


<!-- ## Weaker Claims Weaken Sensitivity  -->

<!-- Let's examine more closely the Bayes factor and the likelihood ratio as measures  -->
<!-- of evidential strength. Likelihood ratios are comparative in nature. Suppose we compare claim $A$ and claim $A\wedge B$ relative to the same body of evidence $a\wedge b$. Thinks of $A$ as 'the defendant physically injured the victim' while $B$ as 'the defendant knew the victim was a firefighter'. Think of $a$ and $b$ as testimonies each supporting one of the two hypotheses. We are still working with the Bayesian networks in Figure \ref{network-conjunction}. -->

<!-- Consider the combined body of evidence $a\wedge b$. Which claim between $A$ and $A\wedge B$ will receive more support by evidence $a\wedge b$?  Intuitively, one might think that $A$ should receive more or at least equal support compared to $A\wedge B$ . After all, $A\wedge B$ is a stronger claim than $A$ and thus more difficult to establish than $A$, other things being equal. In terms of posterior probabilities, this is true. The posterior probability of $A\wedge B$ should not be higher than the posterior probability of $A$ alone given evidence $a \wedge b$.  -->

<!-- Let's now think in terms of evidential support. Formally, the question is whether $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}$ is less than one or greater than one. Given the customary independencies between evidence and hypotheses,  -->

<!-- \[\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}=\frac{\pr{a \vert A} \pr{b \vert A}}{\pr{a \vert A} \pr{b \vert B}}=\frac{\pr{b \vert A}}{\pr{b \vert B}}<1.\] -->

<!-- \noindent -->
<!-- The reason is that $\pr{b \vert A} < \pr{b \vert B}$ since the sensitivity of $b$ relative to $B$ should be higher than the sensitivity of $b$ relative to $A$.\footnote{GIVE PROOF OF THIS} This is obvious if $A$ and $b$ are independent claims, as in the Bayesian networks in Figure \ref{network-conjunction} (bottom). In this case, $\pr{b \vert A}=\pr{b}$.   -->
<!-- So $\frac{\pr{b}}{\pr{b \vert B}}<1$ since $b$, by assumption, positively supports $B$, or in other words,  -->
<!-- $\frac{\pr{b \vert B}}{\pr{b}}>1$. Thus, $a\wedge b$ supports $A\wedge B$ more than it supports $A$ alone. This is not what one would expect intuitively. -->

<!-- The same conslusion holds using the Bayes factor. If $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}<1$, then  -->

<!-- \[\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b}} < \frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a\wedge b}}   \] -->

<!-- \noindent -->
<!-- Thus, evidence $a\wedge b$ better supports, even on an absolute scale, $A \wedge B$ compared to $A$. Note that, even if the Baye factor depends on the priors, the difference here is not due to the difference between the priors of $A$ and the priors of $A\wedge B$ since the denominator is simply $\pr{a\wedge b}$.\footnote{Note that $\pr{a\wedge b \vert A}\pr{A}+ \pr{a\wedge b \vert \neg A}\pr\neg {A}$ is the same as $\pr{a\wedge b \vert (A\wedge B)}\pr{A\wedge B}+ \pr{a\wedge b \vert \neg (A\wedge B)}\pr{\neg (A\wedge B)}$. \textbf{NEED A PROOF FOR THIS BUT IT SHOULD HOLD, RIGHT?}} -->

<!-- What we have said so far agrees with the claim defended in the previous section. That is, even when $a\wedge b$ strongly supports $A \wedge B$, the same evidence need \textit{not} strongly support $A$. Formally,  -->

<!-- \[\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert \neg A}} < \frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a\wedge b \vert \neg (A \wedge B)}}\] -->

<!-- \noindent -->
<!-- In other words, even if $a\wedge b$ favors $A\wedge B$ over its negation to a very high degree, it need not favor equally strongly $A$ over its negation. This is a comparative claim about two compartive claims, and as such, it may not be easy to parse. Evidential support, when it is formalized by the likelihoo ratio, is always relative to a contrast class.  -->
<!-- <!-- such as $A$ versus $\neg A$ or $A\wedge B$ versus $\neg (A\wedge B)$.---> 


<!-- In comparing the support that the same body of evidence provides to $A$ as contrasted to $A\wedge B$, it might be better to include these two hypotheses in the contrast class. So the expression $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}$ is more straightforward.  --> 

<!-- No matter the formulation, the same conclusion holds. Evidence $a\wedge b$ supports the composite claim $A\wedge B$ more than it supports the weaker claim $A$, even assuming that $A$ and $B$ are independent of one another and thus not mutually reinforcing. This conclusion seems paradoxical. How should we make sense of it? At first, we thought the paradox could be due to prior dependency since the combined likelihood ratio $\frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a\wedge b \vert \neg (A \wedge B)}}$  varies depending on the priors of $A$ and $B$. But this argument seems to no longer holds since in case of $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}$ any prior dependency seems to have been eliminated. After all, if $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}<1$, the sensitivity of $a\wedge b$ must be worse relative to $A$ than the composite claim $A \wedge B$.  -->

<!-- Sensitivity is a crucial property of the quality of the evidence. Everything else being equal, the lower the sensitivity of the evidence, the lower its evidential strength. The importance of sensitivity as a factor for assessing the strength of the evidence is hard to dispute. So why is the sensitivity of $a\wedge b$ worse relative to $A$ than $A \wedge B$? Suppose $A$ is the case. If $A$ is the case, in order for $a\wedge b$ to arise, both $a$ and $b$ should pick up on $A$. If $b$ fails to pick up on $A$, then $A \wedge b$ would not arise even if $a$ pick up on $A$.\footnote{The occurrance of $a\wedge b$ is less likely to occur than just $a$ alone picking up on $A$ because $b$ may fail---and fail more often than $a$ would---in picking up on $A$.} Suppose instead $A\wedge B$ holds. In this case, $a\wedge b$ would arise even if $b$ fails to pick up on $A$ so long $a$ picks up on $A$ and $b$ picks up on $B$. Now of course $b$ could also fail to pick on $B$ just like $a$ could fail to pick up on $B$. But we are assuming that $b$ is better than $a$ at tracking $B$. So $b$ will fail less often than $a$ at picking up on $B$. Thus, the sensitivity of $a\wedge b$ relative to $A\wedge B$ is better than the sensitivity of $a$ relative to $A$ alone. This is a subtle point that probability theory helps to bring out clearly.  -->

<!-- How big are these variations? \textbf{PLOT GRAPH TO GET A SENSE OF VARIATIONS OF SENSITIVITY} -->

<!-- One explanation of the paradox, then, is the difference in sensitivity. The sensitivity of $a\wedge b$ relative to $A\wedge B$ is better than the sensitivity of the same evidence relative to $A$. Consequently, other thigns being equal, evidence $a\wedge b$ supports $A\wedge B$ better than $A$. However counterintuitive this might seem, we should accept this fact and admit that our intuitions were wrong. So the fallacy seems to be to assume that the sensitivity of $a\wedge b$ relative to $A$ cannot be lower than the senstivity of $a\wedge b$ relative to $A\wedge B$. The thought would be something like this: if $a \wedge b$ tracks $A\wedge B$ to some degree, it surely must be able to track $A$ alone, at least as well. But we have just shown that we cannot assume that $\pr{a\wedge b \vert A} \geq \pr{a\wedge b \vert A\wedge B}$ and in fact the opposite is the case, $\pr{a\wedge b \vert A} < \pr{a\wedge b \vert A\wedge B}$. -->

<!-- Another way to convince ourselves this is the case is to run a simulation. Suppose we are deciding about the truth of $A$ and the the truth of $A\wedge B$, and we have a fixed body of evidence, say, $a\wedge b$ that speaks in favor of both claims.  \todo{M: Run simulation to show that same diagnostic test for composite claim would perform better then when applied to individual claim (worse LR). How do to do this? HELP!}  -->

<!-- We should circumscribe the point we just made since it does not always hold.  -->
<!-- Suppose, as in Figure \ref{network-unrelated}, that  -->
<!-- $H$ is a claim unrelated to $A$ and evidence $a$. Evidence $a$ supports $A$. Would the composite claim $A\wedge H$ be better supported by $a$ than $A$ alone? It would not. Mere tagging an unrelated hypothesis does not strengthen the evidence. Note that $\pr{a \vert A}=\pr{a \vert A \wedge H}$ because $H$ is indepedent from everything else. Thus,  -->

<!-- \[\frac{\pr{a \vert A}}{\pr{a \vert A \wedge H}}=\frac{\pr{a \vert A}}{\pr{a \vert A}}=1.\] -->

<!-- \noindent -->
<!-- Tagging an unrelated claim $H$ does not strengthen the evidence, but leaves it unchanged.  -->
<!-- Similarly, suppose $B$ constitutes one element of a crime and $A$ constitutes the other element. The two claims are independent, each supported by items of evidence $a$ and $b$ respectively. This is our standard set up. If $a$ supports $A$, would $a$ support $A\wedge B$ more strongly than it supports $A$ alone? Here we no longer have $a\wedge b$, but instead, $a$ alone. The question is whether -->
<!-- $\frac{\pr{a \vert A}}{\pr{a \vert A \wedge B}}>1$ or $\frac{\pr{a \vert A}}{\pr{a \vert A \wedge B}}<1$.  -->
<!-- Given the usual independencies between evidence and hypotheses, -->

<!-- \[\frac{\pr{a\vert A}}{\pr{a \vert A \wedge B}}=\frac{\pr{a \vert A}}{\pr{a \vert A}}=1\] -->

<!-- Evidence $a$ supports $A$ and $A\wedge B$ to the same extent.  One might complain that this is counterintuitive. How can it be that $a$ supports $A$ to the same degree that it supports the more demanding claim $A\wedge H$ or $A\wedge B$? For suppose we have evidence $a$ in favor of $A$ and then wonder whether we could use that evidence in support of another claim $H$ or $B$. By tagging $H$ or $B$ to $A$, we can at least say that we have evidence $a$ for $A\wedge H$ or $A\wedge B$ that is at least as strong as evidence $a$ in support of $A$. But note that $\frac{\pr{a \vert A}}{\pr{a\vert neg A}}>1$ even though $\frac{\pr{a \vert H}}{\pr{a\vert neg H}=1}$ and $\frac{\pr{a \vert B}}{\pr{a\vert neg B}=1}$ (assuming $A$ and $B$ are independent). So $a$ does not support $H$ or $B$ to the same degree that it supports $A$. However, $a$ supports $A$ to the same degree that it supports $A\wedge H$ or $A\wedge B$. -->

<!-- What should we make of this? The commonality between $H$ and $B$ is that they are irrelevant for $a$. Evidence $a$ does not increase nor decrease their probability so the evidence is irrelevant for them. So the upshot here is that, tagging an irrelevant hypothesis does not change evidential support. The other upshot is that tagging a relevant hypothesis---a hypthesis that does bear on the evidence in one way or another, such as $B$ relative to $a\wedge b$---does increase evidential support. \todo{M: How to explain this better? Can we make this plausible? HELP!} -->

<!-- \begin{center} -->
<!-- \begin{figure}[h!] -->
<!-- \begin{tikzpicture}[ -->
<!--   node distance=1cm and 1cm, -->
<!--   mynode/.style={draw,ellipse,text width=1cm,align=center} -->
<!-- ] -->
<!-- \node[mynode] (h1) {$A$}; -->
<!-- \node[mynode, right =of h1] (h) {$H$}; -->
<!-- \node[mynode, below  =of h1] (e1) {$a$}; -->
<!-- \path  -->
<!-- (h1) edge[-latex] (e1); -->
<!-- \end{tikzpicture} -->

<!-- \caption{Bayesian network with wholly unrelated claim $H$.} -->
<!-- \label{network-unrelated} -->
<!-- \end{figure} -->
<!-- \end{center} -->



<!-- The intuition that the same evidence tracks the conjunction $A\wedge B$ better than one of the conjuncts $A$ and $B$  -->
<!-- might rest on another model of what is going on. Say $ab$ is an item of evidence that arises when $A$ or $B$ occurs with $60\%$ probability. That is, $\pr{ab \vert A}=\pr{a \vert B}=60\%$. What would be the sensitivity of $ab$ relative to the conjunction $A\wedge B$, $\pr{ab \vert A\wedge B}$? We can represent this set up in Figure \ref{network-ab}. -->
<!-- Intuitvely, one might reason as follows. There are two paths leading to $ab$, one path starts with $A$ and another path starts with $B$. When both these pathst are active, since we are assuming $A\wedge B$, then the probability of $ab$ must be higher than if just one of the two paths is active. Hence, $\pr{ab \vert A\wedge B}\geq 60\%$. \todo{M: Is this true probabilistically? If not, what assumptions are required? HELP!} -->




<!-- \begin{center} -->
<!-- \begin{figure}[h!] -->
<!-- \begin{tikzpicture}[ -->
<!--   node distance=1cm and 1cm, -->
<!--   mynode/.style={draw,ellipse,text width=1cm,align=center} -->
<!-- ] -->
<!-- \node[mynode] (h1) {$A$}; -->
<!-- \node[mynode, below right=of h1] (e) {$ab$}; -->
<!-- \node[mynode, above right=of e] (h2) {$B$}; -->
<!-- \path  -->
<!-- (h1) edge[-latex] (e) -->
<!-- (h2) edge[-latex] (e); -->
<!-- \end{tikzpicture} -->

<!-- \caption{Bayesian network with $ab$ resulting from $A$ and $B$.} -->
<!-- \label{network-ab} -->
<!-- \end{figure} -->
<!-- \end{center} -->







<!-- ## But Sensitivity Depends On Prior Probabilities -->




<!-- We have shown, given a suitable number of assumptions, that the sensitivity of $a\wedge b$ can  -->
<!-- be greater relative to $A \wedge B$ than $A$ alone. This explains why the evidential support  -->
<!-- of $a\wedge b$ is greater in favor of $A \wedge B$ than alone $A$. Therefore---one  -->
<!-- might conclude---even factoring out differences in priors probabiliies could still lead to differences  -->
<!-- in evidential strength simply due to difference in sensitivity. But note that this argument  -->
<!-- assumes that sensitivity---or specificity---have nothing to do with prior probabilities. -->

<!-- Things are somewhat more complicated. After all, $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}=\frac{\pr{b \vert A}}{\pr{b \vert B}}$. The denominator, which tracks the sensitivity of $a\wedge b$ relative to $A\wedge B$ does not depend on the priors of $A\wedge B$, but solely on the sensitivty of $a$ and $b$ relative to $A$ and $B$. However, the numerator, which tracks the sensitivity of $a\wedge b$ relative to $a$, does depend on the priors of $A$ (or whatvere other hypothesis one choses to computer $\pr{b \vert A}$ or $\pr{b}$). This means that the denominator can vary depeding on the prior probability of a chosen hypothesis. Thus, the lower the prior of $A$, the lower the probability of $b$. One could still insist that here we are not comparing the prior of a hypothesis, say $B$, and the prior of another hypothesis, say $A \wedge B$. Whatever the difference in sensitivity, it cannot be due to the difference in prior probabilities of the hypotheses.   -->
<!-- On other hand hand, $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}$ is comparing two quantitities, one dependent on the priors and the other not dependent on the priors.  -->

<!-- More generally, the intuition that characteristics of the evidence such as sensitivity  -->
<!-- and specificity should be indepedent of the probability of the hypothesis of iterest turns out  -->
<!-- to be incorrect empirically. One study in the medical literature has shown, surprisingly, that the sensitivity of  -->
<!-- a diagnostic test is independent of the prior of the hypothesis beign tested---say whether the patient  -->
<!-- has a medical condition. However, specificity is dependent on the prior of the hypothesis: -->

<!-- \begin{quote} -->
<!-- Overall, specificity tended to be -->
<!-- lower with higher disease prevalence; there -->
<!-- was no such systematic effect for sensitivity (page E537). -->
<!-- Source: Variation of a tests sensitivity and specificity with disease prevalence Mariska M.G. Leeflang, Anne W.S. Rutjes, Johannes B. Reitsma, Lotty Hooft and Patrick M.M. Bossuyt -->
<!-- CMAJ August 06, 2013 185 (11) E537-E544; DOI: https://doi.org/10.1503/cmaj.121286 -->
<!-- \end{quote} -->

<!-- \noindent -->
<!-- The authors of the study, however, caution that  -->

<!-- \begin{quote} -->
<!--  Because sensitivity is estimated in -->
<!-- people with the disease of interest and specificity -->
<!-- in people without the disease of interest, changing the relative number of people  -->
<!-- with and without the disease of interest should not introduce -->
<!-- systematic differences. Therefore, the effects that -->
<!-- we found may be generated by other mechanisms that affect  -->
<!-- both prevalence and accuracy. -->
<!-- \end{quote} -->

<!-- So, according to the authors, changes in prevalence need not directly affect specificity since variations in prevalence and variation in specificity may have a common cause.  -->
<!-- Our earlier calculations about combined specificity and sensitivity agree with experimental  -->
<!-- results, namely, only specificity depends on the priors. Our calculations, in fact, show that different priors for the individual claim do affect specificity. The variation of specificity  -->
<!-- in the result of of splitting the negation of the composite hypthesis $\neg (A \wedge B)$ into three further scenarios, -->
<!-- $\neg A \wedge B$, $B \wedge \neg A$ and $\neg A \wedge neg B$. This prior sensitivity, of course, only applies to composite hyotheses, but to some extent, any hypothesis can be analyzed as a composite hypothesis. The claim that the defedant was running down 5th avnue can be broken down in the conjuction that the defendant was running and that the defendant was at 5th avenue. Any claim, under some level of description, is a composite hypothesis.  -->
<!-- So, perhpas, the quality or strength  -->
<!-- of the evidence should depend on the the priors whether the hypothesis is composite or not.  -->
<!-- Is this another example of base rate neglect? \todo{M: Might be good to have a simulation here that makes vidid why combined specificity is in fact dependent on the priors. Maybe it is, after all, a fallacy  -->
<!-- to think that the quality/strength of the evidence should be independent of the priors. HELP!} -->


<!--  Let's grant that the quality of the evidence should depend, contrary  -->
<!--  to our initial intuition, on the prior probability of the hypotheses.  -->
<!--  If that is so, it would not be natural to see that evidence -- the same evidence -- strongly  -->
<!--  favors $A\wedge B$ without strongly favoring $A$ or $B$. Perhaps we can make sense of this  -->
<!--  if we keep in mind the comparison between hypothesis we are making here.  -->

<!--  NOT SURE HOW TO CONTINUE HERE THOUGH! -->

<!--  TO DO: -->

<!-- 1. NOTE THAT EVEN BY EQUALIZING PRIORS, TE DISTRBUTION PARADOX DOES NOT GO AWAY.  -->
<!-- SO WHAT ELSE IS GOING ON HERE? NEED TO MAKE COMPATISON BETWEEN HYPOTHESES. NEED TO FIGURE THIS OUT! -->

<!--  2. TRY TO MAKE SENSE OF THIS, IT IS INTITVELY ACCEPTABLE THAT SUPPORT FOR COMBINED CLAIM, EVEN HOLDING FIXED THE SAME EVIDENCE, SHOULD BE STRONGER THEN SUPPORT FOR INDIVIDUAL CLAIM? THAT IS CLEARLY ODD AND GOES AGAINST COMMON ASSUMPTIONS. -->


<!-- 3. \textbf{HYPOTHESIS: WHEN THE HYPTHESIS IS NOT MEDIATED (AS IN a toward A OR b TOWARD B, AS OPPOSED TO a TOWARD A-AND-B), THEN SENSITIVITY OR SPECICITIY IS NOT DEPEDENT ON PRIORS} -->

<!-- ## Which Measure of (Combined) Evidential Support? -->

<!-- THIGNS TO ADD: -->

<!-- 1. THE MIN SEEMS TO BE THE MEASURE FOR COMPOSITE CLAIMS THAT CAPTURES AGGREGATION AND DISTRIBUTION BEST.  -->
<!-- SO THE QUESTION IS WHAT PROBABILISTIC MEASURE CAPTURES MIN? -->

<!-- 2. IS DEPEDENCY ON PRIOR ANOTHER EXAMPLE OF BASE RATE NEGLECT? WE NEGLECT BAE RATE IN CALCULATING POSTERIOR BUT ALSO IN CALCULATING STRENGTH OF EVIDENCE? WE JUST NEED TO LIVE WITH THE FACT THAT WE HAVE A POOR UNDERSTANDING OF EVIDENCE BUT PERHPAS GOOD ENOUGH TO GET BY IN THE WORLD. CONNECT TO POINT 1 AND MIN FUNCTION. -->



<!-- ## The likelihood strategy  -->

<!-- Focusing on  -->
<!-- posterior probabilities is not the only approach that legal probabilists  -->
<!-- can pursue. By Bayes' theorem, the following holds, using $G$ and $I$  -->
<!-- as competing hypotheses: -->

<!-- \[ \frac{\Pr(G \vert E)}{\Pr(I \vert E)} = \frac{\Pr(E \vert G)}{\Pr(E \vert I)} \times \frac{\Pr(G)}{\Pr(I)},\] -->

<!-- or using $H_p$ and $H_d$ as competing hypotheses, -->


<!-- \[ \frac{\Pr(H_p \vert E)}{\Pr(H_d \vert E)} = \frac{\Pr(E \vert H_p)}{\Pr(E \vert H_d)} \times \frac{\Pr(H_p)}{\Pr(H_d)},\] -->

<!-- or in words -->

<!-- \[ \textit{posterior odds} = \textit{likelihood ratio} \times \textit{prior odds}.\] -->

<!-- A difficult problem is to assign numbers to the prior probabiliteis  -->
<!-- such as $\Pr(G)$ or $\Pr(H_p)$, or priors odds such as $\frac{\Pr(G)}{\Pr(I)}$ or $\frac{\Pr(H_p)}{\Pr(H_d)}$.  -->

<!-- DISCUSS DIFFICULTIES ABOUT ASSIGNING PRIORS! WHERE? -->
<!--   CAN WE USE IMPRECISE PROBABILKITIES T TALK ABOUT PRIORS -- I.E. LOW PRIORS = TOTAL IGNORANCE = VERY IMPRECISE (LARGE INTERVAL) PRIORS? THE PROBLME WITH THIS WOULD BE THAT THERE IS NO UPDSATING POSSIBLE. ALL UPDATING WOULD STILL GET BACK TO THE STARTING POINT. DO YOU HAVE AN ANSWER TO THAT? WOULD BE INTERETSING TO DISCUSS THIS! -->

<!-- Given these difficulties, both practical  -->
<!-- and theoretical, one option is to dispense with  -->
<!-- priors altogether. This is not implausible. Legal disputes in both  -->
<!-- criminal and civil trials should be decided  -->
<!-- on the basis of the evidence presented by the litigants. But  -->
<!-- it is the likelihood ratio -- not the prior ratio -- that offers the best measure  -->
<!-- of the overall strength of the evidece presented. So it is all too natural to focus on likekihood ratios  -->
<!-- and leave the priors out of the picture. If this is the right, the question is, how would a probabilistic interpretation of standards of proof based on the likelihood rato look like?  -->
<!-- At its simplest, this stratgey will look as follows. Recall our discussion  -->
<!-- of expected utility theory: -->


<!-- \[ \text{convict provided}  		 \frac{cost(CI)}{cost(AG)} < \frac{\Pr(H_p \vert E)}{\Pr(H_d \vert E )}, \] -->

<!-- which is equivalent to -->

<!-- \[ \text{convict provided}  		 \frac{cost(CI)}{cost(AG)} < \frac{\Pr(E \vert H_p)}{\Pr(E \vert H_d)} \times \frac{\Pr(H_p)}{\Pr(H_d)}.\] -->

<!-- By rearraing the terms, -->

<!-- \[ \text{convict provided}  \frac{\Pr(E \vert H_p)}{\Pr(E \vert H_d)} >	\frac{\Pr(H_d)}{\Pr(H_p)} \times 	 \frac{cost(CI)}{cost(AG)} .\] -->

<!-- Then, on this intepretation, the likelihood ratio should be above a suitable threshold  -->
<!-- that is a function of the cost ratio and the prior ratio. The outstanding question  -->
<!-- is how this threshold is to be determined.  -->


<!-- ### Kaplow   -->


<!-- Quite independently, a similar approach to juridical decisions has been proposed by  @kaplow2014likelihood -- we'll call it \textbf{decision-theoretic legal probabilism (DTLP)}. It turns out that  Cheng's suggestion is  a particular case of this more general approach. Let $LR(E)=\pr{E\vert H_\Pi}/\pr{E\vert H_\Delta}$. In whole generality, DTLP invites us to convict just in case $LR(E)>LR^\star$, where $LR^\star$ is some critical value of the likelihood ratio.  -->

<!--  Say we want to formulate the usual preponderance rule: convict iff $\pr{H_\Pi\vert E}>0.5$, that is, iff $\frac{\pr{H_\Pi\vert E}}{\pr{H_\Delta\vert E}}>1$. By Bayes' Theorem we have: -->

<!--  \vspace{-6mm} -->

<!--  \begin{align*} -->
<!-- \frac{\pr{H_\Pi\vert E}}{\pr{H_\Delta\vert E}} =  \frac{\pr{H_\Pi}}{\pr{H_\Delta}}\times \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}} &>1 \Leftrightarrow\\ -->
<!--   \Leftrightarrow \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}} &> \frac{\pr{H_\Delta}}{\pr{H_\Pi}}  -->
<!--  \end{align*} -->
<!--  \noindent So, as expected, $LR^\star$ is not unique and depends on priors. Analogous reformulations are available for thresholds other than $0.5$.  -->

<!-- Kaplow's  point is not that we can reformulate threshold decision rules in terms of priors-sensitive likelihood ratio thresholds. Rather, he insists, when we make a decision, we should factor in its consequences. Let $G$ represent potential gain from correct conviction, and $L$ stand for the potential loss resulting from mistaken conviction. Taking them into account, Kaplow suggests, we should convict if and only if: -->

<!-- \vspace{-6mm} -->

<!--  \begin{align} -->
<!-- \label{eq:Kaplow_decision} -->
<!-- \pr{H_\Pi\vert E}\times G > \pr{H_\Delta\vert E}\times L -->
<!-- \end{align} -->
<!-- \noindent Now, \eqref{eq:Kaplow_decision} is equivalent to: -->

<!-- \vspace{-6mm} -->


<!-- \begin{align} -->
<!-- \nonumber -->
<!-- \frac{\pr{H_\Pi \vert E}}{\pr{H_\Delta \vert E}} & > \frac{L}{G}\\ -->
<!-- \nonumber -->
<!-- \frac{\pr{H_\Pi}}{\pr{H_\Delta}} \times \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}} &> \frac{L}{G}\\ -->
<!-- \nonumber -->
<!-- \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}}  & > \frac{\pr{H_\Delta}}{\pr{H_\Pi}} \times \frac{L}{G}\\ -->
<!-- \label{eq:Kaplow_decision2} LR(E)  & > \frac{\pr{H_\Delta}}{\pr{H_\Pi}} \times \frac{L}{G} -->
<!-- \end{align} -->

<!-- \noindent This is the general format of Kaplow's decision standard.  -->



<!-- ### Dawid -->



<!-- Here is a slightly different perspective, due to @dawid1987difficulty, that also suggests that juridical decisions should be likelihood-based. The focus is  on witnesses for the sake of simplicity. Imagine the plaintiff produces two independent witnesses: $W_A$ attesting to $A$, and $W_B$ attesting to $B$.  Say the witnesses are regarded as $70\%$ reliable and $A$ and $B$ are probabilistically independent, so we infer $\pr{A}=\pr{B}=0.7$ and  $\pr{A\et B}=0.7^2=0.49$.  -->

<!--   But, Dawid argues,  this is misleading, because to reach this result we misrepresented the reliability of the witnesses: $70\%$ reliability of a witness, he continues, does not mean that if the witness testifies that $A$, we should believe that  $\pr{A}=0.7$. To see his point,  consider two potential testimonies: -->


<!-- \begin{center} -->
<!-- \begin{tabular} -->
<!-- {@{}ll@{}} -->
<!-- \toprule -->
<!--   $A_1$ & The sun rose today. \\ -->
<!--    $A_2$ & The sun moved backwards through the sky today.\\ -->
<!-- \bottomrule -->
<!-- \end{tabular} -->
<!-- \end{center} -->

<!-- \noindent     Intuitively, after hearing them, we would still take $\pr{A_1}$ to be close to 1 and $\pr{A_2}$ to be close to 0, because we already have fairly strong convictions about the issues at hand. In general, how we should revise our beliefs  in light of a testimony depends not only on the reliability of the witness, but also on our prior convictions.\footnote{An issue that Dawid does not bring up is the interplay between our priors and our assessment of the reliability of the witnesses. Clearly, our posterior assessment of the credibility of the witness who testified $A_2$ will be lower than that of the other witness.}  And this is as it should be:  as indicated by Bayes' Theorem,  one and the same testimony with different priors might lead to different posterior probabilities. -->



<!--  So far so good. But how should we represent evidence (or testimony) strength then? Well, one pretty standard way to go is to  focus on how much it contributes to the change in our beliefs in a way independent of any particular choice of prior beliefs.  -->
<!--  Let $a$ be the event that the  witness testified that $A$.  It is useful to think about the problem in terms of \emph{odds, conditional odds (O)} and \emph{likelihood ratios (LR)}: -->
<!--  \begin{align*} O(A)  & = \frac{\pr{A}}{\pr{\n A}}\\ -->
<!--  O(A\vert a) &= \frac{\pr{A\vert a}}{\pr{\n A \vert a}}  \\ -->
<!--  LR(a\vert A) &= \frac{\pr{a\vert A}}{\pr{a\vert \n A}}.  -->
<!-- \end{align*} -->




<!-- Suppose our prior beliefs and background knowledge, before hearing a testimony, are captured by the prior probability measure $\prr{\cdot}$, and the only thing that we learn  is $a$. We're interested in what our \emph{posterior} probability measure, $\prp{\cdot}$, and posterior odds should then be. If we're to proceed with Bayesian updating, we should have: -->


<!-- \vspace{-6mm} -->


<!--  \begin{align*} -->
<!--  \frac{\prp{A}}{\prp{\n A}} & = \frac{\prr{A\vert a}}{\prr{\n A\vert a}} -->
<!--  = -->
<!--  \frac{\prr{a\vert A}}{\prr{a\vert \n A}} -->
<!--  \times -->
<!--  \frac{\prr{A}}{\prr{\n A}} -->
<!--   \end{align*} -->
<!--  that is, -->

<!--  \vspace{-6mm} -->

<!--  \begin{align} -->
<!--  \label{bayesodss2} -->
<!--  O_{posterior}(A)& = O_{prior}(A\vert a) = \!\!\!\!\!  \!\!\!\!\!  \!\! \!\!  \underbrace{LR_{prior}(a\vert A)}_{\mbox{\footnotesize conditional likelihood ratio}}  \!\!\!\!\!   \!\!\!\!\!  \!\! \!\!   \times  O_{prior}(A) -->
<!--  \end{align} -->



<!--   The conditional likelihood ratio seems to be a much more direct measure of the value of $a$, independent of our priors regarding $A$ itself.   In general, the posterior probability of an event will equal to the witness's reliability in the sense introduced above only if the prior is $1/2$.\footnote{Dawid gives no general argument, but it is not too hard to  give one. Let $rel(a)=\pr{a\vert A}=\pr{\n a\vert \n A}$. We have in the background $\pr{a\vert \n A}=1-\pr{\n a\vert \n A}=1-rel(a)$. -->
<!--  We want to find the condition under which $\pr{A\vert a} = \pr{a\vert A}$. Set $\pr{A}=p$ and  start with Bayes' Theorem and the law of total probability, and go from there: -->
<!--  \begin{align*} -->
<!--  \pr{A\vert a}& = \pr{a\vert A}\\ -->
<!--  \frac{\pr{a\vert A}p}{\pr{a\vert A}p+\pr{a\vert \n A}(1-p)} &= \pr{a\vert A} \\ -->
<!--  \pr{a\vert A}p & = \pr{a\vert A}[\pr{a\vert A}p+\pr{a\vert \n A}(1-p)]\\ -->
<!--  p & = \pr{a\vert A}p + \pr{a\vert \n A} - \pr{a\vert \n A}p\\ -->
<!--  p &= rel(a) p + 1-rel(a)- (1-rel(a))p\\ -->
<!--  p & = rel(a)p +1 - rel(a) -p +rel(a)p \\ -->
<!--  2p & =  2rel(a)p + 1 - rel(a)  \\ -->
<!--  2p - 2 rel(a)p & = 1-rel(a)\\ -->
<!--  2p(1-rel(a)) &= 1-rel(a)\\ -->
<!--  2p & = 1 -->
<!--  \end{align*} -->

<!-- \noindent  First we multiplied both sides by the denominator. Then we divided both sides by $\pr{a\vert A}$ and multiplied on the right side. Then we used our background notation and information. Next, we manipulated the right-hand side algebraically and  moved  $-p$ to the left-hand side. Move $2rel(a)p$ to the left and manipulate the result algebraically to get to the last line.} -->




















<!-- ## Likelihood and DAC -->

<!--  But how does our preference for the likelihood ratio as a measure of evidence strength relate to DAC? Let's go through Dawid's reasoning.  -->

<!--  A sensible  way to probabilistically interpret the  $70\%$ reliability of a witness who testifies that $A$  is to take it to consist in the fact that the probability of a positive testimony  if $A$ is the case, just as the probability of a negative testimony  (that is, testimony that $A$ is false) if $A$ isn't the case, is 0.7:\footnote{In general setting, these are called the \emph{sensitivity} and \emph{specificity} of a test (respectively), and they don't have to be equal. For instance, a degenerate test for an illness which always responds positively, diagnoses everyone as ill, and so has sensitivity 1, but specificity 0.}  -->
<!--   \[\prr{a\vert A}=\prr{\n a\vert\n  A}=0.7.\] -->
<!--  \noindent   $\prr{a\vert \n A}=1- \prr{\n a\vert \n A}=0.3$, and so the same information is encoded in the appropriate likelihood ratio: -->
<!--  \[LR_{prior}(a\vert A )=\frac{\prr{a\vert A}}{\prr{a\vert \n A}}= \frac{0.7}{0.3}\]  -->


<!--  Let's say that $a$ \emph{provides (positive) support} for $A$ in case  -->
<!--  \[O_{posterior}(A)=O_{prior}(A\vert a)> O_{prior}(A)\] -->
<!--  \noindent  that is, a testimony $a$ supports $A$ just in case the posterior odds of $A$ given $a$ are greater than the prior odds of $A$ (this happens just in case $\prp{A}>\prr{A}$). By \eqref{bayesodss2}, this will be the case if and only if $LR_{prior}(a\vert A)>1$. -->




<!--  One question that Dawid addresses is this: assuming reliability of witnesses  $0.7$, and assuming that  $a$ and $b$, taken separately, provide positive support for their respective claims, does it follow that  $a \et b$ provides positive support for $A\et B$? -->

<!-- Assuming the  independence of the witnesses, this will hold  in non-degenerate cases that do not  involve extreme probabilities, on the assumption of independence of $a$ and $b$ conditional on all combinations: $A\et B$, $A\et \n B$, $\n A \et B$ and $\n A \et \n B$.\footnote{Dawid only talks about the independence of witnesses without reference to  conditional independence. Conditional independence does not follow from independence, and it is the former that is needed here (also, four non-equivalent different versions of it).}$^,$~\footnote{In terms of notation and derivation in the optional content that will follow, the claim holds  if and only if $28 > 28 p_{11}-12p_{00}$.  This inequality is not  true for all admissible values of $p_{11}$ and $p_{00}$. If $p_{11}=1$ and $p_{00}=0$, the sides are equal. However, this is a rather degenerate example. Normally, we are  interested in cases where $p_{11}< 1$. And indeed, on this assumption, the inequality holds.} -->



<!-- Let us see why the above claim holds. The calculations are my reconstruction and are not due to Dawid. The reader might be annoyed with me working out the mundane details of Dawid's claims, but it turns out that in the case of Dawid's strategy, the devil is in the details. The independence of witnesses gives us: -->
<!-- \begin{align*} -->
<!--  \pr{a \et b \vert A\et B}& =0.7^2=0.49\\ -->
<!--  \pr{a \et b \vert A\et \n B}& =  0.7\times 0.3=0.21\\ -->
<!--  \pr{a \et b \vert \n A\et B}& =  0.3\times 0.7=0.21\\ -->
<!--  \pr{a \et b \vert \n A\et \n B}& =  0.3\times 0.3=0.09 -->
<!--  \end{align*} -->
<!--   Without assuming $A$ and $B$ to be independent, let the probabilities of $A\et B$, $\n A\et B$, $A\et \n B$, $\n A\et \n B$ be $p_{11}, p_{01}, p_{10}, p_{00}$. First, let's see what $\pr{a\et b}$ boils down to. -->

<!-- By the law of total probability we have: -->
<!--  \begin{align}\label{eq:total_lower} -->
<!--  \pr{a\et b} & =  -->
<!--                      \pr{a\et b \vert A \et B}\pr{A\et B} + \\ &  \nonumber -->
<!--                      +\pr{a\et b \vert A \et \n B}\pr{A\et \n B} \\ &  \nonumber -->
<!--  + \pr{a\et b \vert \n A \et B}\pr{\n A\et B} + \\ & \nonumber -->
<!--                      + \pr{a\et b \vert \n A \et \n B}\pr{\n A\et \n B} -->
<!--  \end{align} -->
<!--  \noindent which, when we substitute our values and constants, results in: -->
<!--  \begin{align*} -->
<!--                      & = 0.49p_{11}+0.21(p_{10}+p_{01})+0.09p_{00} -->
<!--  \end{align*} -->
<!--  Now, note that because $p_{ii}$s add up to one, we have $p_{10}+p_{01}=1-p_{00}-p_{11}$. Let us continue. -->
<!--  \begin{align*} -->
<!--     & = 0.49p_{11}+0.21(1-p_{00}-p_{11})+0.09p_{00} \\ -->
<!--                      & = 0.21+0.28p_{11}-0.12p_{00} -->
<!--  \end{align*} -->

<!--  Next, we ask what the posterior of $A\et B$ given $a\et b$ is (in the last line, we also multiply the numerator and the denominator by 100). -->
<!--  \begin{align*} -->
<!--  \pr{A\et B\vert a \et b} & = -->
<!--          \frac{\pr{a\et b \vert A \et B}\pr{A\et B}} -->
<!--              {\pr{a\et b}}\\ -->
<!--          & = -->
<!--                      \frac{49p_{11}} -->
<!--                            {21+28p_{11}-12p_{00}}  -->
<!--          \end{align*} -->

<!--  In this particular case, then, our question whether $\pr{A\et B\vert a\et b}>\pr{A\et B}$ boils down to asking whether -->
<!--  \[\frac{49p_{11}}{21+28p_{11}-12p_{00}}> p_{11}\] -->
<!--  that is, whether $28 > 28 p_{11}-12p_{00}$ (just divide both sides by $p_{11}$, multiply by the denominator, and manipulate algebraically).  -->




<!--  Dawid continues working with particular choices of values and provides neither a general statement of the fact that the above considerations instantiate nor a proof of it. In the middle of the paper he says:  -->
<!--  \begin{quote} -->
<!--  Even under prior dependence, the combined support is always positive, in the sense that the posterior probability of the case always exceeds its prior probability\dots When the problem is analysed carefully, the `paradox' evaporates [pp. 95-7]\end{quote} -->
<!--  \noindent where he  still means the case with the particular values that he has given, but he seems to suggest that the claim generalizes to a large array of cases.  -->


<!--  The paper does not contain a precise statement making the conditions required explicit and, \emph{a fortriori}, does not contain a proof of it. -->
<!-- Given the example above and Dawid's informal reading, let us develop a more precise statement of the claim and  a proof thereof.  -->


<!-- \begin{fact}\label{ther:increase} -->
<!-- Suppose that  $rel(a),rel(b)>0.5$ and witnesses are independent conditional on all Boolean combinations of $A$ and $B$  (in a sense to be specified), and that none of the Boolean combinations of $A$ and $B$ has an extreme probability (of 0 or 1). It follows that  $\pr{A\et B \vert a\et b}>\pr{A\et B}$. (Independence of $A$ and $B$ is not required.) -->
<!-- \end{fact} -->


<!-- Roughly, the theorem says that if independent and reliable witnesses provide positive  support of their separate claims, their joint testimony provides positive support of the conjunction of their claims.  -->




<!-- Let us see why the claim holds. First, we introduce an abbreviation for witness reliability:  -->
<!--   \begin{align*}\mathbf{a} &=rel(a)=\pr{a\vert A}=\pr{\n a\vert \n A}>0.5\\  -->
<!-- \mathbf{b} &=rel(b)=\pr{b\vert B}=\pr{\n b\vert \n A}>0.5 -->
<!-- \end{align*} -->
<!-- Our independence assumption means: -->
<!-- \begin{align*} -->
<!-- \pr{a\et b \vert A\et B}  &= \mathbf{ab}\\ -->
<!-- \pr{a\et b \vert A\et \n B} & = \mathbf{a(1-b)}\\ -->
<!-- \pr{a\et b \vert \n A\et B}  & = \mathbf{(1-a)b}\\ -->
<!-- \pr{a\et b \vert \n A\et \n  B}  & = \mathbf{(1-a)(1-b)} -->
<!-- \end{align*} -->

<!-- \vspace{-2mm} -->

<!-- Abbreviate the probabilities the way we already did: -->

<!-- \begin{center} -->
<!-- \begin{tabular}{ll} -->
<!-- $\pr{A\et B} = p_{11}$ & $\pr{A\et \n B} = p_{10}$\\ -->
<!-- $\pr{\n A \et B} = p_{01}$ & $\pr{\n A \et \n B}=p_{00}$ -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- Our assumptions entail $0\neq p_{ij}\neq 1$ for $i,j\in \{0,1\}$ and: -->
<!-- \begin{align}\label{eq:sumupto1} -->
<!-- p_{11}+p_{10}+p_{01}+p_{00}&=1 -->
<!-- \end{align} -->

<!-- \noindent So, we can use this with \eqref{eq:total_lower} to get: -->
<!-- \begin{align}\label{eq:aetb} -->
<!-- \pr{a\et b} & =  \mathbf{ab}p_{11} + \mathbf{a(1-b)}p_{10}+\mathbf{(1-a)b}p_{01} + \mathbf{(1-a)(1-b)}p_{00}\\ \nonumber -->
<!-- & = p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab}) -->
<!-- \end{align} -->

<!-- Let's now work out what the posterior of $A\et B$ will be, starting with an application of the Bayes' Theorem: -->
<!-- \begin{align} \nonumber -->
<!-- \pr{A\et B \vert a\et b} & = \frac{\pr{a\et b \vert A \et B}\pr{A\et B}}{\pr{a\et b}} -->
<!-- \\ \label{eq:boiled} -->
<!-- & = \frac{\mathbf{ab}p_{11}}{p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})} -->
<!-- \end{align} -->
<!-- To answer our question we therefore have to compare the content of \eqref{eq:boiled} to $p_{11}$ and our claim holds just in case: -->
<!-- \begin{align*} -->
<!-- \frac{\mathbf{ab}p_{11}}{p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})} &> p_{11} -->
<!-- \end{align*} -->
<!-- \begin{align*} -->
<!--  \frac{\mathbf{ab}}{p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})} & > 1\end{align*} -->
<!--  \begin{align}   -->
<!--  \label{eq:goal} -->
<!-- p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab}) & < \mathbf{ab} -->
<!-- \end{align} -->
<!-- Proving \eqref{eq:goal} is therefore our goal for now. This is achieved by the following reasoning:\footnote{Thanks to Pawel Pawlowski for working on this proof with me.} -->



<!-- \hspace{-7mm} -->
<!-- \resizebox{13.5cm}{!}{ -->
<!-- \begin{tabular}{llr} -->
<!--  1. & $\mathbf{b}>0.5,\,\,\, \mathbf{a}>0.5$ & \mbox{assumption}\\ -->
<!--  2. & $2\mathbf{b}>1,\,\,\, 2\mathbf{a}> 1$ & \mbox{from 1.}\\ -->
<!--  3. & $2\mathbf{ab}>\mathbf{a},\,\,\, 2\mathbf{ab}>\mathbf{b}$ & \mbox{multiplying by $\mathbf{a}$ and $\mathbf{b}$ respectively}\\ -->
<!--  4.  & $p_{10}2\mathbf{ab}>p_{10}\mathbf{a}$,\,\,\, $p_{01}2\mathbf{ab}>p_{01}\mathbf{b}$ & \mbox{multiplying by $p_{10}$ and $p_{01}$ respectively}\\ -->
<!--  5.  & $p_{10}2\mathbf{ab} +  p_{01}2\mathbf{ab} > p_{10}\mathbf{a} + p_{01}\mathbf{b}$ & \mbox{adding by sides, 3., 4.}\\ -->
<!--  6. & $1- \mathbf{b}- \mathbf{a} <0$ & \mbox{from 1.}\\ -->
<!--  7. & $p_{00}(1-\mathbf{b}-\mathbf{a})<0$ & \mbox{From 6., because $p_{00}>0$}\\ -->
<!--   8.  &  $p_{10}2\mathbf{ab} +  p_{01}2\mathbf{ab} > p_{10}\mathbf{a} + p_{01}\mathbf{b} + p_{00}(1-\mathbf{b}-\mathbf{a})$ & \mbox{from 5. and 7.}\\ -->
<!--   9.  & $p_{10}\mathbf{ab} + p_{10}\mathbf{ab} + p_{01}\mathbf{ab} + p_{01}\mathbf{ab} + p_{00}\mathbf{ab} - p_{00}\mathbf{ab}> p_{10}\mathbf{a} + p_{01}\mathbf{b} + p_{00}(1-\mathbf{b}-\mathbf{a})$ & \mbox{8., rewriting left-hand side}\\ -->
<!--   10.  & $p_{10}\mathbf{ab} + p_{01}\mathbf{ab}  + p_{00}\mathbf{ab} > - p_{10}\mathbf{ab}  -  p_{01}\mathbf{ab} + p_{00}\mathbf{ab} +  p_{10}\mathbf{a} + p_{01}\mathbf{b} + p_{00}(1-\mathbf{b}-\mathbf{a})$ &  \mbox{9., moving from left to right}\\ -->
<!-- 11. & $\mathbf{ab}(p_{10}+p_{01}+p_{00})> p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{10., algebraic manipulation}\\ -->
<!-- 12. & $\mathbf{ab}(1-p_{11})> p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{11. and equation \eqref{eq:sumupto1}}\\ -->
<!-- 13. & $\mathbf{ab}- \mathbf{ab}p_{11}> p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{12., algebraic manipulation}\\ -->
<!-- 14. & $\mathbf{ab}> \mathbf{ab}p_{11}+ p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{13., moving from left to right}\\ -->
<!-- \end{tabular}} -->
<!-- %\end{adjustbox} -->

<!-- \vspace{1mm} -->

<!-- The last line is what we have been after. -->

<!-- \intermezzob -->





<!-- Now that we have as a theorem an explication of what Dawid informally suggested, let's see whether it helps the probabilist handling of DAC.  -->


<!-- ### Kaplow -->

<!--  On RLP, at least in certain cases, the decision rule leads us to \eqref{eq:Cheng:compar2}, which tells us to decide the case based on whether the likelihood ratio is greater than 1. -->



<!-- \footnote{Again, the name of the view is by no means standard, it is  just a term I coined to refer to various types of legal probabilism in a fairly uniform manner.}  While Kaplow did not discuss DAC or the gatecrasher paradox, it is only fair to evaluate Kaplow's proposal from the perspective of these difficulties.  -->



<!-- Add here stuff from Marcello's Mind paper about the prisoner hypothetical.  -->
<!-- Then, discuss Rafal's critique of the likelihood  -->
<!-- ratio threshold and see where we end up.  -->



<!-- ## Dawid's likelihood strategy doesn't help -->

<!--  Recall that DAC was a problem posed for the decision standard proposed by TLP, and the real question is how the information resulting from Fact \ref{ther:increase} can help to avoid that problem.  Dawid does not mention any decision standard, and so addresses quite a different question, and so it is not clear that  ``the `paradox'  evaporates'', as Dawid suggests. -->

<!--  What Dawid correctly suggests (and we establish in general as Fact \ref{ther:increase})  is that  the support of the conjunction by two witnesses will be positive as soon as their separate support for the conjuncts is positive. That is, that the posterior of the conjunction will be higher that its prior. But  the critic of probabilism never denied that the conjunction of testimonies might raise the probability of the conjunction if the testimonies taken separately support the conjuncts taken separately. Such a critic can still insist that Fact \ref{ther:increase} does nothing to alleviate her concern.  After all, at least \emph{prima facie} it still might be the case that: -->
<!-- \begin{itemize} -->
<!-- \item  the posterior probabilities of the conjuncts are above a given threshold, -->
<!-- \item   the posterior probability of the conjunction is higher than the prior probability of the conjunction, -->
<!-- \item   the posterior probability of the conjunction  -->
<!--  is still below the threshold. -->
<!-- \end{itemize} -->
<!-- That is, Fact \ref{ther:increase} does not entail that once the conjuncts satisfy a decision standard, so does the conjunction.  -->



<!-- At some point, Dawid makes a  general claim that is somewhat stronger than the one already cited: -->
<!--  \begin{quote} When the problem is analysed carefully, the `paradox' evaporates: suitably measured, the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents. -->

<!--   [p. 97]\end{quote} -->
<!-- This is quite a different claim from the content of Fact \ref{ther:increase}, because previously the joint probability was claimed only to increase as compared to the prior, and here it is claimed to increase above the level of the separate increases provided by separate testimonies. Regarding this issue Dawid elaborates (we still use the $p_{ij}$-notation that we've already introduced): -->
<!--  \begin{quote} -->
<!--  ``More generally, let $\pr{a\vert A}/\pr{a\vert \n A}=\lambda$, $\pr{b\vert B}/\pr{b\vert \n B}=\mu$, with $\lambda, \mu >0.7$, as might arise, for example, when there are several available testimonies. If the witnesses are -->
<!--   independent, then \[\pr{A\et B\vert  a\et b} = \lambda \mu p_{11}/(\lambda \mu p_{11} + \lambda p_{10} +\mu p_{01} + p_{00})\] which  increases with -->
<!--  each of $\lambda$ and $\mu$, and is never less than the larger of $\lambda p_{11}/(1-p_{11}+\lambda p_{11}), -->
<!--  \mu p_{11} /(1- p_{11} 1 + \mu p_{11})$, the posterior probabilities appropriate to the individual testimonies.'' [p. 95] -->
<!--  \end{quote} -->
<!-- This claim, however, is false. -->

<!-- \intermezzoa -->


<!-- Let us see why.   The quoted passage is a bit dense. It contains four claims for which no arguments are given in the paper. The first three are listed below as \eqref{eq:lambdamu}, the fourth is that if the conditions in \eqref{eq:lambdamu} hold,  $\pr{A\et B\vert  a\et b}>max(\pr{A\vert a},\pr{B\vert b})$.  Notice that $\lambda=LR(a\vert A)$ and $\mu=LR(b\vert B)$. Suppose the first three claims hold, that is: -->
<!--  \begin{align}\label{eq:lambdamu} -->
<!--  \pr{A\et B\vert  a\et b} &= \lambda \mu p_{11}/(\lambda \mu p_{11} + \lambda p_{10} +\mu p_{01} + p_{00})\\ -->
<!--  \pr{A\vert a} & = \frac{\lambda p_{11}}{1-p_{11}+\lambda p_{11}}\nonumber \\ -->
<!--  \pr{B\vert b} & = \frac{\mu p_{11}}{1-p_{11}+\mu p_{11}} \nonumber  -->
<!--  \end{align} -->
<!--  \noindent Is it really the case that  $\pr{A\et B\vert  a\et b}>\pr{A\vert a},\pr{B\vert b}$? It does not  seem so. Let $\mathbf{a}=\mathbf{b}=0.6$, $pr =\la p_{11},p_{10},p_{01},p_{00}\ra=\la 0.1, 0.7, 0.1, 0.1 \ra$. Then, $\lambda=\mu=1.5>0.7$ so the assumption is satisfied.  -->
<!-- Then we have $\pr{A}=p_{11}+p_{10}=0.8$, $\pr{B}=p_{11}+p_{01}=0.2$. We can also easily compute $\pr{a}=\mathbf{a}\pr{A}+(1-\mathbf{a})\pr{\n A}=0.56$ and $\pr{b}=\mathbf{b}\pr{B}+(1-\mathbf{b})\pr{\n B}=0.44$.  -->
<!--  Yet: -->

<!--  \begin{align*} -->
<!--  \pr{A\vert a} & = \frac{\pr{a\vert A}\pr{A}}{\pr{a}} = \frac{0.6\times 0.8}{0.6\times 0.8 + 0.4\times 0.2}\approx 0.8571 \\ -->
<!--  \pr{B\vert b} & = \frac{\pr{b\vert B}\pr{B}}{\pr{b}} = \frac{0.6\times 0.2}{0.6\times 0.2 + 0.4\times 0.8}\approx 0.272 \\ -->
<!--  \pr{A\et B \vert a \et b} & = \frac{\pr{a\et b\vert A \et B}\pr{A\et B}}{\splitfrac{\pr{a\et b \vert A\et B}\pr{A\et B}+ -->
<!--    \pr{a\et b\vert A\et \n B}\pr{A\et \n B} +}{+  -->
<!--  \pr{a\et b \vert \n A \et B}\pr{\n A \et B} + \pr{a\et b \vert \n A \et \n B}\pr{\n A \et \n B}}} \\ -->
<!--  & = \frac{\mathbf{ab}p_{11}}{ -->
<!--    \mathbf{ab}p_{11} + \mathbf{a}(1-\mathbf{b})p_{10} + (1-\mathbf{a})\mathbf{b}p_{01} + (1-\mathbf{a})(1-\mathbf{b})p_{00} -->
<!--  }   -->
<!--     \approx 0.147 -->
<!--  \end{align*} -->
<!-- The posterior probability of $A\et B$ is not only lower than the larger of the individual posteriors, but also lower than any of them!  -->

<!-- So what went wrong in Dawid's calculations in \eqref{eq:lambdamu}? Well, the first formula is correct. However, let us take a look at what the second one says (the problem with the third one is pretty much the same): -->
<!-- \begin{align*} -->
<!-- \pr{A\vert a } & = \frac{\frac{\pr{a\vert A}}{\pr{\n a \vert A}}\times \pr{A\et B}}{\pr{\n (A\et B)}+ \frac{\pr{a\vert A}}{\pr{\n a \vert A}} \times \pr{A\et B}} -->
<!-- \end{align*} -->
<!-- Quite surprisingly, in Dawid's formula for $\pr{A\vert a}$, the probability of $A\et B$ plays a role. To see that it should not take any $B$ that excludes $A$ and the formula will lead to the conclusion that \emph{always} $\pr{A\vert a}$ is undefined. The problem with Dawid's formula is that instead of $p_{11}=\pr{A\et B}$ he should have used $\pr{A}=p_{11}+p_{10}$, in which case the formula would rather say this: -->
<!-- \begin{align*} -->
<!-- \pr{A\vert a } & = \frac{\frac{\pr{a\vert A}}{\pr{\n a \vert A}}\times \pr{A}}{\pr{\n A}+ \frac{\pr{a\vert A}}{\pr{\n a \vert A}} \times \pr{A}}\\ -->
<!-- & = \frac{\frac{\pr{a\vert A}\pr{A}}{\pr{\n a \vert A}}}{\frac{\pr{\n a\vert A}\pr{\n A}}{\pr{\n a\vert A}}+ \frac{\pr{a\vert A}\pr{A}}{\pr{\n a \vert A}}}\\ -->
<!-- & = \frac{\pr{a\vert A}\pr{A}}{\pr{\n a\vert A}\pr{\n A} + \pr{a\vert A}\pr{A}} -->
<!-- \end{align*} -->
<!-- Now, on the assumption that witness' sensitivity is equal to their specificity, we have $\pr{a\vert \n A}=\pr{\n a \vert A}$ and can substitute this in the denominator: -->
<!--  \begin{align*} & = \frac{\pr{a\vert A}\pr{A}}{\pr{ a\vert \n A}\pr{\n A} + \pr{a\vert A}\pr{A}}\end{align*} -->
<!-- and this would be a formulation of Bayes' theorem.  And indeed with $\pr{A}=p_{11}+p_{10}$ the formula works (albeit its adequacy rests on the identity of $\pr{a\vert \n A}$ and $\pr{\n a \vert A}$), and yields the result that we already obtained: -->
<!-- \begin{align*} -->
<!-- \pr{A\vert a} &= \frac{\lambda(p_{11}+p_{10})}{1-(p_{11}+p_{10})+\lambda(p_{11}+p_{10})}\\ -->
<!-- &= \frac{1.5\times 0.8}{1- 0.8+1.5\times 0.8} \approx 0.8571 -->
<!-- \end{align*} -->



<!--   The situation cannot be much improved by taking $\mathbf{a}$ and $\mathbf{b}$ to be high. For instance, if they're both 0.9 and $pr=\la0.1, 0.7, 0.1, 0.1 \ra$, the posterior of $A$ is $\approx 0.972$, the posterior of $B$ is $\approx 0.692$, and yet the joint posterior of $A\et B$ is $0.525$. -->

<!--  The situation cannot also be improved by saying that at least if the threshold is 0.5, then as soon as $\mathbf{a}$ and $\mathbf{b}$  are above 0.7 (and, \emph{a fortriori}, so are $\lambda$ and $\mu$), the individual posteriors being above 0.5 entails the joint posterior being above 0.5 as well. For instance, for $\mathbf{a}=0.7$ and $\mathbf{b}=0.9$ -->
<!--  with $pr= \la 0.1, 0.3, 0.5, 0.1\ra$, the individual posteriors of $A$ and $B$ are $\approx 0.608$ and $\approx 0.931$ respectively, while the joint posterior of $A\et B$ is $\approx 0.283$. -->



<!-- \intermezzob -->

<!--  The situation cannot be improved by saying that what was meant was rather that the joint likelihood is going to be at least as high as the maximum of the individual likelihoods, because quite the opposite is the case: the joint likelihood is going to be lower than any of the individual ones. -->

<!--  \intermezzoa -->

<!-- Let us make sure this is the case.  We have:  -->
<!--  \begin{align*} -->
<!--  LR(a\vert A) & = \frac{\pr{a\vert A}}{\pr{a\vert \n A}}\\ -->
<!--  &= \frac{\pr{a\vert A}}{\pr{\n a\vert  A}} \\ -->
<!-- & =  \frac{\mathbf{a}}{\mathbf{1-a}}. -->
<!-- \end{align*} -->
<!-- where the substitution in the denominator is legitimate only because witness' sensitivity is identical to their specificity.  -->

<!-- With the joint likelihood, the reasoning is just a bit more tricky. We will need to know what $\pr{a\et b \vert \n (A\et B)}$ is. There are three disjoint possible conditions in which the condition holds: $A\et \n B, \n A \et B$, and $\n A \et \n B$. The probabilities of $a\et b$ in these three scenarios are respectively $\mathbf{a(1-b),(1-a)b,(1-a)(1-b)}$ (again, the assumption of independence is important), and so on the assumption $\n(A\et B)$ the probability of $a\et b$ is: -->
<!-- \begin{align*} -->
<!-- \pr{a\et b \vert \n (A\et B)} & =  -->
<!-- \mathbf{a}(1-\mathbf{b})+(1-\mathbf{a})\mathbf{b}+(1-\mathbf{a})(1-\mathbf{b})\\  -->
<!-- & =  -->
<!-- \mathbf{a}(1-\mathbf{b})+(1-\mathbf{a})(\mathbf{b} + 1-\mathbf{b})\\ -->
<!-- & = \mathbf{a}(1-\mathbf{b})+(1-\mathbf{a})\\ -->
<!-- & = \mathbf{a}-\mathbf{a}\mathbf{b}+1-\mathbf{a} = 1- \mathbf{a}\mathbf{b} -->
<!-- \end{align*} -->
<!-- So, on the assumption of witness independence, we have: -->
<!-- \begin{align*} -->
<!-- LR(a\et b \vert A \et B) & = \frac{\pr{a\et b \vert A \et B}}{\pr{a \et b\vert \n (A \et B)}} \\ -->
<!-- & = \frac{\mathbf{ab}}{\mathbf{1-ab}} -->
<!-- \end{align*} -->

<!--  With $0<\mathbf{a},\mathbf{b}<1$ we have $\mathbf{ab}<\mathbf{a}$, $1-\mathbf{ab}>1-\mathbf{a}$, and consequently: -->
<!--  \[\frac{\mathbf{ab}}{\mathbf{1-ab}} < \frac{\mathbf{a}}{\mathbf{1-a}}\] -->
<!--  which means that the joint likelihood is going to be lower than any of the individual ones. -->




<!-- \intermezzob -->



<!--   Fact \ref{ther:increase} is so far the most optimistic reading of the claim that if witnesses are independent and fairly reliable, their testimonies are going to provide positive support for the conjunction,\footnote{And this is the reading that Dawid in passing suggests: ``the combined support is always positive, in the sense that the posterior probability of the case always exceeds its prior probability.'' [@dawid1987difficulty: 95] and any stronger reading of Dawid's suggestions fails. But Fact \ref{ther:increase} is not too exciting when it comes to answering the original DAC. The original question focused on the adjudication model according to which the deciding agents are to evaluate the posterior probability of the whole case conditional on all evidence, and to convict if it is above a certain threshold. The problem, generally, is that  it might be the case that  the pieces of evidence for particular elements of the claim can have high likelihood and posterior probabilities of particular elements can be above the threshold while the posterior joint probability will still fail to meet the threshold. The fact that the joint posterior will be higher than the joint prior does not  help much. For instance, if $\mathbf{a}=\mathbf{b}=0.7$, $pr=\la 0.1, 0.5, 0.3, 0.1\ra$, the posterior of $A$ is $\approx 0.777$, the posterior of $B$ is $\approx 0.608$ and the joint posterior is $\approx 0.216$ (yes, it is  higher than the joint prior $=0.1$, but this does not help the conjunction to satisfy the decision standard). -->




<!--  To see the extent to which Dawid's strategy is helpful here, perhaps the following analogy might be useful.   -->
<!--  Imagine it is winter, the heating does not work in my office and I am quite cold. I pick up the phone and call maintenance. A rather cheerful fellow picks up the phone. I tell him what my problem is, and he reacts: -->

<!--  \vspace{1mm} -->
<!--  \begin{tabular}{lp{10cm}} -->
<!--  --- & Oh, don't worry. \\ -->
<!--  --- & What do you mean? It's cold in here! \\ -->
<!--  --- & No no, everything is fine, don't worry.\\ -->
<!--  --- & It's not fine! I'm cold here! \\ -->
<!--  --- & Look, sir, my notion of it being warm in your office is that the building provides some improvement to what the situation would be if it wasn't there. And you agree that you're definitely warmer than you'd be if your desk was standing outside, don't you? Your, so to speak, posterior warmth is higher than your prior warmth, right?  -->
<!--  \end{tabular} -->
<!--  \vspace{1mm} -->

<!--  Dawid's discussion is in the vein of the above conversation. In response to a problem with the adjudication model under consideration Dawid simply invites us to abandon thinking in terms of it and to abandon requirements crucial for the model.  Instead, he puts forward a fairly weak notion of support (analogous to a fairly weak sense of the building providing improvement), according to which,  assuming witnesses are fairly reliable,  if separate fairly reliable witnesses provide positive support to the conjuncts, then their joint testimony provides positive support for the conjunction.  -->

<!--  As far as our assessment of the original adjudication model and dealing  with DAC, this leaves us hanging. Yes, if we abandon the model, DAC does not worry us anymore. But should we? And if we do, what should we change it to, if we do not want to be banished from the paradise of probabilistic methods?   -->




<!--    Having said this, let me emphasize that Dawid's paper is important in the development of the debate, since it shifts focus on the likelihood ratios, which for various reasons are much better measures of evidential support provided by particular pieces of evidence than mere posterior probabilities.  -->



<!-- Before we move to another attempt at a probabilistic formulation of the decision standard, let us introduce the other hero of our story: the gatecrasher paradox. It is against DAC and this paradox that the next model will be judged.  -->















<!-- \intermezzoa -->

<!-- In fact, Cohen replied to Dawid's paper [@cohen1988difficulty]. His reply, however, does not have much to do with the workings of Dawid's strategy, and is rather unusual. Cohen's first point is that the calculations of posteriors require odds about unique events, whose meaning is usually given in terms of potential wagers -- and the key criticism here is that in practice such wagers cannot be decided. This is not a convincing criticism, because the betting-odds interpretations of subjective probability do not require that on each occasion the bet should really be practically decidable. It rather invites one to imagine a possible situation in which the truth could be found out and asks: how much would we bet on a certain claim in such a situation? In some cases, this assumption is false, but there is nothing in principle wrong with thinking about the consequences of false assumptions.  -->



<!-- Second, Cohen says that Dawid's argument works only for testimonial evidence, not for other types thereof. But this claim is simply false -- just because Dawid used testimonial evidence as an example that he worked through it by no means follows that the approach cannot be extended. After all, as long as we can talk about sensitivity and specificity of a given piece of evidence, everything that Dawid said about testimonies can be repeated \emph{mutatis mutandis}. -->



<!-- Third, Cohen complaints that Dawid in his example worked with rather high priors, which according to Cohen would be too high to correspond to the presumption of innocence. This also is not a very successful rejoinder. Cohen picked his priors in the example for the ease of calculations, and the reasoning can be run with lower priors. Moreover, instead of discussing the conjunction problem, Cohen brings in quite a different problem: how to probabilistically model the presumption of innocence, and what priors of guilt should be appropriate? This, indeed, is an important problem; but it does not have much to do with DAC, and should be discussed separately. -->




<!-- ## Problem's with Kaplow's stuff -->



<!--  Kaplow does not discuss the conceptual difficulties that we are concerned with, but this will not stop us from asking whether DTLP can handle them (and answering to the negative). Let us start with DAC. -->

<!--   Say we consider two claims, $A$ and $B$. Is it generally the case that if they separately satisfy the decision rule, then so does $A\et B$? That is, do the assumptions: -->
<!--  \begin{align*} -->
<!--  \frac{\pr{E\vert A}}{\pr{E\vert \n A}}  & > \frac{\pr{\n A}}{\pr{A}} \times \frac{L}{G}\\ -->
<!--  \frac{\pr{E\vert B}}{\pr{E\vert \n B}}  & > \frac{\pr{\n B}}{\pr{B}} \times \frac{L}{G} -->
<!--  \end{align*} -->
<!--  \noindent entail -->
<!--  \begin{align*} -->
<!--  \frac{\pr{E\vert A\et B}}{\pr{E\vert \n (A\et B)}}  & > \frac{\pr{\n (A\et B)}}{ -->
<!--  \pr{A\et B}} \times \frac{L}{G}? -->
<!--  \end{align*} -->

<!-- Alas, the answer is negative. -->

<!-- \intermezzoa -->

<!-- This can be seen from the following example.  Suppose a random digit from 0-9 is drawn; we do not know the result; we are  told that the result is $<7$ ($E=$`the result is $<7$'), and  we are to decide whether to accept the following claims: -->
<!--  \begin{center} -->
<!--  \begin{tabular}{@{}ll@{}} -->
<!--  \toprule -->
<!--  $A$ & the result is $<5$. \\ -->
<!--  $B$  & the result is an even number.\\ -->
<!--  $A\et B$ & the result is an even number $<5$. \\ -->
<!--  \bottomrule -->
<!--  \end{tabular} -->
<!--  \end{center} -->
<!--  Suppose that $L=G$ (this is for simplicity only --- nothing hinges on this, counterexamples for when this condition fails are analogous). First, notice that $A$ and $B$ taken separately satisfy \eqref{eq:Kaplow_decision2}. $\pr{A}=\pr{\n A}=0.5$, $\pr{\n A}/\pr{A}=1$ $\pr{E\vert A}=1$, $\pr{E\vert \n A}=0.4$. \eqref{eq:Kaplow_decision2} tells us to check: -->
<!--  \begin{align*} -->
<!--  \frac{\pr{E\vert A}}{\pr{E\vert \n A}}&> \frac{L}{G}\times \frac{\pr{\n A}}{\pr{A}}\\ -->
<!--  \frac{1}{0.4} & > 1 -->
<!--  \end{align*} -->


<!--  \noindent so, following DTLP, we should accept $A$.   -->
<!--   For analogous reasons, we should also accept $B$. $\pr{B}=\pr{\n B}=0.5$, $\pr{\n B}/\pr{B}=1$ $\pr{E\vert B}=0.8$, $\pr{E\vert \n B}=0.6$, so we need  to check that indeed: -->
<!--  \begin{align*} -->
<!--  \frac{\pr{E\vert B}}{\pr{E\vert \n B}}&> \frac{L}{G}\times \frac{\pr{\n B}}{\pr{B}}\\ -->
<!--  \frac{0.8}{0.6} & > 1  -->
<!--  \end{align*} -->

<!--  But now, $\pr{A\et B}=0.3$, $\pr{\n (A \et B)}=0.7$, $\pr{\n (A\et B)}/\pr{A\et B}=2\frac{1}{3}$, $\pr{E\vert A \et B}=1$, $\pr{E\vert \n (A\et B)}=4/7$ and it is false that: -->
<!--   \begin{align*} -->
<!--  \frac{\pr{E\vert A \et B}}{\pr{E\vert \n (A\et B)}}&> \frac{L}{G}\times \frac{\pr{\n (A \et B)}}{\pr{A \et B}}\\ -->
<!--  \frac{7}{4} & > \frac{7}{3}  -->
<!--  \end{align*} -->

<!-- The example was easy, but the conjuncts are probabilistically dependent. One might ask: are there counterexamples that  involve claims which are  probabilistically independent?\footnote{Thanks to Alicja Kowalewska for pressing me on this.}  -->

<!-- Consider an experiment in which someone tosses a six-sided die twice. Let the result of the first toss be $X$ and the result of the second one $Y$. Your evidence is that the results of both tosses are greater than one ($E=: X>1 \et Y>1$). Now, let $A$ say that $X<5$ and $B$ say that $Y<5$. -->

<!-- The prior probability of $A$ is $2/3$ and the prior probability of $\n A$ is $1/3$ and so $\frac{\pr{\n A}}{\pr{A}}=0.5$. Further, $\pr{E\vert A}=0.625$, $\pr{E\vert \n A}= 5/6$ and so $\frac{\pr{E\vert A}}{\pr{E\vert \n A}}=0.75$ Clearly, $0.75>0.5$, so $A$ satisfies the decision standard. Since the situation with $B$ is symmetric, so does $B$.  -->

<!--  Now, $\pr{A\et B}=(2/3)^2=4/9$ and $\pr{\n (A\et B)}=5/9$. So $\frac{\pr{\n(A\et B)}}{\pr{A\et B}}=5/4$.  -->
<!--  Out of 16 outcomes for which $A\et B$ holds, $E$ holds in 9, so $\pr{E\vert A\et B}=9/16$. Out of 20 remaining outcomes for which $A\et B$ fails, $E$ holds in 16, so -->
<!--   $\pr{E\vert \n (A\et B)}=4/5$. Thus, $\frac{\pr{E\vert A\et B}}{\pr{E\vert \n (A\et B)}}=45/64 <5/4$, so the conjunction does not satisfy the decision standard. -->



<!-- \intermezzob -->


<!-- Let us turn to the gatecrasher paradox.  -->


<!--  Suppose $L=G$ and recall our abbreviations: $\pr{E}=e$, $\pr{H_\Pi}=\pi$. DTLP tells us to convict just in case: -->
<!--  \begin{align*} -->
<!--  LR(E) &> \frac{1-\pi}{\pi} -->
<!--  \end{align*} -->
<!--  \noindent From \eqref{eq:Cheng_lre} we already now that  -->
<!--  \begin{align*} -->
<!--  LR(E) & = \frac{0.991-0.991\pi}{0.009\pi} -->
<!--  \end{align*} -->
<!--  \noindent so we need to see whether there are any $0<\pi<1$ for which   -->
<!--  \begin{align*} -->
<!--   \frac{0.991-0.991\pi}{0.009\pi} &> \frac{1-\pi}{\pi} -->
<!--  \end{align*} -->
<!--  \noindent Multiply both sides first by $009\pi$ and then by $\pi$: -->
<!--  \begin{align*} -->
<!--  0.991\pi - 0.991\pi^2 &> 0.09\pi - 0.009\pi^2 -->
<!--  \end{align*} -->
<!--  \noindent Simplify and call the resulting function $f$: -->
<!--  \begin{align*} -->
<!--  f(\pi) = - 0.982 \pi^2 + 0.982\pi &>0  -->
<!--  \end{align*} -->
<!--  \noindent The above condition is satisfied for any $0<\pi <1$ ($f$ has two zeros: $\pi = 0$ and $\pi = 1$). Here is  a plot of $f$: -->

<!--  \includegraphics[width=12cm]{f-gate.png} -->

<!--  Similarly, $LR(E)>1$ for any $0< \pi <1$. Here is a plot of $LR(E)$ against $\pi$: -->

<!--  \includegraphics[width=12cm]{lre-gate.png} -->

<!-- \noindent Notice that $LR(E)$ does not go below 1. This means that for $L=G$ in the gatecrasher scenario DTLP wold tell us to convict for any prior probability of guilt $\pi\neq 0,1$. -->

<!-- One might ask: is the conclusion very sensitive to the choice of $L$ and $G$? The answer is, not too much. -->

<!-- \intermezzoa -->


<!--  How sensitive is our analysis to the choice of $L/G$? Well, $LR(E)$ does not change at all, only the threshold moves. For instance, if $L/G=4$, instead of $f$ we end up with \begin{align*} -->
<!--  f'(\pi) = - 0.955 \pi^2 + 0.955\pi &>0  -->
<!--  \end{align*} -->
<!--  and the function still takes positive values on the interval $(0,1)$. In fact, the decision won't change until $L/G$ increases to $\approx 111$. Denote $L/G$ as $\rho$, and let us start with the general decision standard, plugging in our calculations for $LR(E)$: -->
<!-- \begin{align*} -->
<!-- LR(E) &> \frac{\pr{H_\Delta}}{\pr{H_\Pi}} \rho\\ -->
<!-- LR(E) &> \frac{1-\pi}{\pi} \rho \\ -->
<!-- \frac{0.991-0.991\pi}{0.009\pi} &> \frac{1-\pi}{\pi} \rho\\ -->
<!-- \frac{0.991-0.991\pi}{0.009\pi}\frac{\pi}{1-\pi} &>  \rho\\ -->
<!-- \frac{0.991\pi-0.991\pi^2}{0.009\pi-0.009\pi^2} &>  \rho\\ -->
<!-- \frac{\pi(0.991-0.991\pi)}{\pi(0.009-0.009\pi)} &>  \rho\\ -->
<!-- \frac{0.991-0.991\pi}{0.009-0.009\pi} &>  \rho\\ -->
<!-- \frac{0.991(1-\pi)}{0.009(1-\pi)} &>  \rho\\ -->
<!-- \frac{0.991}{0.009} &>  \rho\\ -->
<!-- 110.1111 &>  \rho\\ -->
<!-- \end{align*} -->








<!-- \intermezzob -->

<!--  So, we conclude, in usual circumstances, DTLP does not handle the gatecrasher paradox.  -->


<!-- ## Conclusions -->





<!-- Where are we, how did we get here, and where can we go from here? -->
<!--  We were looking for a probabilistically explicated condition $\Psi$ such that the trier of fact, at least ideally, should accept any relevant claim (including $G$) just in case $\Psi(A,E)$. -->

<!-- From the discussion that transpired it should be clear that we were looking for a $\Psi$ satisfying the following desiderata: -->

<!-- \begin{description} -->
<!-- \item[conjunction closure] If $\Psi(A,E)$ and $\Psi(B,E)$, then $\Psi(A\et B,E)$. -->
<!-- \item[naked statistics] The account should at least make it possible for convictions based on strong, but naked statistical evidence to be unjustified.  -->
<!-- \item[equal treatment] the condition should apply to any relevant claim whatsoever (and not just a selected claim, such as $G$). -->
<!-- \end{description} -->


<!-- Throughout the paper we focused on the first two conditions (formulated in terms of the difficulty with conjunction (DAC), and the gatecrasher paradox), going over various proposals of what $\Psi$ should be like and evaluating how they fare. The results can be summed up in the following table: -->


<!-- \begin{center} -->
<!-- \footnotesize  -->
<!--  \begin{tabular}{@{}p{3cm}p{2.5cm}p{4cm}p{3cm}@{}} -->
<!-- \toprule -->
<!-- \textbf{View} & \textbf{Convict iff} & \textbf{DAC} & \textbf{Gatecrasher} \\ \midrule -->
<!-- Threshold-based LP (TLP) & Probability of guilt given the evidence is above a certain threshold & fails & fails \\ -->
<!-- Dawid's likelihood strategy & No condition given, focus on $\frac{\pr{H\vert E}}{\pr{H\vert \n E}}$ & - If evidence is fairly reliable, the posterior of $A\et B$ will be greater than the prior. -->

<!-- - The posterior of $A\et B$ can still be lower than the posterior of any of $A$ and $B$. -->

<!-- - Joint likelihood, contrary do Dawid's claim, can also be lower than any of the individual likelihoods. & fails  \\ -->
<!-- Cheng's relative LP (RLP) -->
<!-- & Posterior of guilt higher than the posterior of any of the defending narrations & The solution assumes equal costs of errors and independence of $A$ and $B$ conditional on $E$. It also relies on there being multiple defending scenarios individualized in terms of  combinations of literals involving $A$ and $B$. & Assumes that the prior odds of guilt are 1, and that the statistics is not sensitive to guilt (which is dubious). If the latter fails, tells to convict as long as the prior of guilt $<0.991$. \\ -->
<!-- Kaplow's decision-theoretic LP (DTLP) & -->
<!-- The likelihood of the evidence is higher than the odds of innocence multiplied by the cost of error ratio & fails & convict if cost ratio $<110.1111$ -->
<!-- \end{tabular}  -->
<!--  \end{center}  -->


<!-- Thus, each account either simply fails to satisfy the desiderata, or succeeds on rather unrealistic assumptions.  Does this mean that a probabilistic approach to legal evidence evaluation should be abandoned? No. This only means that if we are to develop a general probabilistic model of legal decision standards, we have to do better. One promising direction is to go back to Cohen's pressure against \textbf{Requirement 1} and push against it. A brief paper suggesting this direction is [@DiBello2019plausibility], where the idea is that the probabilistic standard (be it a threshold or a comparative wrt. defending narrations) should be applied to the whole claim put forward by the plaintiff, and not to its elements. In such a context, DAC does not arise, but \textbf{equal treatment} is violated. Perhaps, there are independent reasons to abandon it, but the issue deserves further discussion. Another strategy might be to go in the direction of employing probabilistic methods to explicate the narration theory of legal decision standards [@urbaniak2018narration], but a discussion of how this approach relates to DAC and the gatecrasher paradox lies beyond the scope of this paper.  -->






# References








