---
title: "The difficulty with Conjunction"
author: "Marcello Di Bello and Rafal Urbaniak"
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        - Rafal_latex6.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: [../references/referencesMRbook.bib]
csl: [../references/apa-6th-edition.csl]
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\tableofcontents



# The problem



A theoretical difficulty that any theory of the standard of proof should 
address is the the conjunction paradox or difficulty about conjunction. First formulated by @Cohen1977The-probable-an, the difficulty about conjunction has enjoyed a great deal of scholarly attention every since [@Allen1986A-Reconceptuali; @Stein05; @allen2013; @haack2011legal; @schwartz2017ConjunctionProblemLogic; @AllenPardo2019relative]. 
This difficulty arises when an accusation of wrongdoing, in a civil or criminal proceeding, 
is broken down into its constituent elements. The basic problem is that the probability of a conjuction 
is often lower than the probability of the conjuncts. Thus, even if each conjunct meets the requisite 
probability threshold, the conjunction does not. This chapter examines the difficulty about conjuction 
and how legal probabilists can respond.

## The Conjunction Principle


Suppose that in order to prevail in a 
criminal trial, the prosecution should establish by the required standard, first, that the defendant caused harm to the victim (call it claim $A$), and second, that the defendant had premeditated the harmful act (call it claim $B$). @Cohen1977The-probable-an argues that common law systems subscribe to a conjunction principle, that is, if $A$ and $B$ are established according to the governing standard of proof, so is their conjunction (and vice versa).  If the conjunction principle holds, the following must be equivalent, where $S$ is a placeholder for the standard of proof:

\begin{center}
\begin{tabular}
{@{}ll@{}}
\toprule
\textbf{Separate} &   A is established according to S and B is established according to S\\   
\textbf{Overall}  &   The conjunction $A \et B$ is established according to S  \\ 
\bottomrule
\end{tabular}
\end{center}

\noindent
Let $S[X]$ mean that claim or hypothesis $X$ is established according 
to standard $S$.  Then, in other words, the conjunction principles requires that:
\[S[A \wedge B] \Leftrightarrow S[A] \wedge S[B].\]



The conjunction principle is 
consistent with---perhaps even required by---the case 
law. For example, the United States Supreme Court 
writes that in criminal cases 

\begin{quote}
the accused [is protected] against conviction except upon proof beyond a reasonable doubt of \textit{every fact} necessary to constitute the crime with which he is charged. In re Winship (1970), 397 U.S. 358, 364. 
\end{quote}

\noindent
A plausible way to interpret this quotation is to posit this 
identity: to establish someone's guilt beyond a 
reasonable doubt \textit{just is} to establish each 
element of the crime beyond a reasonable doubt. Thus, 

\begin{align*}\mathsf{BARD}[A_1 \wedge \cdots \wedge A_n] \Leftrightarrow \mathsf{BARD}[A_1] \wedge \cdots \wedge \mathsf{BARD}[A_n],
\end{align*}

\noindent
where the conjunction $A_1 \et \cdots \et A_n$ comprises all the material 
facts that, according to the applicable law, 
constitute the crime with with the accused is charged.

<!--

One could quibble with this intepretation. Perhpas, what the law requires is only the left-to-right direction---that $\text{if }BARD[A \wedge B] \text{ then } BARD[A] \wedge BARD[B]$---not the right-to-left direction---that $\text{if } BARD[A] \wedge BARD[B] \text{ then }BARD[A \wedge B]$. The left-to-right direction is, indeed, the least controversial.\todo{discussion in terms of equation sides is confusing, figure out terms that are more meaningful} If the conjuction is established to the required standard, so should its conjuncts.\todo{This just repeats the claim, I don't know what job it does.} It would be odd if this was not the case. The other direction is more controversial.\todo{Why?} But it is plausible that one would establish the conjunction by establishing each conjunct one by one. As we shall see later, both directions can be questioned. At least, the conjunction principle has some initial plausibility. \todo{Weakened the claim, check}

-->


The problem for the legal probabilist is that the conjunction principle conflicts 
with a threshold-based probabilistic interpretation 
of the standard of proof. For suppose the prosecution presents evidence that establishes claims $A$ and $B$, separately, to the required probability, say about 95\% each. Has the prosecution met the burden of proof? Each claim was established to the requisite probability threshold, and thus it was established to the requisite standard (assuming the threshold-based interpretation of the standard of proof). And if each claim was established to the requisite standard, then (i) guilt as a whole was established to the requisite standard (assuming the conjunction principle). But even though each claim was established to the requisite probability threshold, the probability of their conjunction---assuming the two claims are independent---is only $95\%\times95\%=90.25\%$, below the required 95\% threshold. So (ii) guilt as a whole was \textit{not} established to the requisite standard (assuming a threshold-based  probabilistic interpretation of the standard). 
Hence, we arrive at two contradictory conclusions: (i) that the prosecution met its burden of proof 
and (ii) that it did not meet its burden.

The difficulty about conjunction---the fact that a probabilistic interpretation of the standard of proof conflicts with the conjunction principle---does not subside when the number of constituent claims increases. If anything, the difficulty becomes more apparent. Say the prosecution has established three separate claims to 95\% probability. Their conjunction---again if the claims are independent---would be about 85\% probable, even further below the 95\% threshold.  Nor does the difficulty about conjunction subside if the claims are no longer regarded as independent.
The probabilty of the conjunction $A \et B$, without the assumption of independence, equals $\pr{A | B} \times \pr{B}$. 
But if claims $A$ and $B$, separately, have been established to 95\% probability, enough for 
each to meet the threshold, the probability of $A \et B$ could still be below 
the 95\% threshold unless $\pr{A | B}=100\%$. 
For example, that someone premediated a harmful act against another (claim $B$) makes it more likely that they did cause harm in the end (claim $A$). Since $\pr{A | B} > \pr{A}$, the two claims are not independent. 
Still, premeditation does not always lead to harm, so $\pr{A | B}$ should be below 100\%. Consequently, in this case, the probability of the conjunction $A \et B$ would be below the 95\% threhsold.\todo{False in whole generality, give a counterexample with more specific numbers. M: I changed things a bit. Maybe it's clear now. The counterexample is basically P(A)=P(B)=0.95, but P(AB)=Pr(A)*P(A|B) and since P(A|B) is below 1, then P(AB) is below 0.95.}



## Aggregating hypotheses and evidence


So far the discussion proceeded without mentioning explicitly 
the evidence proferred in support of the different claims that 
constitute the allegation of wrongdoing. 
This is, however, a simplification. Say evidence $a$ establishes claim $A$ and other evidence 
$b$ establishes a distinct claim $B$. The question now is whether 
the combination of $a$ and $b$ establishes the conjunction $A \et B$, and viceversa. 
<!--The answer to this question can well be affirmative---and without denying 
that the risks of error accumulate. 
No doubt the possible presence of a defeater $D_a$ would undermine 
the support of $a$ in favor of $A$, and another defeater $D_b$ would 
undermine the support of $b$ in favor of $B$. But, absent any defeater $D_a$ or $D_b$, the combination of
items of evidence $a$ and $b$ should jointly support (again, defeasibly or to whatever suitable standard is required) the combined hypothesis $A \wedge B$. \todo{Should we talk more about what we mean by defeaters, perhaps with examples of what the defeaters in this example might be?}
-->
In other words, the question is whether 
the following conjunction principle holds:
\[\text{S[$a, A$] and S[$b, B$] iff S[$a \wedge b, A\wedge B$]},\]

\noindent
where $\text{S}[e, H]$ means that evidence $e$ establishes hypothesis 
$H$ by standard $S$.  This conjunction principle 
differs from the earlier one since it mentions explicitly the 
evidence $a$ and $b$.

<!---
This new formulation of the conjunction principle is compatible 
with the fact that risks accumulate.\todo{I am not convinced; the discussion so far indicates that this is so if $S$ represents defeasible support; but I don't see how the claim follows for any sensible standard $S$ that one could formulate.} Hypothesis $A$ and $B$ could be attacked by a defeater $D_a$ or $D_b$. The conjunction $A \wedge B$ could be attacked by a larger set of defeaters, including $D_a$ or $D_b$. So $A \wedge B$ is more susceptible to attack than any individual claim $A$ or $B$. In this sense, risks do accumulate.\todo{Is there another sense in which they don't? How is this related to the probabilistic risk accumulation that the legal probabilists might care more about?} Still, even though \todo{Why do you say "even though"? I am not sure I see a contrast here...} $A \wedge B$ is more susceptible to attack, it requires a larger body of evidence $a \wedge b$ than each of the individual claim $A$ and $B$. So, the larger body of evidence in support of $A \wedge B$ balances off its greater susceptibility to error.
Defeasible logic provides a formal framework to 
make sense of the conjuction principle (REFERENCES?). 
But defeasible logic aside, the challenge for legal 
probabilists, at this point, is to spell out 
a probabilistic version of the conjunction principle 
for the aggregation of evidence and hypotheses. 
Can this challenge be met? \todo{It's at this point unclear how the probabilistic version of support is supposed to be related to the whole defeasibility buisness. In other words, if probabilistic support is what we're after, why would/should things that hold for defeasible support be our guide?}
-->
Understood in terms of posterior probability, 
the conjunction principle above fails 
in some cases. For suppose that $\pr{A | a}>t$ and $\pr{B | b}>t$, 
for a threshold $t$, or in other words, given the 
evidence $a$ and $b$, both $A$ and $B$ are sufficiently probable (for a fixed threshold).
It does not generally follow that $A \et B$ is sufficienlty probable given the combined evidence $a\et b$. 
By the probability calculus,
 \begin{align*}
\pr{A \wedge  B | a \wedge b}& =\pr{A |a \wedge b} \times \pr{B | a \wedge b \wedge A}\\
 & = \pr{A |a} \times \pr{B | b}
 \end{align*}

\noindent
The second equality holds assuming certain relationships of independence, 
specifically, the independence of $A$ from $b$ given $a$, 
and of $B$ from $a \wedge A$ given $b$. These relationships of independence do 
not always hold, but they do sometimes. For example, 
in an aggravated assault case, evidence $a$ could be a witness testimony that the defendant 
physically injured the victim (claim $A$), and $b$ evidence that the defendant 
knew that the victim was a firefighter (claim $B$), for example, another testimony that the 
defendant earlier called the firefighther for help.  Presumably, $\pr{A \vert a}=\pr{A \vert a \wedge b}$
because the fact that the defendant called a firefighter for help ($b$) does not make it more (or less) likely that 
he would physically injure him ($A$). Further, $\pr{B \vert b}=\pr{B \vert a \wedge b \wedge A}$ because the fact that
the defendant injured the victim ($A$) and there is a testimony to that effect ($a$) does not make it more (or less) 
likely that the victim was a firefighter ($B$). 
<!-- the hypotheses $A$ and $B$,  -->
<!-- and the independence of $A$ from $b$, and  -->
<!-- of $B$ from $a$. In other words, the two hypotheses  -->
<!-- are thought to be independent of one another,  -->
<!-- and their supporting evidence is thought  -->
<!-- to be independent of the other hypothesis.  -->
<!---These assumptions are codified in the Bayesian 
network in Figure \ref{network-conjunction}.---> 
Given these assumptions,  if---as is normally the case---neither $\pr{A \vert a}$ 
nor $\pr{B \vert b}$ equal 1, then
\[\pr{A \wedge B \vert a \wedge b}< \pr{A \vert a} \;\ \& \;\ \pr{A \wedge B \vert a \wedge b} < \pr{B \vert b}. \]

\noindent
This is another manifestation of the difficulty about conjuction.  If each piece of 
evidence $a$ and $b$ establishes claims $A$ and $B$ with 95\% probability, the combined 
evidence $a\et b$ need not establish the conjuction $A\et B$ with 95% probability. 
The conjunction principle fails here. 

Interestingly, even if the independence assumptions are dropped, the difficulty about conjunction still arises in a number of circumstances.  Suppose evidence $a\et b$ establishes claim $A$ and also claim $B$, separately, right above the probability threshold $t$. Since $\pr{A \wedge  B | a \wedge b} =\pr{A |a \wedge b} \times \pr{B | a \wedge b \wedge A}$, it follows that $\pr{A \wedge  B | a \wedge b}$ would be below $t$ so long as $\pr{B | a \wedge b \wedge A}$ is below 100\%, which would often be the case since (i) evidence is fallible and (ii) one hypothesis does not usually entail the other. So even though $A$ and $B$ are established to the required probability, the conjunction is not. 



## Probabilistic (in)dependencies

The conjunction paradox is a difficult problem, as the vast literature on the topic attests. 
Before we move on, it is important to become clear about the assumptions underlying the formulation 
of the paradox, in particular, the assumptions of probabilistic independence. We will then explore a number 
of probability-based proposals and distinguish the promising ones from those that ultimately fail. 

One assumption often made in the formulation 
of the paradox is that claims $A$ and $B$ are probabilistically independent. This is not always the case, but we have seen that the paradox does subside if the two claims are dependent of one another. So the assumption, in formulating the paradox, should be that either the two claims are fully probabilistically independent or positively probabilistically dependent. We exclude cases in which the claim are negatively probabilistically dependent. For it would be odd if the prosecution or the plaintiff would try to combine two claims where one makes the other less probable. 

The other assumption---which is not always stated upfront in the presentation of the paradox---is that the supporting items of evidence are also independent. They are not, however, unconditionally probabilistically independent. They are independent conditionally on the claim they support. This notion of conditional independence captures formally the thought that two or more items of evidence constitute \textit{independent lines of evidence} (SEE DISCUSSION IN EARLIER CHAPTERS). 

Bayesian network are useful for representing graphically these relationships of independence. Consider the networks in Figure \ref{network-conjunction}. The structure of the networks is rather natural because each piece of evidence bears on its hypothesis and is probabilistically independent conditional on one of the hypotheses. One might wonder, however, why the arrows go from $A$ and $B$ into the node representing the conjunction $A\wedge B$. This setting can capture the meaning of the conjunction. The constraint that, for the conjunction to be true, both $A$ and $B$ have to be true, can be defined using conditional probability tables. The two networks only differ in one detail, whether or not an arrow exist between claims $A$ and $B$. If there is no arrow, $A$ and $B$ are probabilistically independent since $A\wedge B$ is a collider node (SEE DISCUSSION IN EARLIER CHAPTERS). 
To eliminate the independence of the two claims, while holding everything else fixed, it is enough to draw an arrow between $A$ and $B$. See the network in Figure \ref{network-conjunction} (bottom). Arrows cab be drawn between the evidence nodes themselves, but this modification would undermine them being independent lines of evidence. 



\begin{center}
\begin{figure}[h!]
\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,ellipse,text width=1cm,align=center}
]
\node[mynode] (h1) {$A$};
\node[mynode,below right =of h1] (h3) {$A \wedge B$};
\node[mynode,above right =of h3] (h2) {$B$};
\node[mynode,below  =of h1] (e1) {$a$};
\node[mynode,below  =of h2] (e2) {$b$};
\path 
(h1) edge[-latex] (h3)
(h2) edge[-latex] (h3)
(h1) edge[-latex] (e1)
(h2) edge[-latex] (e2); 
\end{tikzpicture}

\begin{tikzpicture}[
  node distance=1cm and 1cm,
  mynode/.style={draw,ellipse,text width=1cm,align=center}
]
\node[mynode] (h1) {$A$};
\node[mynode,below right =of h1] (h3) {$A \wedge B$};
\node[mynode,above right =of h3] (h2) {$B$};
\node[mynode,below  =of h1] (e1) {$a$};
\node[mynode,below  =of h2] (e2) {$b$};
\path 
(h1) edge[-latex] (h2)
(h1) edge[-latex] (h3)
(h2) edge[-latex] (h3)
(h1) edge[-latex] (e1)
(h2) edge[-latex] (e2); 
\end{tikzpicture}
\caption{Two Bayesian networks for two pieces of evidence and a composite hypothesis.}
\label{network-conjunction}
\end{figure}
\end{center}

The two Bayesian networks above provide a compact representation of 
the class of models 
we will be concerned with.  We will assume throughout that the 
items of evidence are independent lines of evidence. 
Often, we will also assume that the 
claims themselves are independent, and occasionally 
consider the situation in which they are positively 
probabilistically dependent. 



# Rejecting the Conjunction Principle?

Let's now turn to possible strategies that legal probabilists 
can pursue to address the conjunction paradox. 
The first thing to note is that the 
paradox would not arise without 
the conjunction principle. So could legal 
probabilists reject this principle and let 
the paradox disappear? 
<!-- On its face, the conjunction principle is no different from closure principles 
used in epistemic logic. Let $\Box$ the modal operator for belief or knowledge.
The conjunction principle is a special version 
of the closure principle $\Box A \wedge \Box B \leftrightarrow \Box (A \et B)$. (CITE MODAL LOGICS THAT ADOPT THIS PRINCIPLE.) \todo{Ok, I will add a footnote about this.}
--> 

In current discussions in epistemology, an analogous principle about knowledge or justification has been contested because 
it appears to deny the fact that risks of error accumulate (CITE). \todo{Hey, we can quote a paper that's out by Alicja!:) Also, I guess you want me to find the right refs? M: Yes, if you can} If one is justifiably sure about the truth of 
each claim considered seperataly, one should not be equally sure of their conjunction. You have checked 
each page of a book and found no error. So, for each page, you are nearly sure there is no error. Having checked each page and found no error, can you be sure that the book as a whole contains no error? Not really. As the number of pages grow, it becomes virtually certain that there is at least one error in the book you have overlooked, although for each page you are nearly sure there is no error. (ADD CITATION ABOUT PREFACE PARADOX) The same applies to other contexts, say product quality control. You may be sure, for each product you checked, that it is free from defects. But you cannot, on this basis alone, be sure that all products you checked are free from defects. Since the risks of error accumulate, you must have missed at least one defective product. 

Risk accumulation challenges one direction of the conjunction principle, what we will call 
\textit{aggregation}. Even if the probability of several claims, considered individually, is above a threhsold $t$,
their conjuction need not be above $t$. However, the other direction of the conjunction, what we will call \textit{distribution}, still holds. Probability theory ensures that, if the probabiliity of the conjunction of several claims is above $t$, so is the probability of each individual claim. 


## Atomistic and Holistic Approches

Suppose the legal probabilist does away with the conjunction principle. Now what?
How should they define standards of proof? Two immediate options come to mind, but neither is without problems.  

One option stipulates that, in order to establish the defendant's guilt beyond a reasonable doubt (or civil liability by preponderance of the evidence), the party making the accusation should establish each claim, separately, to the requisite probability, say at least 95\%, without needing to establish the conjunction to the requisite probability. Call this the \textit{atomistic account}.  On this view, the prosecution could be in a position to establish guilt beyond a reasonable doubt without estalishing the conjunction of different claims with a sufficiently high probability. This account would allow convictions in cases in which the probability of the defendant's guilt, call it $G$, is low, just because $G$ is a conjunction of several independent claims that separately satisfy the standard of proof. For example, if each constituent claim is established with 95\% probability, the composite claim---assuming, as usual, probabilistic independence between individual claims---would only be established with 59% probability, a far cry from proof beyond a reasonable doubt. This is counterintuitve as it would allow convictions when the defendant is most likely innocent. Under the atomistic account, the composite claim representing the case as a whole would often be established with a probability below the required threshold.

The other option is to require that the prosecution in a criminal case (or the plantiff in a civil case) establish the accusation as a whole---say the cojunction of $A$ and $B$---to the requisite probability. Call this the \textit{holistic account}. This account is not without problems either. The proof of $A\et B$ would impose a higher requirement on the separate probabilities of the conjuncts. If the conjunction $A\et B$ is to be proven with at least 95\% probability, the individual conjuncts should be established with probability higher than the 95\% threshold. So the more conjuncts, the higher their required probability.  The more constituent claims, the higher the posterior probability for each claim needed 
to meet the requisite probability threhsold. 

Assume, for the sake of illustration, the independence and equiprobability of the constituent claims. If a composite claim consists of $k$ individual claims, these individual claims will have to be established with proabilility of at least
$t^{1/k}$, where $t$ is the threhsold applicable to the composite claim.\footnote{Let $p$ the probability of each consitutuent claim. To meet threshold $t$, the probability of the composite claim, $p^k$, should satisfy the constraint $p^k>t$, or in other words, $p>t^{1/k}$.} For example, if there are ten constituent claims, they will have to be proven with $0.5^{1/10}=0.93$ even if the standard of proof is only $>0.5$. If the standard is more stringent, as is appropriate in criminal cases, say $>0.95$, each individual claim will have to be proven with near certainity, which would make the task extremely demanding on the prosecution. For example, if there are ten constituent claims, they will have to be proven with $0.95^{1/10}=0.995$. So the plaintiff or the prosecution would face the demanding task of establishing each element of the accusation beyond what the standard of proof would seem to require.

Another problem with the holistic account is that the standard that applies to one of the conjuncts would depend on what has been achieved for the other conjuncts. For instance, assuming independence, if  $\pr{A}$ is $96\%$, then $\pr{B}$ must be at least $99\%$ so that $\pr{A\et B}$ is above a $95\%$ threshold. But if $\pr{A}$ is $99.99\%$, then $\pr{B}$ must only be greater than $95\%$ to reach the same threshold. Thus, the holistic account would require that the elements of an accusation be proven to different probabilities---and thus different standards---depeding on how well other claims have been established.
This result runs counter to the tacit assumption that each element should be established to the same standard of proof. 



```{r, echo=FALSE}
0.5^(1/10)
```

```{r, echo=FALSE}
0.95^(1/10)
```


```{r, echo=FALSE}
0.95^(10)
```



We have a dilemma here: either (under the holistic approach) the standard is too demanding on the prosecution (or the plantiff) because it would require the individual claims to be established to extremely high probabilities, or (under the atomistic approach) the standard is too lax because it would allow findings of liability when the defendant most likely committed no wrong. Denying the conjunction principle, then, is not without difficulties of its own. Absent the conjuction principle, legal probabilists should still explain how individual claims relate to larger claims in the process of legal proof. 


## Prior Probabilities

It is worth examining the holistic account more closely, 
focusing in particular on the role of prior 
probabilities, an aspect that has gone unnoticed so far. 
The main problem with the holistic approach is that it would require, especially in 
criminal cases, individual claims to 
be established with a very high probability, often making the task unsurmountable 
for the prosecution. Or so it would seem. But a composite claim such as $A\wedge B$ will have, 
other things being equal, a lower prior probability than any individual claim $A$ or $B$.
 In general,  a composite claim consists of $k$ individual claims. If the composite claim has a prior probability of $\pi$, each constituent claim, assuming they are independent and equiprobable, will have a prior probability of $\pi^{1/n}$. The prior probability of the individual claims will approach one as the number of constituent claims increases.  
 
<!--

Compare the impact of one item of evidence on the probability of an individual claim, for different level of sensitivity and specificity of the evidence, with the impact of $k$ items of evidence on the probability of a composite claim that consists of $k$ individual claims, again for different levels of specificity and sensitivity of the evidence. More specifically, let $c_i$ be a constituent claim and $e_i$ its supporting evidence. The comparison is between prior probabilities $\pr{C_i}$ and
$\pr{C_1 \wedge C_2 \wedge \dots \wedge C_k}$, contrasted with the posterior probabilities $\pr{C_i \vert E_i}$ and
$\pr{C_1 \wedge C_2 \wedge \dots \wedge C_k \vert E_1 \wedge E_2 \wedge \dots \wedge E_k}$. Figure \ref{fig:post-indiv-joint-first} (top) compares one item of evidence supporting an individual claim and five items of evidence supporting a composite claim consisting of five claims. As is customary, the items of evidence and constituent claims are independent. In addition, for the sake of simplicity, the prior probabilities of the constituent claims are assumed to be the same. There is a significant difference in posterior probabilities, as expected, but there is a also a significant difference in prior probabilities. Since the composite claim starts out less likely than any individual claim, it is natural---other things being equal---that its posterior probability would be correspondingly lower. 
 
 \begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


postn <- function (x, s1, s2, n){
  ((x)^n)*
    ((
      s1/
        (s1*x+(1-s2)*(1-x))
     )^n)
    
}

x <-seq(0,1,by=0.001)
y0908 <- post(x, 0.9, 0.8)
y0909 <- post(x, 0.99, 0.99)
yy0908 <- postn(x, 0.9, 0.8, 2)
yy0909 <- postn(x, 0.99, 0.99, 2)
yynull <- postn(x, 0.5, 0.5, 2)
y50908 <- postn(x, 0.9, 0.8, 5)
y50909 <- postn(x, 0.99, 0.99, 5)
y5null <- postn(x, 0.5, 0.5, 5)


ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, sens=0.5, spec=0.5"))+
  #geom_line(aes(x = x, y = yynull,color = "2, sens=0.5, spec=0.5")) +
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  #geom_line(aes(x = x, y = yy0908,color = "2, sens=0.9, spec=0.8")) +
  # geom_line(aes(x = x, y = yy0909,color = "2, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y5null, color = "5, sens=0.5, spec=0.5")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.9, 0.2), legend.title = element_blank())
```


\caption{The comparison here is betwen individual support and joint support. 
The null line for joint support ($y=x*x$) is 
much below the null line for individual support ($y=x$).}
\label{fig:post-indiv-joint-first}
\end{figure}
 
 -->
 
Cohen worried that, as the number of constituent claims increases, the prosection or the plaintiff
would see their case against the defendant become progressively weaker and it would 
become impossisble for them to establish liability. But this worry is an 
exaggeration. The paradox, as is commonly formulated, starts by assuming that the constituent claims are
established by the required probability threshold and then shows that the probability of the conjuction may fall below the threshold. However, following the holistic approach, the order of presentation can be reversed. Start by assuming that the composite claim is established by the required probability threshold. No doubt the individual claims will have to be established with a higher probability, a violation of the conjuction principle. Yet, this violation is not as counterintuitive as it might first appear for two reasons. First, since risks aggregate, it is natural that the probability of a conjunction would be lower than the probability of the conjunct. Second, the prior probabilities of the conjuncts will be higher than the prior probability of the conjunction. Thus, establishing the conjuncts with a higher probability will not be exceedingly demanding.
 
Along this lines, @dawid1987difficulty, in one of the earliest attempts to solve the conjunction 
 paradox from a probabilistic perspective, wrote:

\begin{quote}
\dots it is not asking too much of the plaintiff to establish the case as a whole with a posterior probability exceeding one half, even though this means  that the several component issues must be established with much larger posterior probabilities; for the \textit{prior}  probabilities of the components will also be correspondingly larger, compared with that of their conjunction. The overall effect of subdiving a case into more and more component issues ... [gives] an advantage to the plaintiff, even though he has to establish each with a high probability. (p. 97)
 \end{quote}
 
\noindent 
The price of this strategy, however, is the denial of the conjunction principle, the very motivation 
behind the conjunction paradox. Cohen could insist that this solution 
amounts to denying the paradox itself. To satisfy Cohen, legal probabilists 
should offer a justification of the conjunction principle in probabilistic terms, 
something that Cohen maintains cannot be done. Or can it be done? 


## Evidential Strength 

We have seen that the aggregation direction 
of the conjunction principle fails if the standard of proof is understood as a posterior probability threhsold.
Naturally,  legal probabilists cannot justify  aggregation if they equate the 
standard of proof to a posterior probability threshold. Instead , they can think of the standard of proof as a sufficiency criterion for how strong the evidence should be in order to justify a finding of criminal or civil liability against the defendant. Crucially, from a probabilistic perspective, the notion of the strength of the evidence in favor of a hypothesis---say the defendant is guilty of murder---need not be understood as the posterior probability of the hypothesis given the evidence. 

So suppose the standard of proof is no longer understood as a threshold on 
the posterior probability given the evidence, but rather, as threshold on evidential strength. 
Two common probabilistic measures of evidential strength are the Bayes factor or the likelihood ratio.
We discussed this topic in earlier chapters (REFER TO EARLIER CHAPTERS).  As we will show in detail later, under plausible assumptions, these measures of evidential strength validate one direction of the conjunction principle, namely aggregation. If $a$ is sufficiently strong evidence in favor of $A$ and $b$ is sufficiently strong evidence in favor of $B$, then $a\wedge b$ is sufficiently strong evidence in favor of the conjunction $A \wedge B$. In fact, the evidential support for the conjunction will often exceed that for the individual claims, a point already made by @dawid1987difficulty:

 \begin{quote} suitably measured, the support  supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents.
 \end{quote}

\noindent
Dawid thought this fact was enough for the conjuction paradox to 'evaporate'. To some extent, this is true since we are vindicating one direction of the conjunction principle, aggregation. That is, if distinct items of evidence $a$ and $b$ constitute sufficiently strong evidence for claim $A$ and $B$, so does their conjunction $a\wedge b$ for the composite claim $A\wedge B$. 

However, we will show that the other direction of the cojunction principle does not hold, what we call distribution.
If $a \wedge b$ is sufficiently strong evidence in favor of $A \wedge B$, it does not follow that $a$ is sufficiently strong evidence in favor of $A$ or $b$ sufficiently strong evidence in favor of $B$. It is not even true that, if $a \wedge b$ is sufficiently strong evidence in favor of $A \wedge B$, then $a\wedge b$ is sufficiently strong evidence in favor of $A$ or $B$. This is  odd.  It would mean that, given a body of evidence, one can establish beyond a reasonable doubt that $A \wedge B$ (say the defendant killed the victim \textit{and} did so intentionally) while failing to establish by the same standard one of the conjuncts. 

Interestingly, if instead of probabilistic measures of evidential strength such as the Bayes factor or the likelihood ratio, we use posterior probabilities, the other direction of conjuction principle---aggregation, not distribition---fails. So we are in a dilemma. If the standard of proof is understood as a threshold relative to the posterior probability, the conjunction principle fails because aggregation fails while distribution succeeds. If, on the other hand, the standard of proof is understood as a threshold relative to measures of evidential strength, the conjuction principle fails because distribution fails while aggregation succeeds. From a probabilistic perspective, it seems impossible to capture both directions of the conjunction principle into one unified account. So, ultimately, Cohens was right that legal probabilists cannot justify the conjuction principle. But, as we shall explain, legal probabilists have very good reasons to reject one or the other direction of the conjunction principle. So Cohen was wrong in holding on to the conjunction principle.  

All in all, the legal probabilists has two routes to address the conjunction paradox: either to deny aggregation (and understanding the standard of proof in terms of posterior probabilities) or to deny distribution (and understanding the standard of proof in terms of evidential strenght). For reasons we shall explain, we think the former option is the best course of action.


# Aggregation succeeds, distribution fails

This section will establish the two claims that were announced without proof 
at the end of last section: first, that aggregation can be vindicated probabilistically if the standard of proof is understood in terms of evidential stregth instead of posterior probabilities; second, that under the same approach distribution ultimately fails. By using the theory of Bayesian networks, we will first verify under what conditions Dawid's claim that `suitably measured, the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents' holds. Next, we will show---perhpas surprisingly---that after switching from posterior probabilities to measures of evidential support, another paradox, what we call the distrubition paradox, arises. The argument for these two claims is somewhat tedious.  The reader should arm themselves with patience, or alternatively take out word for it and jump ahead to the next section. 

## Bayes factor threshold

A common probabilistic measure of the support of $E$ in 
favor of $H$ is the Bayes factor $\pr{E \vert H}/\pr{E}$. Since by Bayes' theorem

\[\pr{H \vert E} = \frac{\pr{E \vert H}}{\pr{E}}\times \pr{H},\]

\noindent
the Bayes factor measures the extent to which 
a piece of evidence increases the probability 
of a hypothesis. The greater the Bayes factor (for values above one), the stronger 
the support of $E$ in favor of $H$. Putting aside reservations about this measure 
of evidential support (discussed earlier in Chapter CROSSREF), 
the Bayes factor $\pr{E | H}/\pr{E}$, unlike the conditional probability 
$\pr{H | E}$, offers a potential way to overcome 
the difficulty about conjunction.


### Aggregating evidence 


Say  $a$ and $b$, separately, support $A$ and $B$
to degree $s_A$ and $s_B$ respectively, that is, $\pr{a | A}/\pr{a}=s_A$ and 
$\pr{b | B}/\pr{b}=s_B$, where both $s_A$ and $s_B$ are greater than one. 
Does the combined evidence $a \wedge b$ provide at least as much support 
in favor of the combined claim $A \wedge B$ as the individual support by $a$ and $b$ 
in favor of $A$ and $B$ considered separately? The combined support should be 
measured by the combined Bayes factor $\pr{a \wedge b| A\wedge B}/\pr{a \wedge b}$.
The latter, under suitable independence assumptions, equals the product of the individual 
supports $s_{A}$ and $s_{B}$.\footnote{By the probability calculus,
 \begin{align*}
\frac{\pr{a \wedge b| A\wedge B}}{\pr{a \wedge b}} & =  \frac{\pr{A \et B| a\wedge b}}{\pr{A \et B}}\\
& =  \frac{\frac{ \pr{A \et B \et a \wedge b }}{\pr{a \et b}}}{\pr{A \et B}} \\ 
& =  \frac{\frac{ \pr{A} \times \pr{B|A} \times \pr{a | A \wedge B} \times \pr{b | A \wedge B \wedge a}}{\pr{a} \times \pr{b | a}}}{\pr{A \et B}} \\ 
& =^*  \frac{\frac{\pr{A} \times \pr{B} \times \pr{a | A} \times \pr{b | B}}{\pr{a} \times \pr{b}}}{\pr{A} \times \pr{B}} \\ 
& =  \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b}} \\
s_{AB}& =  s_{A}\times s_{B} 
 \end{align*}

\noindent
The step marked by the asterisk rests on the 
independence assumptions codified in the Bayesian network 
in Figure \ref{network-conjunction} (top).} That is, 
 \begin{align*}
\frac{\pr{a \wedge b| A\wedge B}}{\pr{a \wedge b}} & =  \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b}} \\
s_{AB}& =  s_{A}\times s_{B}.
 \end{align*}

 \noindent 
 Thus, the combined support $s_{AB}$ will always be higher 
than the individual support so long as $s_{A}$ and 
$s_{B}$ are greater than one. This result can be generalized 
beyond two pieces of evidence. Figure REFERENCE TO FIGURE BELOW compares the Bayes 
factor of one item of evidence, say $\frac{\pr{a \vert A}}{\pr{a}}$ with the 
combined Bayes factor for five items of evidence, say $\frac{\pr{a_1 \wedge \dots a_5 \vert A_1 \wedge \dots A_5}}{\pr{a_1\wedge \dots a_5}}$, 
for dfferent values of sensitivity and specificity of the evidence. The latter always exceeds the fomer. 


\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


bf <- function (x, s1, s2){
  
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


bfn <- function (x, s1, s2, n){
  
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )^n
    
}

x <-seq(0,1,by=0.001)
y0908 <- bf(x, 0.9, 0.8)
y0909 <- bf(x, 0.99, 0.99)
yy0908 <- bfn(x, 0.9, 0.8, 2)
yy0909 <- bfn(x, 0.99, 0.99, 2)
yynull <- bfn(x, 0.5, 0.5, 2)
y50908 <- bfn(x, 0.9, 0.8, 5)
y50909 <- bfn(x, 0.99, 0.99, 5)
y5null <- bfn(x, 0.5, 0.5, 5)


ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, sens=0.5, spec=0.5"))+
  #geom_line(aes(x = x, y = yynull,color = "2, sens=0.5, spec=0.5")) +
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  #geom_line(aes(x = x, y = yy0908,color = "2, sens=0.9, spec=0.8")) +
  # geom_line(aes(x = x, y = yy0909,color = "2, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y5null, color = "5, sens=0.5, spec=0.5")) +
        ylim(c(0,10))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.9, 0.8), legend.title = element_blank())
```

\end{figure}

Dawid's claim that `the support supplied 
by the conjunction of several independent testimonies exceeds 
that supplied by any of its constituents' is therefore vindicated. 
The claim holds in the class of cases characterized by the relationships of probabilistic 
 independence encoded in the  Bayesian network in Figure \ref{network-conjunction} (top). 
 As noted before, this network ensures that claim $A$ and $B$ are probabilistically independent, 
 as well as that items of evidence $a$ and $b$ are conditionally independent (which captures formally 
 the fact that they are independent lines of evidence).
 
What happens if $A$ and $B$ are not necessarily 
probabilistically independent as in the Bayesian network 
in Figure \ref{network-conjunction} (bottom)? 
Given this network, the following holds:

 \begin{align*}
\frac{\pr{a \wedge b| A\wedge B}}{\pr{a \wedge b}} & =  \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b|a}} \\
s^{'}_{AB}& =  s_{A}\times s^{'}_{B} 
 \end{align*}

\noindent 
Note that factor $s_{B}= \frac{\pr{b |B}}{\pr{b}}$ was replaced by
$s^{'}_{B}=\frac{\pr{b |B}}{\pr{b|a}}$.\footnote{Given the second Bayesian network, 
$b$ need not be probabilistically independent of $a$, and thus there is no guarantee that 
$\pr{b \vert a}=\pr{a}$.} Now, $s^{'}_{B}$ is usually lower than $s_{B}$ 
because $\pr{b | a} > \pr{b}$ (assuming, at least, $a$ and $b$ are convergent pieces 
of evidence; SEE DISCUSSION IN EARLIER CHAPTERS). At the same time, $s^{'}_{B}$ should 
still be greater than one since $b$, by assumption, positively 
supports $B$ even in combination with $a$.\footnote{Note that 
$\frac{\pr{b |B}}{\pr{b|a}}=\frac{\pr{b |B \wedge a}}{\pr{b|a}}$ 
(by the probabilistic independence of $a$ and $b$ given $B$). 
So the claim that $\frac{\pr{b |B}}{\pr{b|a}}>1$ is 
equivalent $\pr{B | b \wedge a}> \pr{B|a}$ since by Bayes' theorem 
$\pr{B | b \wedge a} = \frac{\pr{b |B \wedge a}}{\pr{b|a}} \times \pr{B|a}$.
Presumably, evidence $b$ should still raise the probability of $B$ 
even in cojunction with $a$, or else $b$ would be useless evidence. \textbf{M: THIS CLAIM NEEDS 
TO BE PROVEN MORE RIGOROUSLY BUT I THINK IT'S CORRECT. BASIC INTUITION IS THAT EVIDENCE a 
RAISES THE PROBABILITY OF CLAIM A (OR B) AND THEN EVIDENCE 
b FURTHER RAISES THE PROBABILITY OF CLAIM A (OR B). THIS HAPPENS WHEN WE HAVE CONVERGENT 
EVIDENCE.}} Hence, $s^{'}_{AB}$ should still be greater than one provided $s_A$ and $S_B$ are both 
greater than one, but it might not always exceed the support supplied by $s_A$ and $s_B$ individually. For suppose $s_A=2$ 
and $s_B=3$, but $s^{'}_B=1.2$. Then, $s^{'}_{AB}=2\times 1.2=2.4$, which is below 
the individual support $s_B$, but still above $s_A$.  For the case in which 
$A$ and $B$ are probabilistically dependent, 
Dawid's claim should be amended as follows. Even though the support supplied by the conjunction of several independent testimonies need not always exceed that supplied by any of its constituents, it is always at least as great as the smallest support supplied by its constituents.\footnote{DO WE NEED A PROOF OF THIS? THIS SHOULD BE CLEAR BY PLOTTING. IN PLITTING THIS, WE SHOULD ENSURE THAT BOTH BF ARE GREATER THAN ONE, THEN THE COMBINED ONE WILL BE GREATER THAN THE SMALLEST EVEN WHEN A AND B ARE DEPENDENT.}


<!-- 
Full derivation:
 \begin{align*}
\frac{\pr{a \wedge b| A\wedge B}}{\pr{a \wedge b}} & =  \frac{\pr{A \et B| a\wedge b}}{\pr{A \et B}}\\
& =  \frac{\frac{ \pr{A \et B \et a \wedge b }}{\pr{a \et b}}}{\pr{A \et B}} \\ 
& =  \frac{\frac{ \pr{A} \times \pr{B|A} \times \pr{a | A \wedge B} \times \pr{b | A \wedge B \wedge a} }{\pr{a} \times \pr{b | a}}}{\pr{A \et B}} \\ 
& =^*  \frac{\frac{\pr{A} \times \pr{B|A} \times \pr{a | A} \times \pr{b | B}}{\pr{a} \times \pr{b |a}}}{\pr{A} \times \pr{B | A}} \\ 
& =  \frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b|a}} \\
s^{'}_{AB}& =  s_{A}\times s^{'}_{B} 
 \end{align*}
 -->


### Variable Threshold

If the combined support equals $s_{A}\times s_{B}$ or
$s_{A}\times s^{'}_{B}$, does 
the difficulty about conjunction evaporate, as Dawid thought? 
<!---To some extent 
it does. The crux of the matter here is whether the conjunction of two or more claims 
 satisfies a given standard of proof just in case the individual conjuncts do. 
 --->
 One hurdle here is that
 the standard of proof would no longer be formalized as a posterior probability threshold, but instead as a threshold about the Bayes factor. The threshold would no longer be a probability between 0% and 100%, but rather a number somewhere above 1. The greater this number, the more stringent the standard of proof, for any value above one. In criminal trials, for example, the rule of decision would be: guilt is proven beyond a reasonable doubt if and only if the evidential support in favor of $G$---as measured by the Bayes factor $\frac{\pr{E | G}}{\pr{E}}$---meets a suitably high threshold $t_{BF}$. The 
obvious question at this point is, how do we identify the appropriate threshold?\todo{good question, will think about it, will need to take a look at "Bayesian Choice", also need to think about a counterexample}

One strategy is to derive the Bayes factor threshold, call it $t_{BF}$, from the posterior threshold $t$. Since $\textit{posterior }=\textit{ Bayes factor }\times \textit{ prior}$, the Bayes factor threshold can be determined as follows:

\[\frac{t}{\textit{prior}}=t_{BF}\]

\noindent
The higher the prior probability, the lower $t_{BF}$. Whether this is a desirable property for a decision threshold can be questioned, but the same can be said about the posterior threshold $t$. The higher the prior probability, the easier to meet the posterior threshold.  

Presumably, the threshold $t_{BF}$ should be applied to individual as well as composite claims. Since the threshold varies 
depending on the priors, the thresholds for the individual claims $A$ and $B$, denoted by
$t_{BF}^A$ and $t_{BF}^B$, will differ from the threshold for the composite claim $A \wedge B$, denoted by $t_{BF}^{A\wedge B}$. 

At issue here is whether the conjunction principle can be formalized in a plausible manner with the Bayes factor. Unfortuantely, the answer is negative. To see why, first recall the conjunction principle:
 
 \[\text{S[$a, A$] and S[$b, B$] iff S[$a \wedge b, A\wedge B$]},\]

\noindent
where $\text{S}[E, H]$ means that evidence $E$ establishes hypothesis $H$ by standard $S$.  If the standard of proof is formalized using the Bayes factor, the conjunction principle would boil down to:

\[  \text{ $\frac{\pr{a | A }}{\pr{a}}>t^A_{BF}$ and $\frac{\pr{ b | B}}{\pr{b}}>t^B_{BF}$ iff $\frac{\pr{a \et b | A \et B}}{\pr{a \et b}}>t^{A\wedge B}_{BF}$ } \]

\noindent
Consider a posterior threshold $t=0.95$, as might be appropriate in a criminal case. If $A$ and $B$ both have a prior probability of $10\%$, the threshold $t^A_{BF}=t_{BF}^B=0.95/0.1=9.5$ for $A$ or $B$ individually. Assuming independence of $A$ and $B$, the composite claim $A \wedge B$ will be associated with the threshold $t^{A\wedge B}_{BF}=0.95/(0.1*0.1)=95$, a much higher value. But if each individual claim meets its Bayes factor threshold of 9.5 and the two claims are independent, the joint Bayes factor would equal the multiplication of the individual Bayer factors, that is, $9.5*9.5=90.25$. This is not quite enough to meet $t^{A\wedge B}_{BF}=95$, but is fairly close. The difference in absolute  terms grows as the prior probability of the individual claims becomes lower, but the combined Bayes factor remains only $5\%$ below the value needed to meet $t_{BF}^{A\wedge B}$.\footnote{The difference at here is between $t_{BF}^{A\wedge B}=0.95/p^2$ and $t_{BF}^{A}*t_{BF}^{B}=(0.5/p)^{2}$. Note that $\frac{0.95/p^2 - (0.5/p)^{2}}{0.95/p^2}=5\%$, for any value of the prior $p$.} Perhaps this is a good enough approximation. However, as the number of constituent claims grows, the difference becomes larger.\footnote{Given five constituent claims, $\frac{0.95/p^5 - (0.95/p)^{5}}{0.95/p^5}=18\%$.} In addition, the difference becomes larger with a lower posterior probability threshold, say $0.5$. Even with just two claims, $t^{A\wedge B}_{BF}=0.5/(0.1*0.1)=50$, but $t^A_{BF}*t_{BF}^B=(0.5/0.1)*(0.5/0.1)=25$, only half the required value. The conjunction principle therefore fails in a large number of cases even using the Bayes factor threshold. 


```{r, echo=FALSE}
0.95/0.1
0.95/(0.1)^2
(0.95/0.1)^2
0.95/0.01
0.95/(0.01)^2
(0.95/0.01)^2
0.95/(0.001)^2
(0.95/0.001)^2
0.5/0.1
0.5/(0.1)^2
(0.5/0.1)^2
```


<!--
\noindent
If the conjunction principle holds,\todo{In which direction? Note that at least in one direction the principle still fails. For instance, if your threshold is 15, $14*25$ is 350 but still one of the elements fails to be sufficiently supported.}  the statement above would be equivalent \todo{not sure about the equivalence; at best, this would be a sufficient condition, no? you do talk about this later so putting it this way is a bit misleading} to:
  \begin{quote}
Guilt is proven beyond a reasonable doubt if and only if the evidential support in favor of each constituent claim 
 $C_i$ suported by evidence $E_i$---as measured by the Bayes factor $\frac{\pr{E_i | C_i}}{\pr{E_i}}$---meets a suitably high threshold $t$.
 \end{quote}
 \noindent
-->

### Fixed Threshold

The alternative here is to fix the Bays factor 
threshold regardless of the prior probability of the 
claim of interest.  This raises the difficult 
question of how to fix the Bayesian factor threshold irrespective 
of the priors. Standard decision theory can no longer be used. 
As it turns out, even if the question can be satisfactorily answered, 
the fixed threshold approach gives rise to a complication that proves fatal.
If the standard of proof is formalized using a fixed Bayes 
factor threshold $t_{BF}$, the conjunction principle 
would boil down to:

\[  \text{ $\frac{\pr{a | A }}{\pr{a}}>t_{BF}$ and $\frac{\pr{ b | B}}{\pr{b}}>t_{BF}$ iff $\frac{\pr{a \et b | A \et B}}{\pr{a \et b}}>t_{BF}$ } \]

\noindent
The left-to-right direction---aggregation---is likely to hold for 
any threshold $t_{BF}$ greater than one. As shown earlier, the combined evidential 
support is greater than the individual evidential support 
<!---
because $\frac{\pr{a \et b | A \et B}}{\pr{a \et b}}$ 
equals $\frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b}}$
-->
if A and B are independent, or greater than the smallest individual support 
<!---or $\frac{\pr{a |A}}{\pr{a}} \times \frac{\pr{b |B}}{\pr{b \vert a}}$ -->
if A and B are dependent. Aggregation could not be justified using posterior probabilities 
$\pr{A | a}$ and $\pr{B | b}$ nor could it be justified generally 
using a variable Bayes factor threshold. 
So it is an advantage of the fixed Bayes factor threshold 
that it can justify this direction of the conjunction principle. 

However, the right-to-left direction---distribution---has now become problematic. 
For suppose the combined evidential support, $\frac{\pr{a \et b | A \et B}}{\pr{a \et b}}$, barely meets 
the threshold. This implies that the individual support, say $\frac{\pr{a |A}}{\pr{a}}$, 
could be be below the threshold unless $\frac{\pr{b |B}}{\pr{b}}=1$
\todo{Not always true; just give a specific numerical counterexample. M: I added 'often'. Is this enough?} (which should not happen if $b$ positively supports $B$). So, curiously, there would be cases in which, even though the conjunction $A\et B$ is established to the desired standard of proof, one of the individual claims  fails to meet the standard. This is odd. More specifically, the following distribution 
principle fails in some cases:

\[\text{If S[$a \wedge b, A\wedge B$], then S[$a, A$] and S[$b, B$].} \tag{DIS1}\]

\noindent
Could this principle be rejected? Perhaps, it is not as essential as we thought at first. Since 
the evidence is not held constant, the support supplied by $a\wedge b$ could be stronger than 
that supplied by $a$ and $b$ individually. So even when the conjunction 
$A \wedge B$ is established to the requisite standard given evidence 
$a\wedge b$, it might still be that $A$ does not meet the requisite 
standard (given $a$) nor does $B$ (given $b$). 

But consider a less controversial version, holding the evidence constant:

 \[\text{If S[$a \wedge b, A\wedge B$], then S[$a \wedge b, A$] and S[$a \wedge b, B$].} \tag{DIS2}\]

\noindent
This principle is harder to deny. That is, one would not want 
to claim that, holding fixed evidence $a\wedge b$, establishing the conjunction 
might not be enough for establishing one of the conjuncts. 
One cannot be willing to assent to the 
conjunction without being willing to assent to one of the conjuncts 
against a fixed body of evidence. Certainly any formalization of the standard of proof should obey 
(DIS2). And yet, it is this very principle that we should deny if we understand 
the standard of proof using the Bayes factor. \footnote{To show that (DIS2) fails, it is enough to show that $\frac{\pr{B \vert a\wedge b \wedge A}}{\pr{B \vert A}}>1$ because $S[a\wedge b, A\wedge B]>S[a\wedge b, A]$ iff $\frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a\wedge b}}>\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b}}$ iff $\frac{\pr{A\wedge B \vert a \wedge b}}{\pr{A\wedge B}}>\frac{\pr{A \vert a \wedge b }}{\pr{A}}$ iff $\frac{\pr{A \vert a \wedge b} \times \pr{B \vert a\wedge b\wedge A}}{\pr{A} \times \pr{B \vert A}}>\frac{\pr{A \vert a \wedge b }}{\pr{A}}$. Now, $\frac{\pr{B \vert a\wedge b \wedge A}}{\pr{B \vert A}}>1$ so long as $a\wedge b$ positively supports $B$ even under the assumption of $A$, unless $A$ entailed $B$. We naturally exclude the situation in which one claim entails the other because otherwise there would be no need to establish the two claims. Establishing one claim alone would suffice. To see why $a\wedge b$ positively supports $B$ (or $A$), note that $S[a, A]=\frac{\pr{a |A}}{\pr{a}} \leq \frac{\pr{a |A} \times \pr{b | A \wedge a}}{\pr{a}\times \pr{b | a}}=\frac{\pr{a \wedge b |A}}{\pr{a\wedge b}}=S[a \wedge b, A]$. The key step here is $\frac{\pr{a |A}}{\pr{a}} \leq \frac{\pr{a |A} \times \pr{b | A \wedge a}}{\pr{a}\times \pr{b | a}}$. The latter holds because $\frac{\pr{b | A \wedge a}}{\pr{b | a}}\geq 1$. It is useful to distinguish two cases. First, if $A$ and $B$ are probabilistically independent, as in the Bayesian network in Figure \ref{network-conjunction} (top), then $\frac{\pr{b | A \wedge a}}{\pr{b | a}}= \frac{\pr{b}}{\pr{b}} = 1$. Second, if $A$ and $B$ are probabilistically dependent, as in the Bayesian network in Figure \ref{network-conjunction} (bottom), evidence $b$ positively supports claim $A$ (even conditional on $a$) so long as $b$ positively supports $B$. The assumption is that claim $A$ and $B$ are positively correlated, and thus, any evidence that supports one of the claims is going to support the other claim, as well. \textbf{SEE EARLIER CHAPTERS FOR A MORE RIGOROUS PROOF OF THIS LAST POINT.}}  In case $A$ and $B$ are probabilistically independent, (DIS1) and (DIS2) are in fact equivalent, so rejectign one requires rejecting the other.\footnote{$\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b}}=\frac{\pr{a \vert A}\pr{b \vert A}}{\pr{a}\pr{b}}=\frac{\pr{a \vert A}}{\pr{a}}$. In other words, $S[a \wedge b, A]=S[a, A]$.} The intuitive reason for this--- perhpas surprising---result is that $A \wedge B$ has a much lower prior probability than $A$ (or $B$) considered separately. Thus, the same body of evidence is going to have a larger impact on a hypothesis with a lower prior probability, other things being equal. This larger impact on the prior is reflected in a larger Bayes factor. 


All in all, using Bayes factor to understad the standard of proof 
has counterintuitive consequences, what we will call the distribution paradox. 
For suppose the prosecution provided evidence for claim $A$, but this evidence still falls short of the threshold $t$ (a certain number above 1). Just by tagging an additional claim $B$ and without doing any further evidentiary work, the prosecution could provide sufficiently strong evidence (which meets the threshold $t$) in favor of claim $A \wedge B$. So, it could well happen that, while the prosecution failed to prove beyond a reasonable doubt that the defendant injured the victim, the prosecution could nevertheless prove beyond a reasonable doubt that the defendant injured the victim and did so intentionally. This is odd. 






<!---

To see why, note that 
given the Bayesian network in Figure \ref{network-conjunction} (top), 
the following equalities hold:

\[ \text{S[$a \wedge b, A$] = S[$a, A$] and  S[$a \wedge b, B$] = S[$b, B$]}. \]

\noindent
If so, (EXT1) and (EXT2) are equivalent, and 
denying one extrapolation principle implies denying the other.\footnote{To show that 
S[$a \wedge b, A$] = S[$a, A$], note that $S[a, A]=\frac{\pr{a |A}}{\pr{a}}=\frac{\pr{a |A} \times \pr{b}}{\pr{a}\times \pr{b}}=\frac{\pr{a |A} \times \pr{b | A \wedge a}}{\pr{a}\times \pr{b | a}}=\frac{\pr{a \wedge b |A}}{\pr{a\wedge b}}=S[a \wedge b, A]$. The key step here is the unconditional probabilistic independence of $a$ and $b$ encoded in the Bayesian network in Figure \ref{network-conjunction} (top). The same reasoning applies for $B$. A related claim is that
$S[a, A] = S[a, A\wedge B]$ because $S[a, A]=\frac{\pr{a |A}}{\pr{a}}=\frac{\pr{a |A \wedge B}}{\pr{a}}=S[a, A \wedge B]$, and similarly for $S[b, B] = S[b, A\wedge B]$.} Thus, if the standard of proof is formalized 
using Bayes factor, the extremely plausible, almost undeniable, extrapolation principle (EXT2) would have to go. 


If we switch to the Bayesian network in Figure \ref{network-conjunction} (bottom), 
the evidential support of $a\wedge b$ 
in favor of $A$ (and the same would apply for $B$) would typically be \textit{stronger} 
than the evidential support of $a$ alone in favor of $A$. So the equalities 
above fail.

-->

## Likelihood ratio threshold

Let's now replace the Bayes factor with the likelihood ratio, another probabilistic 
measure of evidential support. As we shall understand it for now, the likelihood ratio compares
the probability of the evidence on the assumption that 
a hypothesis of interest is true and the probability of the evidence on the assumption that 
the negation of the hypothesis is true, 
that is, $\frac{\pr{E \vert H}}{\pr{E \vert \neg H}}$. The greater the 
likelihood ratio (for values above one), the stronger the evidential support 
in favor of the hypothesis (as contrasted to the its negation). 
We discussed extensively the advantages and 
limitations of this account in \textbf{REFERENCE TO EARLIER CHAPTER}.

We can think of the the likelihood rato as the following:

\[\frac{\textit{sensitivity}}{\textit{1- specificity}}\]

\noindent
Unlike the Bayes factor, the likleihood ratio is not sensitive to the priors so long as
sensitivity and specificity are not. In this sense, it is a more suitable measure 
if we want to factor out the effects that 
priors may have on the assessment of evidential strength. 
But this advantage is short lived. The likelihood ratio 
is sensitive to priors when one 
considers a composite claim instead of an individual claim. 

### Aggregating evidence

To see why, consider the combined 
ikelihood ratio $\frac{\pr{ a \et b |A \et B}}{\pr{a \et b | \neg (A \et B)}}$.
The numerator can be computed easily:\footnote{ \begin{align*}
\pr{a \wedge b| A\wedge B} & =  \frac{\pr{A \et B \et a\et b}}{\pr{A \et B}}\\
&= \frac{   \pr{A} \times \pr{B|A} \times \pr{a | A \wedge B} \times \pr{b | A \wedge B \wedge a} }{\pr{A} \times \pr{B \vert A}}\\
& =^* \frac{\pr{A} \times \pr{B | A} \times \pr{a | A} \times \pr{b | B}}{\pr{A}  \times \pr{B | A}} \\
& = \pr{a | A} \times \pr{b | B} 
 \end{align*}
  The asterisk marks the step that requires the indepedence assumptions in Figure \ref{network-conjunction}.
  }

\[ \pr{a \wedge b| A\wedge B} = \pr{a | A} \times \pr{b | B} \]
 \noindent
 The equality requires the independence assumptions 
 codified in the Bayesian networks 
 in Figure \ref{network-conjunction}. That is, the two items of evidence should be independent 
 of one another conditional on the hypothesis they support. 
The numerator does not depend on the priors associated with $A\wedge B$.
Call it \textit{combined sensitivity}, simply resulting from multiplying 
the sensitivity of the individual items of evidence, $a$ and $b$, 
relative to their respective hyptheses, $A$ and $B$.

The denominator is more involved:\footnote{   \begin{align*}
\pr{a \et b| \neg (A\et B)} & = \frac{\pr{a \et b \et \neg (A\et B)}}{\pr{\neg (A \et B)}} \\
& = \frac{\pr{a \et b \et \neg A\et B} +  \pr{a \et b \et A\et \neg B} + \pr{a \et b \et \neg A\et \neg B}  }{\pr{\neg A \et B} + \pr{A \et \neg B} + \pr{\neg A \et \neg B} } \\
& =^* \frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a | \neg A}\pr{b | B} + \pr{A}\pr{\neg B \vert A} \pr{a | A }\pr{b | \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a | \neg A}\pr{b | \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} }  
 \end{align*}
   The asterisk marks the step that requires the indepedence assumptions in Figure \ref{network-conjunction}.
   }

\[\pr{a \et b| \neg (A\et B)} = \frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a | \neg A}\pr{b | B} + \pr{A}\pr{\neg B \vert A} \pr{a | A }\pr{b | \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a | \neg A}\pr{b | \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} } \]

\noindent 
The same independence assumptions 
invoked before are needed here. 
Unlike the numerator, the denominator---call it \textit{combined specificity}---depends on the 
priors of $A$ and $A$ and thus on the priors of $A\wedge B$. Putting numerator 
and denominator together yields the formula 
for the combined likelihood ratio. 


  \begin{align*}
\frac{\pr{a \wedge b \vert A\wedge B}}{\pr{a \et b| \neg (A\et B)}} & = \frac{\pr{a | A} \times \pr{b | B}}{\frac{\pr{\neg A}\pr{B \vert \neg A} \pr{a | \neg A}\pr{b | B} + \pr{A}\pr{\neg B \vert A} \pr{a | A }\pr{b | \neg B} + \pr{\neg A}\pr{\neg B \vert \neg A } \pr{a | \neg A}\pr{b | \neg B}}{\pr{\neg A}\pr{B \vert \neg A} + \pr{A}\pr{\neg B \vert A } + \pr{\neg A}\pr{\neg B \vert \neg A} } }
 \end{align*}


As with the Bayes factor, under suitable independence assumptions, the combined likelihood ratio exceeds 
the individual likelihood ratio so logn as the two pieces of evidence have the same sensitivity and specificity. If they have differnt levels of sensitivity and specificity, the combined likelihood ratio never goes 
below the lowest of the two individual likelihood ratios. 

Becasue of the many variables at play, it is not easy to compare the combined evidential support and 
the individual support supplied by $a$ and $b$ towards $A$ and $B$, as measured by the individual and combined likelihood ratio. To circumvent this difficulty, we make three simplifying assumptions. First, the sensitivity of a piece of evidence, say $\pr{a |A}$, is the same as its specificity, $\pr{\neg a | \neg A}$. Let $\pr{a |A}=x$ and $\pr{b |B}=y$. So $\pr{a |\neg A}=1-x$ and $\pr{b | \neg B}=1-y$. Finally, the sensitivity (and thus the specificity) of the two pieces of evidence is the same, that is, $\pr{a |A}=x=\pr{b |B}=y$. Finally, as is customary, 
claims $A$ and $B$ are independent of one another.  The combined likelihood ratio therefore reduces to the following, where $\pr{A}=k$ and $\pr{B}=t$:

  \begin{align*}
\frac{\pr{a \wedge b \vert A\wedge B}}{\pr{a \et b| \neg (A\et B)}} & = \frac{xx}{\frac{(1-k)t(1-x)x + k(1-t)x(1-x) + (1-k)(1-t)(1-x)(1-x)}{ \left(1-k\right) t +\left(1-t\right) k+\left(1-k\right) \left(1-t\right)}}
 \end{align*}

\noindent
The graph of the combined likelihood ratio can now be easily plotted against the single likelihood ratios. As Figure \ref{fig:jointLRMarcello} shows, the combined likelihood  ratio varies depeding on the prior probabilities $\pr{A}$ and $\pr{B}$, as expected, but always execeeds the individual likelihood ratios whenever they are greater than one (that is, the two piece of evidence provides positive support for their respective hypothesis)

 
\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)

combined <- function (x,k,t) {
  (x^2)/
(
  ((1-k)*t*(1-x)*x + k*(1-t)*x*(1-x) + (1-k)*(1-t)*(1-x)*(1-x))
  /
    ((1-k)*t +(1-t)*k+(1-k)*(1-t))
)
}

combinedMarcello <- function(x,k,t){
  (x^2)/(k*(1-t)*(x)*(1-x)+(1-k)*t*x*(1-x)+(1-k)*(1-t)*(1-x)*(1-x))
}

x <-seq(0,1,by=0.001)
y0102 <- combined(x,0.1,0.2)
y0608 <- combined(x,0.6,0.8)

ggplot() +
  stat_function(fun=function(x)(x/(1-x)), geom="line", aes(colour="LR(a)=LR(b)"))+
  geom_line(aes(x = x, y = y0102,color = "joint LR for k = 0.1, t=0.2")) +
  geom_line(aes(x = x, y = y0608,color = "joint LR for k = 0.6, t=0.8")) +
        ylim(c(0,5))+
  xlab("x=sensitivity(a)=sensitivity(b)")+ ylab("LR")+theme_tufte()+
  theme(legend.position = c(0.8, 0.2))
```

\caption{Combined likelihood ratios exceeds individual Likelihood ratios. Changes in the prior 
probabilities $t$ and $k$ do not invalidate this result.}
\label{fig:jointLRMarcello}
\end{figure}


<!--

\noindent
The combined likelihood ratio is therefore:
\begin{align}\label{eq:combinedLRMarcello}
\frac{\pr{ a \et b |A \et B}}{\pr{a \et b | \neg (A \et B)}} & = \frac{xy}{\frac{(1-k)t(1-x)y + k(1-t)x(1-y) + (1-k)(1-t)(1-x)(1-y)}{ \left(1-k\right) t +\left(1-t\right) k+\left(1-k\right) \left(1-t\right) }}
 \end{align}


\noindent

  \begin{align*}
\frac{\pr{ a \et b |A \et B}}{\pr{a \et b | \neg (A \et B)}} & = \frac{xx}{\frac{(1-k)t(1-x)x + k(1-t)x(1-x) + (1-k)(1-t)(1-x)(1-x)}{ \left(1-k\right) t +\left(1-t\right) k+\left(1-k\right) \left(1-t\right) }}
 \end{align*}

-->

What happens if we relax the three simplifying assumptions?
Suppose the sensitivity and specificity of the two pieces of 
evidence are not the same. Their likelihood ratios will then 
also be different. In this case, the combined likelihood ratio is not always 
 greater than the individual ratios, but it is always 
greater than the smallest of the two provided the individual likelihood 
 ratios are greater than one. 
\textbf{M: what about dropping other assumptions?}

\todo{M: Need to add simuation results to make this argument 
fully general and drop all the simplifying assumptions. For example, what if the two 
hyptheses are not independent?}

<!-- ```{r echo=FALSE} -->
<!-- library(ggplot2) -->
<!-- library("gridExtra") -->
<!-- k <- 0.1 -->
<!-- t <- 0.2 -->
<!-- y <- 0.7 -->
<!-- b <- ggplot(data.frame(x=c(0,1)), aes(x)) +   ylim(0, 30) + -->
<!--   stat_function(fun=function(x)(x/(1-x)), geom="line", aes(colour="item of evidence a, LR-x=(1-x)")) + -->
<!--   stat_function(fun=function(x)(y/(1-y)), geom="line", aes(colour="item of evidence b, LR=y/(1-y)")) + -->
<!--   stat_function(fun=function(x)((x*y)/(k*(1-t)*(x)*(1-y)+(1-k)*t*y*(1-x)+(1-k)*(1-t)*(1-x)*(1-y))), geom="line", aes(colour="two items combined Pr(A)=0.1, Pr(B)=0.2")) + -->
<!--    stat_function(fun=function(x)((x*y)/(k*(1-0.4)*(x)*(1-x)+(1-k)*0.4*x*(1-x)+(1-k)*(1-0.4)*(1-x)*(1-x))), geom="line", aes(colour="two items combined Pr(A)=0.1, Pr(B)=0.4")) + -->
<!--     stat_function(fun=function(x)((x*y)/(0.3*(1-0.4)*(x)*(1-y)+(1-0.3)*0.4*y*(1-x)+(1-0.3)*(1-0.4)*(1-x)*(1-y))), geom="line", aes(colour="two items combined Pr(A)=0.3, Pr(B)=0.4")) + -->
<!--   xlab("x") + ylab("LR") +labs(colour="one versus many") + -->
<!--        ggtitle("Combined v individual support measured by LR with y=0.7") -->


The argument, once again, verified Dawid's claim that  `the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents'. One caveat is that, if evidential support is measured by the likelihood ratio, the support supplied by the conjunction of different independent pieces of evidence always exceeds the smallest of the support supplied by its consitituents, but there are cases in which it does not exceed 
the support supplied by some of its constituents.


### Variable Threshold

Like the Bayes factor, the likelihood ratio can be used 
to formalize the standard of proof by equating the standard 
to a threshold $t$ above one. The greater the threshold, the more stringent the standard. 
In criminal trials, for example, the rule of decision would be: guilt is proven beyond a reasonable doubt if and only 
if the evidential support in favor of $G$---as measured by the likelihood ratio 
$\frac{\pr{E \vert  G}}{\pr{E \vert \neg G}}$---meets a suitably 
high threshold $t$ above one. 

By the ratio version of Bayes' theorem, 

\[\textit{posterior ratio} = \textit{likelihood ratio} \times \textit{prior ratio},\]

and thus

\[\frac{\textit{posterior ratio}}{\textit{prior ratio}} = \textit{likelihood ratio}.\]

If the posterior ratio is fixed at, say $t/1-t$, the threshold on the 
likelihood ratio, call it $t_{LR}$ is defined as:

\[\frac{t/1-t}{\textit{prior ratio}} = t_{LR}.\]

\noindent
The likelihood ratio threshold will vary depending on the prior. The higher the prior, 
the lower the likelihood ratio threshold. So 


Consider the individual claim $A$ and $B$ 
and compared them with the composite claim $A \wedge B$. Suppose the two 
claims are independent. Consider a posterior threshold of $95\%$ as might be appropriate 
in a criminal case. Assuming $A$ and $B$ have a prior probability of 
$20\%$ and $30\%$ respectively, the likelihood ratio threshold for $A$ and $B$ will be 
$t_{LR}^{A}\approx 76$ and $t_{LR}^{B}a\approx 44$. The likelihood ratio threshold for 
the composite claim $A \wedge B$ will be $t^{A\wedge B}_{LR}\approx 297$. 




```{r, echo=FALSE}
(0.95/(1-0.95))/(0.2/(1-0.2))
(0.95/(1-0.95))/(0.3/(1-0.3))
(0.95/(1-0.95))/((0.3*0.2)/(1-(0.3*0.2)))
```


Consider a posterior threshold of 50\% as might be appropriate 
in a civil case. Assuming $A$ and $B$ have a prior probability of 
20\% and 30\% respectively, the likelihood ratio threshold for $A$ and $B$ will be 
$t_{LR}^{A}\approx 4$ and $t_{LR}^{B}a\approx 2$. The likelihood ratio threshold for 
the composite claim $A \wedge B$ will be $t^{A\wedge B}_{LR}\approx 15$.


```{r, echo=FALSE}
(0.5/(1-0.5))/(0.2/(1-0.2))
(0.5/(1-0.5))/(0.3/(1-0.3))
(0.5/(1-0.5))/((0.3*0.2)/(1-(0.3*0.2)))
```


As expected, other things being equal, the likelihood ratio threshold is lower 
for civil than criminal cases. The threshold is variable and depends on then prior, so claim that have higher priors are asociated with lower likelihood ratio threshold, such as $A$ and $B$, than claims 
associated with lower priors such as $A \wedge B$. 


Now suppoose the individual likelihood ratios meet the threshold
$t_{LR}^{A}$ and $t_{LR}^{B}$ give a posterior threshold of $0.95$. 
To ensure that $t_{LR}^{A}$ is met, 
evidence $a$ should have a sensitivity of at least (roughly) 0.99 (and a 
specificity of 1-sensitivity). To ensure that $t_{LR}^{B}$ is met, 
evidence $b$ should have a sensitivity of at least (roughly) 0.98 (and a 
specificity of 1-sensitivity). Holding fixed the values for sensitivity and specificity, 
does the combined likelihood ratio meet the threshold $t_{LR}^{A\wedge B}$? Not quite. The combined likelihood ratio equals about 145, far short that what the threshold $t^{A\wedge B}_{LR}$ requires, namely a likelihood ratio of 297



```{r, echo=FALSE}
(0.95/(1-0.95))/(0.2/(1-0.2))
(0.987/(1-0.987))

(0.95/(1-0.95))/(0.3/(1-0.3))
(0.978/(1-0.978))

(0.987*0.978)/
(
  ((1-0.2)*(1-0.987)*0.3*0.978+0.2*0.987*(1-0.3)*(1-0.978)+(1-0.987)*(1-0.2)*(1-0.978)*(1-0.3))/
  ((1-0.2)*0.3+0.2*(1-0.3)+(1-0.2)*(1-0.3))
)
```


Things do not look any better for a lower threshold. Now suppoose the 
individual likelihood ratios meet the threshold
$t_{LR}^{A}$ and $t_{LR}^{B}$ give a posterior threshold of $0.5$. 
To ensure that $t_{LR}^{A}$ is met, 
evidence $a$ should have a sensitivity of at least 0.8 (and a 
specificity of 1-sensitivity). To ensure that $t_{LR}^{B}$ is met, 
evidence $b$ should have a sensitivity of at least 0.7 (and a 
specificity of 1-sensitivity). To ensure that $t_{LR}^{B}$ is met, 
evidence $b$ should have a sensitivity of at least 0.7 (and a 
specificity of 1-sensitivity). Holding fixed the values for sensitivity and specificity, 
does the combined likelihood ratio meet the threshold $t_{LR}^{A\wedge B}$? Note quite. The combined likelihood ratio equals about 5, far short that what the threshold $t^{A\wedge B}_{LR}$ requires, namely a likelihood ratio of 15.



```{r, echo=FALSE}
(0.5/(1-0.5))/(0.2/(1-0.2))
(0.8/(1-0.8))

(0.5/(1-0.5))/(0.3/(1-0.3))
(0.7/(1-0.7))


(0.8*0.7)/
(
  ((1-0.2)*(1-0.8)*0.3*0.7+0.2*0.8*(1-0.3)*(1-0.7)+(1-0.8)*(1-0.2)*(1-0.7)*(1-0.3))/
  ((1-0.2)*0.3+0.2*(1-0.3)+(1-0.2)*(1-0.3))
)
```


### Fixed Threshold

By using a likelihood ratio threshold that is prior dependent, 
the conjunction principle---in particular, what we called aggregation---fails. The alternative 
is to fix a likelihood ratio threshold irrespective of the prior. Setting aside the problem of how ot identify 
the appropriate threshold, we will see that this approach 
is able to justify one direction of the conjunction principle---what we 
called aggregation---but still fails to justify the other direction---what we 
called distribution So the distribution paradox arises here again. 

Consider aggregation first. Say both individual likelihood 
 ratios $\frac{\pr{a |A}}{\pr{a | \neg A}}$ and $\frac{\pr{b |B}}{\pr{b | \neg B}}$ are above the requisite 
 threshold $t$ for meeting the standard of proof. Will the combined likelihood 
 ratio $\frac{\pr{ a \et b |A \et B}}{\pr{a \et b | \neg (A \et B)}}$ also 
 be above the threshold? The answer is affirmative. As we argued earlier, the combined likelihood ratio is never below the lowest of the individual likelihood ratio. If both individual likelihood ratios meet the threhsold, so does the combined likelihood ratio.  But this approach faces another problem, the same problem that plagues the Bayes factor. That is, likelihood ratios still fail to capture the other direction of the conjunction principle, what we called distribution. 

For suppose evidence $a \et b$ supports $A \et B$ to 
 the required threshold $t$. If evidential support is measured by the likelihood ratio, the threshold in this case should be  some order of magnitude greater than one. If the combined likelihood ratio meets the threshold $t_{LR}$, one of the individual likelihood ratios may well be below $t_{LR}$ So---if the standard of proof is interpreted using evidential support measured by the likelihood ratio---even though the conjuction $A \et B$ was proven according to the desired standard, one of individual claims might not. The right-to-left direction of the conjunction principle---that is, if $S[a \et b, A \et B]$, then $S[a, A] \et S[b, B]$, what we called earlier the distribution principle (DIS1)---fails. 
 
 Consider now the second and less objectionale extrapolation principle discussed earlier. This is the principle that holds 
the evidence fixed throughout, repeated below for convenience:
 
 \[\text{If S[$a \wedge b, A\wedge B$], then S[$a \wedge b, A$] and S[$b \wedge b, B$].} \tag{DIS2}\]

\noindent
Here again, the support of $a\wedge b$ in favor of $A\wedge B$ 
could exceed that of $a\wedge b$ in favor of $A$ alone (or $B$ alone) if evidential support 
is measured using likelihood ratios.\footnote{Note that, assuming either of the Baysian networks 
in Figure \ref{network-conjunction},  $S[a\wedge b, A]=\frac{\pr{a \wedge b \vert A}}{\pr{a\wedge b\vert \neg A}}=\frac{\pr{a \vert A}}{\pr{a \vert \neg A}}\times \frac{\pr{b \vert A}}{\pr{b \vert \neg A}}$, where

\[\frac{\pr{b \vert A}}{\pr{b \vert \neg A}} = \frac{\pr{B \vert A} \times \frac{\pr{b \vert B}}{\pr{b \vert \neg B}} + (1- \pr{B \vert A})}{\pr{B \vert \neg A} \times \frac{\pr{b \vert B}}{\pr{b \vert \neg B}} + (1- \pr{B \vert \neg A})}.\]

\textbf{SEE PROOF IN EARLIER CHAPTERS.}
 If $A$ and $B$ are assumed to be probabilistically independent, the numerator and the denominator will be the same, 
 so $\frac{\pr{b \vert A}}{\pr{b \vert \neg A}}=1$. Thus, 
 $S[a\wedge b, A]=\frac{\pr{a \vert A}}{\pr{a \vert \neg A}}=S[a, A]$. Since $S[a, A]< S[a\wedge b, A\wedge B]$ (see Figure \ref{fig:jointLRMarcello}), it follows $S[a\wedge b, A]<S[a\wedge b, A\wedge B]$. \textbf{What if A and B are dependent? Need simulation data here.}} Even this second, seemingly unobjectionable version of extrapolation fails. The same counterintuitive consequences that arose with Bayes factor manifest themselves here. The distribution paradox persists. 



\todo{M: This whole section should be generalized to the case in which A and B are not 
independent using the simulation data.}

\textbf{M: Might be goot to emphasize how devastaing this finding is for legal probabilists 
who endorsed likelihood ratios as the solution to many problems with legal probabilism.}

## The comparative strategy

Instead of thinking in terms of absolute thresholds---whether relative to posterior probabilities, 
the Bayes factor or the likelihood ratio---the standard of proof can be understood comparatively. This suggestion 
has been advanced by @cheng2012reconceptualizing following the theory 
of relative plausibility by \textbf{REFERENCE TO ALLEN AND PARDO HERE}. Say the prosecutor 
or the plaintiff puts foward a hypothesis $H_p$ about what happened. The defense offers an alternative hypothesis, 
call it $H_d$. On this approach, rather than directly evaluating the 
support of $H_p$ given the evidence and comparing it to a threshold, we compare the 
support that the evidence provides for two competing hypotheses $H_p$ and $H_d$, 
and decide for the one for which the evidence provides better support.

It is controversial whether this is what happens in all trial 
proceedings, especially in criminal trials. The defense may elect to challenge 
the hypothesis put foward by the other party without proposing one of its own. 
For example, in the O.J.\ Simpson trial the defense did not advance 
its own story about what happened, but simply argued that the 
evidence provided by the prosecution, while significant on its face to establish 
OJ's guilt, was riddled with problems and deficencies. This defense strategy 
was enough to secure an acquittal.  So, in order to create a reasonable doubt about 
guilt, the defense does not always provide a full-fledged alternative hypothesis. 
The supporters of the comparative approach, however, will respond that this could happen 
in a small number of cases, even though in general---especially for tactical reasons---the defense 
will provide an alternative hypothesis. After all, not to provide one would usually 
amount to an admission of criminal or civil liability.  

Setting aside this controversy for the time being, 
let's first work out the comparative strategy 
using posterior probabilities. More specifically, given a body of evidence $E$ and two competing hypotheses 
$H_p$ and $H_d$, the probability $\pr{H_p | E}$ should be suitably higher 
than $\pr{H_d | E}$, or in other words, the ratio $\frac{\Pr{H_p | E}}{\Pr{H_d | E}}$ should 
be above a suitable threshold. Presumably, the ratio threshold shoud be higher 
for criminal than civil cases.  In fact, in civil cases it seems enough to require that the 
ratio $\frac{\Pr{H_p | E}}{\Pr{H_d | E}}$ be avove 1, or in other words, 
$\pr{H_p | E}$ should be higher than $\pr{H_d | E}$. Note that $H_p$ and $H_d$ need not be one the negation of the other. 
Whenever two hypotheses are one the negation of the other, $\frac{\pr{H_p | E}}{\pr{H_d | E}}>1$ 
implies that $\pr{H_p | E}>50\%$, the standard probabilistic intepretation of the preponderemca standard. 

One advantage of this approach---as Cheng shows---is that expected utility theory can 
set the appropriate comparative threshold $t$ as a function of the costs and benefits 
of trial decisions. For simplicity, suppose that if the decision is correct, no costs result, 
but incorrect decisions have their price (\textbf{REFERENCE TO EARLIER CHAPTER 
FOR MORE COMPLEX COST STRUCTURE}). The costs of a false positive is $c_{FP}$ and
false negative is $C_{FN}$, both greater than zero. Intuitively, 
the decision rule should minimize the expected costs. That is, a finding against the defendant 
would be acceptable whenever its expected costs---$\pr{H_d \vert E} \times c_{FP}$---are smaller 
than the expected costs of an acquittal---$\pr{H_p \vert E}\times c_{FN}$--- 
or in other words:

\[\frac{\pr{H_p \vert E}}{\pr{H_d \vert E}} > \frac{c_{FP}}{c_{FN}}.\]

\noindent
In civil cases, it is customary to assume the costs ratio of 
false postives to false negatives equals one. So the rule of decision would be: Find against the defedant whenever $\frac{\pr{H_p \vert E}}{\pr{H_d \vert E}} > 1$ or in other words
$\pr{H_p \vert E}$ is greater than $\pr{H_d \vert E}$. In criminal trials, the costs ratio is usually considered higher, since convicting an innocent (false positive) should be more harmful or morally objectionable 
than acquitting a guilty defendant (false negative). Thus, the rule of decision in criminal proceedings would be: 
Convict whenever $\pr{H_p \vert E}$ is significantly greater 
than $\pr{H_d \vert E}$.





<!---
\begin{center}
\begin{tabular}
{@{}llll@{}}
\toprule
& & \multicolumn{2}{c}{Decision}\\
& &  $D_\Delta$ & $D_\Pi$ \\
\cmidrule{3-4}
\multirow{2}{*}{Truth} &  $H_d$    & $0$    & $c_FP$\\
                       &  $H_p$       &  $c_FN$   & $0$ \\ 
\bottomrule
\end{tabular}
\end{center}


Say that given our total evidence $E$ the  relevant conditional probabilities are:
Let us say that  if the defendant is right and we find against them, 
the cost is $c_1$, and if the plaintiff is right and we find against them, the cost is $c_2$:
Intuitively, it seems that we want a  decision rule which minimizes 
the expected cost. Say that given our total evidence $E$ the  relevant conditional probabilities are:

\vspace{-6mm}

\begin{align*}
p_\Delta &= \pr{H_\Delta \vert E} \\
p_\Pi & = \pr{H_\Pi \vert E}
\end{align*}
<!-- \noindent where $\mathtt{P}$ stands for the prior probability (this will be the case throughout our discussion of Cheng). 
\noindent The expected costs for deciding that $H_\Delta$ and $H_\Pi$, respectively, are:
\begin{align*}
E(D_\Delta) & = \pr{H_\Delta \vert E} 0 + p_\Pi c_2 = c_2p_\Pi\\
E(D_\Pi) & = \pr{H_\Pi \vert E} c_1 + p_\Pi 0 = c_1 p_\Delta
\end{align*}
\noindent For this reason, on these assumptions,  we would like to choose $H_\Pi$ just in case $E(D_\Pi) < E(D_\Delta)$. This condition is equivalent to:

\vspace{-6mm}


\begin{align}
\nonumber c_1p_\Delta &< c_2p_\Pi \\
\nonumber c_1 & < \frac{c_2p_\Pi}{p_\Delta}\\
\label{eq:cheng_frac1}\frac{c_1}{c_2} & < \frac{p_\Pi}{p_\Delta}
\end{align}


\noindent @cheng2012reconceptualizing (1261)  notes:
\begin{quote}
At the same time, in a civil trial, the legal system expresses no preference between finding erroneously for the plaintiff (false positives) and finding erroneously for the defendant (false negatives). The costs $c_1$ and $c_2$ are thus equal\dots
\end{quote}
\noindent If we grant this assumption, $c_1=c_2$, \eqref{eq:cheng_frac1} reduces to:

\vspace{-6mm}


\begin{align}
\nonumber 1 &< \frac{p_\Pi}{p_\Delta} \\
\label{eq:cheng_comp1} p_\Pi &> p_\Delta 
\end{align}
\noindent That is, in standard civil litigation we are to find for the plaintiff just in case $H_\Pi$ is more probable given the evidence than $H_\Delta$, which seems plausible.

This instruction is somewhat more general than the usual suggestion of the preponderance standard in civil litigation,  according to which the court should find for the plaintiff just in case $\pr{H_\Pi\vert E} >0.5$. This threshold, however, results from \eqref{eq:cheng_comp1} if it so happens that $H_\Delta$ is $\n H_\Pi$, that is, if the defendant's claim is simply the negation of the plaintiff's thesis.  By no means, Cheng argues, this is always the case. 

-->

Does the comparative strategy just outlined solve 
the difficulty about conjuction? 
We will work through a stylized case used by Cheng himself. 
Suppose, in a civil case, the plaintiff claims that the defendant was speeding ($S$) and that
the crash caused her neck injury ($C$). Thus, the plaintiff's hypothesis $H_p$ is $S\et C$. Given 
the total evidence $E$, the conjuncts, taken separately, meet the decision threshold:
\begin{align}
 \nonumber 
 \frac{\pr{S\vert E}}{\pr{\neg S \vert E}} > 1   & & \frac{\pr{C\vert E}}{\pr{\neg C \vert E}} > 1
\end{align}
\noindent The question is whether $\frac{\pr{S\et C\vert E}}{\pr{H_d \vert E}}>1$. To answer it, we have to decide what the defense hypothesis $H_d$ should. Cheng reasons that there are 
three alternative defense scenarios: $H_{d_1}= S\et \n C$, $H_{d_2}=\n S \et C$, and  $H_{d_3}=\n S \et \n C$. How does the  hypothesis $H_p$ compare to each of them? 
Assuming independence between $C$ and $S$, we have

\begin{align}\label{eq:cheng-multiplication}
\frac{\pr{S\et C\vert E}}{\pr{S\et \n C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{S \vert E}\pr{\n C \vert E}}  =\frac{\pr{C\vert E}}{\pr{\n C \vert E}} > 1 \\
\nonumber
\frac{\pr{S\et C\vert E}}{\pr{\n S\et C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{C\vert E}}  = \frac{\pr{S\vert E}}{\pr{\n S \vert E}} > 1 \\
\nonumber
\frac{\pr{S\et C\vert E}}{\pr{\n S\et \n C\vert E}} & = \frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{\n C \vert E}}   > 1 
\end{align}

\noindent So, whatever the defense hypothesis, the plaintiff's hypothesis is more probable. At least in this case, whenever the elements of a plaintiff's claim satisfy the decision threshold, so does their conjunction. The left-to-right direction of the conjunction principle---what we called aggregation---has been vidicated. But what about the opposite direction, what we called extrapolation? Interestingly, if the threshold is just 1---as might be appropriate in civil cases---extrapolation would be satisfied. Even if $\frac{\pr{S\vert E}\pr{C\vert E}}{\pr{\n S \vert E}\pr{\n C \vert E}}$ might be strictly greater than $\frac{\pr{C\vert E}}{\pr{\n C \vert E}}$ or $\frac{\pr{S\vert E}}{\pr{\n S \vert E}}$, wehenver the former is greater than one the latter must be greater than one. However, suppose the threshold is more stringent than one, as might be appropriate for criminal cases. For some constituent claims $A$ and $B$ in a criminal case, whenever $\frac{\pr{A\vert E}\pr{B\vert E}}{\pr{\n A \vert E}\pr{\n B \vert E}}$ barely meet the threhold $t$, $\frac{\pr{A\vert E}}{\pr{\n A \vert E}}$ or $\frac{\pr{B\vert E}}{\pr{\n B \vert E}}$ could be below $t$. Since the evidence is held fixed throughout, this would be a violation of the extrapolation principle (EXT2). 


Another problem with this approach is that much of the heavy lifting here is done by the strategic splitting of the defense line into multiple scenarios. Now suppose $\pr{H_p\vert E}=0.37$ and the probability of each of the defense lines given $E$ is $0.21$. This means that $H_p$ wins with each of the scenarios, so we should find against the defendant. But should we? Given the evidence, the accusation is very likely to be false, because $\pr{\n H_p \vert E}=0.63$?  The problem generalizes. If, as here, we individualize scenarios by boolean combinations of elements of a case, the more elements there are, into more  scenarios $\n H_p$ needs to be divided. This normally would lead to the probability of each of them being even lower  (because now $\pr{\n H_p}$ needs to be ``split'' between more scenarios). So, if we take this approach seriously, the more elements a case has, the more at disadvantage the defense is. This seems undesirable. 

  REPEAT SAME ARGUMENT USING LR AND COMPARATIVE STRATEGY
 
 
# The Conjunction Principle Is False


Neither the Bayes factor nor the likelihood ratio 
managed to fully justify both directions 
of the conjunction principle. One direction, aggregation, was justified.
So the original concern that was driving Cohen's formulation of the 
conjuction paradox was addressed. But the 
other direction, distribution, failed.  The failure of distribution creates a paradox 
of its own, what we called distribution paradox. It is odd that one could have sufficiently 
strong evidence in support of $A\wedge B$, while not having sufficiently strong evidence for $A$ or $B$. 
This occurs even when $A$ and $B$ are probabilistically independent. If they were dependent of one another---say $A$ and $B$ were mutually reinforcing---it is possible the evidence would strongly support the conjunction, but not one of the conjuncts in isolation (becuase the additional support from the other claim, $A$ or $B$, would be missing). But the failure of distribution manifests itself even when $A$ and $B$ are independent. What should we make of this?
This problem exists for both the Bayes factor and the likelihood ratio. 

## Factoring Out Prior Probabilities

Let us return to the role of prior probabilities and their effect on measures of evidential strength.
Dawid observed that the prior probabilities of the conjuncts are correspondingly higher 
than the prior probability of the conjunction. The conjunction principle, instead, 
ignores the role of prior probabilities and treat the conjuncts and the conjuction only in relation to the 
evidence, irrespective of the prior probabilities. So, in order to capture the conjunction principle, legal probabilists 
should rely on probabilistic measures that are not heavily depend on prior probabilities. But neither the Bayes factor nor the likelihood ratio are such measures. 

The larger Bayes factor associated with the 
composite claim, holding the evidence fixed, need not be a sign of 
stronger evidence, but merely an artifact 
of the lower prior probability of the composite claim. The same can be said for the combined likelihood ratio. 
Holding fixed the sensitivty and specificity of $a$ and $b$, the combined likelihood 
ratio can be changed by varying the priors of $A$ and $B$. The lower the priors, the stronger 
the likelihood ratio.  Perhpas, arguably, the same body of evidence may strongly suport the composite 
hypothesis $A\wedge B$, while failing to strongly support $A$ or $B$ simply because $A\wedge B$ 
has a lower prior probability and this lower prior probability, everything else being equal, 
inflates the likelihood ratio or the Bayes factor qua measures of evidential strength. 

To circumvent the phenomenon of prior dependency, evidential strength can be thought as a relationship between prior and posterior probabilities. The graph in Figure \ref{fig:strength-prior-post}
below represents to what extent the evidence changes the probability of a select hypothesis for any value of the prior probability of the hypothesis. The graph compares the 'base line' (representing no change in probability) and the 'posterior line' (representing the posterior probability of the hypthesis as a function of the prior for a given assignment of sensitivity and specificity of the evidence). Roughly, the larger the area between the base line and the posterior line, the stronger the evidence. Crucially, this area does not depend on the prior probability of the hypothesis, but solely on the sensitivity and specificity of the evidence. As expected, any improvement in sensitivity or specificity will increase the area between the base line and the posterior line. To be sure, what matters is the ratio of sensitivity to 1-specificity, not their absolute value. So evidence with sensitivity and specificity of 0.9 and 0.9 would be equally strong as evidence with sensitivity and specificity at 0.09 and 0.09 becaue $0.9/(1-0.9) = 0.09/(1-0.09)$.


\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}

x <-seq(0,1,by=0.001)
y0906 <- post(x, 0.9, 0.6)
y0609 <- post(x, 0.6, 0.9)
y0909 <- post(x, 0.999, 0.999)

ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="null, sensitivity=specificity=0.5"))+
  geom_line(aes(x = x, y = y0906,color = "sensitivity=0.9, specificity=0.6")) +
  geom_line(aes(x = x, y = y0609,color = "sensitivity=0.6, specificity=0.9")) +
  geom_line(aes(x = x, y = y0909,color = "sensitivity=0.999, specificity=0.999")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.8, 0.2))
```

\caption{The further away the posterior line from the base 
line, the stronger the evidence irrespective 
of the prior probability of the hypothesis.}
\label{fig:strength-prior-post}
\end{figure}

The same approch can model the joint evidential strength of two items of evidence, $a \wedge b$, 
relative to the combined hypothesis, $A \wedge B$. For simplicity, assume $a$ and $b$ are independent 
lines of evidence supporting their respective hypothesis $A$ and $B$. Further, assume 
$A$ and $B$ are probabilistically independent of the other, as in the Bayesian network 
in Figure \ref{network-conjunction} (top). The graph in Figure 
\ref{fig:post-indiv-joint} (top) shows how the prior probabilities 
are impacted by evidence in support of a single hypothesis---say $a\wedge b$ supports $A$\footnote{Given the assumptions of independence we are working with, the strength of $a$ in support of $A$ is the same the strength of 
$a\wedge b$ in support of $A$ since $\frac{\pr{a\wedge b \vert A}}{\pr{a \wedge b}}$ $=\frac{\pr{a \vert A}\pr{b \vert A}}{\pr{a}\pr{b}}=\frac{\pr{a \vert A}}{\pr{a}}$.}---versus 
evidence in support of a joint hypothesis---say $a\wedge b$ supports $A \wedge B$.
The base line is lower in the latter than in the former case because the prior probability of 
$A \wedge B$ is lower than the prior probability of $A$. The prior 
of $A$ equals $x$ and the prior of $A\wedge B$ equals $x^2$ (assuming 
$A$ and $B$ have the same prior probability, and as noted before, 
are probabilistically independent of one another).


\begin{figure}

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


postn <- function (x, s1, s2, n){
  ((x)^n)*
    ((
      s1/
        (s1*x+(1-s2)*(1-x))
     )^n)
    
}

x <-seq(0,1,by=0.001)
y0908 <- post(x, 0.9, 0.8)
y0909 <- post(x, 0.99, 0.99)
yy0908 <- postn(x, 0.9, 0.8, 2)
yy0909 <- postn(x, 0.99, 0.99, 2)
yynull <- postn(x, 0.5, 0.5, 2)
y50908 <- postn(x, 0.9, 0.8, 5)
y50909 <- postn(x, 0.99, 0.99, 5)
y5null <- postn(x, 0.5, 0.5, 5)


ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, sens=0.5, spec=0.5"))+
  #geom_line(aes(x = x, y = yynull,color = "2, sens=0.5, spec=0.5")) +
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  #geom_line(aes(x = x, y = yy0908,color = "2, sens=0.9, spec=0.8")) +
  # geom_line(aes(x = x, y = yy0909,color = "2, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y5null, color = "5, sens=0.5, spec=0.5")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.9, 0.2), legend.title = element_blank())
```

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


post <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


post2 <- function (x, s1, s2){
  (x)*
    (
      s1/
        (s1*sqrt(x)+(1-s2)*(1-sqrt(x)))
     )*
    (
      s1/
        (s1*sqrt(x)+(1-s2)*(1-sqrt(x)))
     )
}

x <-seq(0,1,by=0.001)
y0908 <- post(x, 0.9, 0.8)
y0909 <- post(x, 0.99, 0.99)
yy0908 <- post2(x, 0.9, 0.8)
yy0909 <- post2(x, 0.99, 0.99)
yynull <- post2(x, 0.5, 0.5)

ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, null"))+
  geom_line(aes(x = x, y = y0908,color = "1, sensitivity=0.8, specificity=0.7")) +
  geom_line(aes(x = x, y = y0909,color = "1, sensitivity=0.99, specificity=0.99")) +
  geom_line(aes(x = x, y = yy0908,color = "2, sensitivity=0.8, specificity=0.7")) +
  geom_line(aes(x = x, y = yy0909,color = "2, sensitivity=0.99, specificity=0.99")) +
        ylim(c(0,1))+
  xlab("prior")+ ylab("posterior")+theme_tufte()+
  theme(legend.position = c(0.8, 0.2))
```

\caption{The comparison is betwen individual support (marked by 1, for one individual 
hypothesis) and joint support (marked by 2, for a two-claim composite claim). 
Top graph: The base line for joint support ($y=x*x$) is 
below the base line for individual support ($y=x$).
Bottom graph: the two base lines are equalized and the 
posterior lines adjusted accordingly. The posterior 
lines for individual and joint support 
get closer especially for high posterior probability values.}
\label{fig:post-indiv-joint}
\end{figure}


What happens if we make the same comparison between individual and composite claims by equalizing their prior probability? If the claims are independent and equiprobable, let $x$ be the prior probability of an individual claim (when it is considered in isolation) and let $x^{1/k}$ the prior probability of the same individual claim when it is part of a composite claim that consists of $k$ claims. In this way---and again, assuming independence and equiprobability of the hypotheses---the prior probability of the composite claim equals the prior probability of the individual claim since $(x^{1/k})^k=x$, as desired. These different claims are then plotted having the same priors. Here we are explicitly factoring out the role of prior probabilities. Figure \ref{fig:post-indiv-joint} (bottom) shows the result of this process of equalization. 

We observe two things. First, the difference in posterior probability, though still present, is less significant, especially for values above the 50\% threshold or even more clearly above the 95\% threshold. Second, whatever remaining difference in posterior probability is now reversed, that is, a composite claim supported by several items of evidence has a higher posterior probability compared to an individual claim supported by one item of evidence. This second observation 
agrees with the analysis based on the Bayes factor and the likelihood ratio in the earlier section. Thet analysis showed that the support for a composite claim by a joint body of evidence often exceeds the support for an individual claim. 

These two observations establish that, by factorig out prior probabilities and under certain independence assumptions, whenever the individual claims meet the applicable posterior threhsold, so does the composite claim. This verifies aggregation. 
Conversely, whenever the composite claim, say $A \wedge B$, meets the appliacable posterior threshold, so do the individual claims insofar as the threshold is about 75\% or higher. This verifies---to some approximation and in a limited class of cases---the other direction of the conjunction principle, what we called distribution. 


<!---

The same equalization can be done with the Bayes factor, arriving at similar results. Compare the difference betwen teh graph (top) in which teh Bayes factor has not been equalized relative to the priors and the graph (bottom) in which it has. In the latter case, individual Bayes factor and joint Bayes factor tend to converge especially for relatively high values of sensitivity and specificity SEE FIGURE BELOW. 


\begin{figure}


```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


bf <- function (x, s1, s2){
  
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


bfn <- function (x, s1, s2, n){

    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )^n
}

x <-seq(0,1,by=0.001)
y0908 <- bf(x, 0.9, 0.8)
y0909 <- bf(x, 0.99, 0.99)
y50908 <- bfn(x, 0.9, 0.8, 5)
y50909 <- bfn(x, 0.99, 0.99, 5)
y5null <- bfn(x, 0.5, 0.5, 5)

ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, null"))+
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
        ylim(c(0,10))+
  xlab("prior")+ ylab("Bayes Factor")+theme_tufte()+
  theme(legend.position = c(0.8, 0.7), legend.title = element_blank())
```

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%"}
library(ggplot2)
library(ggthemes)


bf <- function (x, s1, s2){
  
    (
      s1/
        (s1*x+(1-s2)*(1-x))
     )
}


bfnEq <- function (x, s1, s2, n){

    (
      s1/
        (s1*x^(1/n)+(1-s2)*(1-x^(1/n)))
     )^n
}

x <-seq(0,1,by=0.001)
y0908 <- bf(x, 0.9, 0.8)
y0909 <- bf(x, 0.99, 0.99)
y50908 <- bfnEq(x, 0.9, 0.8, 5)
y50909 <- bfnEq(x, 0.99, 0.99, 5)
y5null <- bfnEq(x, 0.5, 0.5, 5)

ggplot() +
  stat_function(fun=function(x)(x), geom="line", aes(colour="1, null"))+
  geom_line(aes(x = x, y = y0908,color = "1, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y0909,color = "1, sens=0.99, spec=0.99")) +
  geom_line(aes(x = x, y = y50908,color = "5, sens=0.9, spec=0.8")) +
  geom_line(aes(x = x, y = y50909,color = "5, sens=0.99, spec=0.99")) +
        ylim(c(0,10))+
  xlab("prior")+ ylab("Bayes Factor Prior Equalized")+theme_tufte()+
  theme(legend.position = c(0.8, 0.7), legend.title = element_blank())
```

\label{fig:bf-indiv-joint}
\end{figure}


-->

Has the distribution paradox be eliminated then? The approach we have just 
described---equalizing the prior probabilities across individual and composite claims---does 
not entirely eliminate the paradox. There are still cases in which a composite hypothesis, say $A \wedge B$, receives stronger support than an individual hypothesis, given the same body of evidence. Sensitivity to priors seems to play a role. 
But it cannot be the only factor at play, or else the equalization of the prior probabilities would have elimited the paradox entirely. So what else is going on?


## Weaker Claims Weaken Sensitivity 

Let's examine more closely the Bayes factor and the likelihood ratio as measures 
of evidential strength. Likelihood ratios are comparative in nature. Say we compare claim $A$ and claim $A\wedge B$ relative to the same body of evidence $a\wedge b$. Which claim will receive more support?  Intuitively, one might think that $A$ should receive more support than $A\wedge B$ relative to the same body of evidence. After all, $A\wedge B$ is a stronger claim than $A$ and thus more difficult to establish than $A$, other things being equal. But this intuition, on closer scrutnity, does not hold water.  Formally, the question is whether $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}>1$ or $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}<1$. Note that the role of priors---at least on its face---has been eliminated. Given the usual independencies between evidence and hypotheses (see Bayesian networks in Figure \ref{network-conjunction}
), we know that:

\[\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}=\frac{\pr{a \vert A} \pr{b \vert A}}{\pr{a \vert A} \pr{b \vert B}}=\frac{\pr{b \vert A}}{\pr{b \vert B}}<1\]

\noindent
The reason is that $\pr{b \vert A} < \pr{b \vert B}$ since the sensitivity of $b$ relative to $B$ should be higher than the sensitivity of $b$ relative to $A$.\footnote{GIVE PROOF OF THIS} Thus, $a\wedge b$ supports $A\wedge B$ more than it supports $A$ alone. This is not what one would expect intuitively.

But if $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}<1$, the sensitivity of $a\wedge b$ must be worse relative to $A$ than the composite claim $A \wedge B$. Sensitivity is a crucial property of the quality of the evidence. Eveyrthing else being equal, the lower the sensitivity of the evidence, the lower its evidential strength. The importance of sensitivity as a factor for asessing the strength of the evidence is hard to dispute. So why is the sensitivity of $a\wedge b$ worse relative to $A$ than $A \wedge B$? Suppose $A$ is the case. If $A$ is the case, in order for $a\wedge b$ to arise, both $a$ and $b$ should pick up on $A$. If $b$ fails to pick up on $A$, then $A \wedge b$ would not arise even if $a$ pick up on $A$.\footnote{The occurrance of $a\wedge b$ is less likely to occur than just $a$ alone picking up on $A$ because $b$ may fail---and fail more often than $a$ would---in picking up on $A$.} Suppose instead $A\wedge B$ holds. In this case, $a\wedge b$ would arise even if $b$ fails to pick up on $A$ so long $a$ picks up on $A$ and $b$ picks up on $B$. Now of course $b$ could also fail to pick on $B$ just like $a$ could fail to pick up on $B$. But we are assuming that $b$ is better than $a$ at tracking $B$. So $b$ will fail less often than $a$ at picking up on $B$. Thus, the sensitivity of $a\wedge b$ relative to $A\wedge B$ is better than the sensitivity of $a$ relative to $A$ alone. This is a subtle point that probability theory helps to bring out clearly. 

The same holds for  the Bayes factor. This is a non-contrastive measures of evidential support. 
If the sensitivity of $a\wedge b$ is better relative to $A \wedge B$ than relative to $A$, then 

\[\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b}} < \frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a\wedge b}}   \]

\noindent
Thus, evidence $a\wedge b$ better supports, even on an absolute scale, $A \wedge B$ compared to $A$. Note that, even if the Baye factor depends on the priors, the difference here is not due to the difference between the priors of $A$ and the priors of $A\wedge B$ since the denominator is simply $\pr{a\wedge b}$ and $\pr{a\wedge b \vert A}\pr{A}+ \pr{a\wedge b \vert \neg A}\pr\neg {A}$ is the same as $\pr{a\wedge b \vert (A\wedge B)}\pr{A\wedge B}+ \pr{a\wedge b \vert \neg (A\wedge B)}\pr{\neg (A\wedge B)}$.\footnote{NEED A PROOF FOR THIS BUT IT SHOULD HOLD, RIGHT?}

Another way to convince ourselves this is the case is to run a simulation. Suppose we are deciding about the truth of $A$ and the the truth of $A\wedge B$, and we have a fixed body of evidence, say, $a\wedge b$ that speaks in favor of both claims.  \textbf{RUN SIMULATION TO SHOW THAT, ACTUALLY, ONE DIAGNOSTIC TEST FOR COMPOSITE CLAIM WOULD PERFORM BETTER 
THEN THE SAME DIAGNOSTIC TEST FOR INDIVIDUAL CLAIM (WORSE LR). SO TEST COUDL PASS QUALITY CRONTROL IN ONE CASE BUT NOT IN THE OTHER.} 

How big are these variations? \textbf{PLOT GRAPH TO GET A SENSE OF VARIATIONS OF SENSITIVITY}








So the fallacy seems to be to assume that the sensitivity of $a\wedge b$ relative to $A$ cannot be lower than the senstivity of $a\wedge b$ relative to $A\wedge B$. The thought would be something like this: if $a \wedge b$ tracks $A\wedge B$ to some degree, it surely must be able to track $A$ alone, at least as well. But we have just shown that we cannot assume that $\pr{a\wedge b \vert A} \geq \pr{a\wedge b \vert A\wedge B}$ and in fact the opposite is the case,
$\pr{a\wedge b \vert A} < \pr{a\wedge b \vert A\wedge B}$.

We should circumscribe the point we just made. It does not always hold. 
It is important to understand when it holds and when it does not hold. Suppose $H$ is a claim completely unrelated to $A$. We have evidence $a$ that supports $A$. Would the composite claim $A\wedge H$ be better supported by $a$ than $A$ alone? It would not. Mere tagging an unrelated hypothesis does not magically strengthen the evidence. Note that $\pr{a \vert A}=\pr{a \vert A \wedge H}$ because 
$H$ is indepedent from everything else.  It is a holly unrelated claim. Thus, 

\[\frac{\pr{a \vert A}}{\pr{a \vert A \wedge H}}=\frac{\pr{a \vert A}}{\pr{a \vert A}}=1.\]

\noindent
Tagging a completely unrelated claim $H$ does not magically strengthen a body of evidence, but leaves it unchanged. 
A similar point is in order here. Suppose $B$ constitutes one element of a crime and $A$ constitutes the other element The two claims are independent, each supported by items of evidence $a$ and $b$ respectively. This is our standard set up. If $a$ supports $A$, would $a$ support $A\wedge B$ more strongly than it supports $A$ alone? Notice that here we no longer have $a\wedge b$, but instead, $a$ alone. The question is whether
$\frac{\pr{a \vert A}}{\pr{a \vert A \wedge B}}>1$ or $\frac{\pr{a \vert A}}{\pr{a \vert A \wedge B}}<1$. 
Given the usual independencies between evidence and hypotheses, we know that:

\[\frac{\pr{a\vert A}}{\pr{a \vert A \wedge B}}=\frac{\pr{a \vert A}}{\pr{a \vert A}}=1\]

One might complain that this is counterintuitive. How can it be that $a$ supports $A$ to the same degree that it supports the more demanding claim $A\wedge H$ or $A\wedge B$? For suppose we have evidence $a$ in favor of $A$ and then wonder whether we could use that evidence in support of another claim $H$ or $B$. By tagging $H$ or $B$ to $A$, we can at leat say that we have evidence $a$ for $A\wedge H$ or $A\wedge B$ that is at lest as strong evidence $a$ in support of $A$. But note that $\frac{\pr{a \vert A}}{\pr{a\vert neg A}}>1$ even though $\frac{\pr{a \vert H}}{\pr{a\vert neg H}=1}$ and 
$\frac{\pr{a \vert B}}{\pr{a\vert neg B}=1}$ (assuming $A$ and B are independent). So $a$ does not support $H$ or $B$ to the same degree that it supports $A$. However, $a$ supports $A$ to the same degree that it supports $A\wedge H$ or $A\wedge B$.

The moral here is two fold. First, tagging an irrelevant hypothesis---a hypothesis taht does not bear on the evidence in one way or another, such as $H$ relative to $a$---does not change evidential support. Second, tagging a relevant hypothesis---a hypthesis that doe bear on the evidence in one way or another, such as $B$ relative to $a\wedge b$---does increase evidential support. \textbf{How to explain this better?}



## But Sensitivity Depends On Prior Probabilities

What we have said so far agrees with the claim defended in the previous section. That is, even when $a\wedge b$ strongly supports $A \wedge B$, the same evidence need \textit{not} strongly support $A$. Formally, 

\[\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert \neg A}} < \frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a\wedge b \vert \neg (A \wedge B)}}\]

\noindent
In other words, even if $a\wedge b$ favors $A\wedge B$ over its negation to a very high degree, it need not favor equally strongly $A$ over its negation. This is a comparative claim about two compartive claims, and as such, it may not be easy to parse. Evidential support, when it is formalized by the likelihoo ratio, is always relative to a contrast class, such as $A$ versus $\neg A$ or $A\wedge B$ versus $\neg (A\wedge B)$. In comparing the support that the same body of evidence provides to $A$ as contrasted to $A\wedge B$, it might be better to include these two hypotheses in the contrast class. So the expression $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}$ is more straightforward. 

No matter the formulation, the conclusion holds. The same body of evidence, $a\wedge b$, supports the composite claim $A\wedge B$ more than it supports the weaker claim $A$, even assuming that $A$ and $B$ are independent of one another and thus not mutually reinforcing. This seems paradoxical. At first, we thought the paradox could be due to prior dependency since the combined likelihood ratio $\frac{\pr{a\wedge b \vert A \wedge B}}{\pr{a\wedge b \vert \neg (A \wedge B)}}$  varies depending on the priors of $A$ and $B$. But this argument seems to no longer holds since in case of $\frac{\pr{a\wedge b \vert A}}{\pr{a\wedge b \vert A \wedge B}}$ any prior dependency has been eliminated. Or has it been?



We have shown, given a suitable number of assumptions, that the sensitivity of $a\wedge b$ can 
be greater relative to $A \wedge B$ than $A$ alone. This explains why the evidential support 
of $a\wedge b$ is greater in favor of $A \wedge B$ than alone $A$. Therefore---one 
might conclude---even factoring out differences in priors probabiliies coudl still lead to difference 
in evidential strength simply due to difference in sensitivity. But note that this argument 
assumes that sensitivity---or specificity---have nothign to do with prior probabilities.

Intuitively, the strength of the evidence should not depend on the prior probability of the hypothesis, but solely on the quality of the evidence itself. The prior probability of the hypothesis seems extrinsic to the quality of the evidence since the latter should solely depend on the sensitivity and specificity of the evidence relative to the hypothesis of interest. Strength of evidence determines how much the evidence changes, upwards or downwards, the probability of a hypothesis. However, as the prior probability increases, the smaller the impact that the evidence will have on the probability of the hypothesis. If the prior is  close to one, the evidence would have marginal if not null impact. But this does not mean that the evidence weakens as the prior probability of the hypothesis goes up. For consider the same hypothesis which in one context has a very high prior probability and in another has a moderate prior probability (say a disease is common in a  population but rare in another). The outcome of the same diagnostic test (say a positive test result) performed on two people, each drawn from two populations, should not count as stronger evidence in one case than in the other. After all, it is the same test that was performed and thus the quality of the evidence should be the same.  For just on eitem of evidence, Bayes factor does not capture this intuition, but the likelihood ratio does, which can be considered an argument in favor of the latter and against the former measure of evidenetial support. However, we have seen that, for more than one item of evidence, teh Bayes factor as well as the likelihood ratio are prior dependent.  

Interestingly, the intuition that characteristics of the evidence such as sensitivity and specificity should be indepedent of the probability of the hypothesis of iterest turns out to be incorrect empirically, however. 
One study in the medical literature has shown, surprisingly, that the sensitivity of 
a diagnostic test is independent of the prior of the hypothesis beign tested---say whether the patient 
has a medical condition. However, specificity is dependent on the prior of the hypothesis:

\begin{quote}
Overall, specificity tended to be
lower with higher disease prevalence; there
was no such systematic effect for sensitivity (page E537).
Source: Variation of a test’s sensitivity and specificity with disease prevalence Mariska M.G. Leeflang, Anne W.S. Rutjes, Johannes B. Reitsma, Lotty Hooft and Patrick M.M. Bossuyt
CMAJ August 06, 2013 185 (11) E537-E544; DOI: https://doi.org/10.1503/cmaj.121286
\end{quote}

\noindent
The authors of the study, however, caution that 

\begin{quote}
 Because sensitivity is estimated in
people with the disease of interest and specificity
in people without the disease of interest, changing the relative number of people 
with and without the disease of interest should not introduce
systematic differences. Therefore, the effects that
we found may be generated by other mechanisms that affect 
both prevalence and accuracy.
\end{quote}

So, according to the authors, changes in prevalence need not directly affect specificity since variations in prevalence and variation in specificity may have a common cause. 
Our earlier calculations about combined specificity and sensitivity agree with experimental 
results, namely, only specificity depends on the priors. Our calculations, in fact, show that different priors for the individual claim do affect specificity. The variation of specificity 
in the result of of splitting the negation of the composite hypthesis $\neg (A \wedge B)$ into three further scenarios,
$\neg A \wedge B$, $B \wedge \neg A$ and $\neg A \wedge neg B$. This prior sensitivity, of course, only applies to composite hyotheses, but to some extent, any hypothesis can be analyzed as a composite hypothesis. The claim that the defedant was running down 5th avnue can be broken down in the conjuction that the defendant was running and that the defendant was at 5th avenue. Any claim, under some level of description, is a composite hypothesis. 
So, perhpas, the quality or strength 
of the evidence should depend on the the priors whether the hypothesis is composite or not. 
Is this another example of base rate neglect? \todo{M: Might be good to have a simulation here that makes vidid why combined specificity is in fact dependent on the priors. Maybe it is, after all, a fallacy 
to think that the quality/strength of the evidence should be independent of the priors.}


 Let's grant that the quality of the evidence should depend, contrary 
 to our initial intuition, on the prior probability of the hypotheses. 
 If that is so, it would not be natural to see that evidence -- the same evidence -- strongly 
 favors $A\wedge B$ without strongly favoring $A$ or $B$. Perhaps we can make sense of this 
 if we keep in mind the comparison between hypothesis we are making here. 
 
 NOT SURE HOW TO CONTIBUE HERE THOUGH!
 
 TO DO:

1. NOTE THAT EVEN BY EQUALIZING PRIORS, TE DISTRBUTION PARADOX DOES NOT GO AWAY. 
SO WHAT ELSE IS GOING ON HERE? NEED TO MAKE COMPATISON BETWEEN HYPOTHESES. NEED TO FIGURE THIS OUT!

 2. TRY TO MAKE SENSE OF THIS, IT IS INTITVELY ACCEPTABLE THAT SUPPORT FOR COMBINED CLAIM, EVEN HOLDING FIXED THE SAME EVIDENCE, SHOULD BE STRONGER THEN SUPPORT FOR INDIVIDUAL CLAIM? THAT IS CLEARLY ODD AND GOES AGAINST COMMON ASSUMPTIONS.
 


## Which Measure of (Combined) Evidential Support?

THIGNS TO ADD:

1. THE MIN SEEMS TO BE THE MEASURE FOR COMPSUTE CLAIMS TAHT CAPTURE AGGREGATION AND DISRIBUTION BEST. 
SO THE QUESTION IS WHAT PROBABILISTIC MEASURE CAPTURES MIN?

2. CAN USE LR SEEMS IT IS INDEFFERENCT TO PRIORS, BUT THIS IS NOT THE CASE FOR THE COMPOSITE CLAIM. SO SAME PROBLEM AS WITH BF.

3. IS DEPEDENCY ON PRIOR ANOTHER EXAMPLE OF BASE RATE NEGLECT. WE NEGLECT BAE RATE IN CALCULATING POSTERIOR BUT ALSO IN CALCULATING STRENGHT OF EVIDENCE? WE JUST NEED TO LIVE WITH THE FACT THAT WE HAVE A POOR UNDERSTAND OF EVIDENE THOUGHT PERHPAS GOOD ENOUGH TO GET BY IN THE WORLD. CONNECT TO POINT 1 AND MIN FUNCTION.



5. ANY DIFFERENCE BETWEEN AREA UNDER TEH CURVE REPRESENTATION OF EVIDENCE STRENGHT AND LR. WHAT IF WE DECRESES SENSITIVITY AND SPECIFICITY. WHAT WOULD HAPPEN TO THE TWO MEASURES. 

 
# Revising the Holistic Approach

The other route for legal probabilist is to reject aggregation and 
hold on to distribution. This can be accomplished by understading the 
standard of proof as a posterior probability threshold. Following Dawid, rejecting aggregation does 
not make legal probabilism an unworkable theory. The conjunction paradox begins with the assumption 
that each element of the of the alleged worongful act has been established by the required standard. Then, one 
shows that it does not follow that the conjunction has been established by the required standard. Aggreation fails. This is teh paradox. But the force of this paradox---as Dwid showed---can be limited by noting 
that each element will be established by higher probability than the conjunction 
because each element will have a higher prior probability, other things being equal.
We think this is the right strategy. We offer in this section a revised version 
of the holistic approahc elaborating on Dawid's initial insights. 

So far we have assumed the most natural probabilistic interpretation of proof standards, one that 
posits a threshold on the posterior probabilities of a generic hypothesis such as guilt or civil liability. In criminal cases, the requirement is formulated as follows: guilt is proven beyond a reasonable doubt provided  $\Pr(G | E)$ is above a suitable threshold, say 95\%. The threshold is lower in civil trials. Civil liability is proven by preponderance provided  $\Pr(L | E)$ is above a suitable threshold, say 50\%. The general claim $G$ or $L$ is split into several consituent claims, depending on the defintion of the wrongful act in the governing law. But this framing misses a crucial ingredient. 

The claim that the defendant is guilty or civilly liable can be replaced by a more fine-grained hypothesis, call it $H_p$, the hypothesis put foward by the prosecutor (or the plaintiff in a civil case), for example, the hypothesis 
that the defendant killed the victim with a firearm while bulglarizing the victim's apartment.  $H_p$ can be any hypothesis which, if true, would entail the defendanat is civilly or criminally liable (according to the governing law). Hypothesis $H_p$ is a more precise description of what happened that establishes, if true, the defendant's guilt or civil liability. In defining proof standards, instead of saying -- somewhat generically -- that $\pr{G | E}$ or $\pr{L | E}$ should be above a suitable threshold, a probabilistic interpretation could read: civil or criminal liabbility is proven beyond a reasonable doubt provided $\Pr(H_d | E)$ is above a suitable threshold. 

This variation may appear inconsequential. But we argue -- perhpas surprisingly -- it can address the naked statistical evidence problem and the difficulty about conjunction. We have consudsred the naked statistical evidence problem in another chapter \textbf{REFERENCE TO EARLIER HAPER} . The basic argument was this. Consider the prisoner hypothetical. It is true that the naked statistics make him 99\% likely to be guilty, that is, $\pr{G | E_s}$. It is 99\% likely that he is one the prisoners who attacked and killed the guard. Notice that this a generic claim. It is odd for the prosecution to simply assert that the prisoner was one of those who killed the guard, without saying what he did, how he partook in the killing, what role he played in the attack, etc. If the prosecution offerred a more specific incriminating hypothesis, call it $H_p$, the probability $\pr{H_p | E_{s}}$ of this hypothesis based on the naked statistical evidence $E_s$ would be well below 99\%, even though $\pr{G | E_s}=99\%$. The fact the prisoner on trial is most likely guilty is an artifact of the choice of a generic hypothesis $G$. When this hypthesis is made more specific -- as it should be -- this probability drops significantly. 

The same approach can address the difficulty about conjunction. The tacic assumption is that prosecutors or plaintiffs should establish each element in isolation. If they manage to prove each element to the desired standard, they have meet their burden. This is artificial. What prosecutor or plaintiffi should do instead is to establish a specific hypothesis from which the constitutent elements follow (almost) deductively. To illustrate, consider a Washington statute about negligent driving:

\begin{quote}
(1)(a) A person is guilty of negligent driving in the first degree if he or she operates a motor vehicle in a manner that is both negligent and endangers or is likely to endanger any person or property, and exhibits the effects of having consumed liquor or marijuana or any drug or exhibits the effects of having inhaled or ingested any chemical, whether or not a legal substance, for its intoxicating or hallucinatory effects. RCW 46.61.5249
\end{quote}

\noindent
In other words, a prosecutor who wishes to establish beyond a reasonable doubt that the 
defendant is guilty of negligent driving should establish:

\begin{quote}
(a) that the defendant operated a vehicle
(b) that, in operating a vehicle, the defendant did so in  negligente manner
(c) that, in operating a vechicle, the defendant did so in a manner likely to endanger a person pr property
(d) that the defendant -- presumably, immediately after the incident -- exihibited the signs of intoxication by liquor or drugs
\end{quote}

\noindent
It would be odd for the prosecutor to go about establishig each claim in isolation, especially because 
these four claims are specific to a time and place. Clearly, the prosecutor cannot simply establish that the defendant was operating a vehicle at some point in time and at some other point in time the defedant exhibited signs of intoxication.
That would establish nothing. The elements to be established must be combined in a coherent spatio-temporal narrative. So establishing them in isolation makes little sense. Prosecutors could establish, say, that the defendant was driving on a busy Highway 1 north San Francisco at about 8:30 PM; the car was moving erratically left and right, cutting across other lanes; the defedant was stopped by a police officer who conducted a field sobriety test and tested alchoool level with a brethylizer, both tests showing a higher-than-normal quantited of alchool ingested. This narrative, if true, establishes each element of the offense. The prosecutor's task would be to establish the narrative as a whole. Presumably, the prosecutor could establish item (a) and (b) separately, but then, would have to show that they happen as part of the same driving episode. So they would have to be connected through a narratie. The same applies to the other elements of the offense. 

Suppose the prosecutor has established a narrative $N$ to a very high 
probability, say above the required threshold for proof beyond a reaosnabke doubt.  Then,

\[\text{ $\pr{C_a\wedge C_b \wedge C_c\wedge C_d \vert N}=\pr{C_i \vert N}=1$ for any $i=\{a, b, c, d\}$}.\]

Both direction of the conjunction principle, aggregation and distribitin, are now trivially 
satisfied. Once we condition on the narrative $N$, each individual claim has a probability of one and thus their conjunction also has a probability of one. The narrartive, however, has a probability short of one, up to 
whatever value is required to meet the governing standard of proof.

To be sure, not all wrongoful acts, in civil or criminal cases, require the prosecutor 
or the plaintiff to establish a spatio-temporal narrative.  It might not be necessary to show that all elements of an offense occurred at the same point in time or in close succession one after the other. Some wrongful acts may consist of a pattern of acts that stretches for several days, months or even years. There may be temporal and spatial gaps that cannot not be filled. Be that as it may, an accusation of wrongdoing in a criminal or civil case should still have a degree of cohesive unity. The  acts and occurences that constitute the wrongdoing should belong to the same wrongful act. It is this unity which the plaintiff and the prosecution must establish when they make their case. One way to establish this unity is by providing a unifying narrative, but this need not be the only way. Perhaps the expresion 'theory' or 'explanation' are better and more general than 'narrative'.

One upshot of this holistic approach is that it higlights a distinction that is often made, between statements of fact and  statements of law. That the defendant was driving on highway 1 and the car was moving erratically left and right is a statement of fact. That such an occurrence counts as `negligent driving' is a statement of law.  The instance 'car moving erratically' is subsumed under the category 'driving erratically'. A defense lawyer could challange the inference. Perhaps the car was moving erratically because of other reasons, say, mulfunctioning of the brakes due to manifacturing defects not attributable to the driver.




































# The likelihood strategy 

Focusing on 
posterior probabilities is not the only approach that legal probabilists 
can pursue. By Bayes' theorem, the following holds, using $G$ and $I$ 
as competing hypotheses:

\[ \frac{\Pr(G | E)}{\Pr(I | E)} = \frac{\Pr(E | G)}{\Pr(E | I)} \times \frac{\Pr(G)}{\Pr(I)},\]

or using $H_p$ and $H_d$ as competing hypotheses,


\[ \frac{\Pr(H_p | E)}{\Pr(H_d | E)} = \frac{\Pr(E | H_p)}{\Pr(E | H_d)} \times \frac{\Pr(H_p)}{\Pr(H_d)},\]

or in words

\[ \textit{posterior odds} = \textit{likelihood ratio} \times \textit{prior odds}.\]

A difficult problem is to assign numbers to the prior probabiliteis 
such as $\Pr(G)$ or $\Pr(H_p)$, or priors odds such as $\frac{\Pr(G)}{\Pr(I)}$ or $\frac{\Pr(H_p)}{\Pr(H_d)}$. 

DISCUSS DIFFICULTIES ABOUT ASSIGNING PRIORS! WHERE?
  CAN WE USE IMPRECISE PROBABILKITIES T TALK ABOUT PRIORS -- I.E. LOW PRIORS = TOTAL IGNORANCE = VERY IMPRECISE (LARGE INTERVAL) PRIORS? THE PROBLME WITH THIS WOULD BE THAT THERE IS NO UPDSATING POSSIBLE. ALL UPDATING WOULD STILL GET BACK TO THE STARTING POINT. DO YOU HAVE AN ANSWER TO THAT? WOULD BE INTERETSING TO DISCUSS THIS!

Given these difficulties, both practical 
and theoretical, one option is to dispense with 
priors altogether. This is not implausible. Legal disputes in both 
criminal and civil trials should be decided 
on the basis of the evidence presented by the litigants. But 
it is the likelihood ratio -- not the prior ratio -- that offers the best measure 
of the overall strength of the evidece presented. So it is all too natural to focus on likekihood ratios 
and leave the priors out of the picture. If this is the right, the question is, how would a probabilistic interpretation of standards of proof based on the likelihood rato look like? 
At its simplest, this stratgey will look as follows. Recall our discussion 
of expected utility theory:


\[ \text{convict provided}  		 \frac{cost(CI)}{cost(AG)} < \frac{\Pr(H_p | E)}{\Pr(H_d | E )}, \]

which is equivalent to

\[ \text{convict provided}  		 \frac{cost(CI)}{cost(AG)} < \frac{\Pr(E | H_p)}{\Pr(E | H_d)} \times \frac{\Pr(H_p)}{\Pr(H_d)}.\]

By rearraing the terms,

\[ \text{convict provided}  \frac{\Pr(E | H_p)}{\Pr(E | H_d)} >	\frac{\Pr(H_d)}{\Pr(H_p)} \times 	 \frac{cost(CI)}{cost(AG)} .\]

Then, on this intepretation, the likelihood ratio should be above a suitable threshold 
that is a function of the cost ratio and the prior ratio. The outstanding question 
is how this threshold is to be determined. 


## Kaplow  


Quite independently, a similar approach to juridical decisions has been proposed by  @kaplow2014likelihood -- we'll call it \textbf{decision-theoretic legal probabilism (DTLP)}. It turns out that  Cheng's suggestion is  a particular case of this more general approach. Let $LR(E)=\pr{E\vert H_\Pi}/\pr{E\vert H_\Delta}$. In whole generality, DTLP invites us to convict just in case $LR(E)>LR^\star$, where $LR^\star$ is some critical value of the likelihood ratio. 

 Say we want to formulate the usual preponderance rule: convict iff $\pr{H_\Pi\vert E}>0.5$, that is, iff $\frac{\pr{H_\Pi\vert E}}{\pr{H_\Delta\vert E}}>1$. By Bayes' Theorem we have:
 
 \vspace{-6mm}
 
 \begin{align*}
\frac{\pr{H_\Pi\vert E}}{\pr{H_\Delta\vert E}} =  \frac{\pr{H_\Pi}}{\pr{H_\Delta}}\times \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}} &>1 \Leftrightarrow\\
  \Leftrightarrow \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}} &> \frac{\pr{H_\Delta}}{\pr{H_\Pi}} 
 \end{align*}
 \noindent So, as expected, $LR^\star$ is not unique and depends on priors. Analogous reformulations are available for thresholds other than $0.5$. 

Kaplow's  point is not that we can reformulate threshold decision rules in terms of priors-sensitive likelihood ratio thresholds. Rather, he insists, when we make a decision, we should factor in its consequences. Let $G$ represent potential gain from correct conviction, and $L$ stand for the potential loss resulting from mistaken conviction. Taking them into account, Kaplow suggests, we should convict if and only if:

\vspace{-6mm}

 \begin{align}
\label{eq:Kaplow_decision}
\pr{H_\Pi\vert E}\times G > \pr{H_\Delta\vert E}\times L
\end{align}
\noindent Now, \eqref{eq:Kaplow_decision} is equivalent to:

\vspace{-6mm}


\begin{align}
\nonumber
\frac{\pr{H_\Pi \vert E}}{\pr{H_\Delta \vert E}} & > \frac{L}{G}\\
\nonumber
\frac{\pr{H_\Pi}}{\pr{H_\Delta}} \times \frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}} &> \frac{L}{G}\\
\nonumber
\frac{\pr{E\vert H_\Pi}}{\pr{E\vert H_\Delta}}  & > \frac{\pr{H_\Delta}}{\pr{H_\Pi}} \times \frac{L}{G}\\
\label{eq:Kaplow_decision2} LR(E)  & > \frac{\pr{H_\Delta}}{\pr{H_\Pi}} \times \frac{L}{G}
\end{align}

\noindent This is the general format of Kaplow's decision standard. 



## Dawid



Here is a slightly different perspective, due to @dawid1987difficulty, that also suggests that juridical decisions should be likelihood-based. The focus is  on witnesses for the sake of simplicity. Imagine the plaintiff produces two independent witnesses: $W_A$ attesting to $A$, and $W_B$ attesting to $B$.  Say the witnesses are regarded as $70\%$ reliable and $A$ and $B$ are probabilistically independent, so we infer $\pr{A}=\pr{B}=0.7$ and  $\pr{A\et B}=0.7^2=0.49$. 

  But, Dawid argues,  this is misleading, because to reach this result we misrepresented the reliability of the witnesses: $70\%$ reliability of a witness, he continues, does not mean that if the witness testifies that $A$, we should believe that  $\pr{A}=0.7$. To see his point,  consider two potential testimonies:


\begin{center}
\begin{tabular}
{@{}ll@{}}
\toprule
  $A_1$ & The sun rose today. \\
   $A_2$ & The sun moved backwards through the sky today.\\
\bottomrule
\end{tabular}
\end{center}

\noindent     Intuitively, after hearing them, we would still take $\pr{A_1}$ to be close to 1 and $\pr{A_2}$ to be close to 0, because we already have fairly strong convictions about the issues at hand. In general, how we should revise our beliefs  in light of a testimony depends not only on the reliability of the witness, but also on our prior convictions.\footnote{An issue that Dawid does not bring up is the interplay between our priors and our assessment of the reliability of the witnesses. Clearly, our posterior assessment of the credibility of the witness who testified $A_2$ will be lower than that of the other witness.}  And this is as it should be:  as indicated by Bayes' Theorem,  one and the same testimony with different priors might lead to different posterior probabilities.



 So far so good. But how should we represent evidence (or testimony) strength then? Well, one pretty standard way to go is to  focus on how much it contributes to the change in our beliefs in a way independent of any particular choice of prior beliefs. 
 Let $a$ be the event that the  witness testified that $A$.  It is useful to think about the problem in terms of \emph{odds, conditional odds (O)} and \emph{likelihood ratios (LR)}:
 \begin{align*} O(A)  & = \frac{\pr{A}}{\pr{\n A}}\\
 O(A\vert a) &= \frac{\pr{A\vert a}}{\pr{\n A \vert a}}  \\
 LR(a\vert A) &= \frac{\pr{a\vert A}}{\pr{a\vert \n A}}. 
\end{align*}




Suppose our prior beliefs and background knowledge, before hearing a testimony, are captured by the prior probability measure $\prr{\cdot}$, and the only thing that we learn  is $a$. We're interested in what our \emph{posterior} probability measure, $\prp{\cdot}$, and posterior odds should then be. If we're to proceed with Bayesian updating, we should have:


\vspace{-6mm}


 \begin{align*}
 \frac{\prp{A}}{\prp{\n A}} & = \frac{\prr{A\vert a}}{\prr{\n A\vert a}}
 =
 \frac{\prr{a\vert A}}{\prr{a\vert \n A}}
 \times
 \frac{\prr{A}}{\prr{\n A}}
  \end{align*}
 that is,
 
 \vspace{-6mm}
 
 \begin{align}
 \label{bayesodss2}
 O_{posterior}(A)& = O_{prior}(A\vert a) = \!\!\!\!\!  \!\!\!\!\!  \!\! \!\!  \underbrace{LR_{prior}(a\vert A)}_{\mbox{\footnotesize conditional likelihood ratio}}  \!\!\!\!\!   \!\!\!\!\!  \!\! \!\!   \times  O_{prior}(A)
 \end{align}



  The conditional likelihood ratio seems to be a much more direct measure of the value of $a$, independent of our priors regarding $A$ itself.   In general, the posterior probability of an event will equal to the witness's reliability in the sense introduced above only if the prior is $1/2$.\footnote{Dawid gives no general argument, but it is not too hard to  give one. Let $rel(a)=\pr{a\vert A}=\pr{\n a\vert \n A}$. We have in the background $\pr{a\vert \n A}=1-\pr{\n a\vert \n A}=1-rel(a)$.
 We want to find the condition under which $\pr{A\vert a} = \pr{a\vert A}$. Set $\pr{A}=p$ and  start with Bayes' Theorem and the law of total probability, and go from there:
 \begin{align*}
 \pr{A\vert a}& = \pr{a\vert A}\\
 \frac{\pr{a\vert A}p}{\pr{a\vert A}p+\pr{a\vert \n A}(1-p)} &= \pr{a\vert A} \\
 \pr{a\vert A}p & = \pr{a\vert A}[\pr{a\vert A}p+\pr{a\vert \n A}(1-p)]\\
 p & = \pr{a\vert A}p + \pr{a\vert \n A} - \pr{a\vert \n A}p\\
 p &= rel(a) p + 1-rel(a)- (1-rel(a))p\\
 p & = rel(a)p +1 - rel(a) -p +rel(a)p \\
 2p & =  2rel(a)p + 1 - rel(a)  \\
 2p - 2 rel(a)p & = 1-rel(a)\\
 2p(1-rel(a)) &= 1-rel(a)\\
 2p & = 1
 \end{align*}

\noindent  First we multiplied both sides by the denominator. Then we divided both sides by $\pr{a\vert A}$ and multiplied on the right side. Then we used our background notation and information. Next, we manipulated the right-hand side algebraically and  moved  $-p$ to the left-hand side. Move $2rel(a)p$ to the left and manipulate the result algebraically to get to the last line.}




















## Likelihood and DAC

 But how does our preference for the likelihood ratio as a measure of evidence strength relate to DAC? Let's go through Dawid's reasoning. 

 A sensible  way to probabilistically interpret the  $70\%$ reliability of a witness who testifies that $A$  is to take it to consist in the fact that the probability of a positive testimony  if $A$ is the case, just as the probability of a negative testimony  (that is, testimony that $A$ is false) if $A$ isn't the case, is 0.7:\footnote{In general setting, these are called the \emph{sensitivity} and \emph{specificity} of a test (respectively), and they don't have to be equal. For instance, a degenerate test for an illness which always responds positively, diagnoses everyone as ill, and so has sensitivity 1, but specificity 0.} 
  \[\prr{a\vert A}=\prr{\n a\vert\n  A}=0.7.\]
 \noindent   $\prr{a\vert \n A}=1- \prr{\n a\vert \n A}=0.3$, and so the same information is encoded in the appropriate likelihood ratio:
 \[LR_{prior}(a\vert A )=\frac{\prr{a\vert A}}{\prr{a\vert \n A}}= \frac{0.7}{0.3}\] 


 Let's say that $a$ \emph{provides (positive) support} for $A$ in case 
 \[O_{posterior}(A)=O_{prior}(A\vert a)> O_{prior}(A)\]
 \noindent  that is, a testimony $a$ supports $A$ just in case the posterior odds of $A$ given $a$ are greater than the prior odds of $A$ (this happens just in case $\prp{A}>\prr{A}$). By \eqref{bayesodss2}, this will be the case if and only if $LR_{prior}(a\vert A)>1$.




 One question that Dawid addresses is this: assuming reliability of witnesses  $0.7$, and assuming that  $a$ and $b$, taken separately, provide positive support for their respective claims, does it follow that  $a \et b$ provides positive support for $A\et B$?

Assuming the  independence of the witnesses, this will hold  in non-degenerate cases that do not  involve extreme probabilities, on the assumption of independence of $a$ and $b$ conditional on all combinations: $A\et B$, $A\et \n B$, $\n A \et B$ and $\n A \et \n B$.\footnote{Dawid only talks about the independence of witnesses without reference to  conditional independence. Conditional independence does not follow from independence, and it is the former that is needed here (also, four non-equivalent different versions of it).}$^,$~\footnote{In terms of notation and derivation in the optional content that will follow, the claim holds  if and only if $28 > 28 p_{11}-12p_{00}$.  This inequality is not  true for all admissible values of $p_{11}$ and $p_{00}$. If $p_{11}=1$ and $p_{00}=0$, the sides are equal. However, this is a rather degenerate example. Normally, we are  interested in cases where $p_{11}< 1$. And indeed, on this assumption, the inequality holds.}



Let us see why the above claim holds. The calculations are my reconstruction and are not due to Dawid. The reader might be annoyed with me working out the mundane details of Dawid's claims, but it turns out that in the case of Dawid's strategy, the devil is in the details. The independence of witnesses gives us:
\begin{align*}
 \pr{a \et b \vert A\et B}& =0.7^2=0.49\\
 \pr{a \et b \vert A\et \n B}& =  0.7\times 0.3=0.21\\
 \pr{a \et b \vert \n A\et B}& =  0.3\times 0.7=0.21\\
 \pr{a \et b \vert \n A\et \n B}& =  0.3\times 0.3=0.09
 \end{align*}
  Without assuming $A$ and $B$ to be independent, let the probabilities of $A\et B$, $\n A\et B$, $A\et \n B$, $\n A\et \n B$ be $p_{11}, p_{01}, p_{10}, p_{00}$. First, let's see what $\pr{a\et b}$ boils down to.

By the law of total probability we have:
 \begin{align}\label{eq:total_lower}
 \pr{a\et b} & = 
                     \pr{a\et b \vert A \et B}\pr{A\et B} + \\ &  \nonumber
                     +\pr{a\et b \vert A \et \n B}\pr{A\et \n B} \\ &  \nonumber
 + \pr{a\et b \vert \n A \et B}\pr{\n A\et B} + \\ & \nonumber
                     + \pr{a\et b \vert \n A \et \n B}\pr{\n A\et \n B}
 \end{align}
 \noindent which, when we substitute our values and constants, results in:
 \begin{align*}
                     & = 0.49p_{11}+0.21(p_{10}+p_{01})+0.09p_{00}
 \end{align*}
 Now, note that because $p_{ii}$s add up to one, we have $p_{10}+p_{01}=1-p_{00}-p_{11}$. Let us continue.
 \begin{align*}
    & = 0.49p_{11}+0.21(1-p_{00}-p_{11})+0.09p_{00} \\
                     & = 0.21+0.28p_{11}-0.12p_{00}
 \end{align*}
 
 Next, we ask what the posterior of $A\et B$ given $a\et b$ is (in the last line, we also multiply the numerator and the denominator by 100).
 \begin{align*}
 \pr{A\et B\vert a \et b} & =
         \frac{\pr{a\et b \vert A \et B}\pr{A\et B}}
             {\pr{a\et b}}\\
         & =
                     \frac{49p_{11}}
                           {21+28p_{11}-12p_{00}} 
         \end{align*}

 In this particular case, then, our question whether $\pr{A\et B\vert a\et b}>\pr{A\et B}$ boils down to asking whether
 \[\frac{49p_{11}}{21+28p_{11}-12p_{00}}> p_{11}\]
 that is, whether $28 > 28 p_{11}-12p_{00}$ (just divide both sides by $p_{11}$, multiply by the denominator, and manipulate algebraically). 




 Dawid continues working with particular choices of values and provides neither a general statement of the fact that the above considerations instantiate nor a proof of it. In the middle of the paper he says: 
 \begin{quote}
 Even under prior dependence, the combined support is always positive, in the sense that the posterior probability of the case always exceeds its prior probability\dots When the problem is analysed carefully, the `paradox' evaporates [pp. 95-7]\end{quote}
 \noindent where he  still means the case with the particular values that he has given, but he seems to suggest that the claim generalizes to a large array of cases. 


 The paper does not contain a precise statement making the conditions required explicit and, \emph{a fortriori}, does not contain a proof of it.
Given the example above and Dawid's informal reading, let us develop a more precise statement of the claim and  a proof thereof. 


\begin{fact}\label{ther:increase}
Suppose that  $rel(a),rel(b)>0.5$ and witnesses are independent conditional on all Boolean combinations of $A$ and $B$  (in a sense to be specified), and that none of the Boolean combinations of $A$ and $B$ has an extreme probability (of 0 or 1). It follows that  $\pr{A\et B \vert a\et b}>\pr{A\et B}$. (Independence of $A$ and $B$ is not required.)
\end{fact}


Roughly, the theorem says that if independent and reliable witnesses provide positive  support of their separate claims, their joint testimony provides positive support of the conjunction of their claims. 




Let us see why the claim holds. First, we introduce an abbreviation for witness reliability: 
  \begin{align*}\mathbf{a} &=rel(a)=\pr{a\vert A}=\pr{\n a\vert \n A}>0.5\\ 
\mathbf{b} &=rel(b)=\pr{b\vert B}=\pr{\n b\vert \n A}>0.5
\end{align*}
Our independence assumption means:
\begin{align*}
\pr{a\et b \vert A\et B}  &= \mathbf{ab}\\
\pr{a\et b \vert A\et \n B} & = \mathbf{a(1-b)}\\
\pr{a\et b \vert \n A\et B}  & = \mathbf{(1-a)b}\\
\pr{a\et b \vert \n A\et \n  B}  & = \mathbf{(1-a)(1-b)}
\end{align*}

\vspace{-2mm}

Abbreviate the probabilities the way we already did:

\begin{center}
\begin{tabular}{ll}
$\pr{A\et B} = p_{11}$ & $\pr{A\et \n B} = p_{10}$\\
$\pr{\n A \et B} = p_{01}$ & $\pr{\n A \et \n B}=p_{00}$
\end{tabular}
\end{center}
Our assumptions entail $0\neq p_{ij}\neq 1$ for $i,j\in \{0,1\}$ and:
\begin{align}\label{eq:sumupto1}
p_{11}+p_{10}+p_{01}+p_{00}&=1
\end{align}

\noindent So, we can use this with \eqref{eq:total_lower} to get:
\begin{align}\label{eq:aetb}
\pr{a\et b} & =  \mathbf{ab}p_{11} + \mathbf{a(1-b)}p_{10}+\mathbf{(1-a)b}p_{01} + \mathbf{(1-a)(1-b)}p_{00}\\ \nonumber
& = p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})
\end{align}

Let's now work out what the posterior of $A\et B$ will be, starting with an application of the Bayes' Theorem:
\begin{align} \nonumber
\pr{A\et B \vert a\et b} & = \frac{\pr{a\et b \vert A \et B}\pr{A\et B}}{\pr{a\et b}}
\\ \label{eq:boiled}
& = \frac{\mathbf{ab}p_{11}}{p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})}
\end{align}
To answer our question we therefore have to compare the content of \eqref{eq:boiled} to $p_{11}$ and our claim holds just in case:
\begin{align*}
\frac{\mathbf{ab}p_{11}}{p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})} &> p_{11}
\end{align*}
\begin{align*}
 \frac{\mathbf{ab}}{p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})} & > 1\end{align*}
 \begin{align}  
 \label{eq:goal}
p_{11}\mathbf{ab} + p_{10}(\mathbf{a}-\mathbf{ab}) + p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab}) & < \mathbf{ab}
\end{align}
Proving \eqref{eq:goal} is therefore our goal for now. This is achieved by the following reasoning:\footnote{Thanks to Pawel Pawlowski for working on this proof with me.}



\hspace{-7mm}
\resizebox{13.5cm}{!}{
\begin{tabular}{llr}
 1. & $\mathbf{b}>0.5,\,\,\, \mathbf{a}>0.5$ & \mbox{assumption}\\
 2. & $2\mathbf{b}>1,\,\,\, 2\mathbf{a}> 1$ & \mbox{from 1.}\\
 3. & $2\mathbf{ab}>\mathbf{a},\,\,\, 2\mathbf{ab}>\mathbf{b}$ & \mbox{multiplying by $\mathbf{a}$ and $\mathbf{b}$ respectively}\\
 4.  & $p_{10}2\mathbf{ab}>p_{10}\mathbf{a}$,\,\,\, $p_{01}2\mathbf{ab}>p_{01}\mathbf{b}$ & \mbox{multiplying by $p_{10}$ and $p_{01}$ respectively}\\
 5.  & $p_{10}2\mathbf{ab} +  p_{01}2\mathbf{ab} > p_{10}\mathbf{a} + p_{01}\mathbf{b}$ & \mbox{adding by sides, 3., 4.}\\
 6. & $1- \mathbf{b}- \mathbf{a} <0$ & \mbox{from 1.}\\
 7. & $p_{00}(1-\mathbf{b}-\mathbf{a})<0$ & \mbox{From 6., because $p_{00}>0$}\\
  8.  &  $p_{10}2\mathbf{ab} +  p_{01}2\mathbf{ab} > p_{10}\mathbf{a} + p_{01}\mathbf{b} + p_{00}(1-\mathbf{b}-\mathbf{a})$ & \mbox{from 5. and 7.}\\
  9.  & $p_{10}\mathbf{ab} + p_{10}\mathbf{ab} + p_{01}\mathbf{ab} + p_{01}\mathbf{ab} + p_{00}\mathbf{ab} - p_{00}\mathbf{ab}> p_{10}\mathbf{a} + p_{01}\mathbf{b} + p_{00}(1-\mathbf{b}-\mathbf{a})$ & \mbox{8., rewriting left-hand side}\\
  10.  & $p_{10}\mathbf{ab} + p_{01}\mathbf{ab}  + p_{00}\mathbf{ab} > - p_{10}\mathbf{ab}  -  p_{01}\mathbf{ab} + p_{00}\mathbf{ab} +  p_{10}\mathbf{a} + p_{01}\mathbf{b} + p_{00}(1-\mathbf{b}-\mathbf{a})$ &  \mbox{9., moving from left to right}\\
11. & $\mathbf{ab}(p_{10}+p_{01}+p_{00})> p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{10., algebraic manipulation}\\
12. & $\mathbf{ab}(1-p_{11})> p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{11. and equation \eqref{eq:sumupto1}}\\
13. & $\mathbf{ab}- \mathbf{ab}p_{11}> p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{12., algebraic manipulation}\\
14. & $\mathbf{ab}> \mathbf{ab}p_{11}+ p_{10}(\mathbf{a}-\mathbf{ab})+p_{01}(\mathbf{b}-\mathbf{ab})+p_{00}(1-\mathbf{b}-\mathbf{a}+\mathbf{ab})$ & \mbox{13., moving from left to right}\\
\end{tabular}}
%\end{adjustbox}

\vspace{1mm}

The last line is what we have been after.

\intermezzob





Now that we have as a theorem an explication of what Dawid informally suggested, let's see whether it helps the probabilist handling of DAC. 


## Kaplow

 On RLP, at least in certain cases, the decision rule leads us to \eqref{eq:Cheng:compar2}, which tells us to decide the case based on whether the likelihood ratio is greater than 1.



\footnote{Again, the name of the view is by no means standard, it is  just a term I coined to refer to various types of legal probabilism in a fairly uniform manner.}  While Kaplow did not discuss DAC or the gatecrasher paradox, it is only fair to evaluate Kaplow's proposal from the perspective of these difficulties. 



Add here stuff from Marcello's Mind paper about the prisoner hypothetical. 
Then, discuss Rafal's critique of the likelihood 
ratio threshold and see where we end up. 



# Challenges (again)


## Likelihood ratio and the problem of the priors










## Dawid's likelihood strategy doesn't help

 Recall that DAC was a problem posed for the decision standard proposed by TLP, and the real question is how the information resulting from Fact \ref{ther:increase} can help to avoid that problem.  Dawid does not mention any decision standard, and so addresses quite a different question, and so it is not clear that  ``the `paradox'  evaporates'', as Dawid suggests.

 What Dawid correctly suggests (and we establish in general as Fact \ref{ther:increase})  is that  the support of the conjunction by two witnesses will be positive as soon as their separate support for the conjuncts is positive. That is, that the posterior of the conjunction will be higher that its prior. But  the critic of probabilism never denied that the conjunction of testimonies might raise the probability of the conjunction if the testimonies taken separately support the conjuncts taken separately. Such a critic can still insist that Fact \ref{ther:increase} does nothing to alleviate her concern.  After all, at least \emph{prima facie} it still might be the case that:
\begin{itemize}
\item  the posterior probabilities of the conjuncts are above a given threshold,
\item   the posterior probability of the conjunction is higher than the prior probability of the conjunction,
\item   the posterior probability of the conjunction 
 is still below the threshold.
\end{itemize}
That is, Fact \ref{ther:increase} does not entail that once the conjuncts satisfy a decision standard, so does the conjunction. 



At some point, Dawid makes a  general claim that is somewhat stronger than the one already cited:
 \begin{quote} When the problem is analysed carefully, the `paradox' evaporates: suitably measured, the support supplied by the conjunction of several independent testimonies exceeds that supplied by any of its constituents.

  [p. 97]\end{quote}
This is quite a different claim from the content of Fact \ref{ther:increase}, because previously the joint probability was claimed only to increase as compared to the prior, and here it is claimed to increase above the level of the separate increases provided by separate testimonies. Regarding this issue Dawid elaborates (we still use the $p_{ij}$-notation that we've already introduced):
 \begin{quote}
 ``More generally, let $\pr{a\vert A}/\pr{a\vert \n A}=\lambda$, $\pr{b\vert B}/\pr{b\vert \n B}=\mu$, with $\lambda, \mu >0.7$, as might arise, for example, when there are several available testimonies. If the witnesses are
  independent, then \[\pr{A\et B\vert  a\et b} = \lambda \mu p_{11}/(\lambda \mu p_{11} + \lambda p_{10} +\mu p_{01} + p_{00})\] which  increases with
 each of $\lambda$ and $\mu$, and is never less than the larger of $\lambda p_{11}/(1-p_{11}+\lambda p_{11}),
 \mu p_{11} /(1- p_{11} 1 + \mu p_{11})$, the posterior probabilities appropriate to the individual testimonies.'' [p. 95]
 \end{quote}
This claim, however, is false.

\intermezzoa


Let us see why.   The quoted passage is a bit dense. It contains four claims for which no arguments are given in the paper. The first three are listed below as \eqref{eq:lambdamu}, the fourth is that if the conditions in \eqref{eq:lambdamu} hold,  $\pr{A\et B\vert  a\et b}>max(\pr{A\vert a},\pr{B\vert b})$.  Notice that $\lambda=LR(a\vert A)$ and $\mu=LR(b\vert B)$. Suppose the first three claims hold, that is:
 \begin{align}\label{eq:lambdamu}
 \pr{A\et B\vert  a\et b} &= \lambda \mu p_{11}/(\lambda \mu p_{11} + \lambda p_{10} +\mu p_{01} + p_{00})\\
 \pr{A\vert a} & = \frac{\lambda p_{11}}{1-p_{11}+\lambda p_{11}}\nonumber \\
 \pr{B\vert b} & = \frac{\mu p_{11}}{1-p_{11}+\mu p_{11}} \nonumber 
 \end{align}
 \noindent Is it really the case that  $\pr{A\et B\vert  a\et b}>\pr{A\vert a},\pr{B\vert b}$? It does not  seem so. Let $\mathbf{a}=\mathbf{b}=0.6$, $pr =\la p_{11},p_{10},p_{01},p_{00}\ra=\la 0.1, 0.7, 0.1, 0.1 \ra$. Then, $\lambda=\mu=1.5>0.7$ so the assumption is satisfied. 
Then we have $\pr{A}=p_{11}+p_{10}=0.8$, $\pr{B}=p_{11}+p_{01}=0.2$. We can also easily compute $\pr{a}=\mathbf{a}\pr{A}+(1-\mathbf{a})\pr{\n A}=0.56$ and $\pr{b}=\mathbf{b}\pr{B}+(1-\mathbf{b})\pr{\n B}=0.44$. 
 Yet:

 \begin{align*}
 \pr{A\vert a} & = \frac{\pr{a\vert A}\pr{A}}{\pr{a}} = \frac{0.6\times 0.8}{0.6\times 0.8 + 0.4\times 0.2}\approx 0.8571 \\
 \pr{B\vert b} & = \frac{\pr{b\vert B}\pr{B}}{\pr{b}} = \frac{0.6\times 0.2}{0.6\times 0.2 + 0.4\times 0.8}\approx 0.272 \\
 \pr{A\et B \vert a \et b} & = \frac{\pr{a\et b\vert A \et B}\pr{A\et B}}{\splitfrac{\pr{a\et b \vert A\et B}\pr{A\et B}+
   \pr{a\et b\vert A\et \n B}\pr{A\et \n B} +}{+ 
 \pr{a\et b \vert \n A \et B}\pr{\n A \et B} + \pr{a\et b \vert \n A \et \n B}\pr{\n A \et \n B}}} \\
 & = \frac{\mathbf{ab}p_{11}}{
   \mathbf{ab}p_{11} + \mathbf{a}(1-\mathbf{b})p_{10} + (1-\mathbf{a})\mathbf{b}p_{01} + (1-\mathbf{a})(1-\mathbf{b})p_{00}
 }  
    \approx 0.147
 \end{align*}
The posterior probability of $A\et B$ is not only lower than the larger of the individual posteriors, but also lower than any of them! 

So what went wrong in Dawid's calculations in \eqref{eq:lambdamu}? Well, the first formula is correct. However, let us take a look at what the second one says (the problem with the third one is pretty much the same):
\begin{align*}
\pr{A\vert a } & = \frac{\frac{\pr{a\vert A}}{\pr{\n a \vert A}}\times \pr{A\et B}}{\pr{\n (A\et B)}+ \frac{\pr{a\vert A}}{\pr{\n a \vert A}} \times \pr{A\et B}}
\end{align*}
Quite surprisingly, in Dawid's formula for $\pr{A\vert a}$, the probability of $A\et B$ plays a role. To see that it should not take any $B$ that excludes $A$ and the formula will lead to the conclusion that \emph{always} $\pr{A\vert a}$ is undefined. The problem with Dawid's formula is that instead of $p_{11}=\pr{A\et B}$ he should have used $\pr{A}=p_{11}+p_{10}$, in which case the formula would rather say this:
\begin{align*}
\pr{A\vert a } & = \frac{\frac{\pr{a\vert A}}{\pr{\n a \vert A}}\times \pr{A}}{\pr{\n A}+ \frac{\pr{a\vert A}}{\pr{\n a \vert A}} \times \pr{A}}\\
& = \frac{\frac{\pr{a\vert A}\pr{A}}{\pr{\n a \vert A}}}{\frac{\pr{\n a\vert A}\pr{\n A}}{\pr{\n a\vert A}}+ \frac{\pr{a\vert A}\pr{A}}{\pr{\n a \vert A}}}\\
& = \frac{\pr{a\vert A}\pr{A}}{\pr{\n a\vert A}\pr{\n A} + \pr{a\vert A}\pr{A}}
\end{align*}
Now, on the assumption that witness' sensitivity is equal to their specificity, we have $\pr{a\vert \n A}=\pr{\n a \vert A}$ and can substitute this in the denominator:
 \begin{align*} & = \frac{\pr{a\vert A}\pr{A}}{\pr{ a\vert \n A}\pr{\n A} + \pr{a\vert A}\pr{A}}\end{align*}
and this would be a formulation of Bayes' theorem.  And indeed with $\pr{A}=p_{11}+p_{10}$ the formula works (albeit its adequacy rests on the identity of $\pr{a\vert \n A}$ and $\pr{\n a \vert A}$), and yields the result that we already obtained:
\begin{align*}
\pr{A\vert a} &= \frac{\lambda(p_{11}+p_{10})}{1-(p_{11}+p_{10})+\lambda(p_{11}+p_{10})}\\
&= \frac{1.5\times 0.8}{1- 0.8+1.5\times 0.8} \approx 0.8571
\end{align*}



  The situation cannot be much improved by taking $\mathbf{a}$ and $\mathbf{b}$ to be high. For instance, if they're both 0.9 and $pr=\la0.1, 0.7, 0.1, 0.1 \ra$, the posterior of $A$ is $\approx 0.972$, the posterior of $B$ is $\approx 0.692$, and yet the joint posterior of $A\et B$ is $0.525$.

 The situation cannot also be improved by saying that at least if the threshold is 0.5, then as soon as $\mathbf{a}$ and $\mathbf{b}$  are above 0.7 (and, \emph{a fortriori}, so are $\lambda$ and $\mu$), the individual posteriors being above 0.5 entails the joint posterior being above 0.5 as well. For instance, for $\mathbf{a}=0.7$ and $\mathbf{b}=0.9$
 with $pr= \la 0.1, 0.3, 0.5, 0.1\ra$, the individual posteriors of $A$ and $B$ are $\approx 0.608$ and $\approx 0.931$ respectively, while the joint posterior of $A\et B$ is $\approx 0.283$.



\intermezzob

 The situation cannot be improved by saying that what was meant was rather that the joint likelihood is going to be at least as high as the maximum of the individual likelihoods, because quite the opposite is the case: the joint likelihood is going to be lower than any of the individual ones.

 \intermezzoa

Let us make sure this is the case.  We have: 
 \begin{align*}
 LR(a\vert A) & = \frac{\pr{a\vert A}}{\pr{a\vert \n A}}\\
 &= \frac{\pr{a\vert A}}{\pr{\n a\vert  A}} \\
& =  \frac{\mathbf{a}}{\mathbf{1-a}}.
\end{align*}
where the substitution in the denominator is legitimate only because witness' sensitivity is identical to their specificity. 

With the joint likelihood, the reasoning is just a bit more tricky. We will need to know what $\pr{a\et b \vert \n (A\et B)}$ is. There are three disjoint possible conditions in which the condition holds: $A\et \n B, \n A \et B$, and $\n A \et \n B$. The probabilities of $a\et b$ in these three scenarios are respectively $\mathbf{a(1-b),(1-a)b,(1-a)(1-b)}$ (again, the assumption of independence is important), and so on the assumption $\n(A\et B)$ the probability of $a\et b$ is:
\begin{align*}
\pr{a\et b \vert \n (A\et B)} & = 
\mathbf{a}(1-\mathbf{b})+(1-\mathbf{a})\mathbf{b}+(1-\mathbf{a})(1-\mathbf{b})\\ 
& = 
\mathbf{a}(1-\mathbf{b})+(1-\mathbf{a})(\mathbf{b} + 1-\mathbf{b})\\
& = \mathbf{a}(1-\mathbf{b})+(1-\mathbf{a})\\
& = \mathbf{a}-\mathbf{a}\mathbf{b}+1-\mathbf{a} = 1- \mathbf{a}\mathbf{b}
\end{align*}
So, on the assumption of witness independence, we have:
\begin{align*}
LR(a\et b \vert A \et B) & = \frac{\pr{a\et b \vert A \et B}}{\pr{a \et b\vert \n (A \et B)}} \\
& = \frac{\mathbf{ab}}{\mathbf{1-ab}}
\end{align*}

 With $0<\mathbf{a},\mathbf{b}<1$ we have $\mathbf{ab}<\mathbf{a}$, $1-\mathbf{ab}>1-\mathbf{a}$, and consequently:
 \[\frac{\mathbf{ab}}{\mathbf{1-ab}} < \frac{\mathbf{a}}{\mathbf{1-a}}\]
 which means that the joint likelihood is going to be lower than any of the individual ones.




\intermezzob



  Fact \ref{ther:increase} is so far the most optimistic reading of the claim that if witnesses are independent and fairly reliable, their testimonies are going to provide positive support for the conjunction,\footnote{And this is the reading that Dawid in passing suggests: ``the combined support is always positive, in the sense that the posterior probability of the case always exceeds its prior probability.'' [@dawid1987difficulty: 95] and any stronger reading of Dawid's suggestions fails. But Fact \ref{ther:increase} is not too exciting when it comes to answering the original DAC. The original question focused on the adjudication model according to which the deciding agents are to evaluate the posterior probability of the whole case conditional on all evidence, and to convict if it is above a certain threshold. The problem, generally, is that  it might be the case that  the pieces of evidence for particular elements of the claim can have high likelihood and posterior probabilities of particular elements can be above the threshold while the posterior joint probability will still fail to meet the threshold. The fact that the joint posterior will be higher than the joint prior does not  help much. For instance, if $\mathbf{a}=\mathbf{b}=0.7$, $pr=\la 0.1, 0.5, 0.3, 0.1\ra$, the posterior of $A$ is $\approx 0.777$, the posterior of $B$ is $\approx 0.608$ and the joint posterior is $\approx 0.216$ (yes, it is  higher than the joint prior $=0.1$, but this does not help the conjunction to satisfy the decision standard).




 To see the extent to which Dawid's strategy is helpful here, perhaps the following analogy might be useful.  
 Imagine it is winter, the heating does not work in my office and I am quite cold. I pick up the phone and call maintenance. A rather cheerful fellow picks up the phone. I tell him what my problem is, and he reacts:

 \vspace{1mm}
 \begin{tabular}{lp{10cm}}
 --- & Oh, don't worry. \\
 --- & What do you mean? It's cold in here! \\
 --- & No no, everything is fine, don't worry.\\
 --- & It's not fine! I'm cold here! \\
 --- & Look, sir, my notion of it being warm in your office is that the building provides some improvement to what the situation would be if it wasn't there. And you agree that you're definitely warmer than you'd be if your desk was standing outside, don't you? Your, so to speak, posterior warmth is higher than your prior warmth, right? 
 \end{tabular}
 \vspace{1mm}

 Dawid's discussion is in the vein of the above conversation. In response to a problem with the adjudication model under consideration Dawid simply invites us to abandon thinking in terms of it and to abandon requirements crucial for the model.  Instead, he puts forward a fairly weak notion of support (analogous to a fairly weak sense of the building providing improvement), according to which,  assuming witnesses are fairly reliable,  if separate fairly reliable witnesses provide positive support to the conjuncts, then their joint testimony provides positive support for the conjunction. 

 As far as our assessment of the original adjudication model and dealing  with DAC, this leaves us hanging. Yes, if we abandon the model, DAC does not worry us anymore. But should we? And if we do, what should we change it to, if we do not want to be banished from the paradise of probabilistic methods?  




   Having said this, let me emphasize that Dawid's paper is important in the development of the debate, since it shifts focus on the likelihood ratios, which for various reasons are much better measures of evidential support provided by particular pieces of evidence than mere posterior probabilities. 



Before we move to another attempt at a probabilistic formulation of the decision standard, let us introduce the other hero of our story: the gatecrasher paradox. It is against DAC and this paradox that the next model will be judged. 















\intermezzoa

In fact, Cohen replied to Dawid's paper [@cohen1988difficulty]. His reply, however, does not have much to do with the workings of Dawid's strategy, and is rather unusual. Cohen's first point is that the calculations of posteriors require odds about unique events, whose meaning is usually given in terms of potential wagers -- and the key criticism here is that in practice such wagers cannot be decided. This is not a convincing criticism, because the betting-odds interpretations of subjective probability do not require that on each occasion the bet should really be practically decidable. It rather invites one to imagine a possible situation in which the truth could be found out and asks: how much would we bet on a certain claim in such a situation? In some cases, this assumption is false, but there is nothing in principle wrong with thinking about the consequences of false assumptions. 



Second, Cohen says that Dawid's argument works only for testimonial evidence, not for other types thereof. But this claim is simply false -- just because Dawid used testimonial evidence as an example that he worked through it by no means follows that the approach cannot be extended. After all, as long as we can talk about sensitivity and specificity of a given piece of evidence, everything that Dawid said about testimonies can be repeated \emph{mutatis mutandis}.



Third, Cohen complaints that Dawid in his example worked with rather high priors, which according to Cohen would be too high to correspond to the presumption of innocence. This also is not a very successful rejoinder. Cohen picked his priors in the example for the ease of calculations, and the reasoning can be run with lower priors. Moreover, instead of discussing the conjunction problem, Cohen brings in quite a different problem: how to probabilistically model the presumption of innocence, and what priors of guilt should be appropriate? This, indeed, is an important problem; but it does not have much to do with DAC, and should be discussed separately.




## Problem's with Kaplow's stuff



 Kaplow does not discuss the conceptual difficulties that we are concerned with, but this will not stop us from asking whether DTLP can handle them (and answering to the negative). Let us start with DAC.
 
  Say we consider two claims, $A$ and $B$. Is it generally the case that if they separately satisfy the decision rule, then so does $A\et B$? That is, do the assumptions:
 \begin{align*}
 \frac{\pr{E\vert A}}{\pr{E\vert \n A}}  & > \frac{\pr{\n A}}{\pr{A}} \times \frac{L}{G}\\
 \frac{\pr{E\vert B}}{\pr{E\vert \n B}}  & > \frac{\pr{\n B}}{\pr{B}} \times \frac{L}{G}
 \end{align*}
 \noindent entail
 \begin{align*}
 \frac{\pr{E\vert A\et B}}{\pr{E\vert \n (A\et B)}}  & > \frac{\pr{\n (A\et B)}}{
 \pr{A\et B}} \times \frac{L}{G}?
 \end{align*}

Alas, the answer is negative.

\intermezzoa

This can be seen from the following example.  Suppose a random digit from 0-9 is drawn; we do not know the result; we are  told that the result is $<7$ ($E=$`the result is $<7$'), and  we are to decide whether to accept the following claims:
 \begin{center}
 \begin{tabular}{@{}ll@{}}
 \toprule
 $A$ & the result is $<5$. \\
 $B$  & the result is an even number.\\
 $A\et B$ & the result is an even number $<5$. \\
 \bottomrule
 \end{tabular}
 \end{center}
 Suppose that $L=G$ (this is for simplicity only --- nothing hinges on this, counterexamples for when this condition fails are analogous). First, notice that $A$ and $B$ taken separately satisfy \eqref{eq:Kaplow_decision2}. $\pr{A}=\pr{\n A}=0.5$, $\pr{\n A}/\pr{A}=1$ $\pr{E\vert A}=1$, $\pr{E\vert \n A}=0.4$. \eqref{eq:Kaplow_decision2} tells us to check:
 \begin{align*}
 \frac{\pr{E\vert A}}{\pr{E\vert \n A}}&> \frac{L}{G}\times \frac{\pr{\n A}}{\pr{A}}\\
 \frac{1}{0.4} & > 1
 \end{align*}


 \noindent so, following DTLP, we should accept $A$.  
  For analogous reasons, we should also accept $B$. $\pr{B}=\pr{\n B}=0.5$, $\pr{\n B}/\pr{B}=1$ $\pr{E\vert B}=0.8$, $\pr{E\vert \n B}=0.6$, so we need  to check that indeed:
 \begin{align*}
 \frac{\pr{E\vert B}}{\pr{E\vert \n B}}&> \frac{L}{G}\times \frac{\pr{\n B}}{\pr{B}}\\
 \frac{0.8}{0.6} & > 1 
 \end{align*}

 But now, $\pr{A\et B}=0.3$, $\pr{\n (A \et B)}=0.7$, $\pr{\n (A\et B)}/\pr{A\et B}=2\frac{1}{3}$, $\pr{E\vert A \et B}=1$, $\pr{E\vert \n (A\et B)}=4/7$ and it is false that:
  \begin{align*}
 \frac{\pr{E\vert A \et B}}{\pr{E\vert \n (A\et B)}}&> \frac{L}{G}\times \frac{\pr{\n (A \et B)}}{\pr{A \et B}}\\
 \frac{7}{4} & > \frac{7}{3} 
 \end{align*}

The example was easy, but the conjuncts are probabilistically dependent. One might ask: are there counterexamples that  involve claims which are  probabilistically independent?\footnote{Thanks to Alicja Kowalewska for pressing me on this.} 

Consider an experiment in which someone tosses a six-sided die twice. Let the result of the first toss be $X$ and the result of the second one $Y$. Your evidence is that the results of both tosses are greater than one ($E=: X>1 \et Y>1$). Now, let $A$ say that $X<5$ and $B$ say that $Y<5$.

The prior probability of $A$ is $2/3$ and the prior probability of $\n A$ is $1/3$ and so $\frac{\pr{\n A}}{\pr{A}}=0.5$. Further, $\pr{E\vert A}=0.625$, $\pr{E\vert \n A}= 5/6$ and so $\frac{\pr{E\vert A}}{\pr{E\vert \n A}}=0.75$ Clearly, $0.75>0.5$, so $A$ satisfies the decision standard. Since the situation with $B$ is symmetric, so does $B$. 

 Now, $\pr{A\et B}=(2/3)^2=4/9$ and $\pr{\n (A\et B)}=5/9$. So $\frac{\pr{\n(A\et B)}}{\pr{A\et B}}=5/4$. 
 Out of 16 outcomes for which $A\et B$ holds, $E$ holds in 9, so $\pr{E\vert A\et B}=9/16$. Out of 20 remaining outcomes for which $A\et B$ fails, $E$ holds in 16, so
  $\pr{E\vert \n (A\et B)}=4/5$. Thus, $\frac{\pr{E\vert A\et B}}{\pr{E\vert \n (A\et B)}}=45/64 <5/4$, so the conjunction does not satisfy the decision standard.



\intermezzob


Let us turn to the gatecrasher paradox. 


 Suppose $L=G$ and recall our abbreviations: $\pr{E}=e$, $\pr{H_\Pi}=\pi$. DTLP tells us to convict just in case:
 \begin{align*}
 LR(E) &> \frac{1-\pi}{\pi}
 \end{align*}
 \noindent From \eqref{eq:Cheng_lre} we already now that 
 \begin{align*}
 LR(E) & = \frac{0.991-0.991\pi}{0.009\pi}
 \end{align*}
 \noindent so we need to see whether there are any $0<\pi<1$ for which  
 \begin{align*}
  \frac{0.991-0.991\pi}{0.009\pi} &> \frac{1-\pi}{\pi}
 \end{align*}
 \noindent Multiply both sides first by $009\pi$ and then by $\pi$:
 \begin{align*}
 0.991\pi - 0.991\pi^2 &> 0.09\pi - 0.009\pi^2
 \end{align*}
 \noindent Simplify and call the resulting function $f$:
 \begin{align*}
 f(\pi) = - 0.982 \pi^2 + 0.982\pi &>0 
 \end{align*}
 \noindent The above condition is satisfied for any $0<\pi <1$ ($f$ has two zeros: $\pi = 0$ and $\pi = 1$). Here is  a plot of $f$:

 \includegraphics[width=12cm]{f-gate.png}

 Similarly, $LR(E)>1$ for any $0< \pi <1$. Here is a plot of $LR(E)$ against $\pi$:

 \includegraphics[width=12cm]{lre-gate.png}

\noindent Notice that $LR(E)$ does not go below 1. This means that for $L=G$ in the gatecrasher scenario DTLP wold tell us to convict for any prior probability of guilt $\pi\neq 0,1$.

One might ask: is the conclusion very sensitive to the choice of $L$ and $G$? The answer is, not too much.

\intermezzoa


 How sensitive is our analysis to the choice of $L/G$? Well, $LR(E)$ does not change at all, only the threshold moves. For instance, if $L/G=4$, instead of $f$ we end up with \begin{align*}
 f'(\pi) = - 0.955 \pi^2 + 0.955\pi &>0 
 \end{align*}
 and the function still takes positive values on the interval $(0,1)$. In fact, the decision won't change until $L/G$ increases to $\approx 111$. Denote $L/G$ as $\rho$, and let us start with the general decision standard, plugging in our calculations for $LR(E)$:
\begin{align*}
LR(E) &> \frac{\pr{H_\Delta}}{\pr{H_\Pi}} \rho\\
LR(E) &> \frac{1-\pi}{\pi} \rho \\
\frac{0.991-0.991\pi}{0.009\pi} &> \frac{1-\pi}{\pi} \rho\\
\frac{0.991-0.991\pi}{0.009\pi}\frac{\pi}{1-\pi} &>  \rho\\
\frac{0.991\pi-0.991\pi^2}{0.009\pi-0.009\pi^2} &>  \rho\\
\frac{\pi(0.991-0.991\pi)}{\pi(0.009-0.009\pi)} &>  \rho\\
\frac{0.991-0.991\pi}{0.009-0.009\pi} &>  \rho\\
\frac{0.991(1-\pi)}{0.009(1-\pi)} &>  \rho\\
\frac{0.991}{0.009} &>  \rho\\
110.1111 &>  \rho\\
\end{align*}






  

\intermezzob

 So, we conclude, in usual circumstances, DTLP does not handle the gatecrasher paradox. 








# Probabilistic Thresholds Revised

## Likelihood ratios and naked statistical evidence

## Conjunction paradox and Bayesian networks













# Conclusions





Where are we, how did we get here, and where can we go from here?
 We were looking for a probabilistically explicated condition $\Psi$ such that the trier of fact, at least ideally, should accept any relevant claim (including $G$) just in case $\Psi(A,E)$.

From the discussion that transpired it should be clear that we were looking for a $\Psi$ satisfying the following desiderata:

\begin{description}
\item[conjunction closure] If $\Psi(A,E)$ and $\Psi(B,E)$, then $\Psi(A\et B,E)$.
\item[naked statistics] The account should at least make it possible for convictions based on strong, but naked statistical evidence to be unjustified. 
\item[equal treatment] the condition should apply to any relevant claim whatsoever (and not just a selected claim, such as $G$).
\end{description}


Throughout the paper we focused on the first two conditions (formulated in terms of the difficulty about conjunction (DAC), and the gatecrasher paradox), going over various proposals of what $\Psi$ should be like and evaluating how they fare. The results can be summed up in the following table:


\begin{center}
\footnotesize 
 \begin{tabular}{@{}p{3cm}p{2.5cm}p{4cm}p{3cm}@{}}
\toprule
\textbf{View} & \textbf{Convict iff} & \textbf{DAC} & \textbf{Gatecrasher} \\ \midrule
Threshold-based LP (TLP) & Probability of guilt given the evidence is above a certain threshold & fails & fails \\
Dawid's likelihood strategy & No condition given, focus on $\frac{\pr{H\vert E}}{\pr{H\vert \n E}}$ & - If evidence is fairly reliable, the posterior of $A\et B$ will be greater than the prior.

- The posterior of $A\et B$ can still be lower than the posterior of any of $A$ and $B$.

- Joint likelihood, contrary do Dawid's claim, can also be lower than any of the individual likelihoods. & fails  \\
Cheng's relative LP (RLP)
& Posterior of guilt higher than the posterior of any of the defending narrations & The solution assumes equal costs of errors and independence of $A$ and $B$ conditional on $E$. It also relies on there being multiple defending scenarios individualized in terms of  combinations of literals involving $A$ and $B$. & Assumes that the prior odds of guilt are 1, and that the statistics is not sensitive to guilt (which is dubious). If the latter fails, tells to convict as long as the prior of guilt $<0.991$. \\
Kaplow's decision-theoretic LP (DTLP) &
The likelihood of the evidence is higher than the odds of innocence multiplied by the cost of error ratio & fails & convict if cost ratio $<110.1111$
\end{tabular} 
 \end{center} 


Thus, each account either simply fails to satisfy the desiderata, or succeeds on rather unrealistic assumptions.  Does this mean that a probabilistic approach to legal evidence evaluation should be abandoned? No. This only means that if we are to develop a general probabilistic model of legal decision standards, we have to do better. One promising direction is to go back to Cohen's pressure against \textbf{Requirement 1} and push against it. A brief paper suggesting this direction is [@DiBello2019plausibility], where the idea is that the probabilistic standard (be it a threshold or a comparative wrt. defending narrations) should be applied to the whole claim put forward by the plaintiff, and not to its elements. In such a context, DAC does not arise, but \textbf{equal treatment} is violated. Perhaps, there are independent reasons to abandon it, but the issue deserves further discussion. Another strategy might be to go in the direction of employing probabilistic methods to explicate the narration theory of legal decision standards [@urbaniak2018narration], but a discussion of how this approach relates to DAC and the gatecrasher paradox lies beyond the scope of this paper. 






# References








