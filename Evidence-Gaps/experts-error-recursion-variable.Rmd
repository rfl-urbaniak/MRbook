---
title: 'Multiple Pieces of Evidence With Different Reliability'
author: "Marcello"
date: "March 2023"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r}
library(ggplot2)
library(grid)
library(gridExtra)
```

### The Question

This document contains an R-based implementation that examines whether relying on more items of evidence improves the accuracy of our belief forming processes. The running example is as follows. You go to a dentist who recommends a certain medical procedure for you, say deep cleaning.  You are unsure whether the dentist is right. You might in fact really need a deep cleaning, or you might actually not need one. You ask other dentists for a second opinion. Are you more likely to arrive at the correct belief if you ask more than just one dentist? This is the question. Each dentist gives you a piece of evidence. Any other example would do: expert opinions in court, multiple pieces of evidence, etc. Talks of "experts" and "items of evidence" are interchangeable since an expert provides evidence. 


### Expert report/item of evidence

Let's introduce a few variables:

- w refers to the true state of the world, i.e. you need treatment ('1') or you don't need treatment ('0')

- r1 refers to the sensitivity of the expert, i.e. the conditional probability Pr(expert says "yes you need treatment" / you do actually need treatment)

- r2 refers to the specificity of the expert, i.e. the conditional probability Pr(expert says "yes you don't need treatment" / you don't actually need treatment)

The function 'expert_report' simulates an expert report (in terms of '1' or '0') depeding on its sensitivity ('r1') and specificity ('r2'), the state of the world ('w') and the number of simulated iterations ('s'). By using the function rbinom(s, 1, r) that simulates Bernoulli trials with s the number of trials and r the probability of outcome 1 at each trial, the function 'expert_report' simulates expert reports, as follows:

- expert says '1' with probability r1 and  '0' with probability 1-r1, given the world is in state 1

- expert says '1' with probability 1-r2 and  '0' with probability r2, given the world is in state 0

```{r}
expert_report <- function(r1, r2, w, s) {
  
      if (w == 1)
        return(  rbinom(s, 1, r1)
              )  
      else
        return(  rbinom(s, 1, (1-r2))
              )
      }
```



## Variable expert reliability 


We have assumed that the reliability of experts is fixed by some numbers r1 and r2. Another option is to assume that experts have some reliability that is inaccessible. So the values of r1 and r2 are the result of a random draw inside the function expert itself. This seems a more realistic assumption.




```{r}

expert_report_variable <- function(w, s){
  
  r1 <- sample(30:90, 1)/100
  r2 <- sample(30:90, 1)/100
  expert_report(r1, r2, w, s) 
}

```


Note that we have assumed that sometimes the reliability of the expert can be as low as .03.


Below are examples of the outputs of expert reports, over 100 simulated iterations, under the assumption  that the world is in state 1 (say, you actually need treatment)  or in the state 0 (say, you don't actually need treatment):

```{r}
w <- 1
s <- 100
expert_report_variable(w, s)
prop.table(table(expert_report_variable(w, s)))
```

```{r}
w <- 0
expert_report_variable(w, s)
prop.table(table(expert_report_variable(w, s)))
```

The simulation outcome will be different if we call another expert since experts can have different reliability.

```{r}
s <- 10000
w <- 1
prop.table(table(expert_report_variable(w, s)))
```

```{r}
w <- 0
prop.table(table(expert_report_variable(w, s)))
```



### Posterior odds (updated given expert reports)

Let H be the hypothesis that you need treatment (say, a deep cleaning). Let `prior' denote the initial subjective probability you assign to H prior to having considered any evidence or expert opinion. You want to know the posterior probability of H depending on the expert opinion you have available. This posterior probability is calculated using Bayes's theorem, as follows:

- prior_odds * likehood_ratio = posterior_odds

The prior_odds are calculated from the prior probability assigned in `prior' according to the formula:

- -prior/(prior-1)

The likelihood_ratio is given by the formula (sr1/(1-sr2)) in case the expert says 'yes, you need treatment' (report value '1'), where

- sr1 is the subjective estimate of Pr(expert says "yes you need treatment" / you do actually need entreating); and

- sr2 is the subjective estimate of Pr(expert says "yes you don't need treatment" / you don't actually need treatment).

The likelihood_ratio is given by the formula ((1-sr1)/(sr2)) in case the expert says 'no, you don't need treatment' (report value '0').

Note that there is a conceptual difference between r1 and sr1, as well as between r2 and sr2. For example, it could be that the reliability of an expert is overestimated (so that sr1 > r1 and sr2 > r2) or underestimated (so that sr1 < r1 and sr2 < r2). In many cases, however, the values of r1 and sr1, and r2 and sr2, will be the same. 

The function 'posterior_odds' updates prior_odds to posterior_odds of hypothesis H. The function is recursive so it considers case n==1 when only 1 expert is asked, and cases n+1 with more experts are asked.

Mind the difference: n refers to the number of experts being asked for an opinion, while s refers to the number of iterated simulations. 

An important thing is that each expert in the iteration might not have the same level of reliability. So we use the function expert_report_variable (with r1 and r2 inaccessible and randomly assigned within certain boundaries). 

We keep, however, the values of sr1 and sr2 fixed throughout the process. So the idea is that---objectively speaking---experts have different degrees of reliability (not knowable), but when we assess experts we keep fixed our assessment of their reliability. This seems a realistic assumption, but can also be changed. 

```{r}
posterior_odds <- function(n, w) {
  
         if (n == 1)                              # base case 
            return(
               ifelse(
                  expert_report_variable(w, s) == 1, 
                  ((-prior)/(prior-1))*(sr1/(1-sr2)), 
                  ((-prior)/(prior-1))*((1-sr1)/(sr2))
                    )
                  )
        else                                      # inductive case
            return(
              ifelse(
                expert_report_variable(w, s) == 1, 
                posterior_odds(n-1, w)*(sr1/(1-sr2)), 
                posterior_odds(n-1, w)*((1-sr1)/(sr2))
                    )
                  )
                }    

```

### Posterior probabilities (updated given expert reports)


The function 'odds_to_posterior' gives posterior probabilities from posterior odds using this formula:

- posterior_odds/(1+posterior_odds)


```{r}
odds_to_posterior <- function(n, w) {
  
  odds <- posterior_odds(n, w)
  
  return((odds/(1+odds)))
  }
```


### Probability distributions

We can visually see, by means of density histograms, how the subjective posterior probability assignments, relative to a hypothesis H (say, you need treatment), change as the number of experts being asked increases. Let first make the following numerical assignments to the variables:


```{r}

s <- 10000
prior <- 0.5
sr1 <- 0.7
sr2 <- 0.7

```

In other words, we will be working with 10,000 simulated iterations in which the priors for hypothesis H equal 0.5, and the subjective estimates of r1 and r2, namely sr1 and sr2, are 0.7 and 0.7.


Below are the graphs representing the posterior probability distributions depending on how many experts are asked for an opinion. The red graphs represent worlds in state 1 (say, you need treatment) and the blue graphs represent worlds in state 0 (say, you don't need treatment). 




```{r, echo=FALSE}

prior <- 0.5

post_df <- data.frame(odds_to_posterior(1, 1))
names(post_df) <- ("posterior")
p1 <- ggplot(post_df, aes(x=posterior)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.01, colour="red", fill="white") +
  xlim(0, 1.02) +
  ylim(0, 95) +
  ggtitle("One expert") 

post_df <- data.frame(odds_to_posterior(2, 1))
names(post_df) <- ("posterior")
p2 <- ggplot(post_df, aes(x=posterior)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.01, colour="red", fill="white") +
  xlim(0, 1.02) +
  ylim(0, 95) +
  ggtitle("Two experts") 

post_df <- data.frame(odds_to_posterior(3, 1))
names(post_df) <- ("posterior")
p3 <- ggplot(post_df, aes(x=posterior)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.01, colour="red", fill="white") +
  xlim(0, 1.02) +
  ylim(0, 95) +
  ggtitle("Three experts")  


post_df <- data.frame(odds_to_posterior(4, 1))
names(post_df) <- ("posterior")
p4 <- ggplot(post_df, aes(x=posterior)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.01, colour="red", fill="white") +
  xlim(0, 1.02) +
  ylim(0, 95) +
  ggtitle("Four experts")

post_df <- data.frame(odds_to_posterior(1, 0))
names(post_df) <- ("posterior")
p1n <- ggplot(post_df, aes(x=posterior)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.01, colour="blue", fill="white") +
  xlim(-0.02, 1) +
  ylim(0, 100)

post_df <- data.frame(odds_to_posterior(2, 0))
names(post_df) <- ("posterior")
p2n <- ggplot(post_df, aes(x=posterior)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.01, colour="blue", fill="white") +
  xlim(-0.02, 1) +
  ylim(0, 100) 

post_df <- data.frame(odds_to_posterior(3, 0))
names(post_df) <- ("posterior")
p3n <- ggplot(post_df, aes(x=posterior)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.01, colour="blue", fill="white") +
  xlim(-0.02, 1) +
  ylim(0, 100)  


post_df <- data.frame(odds_to_posterior(4, 0))
names(post_df) <- ("posterior")
p4n <- ggplot(post_df, aes(x=posterior)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.01, colour="blue", fill="white") +
  xlim(-0.02, 1) +
  ylim(0, 100)  

grid.arrange(p1, p2, p3, p4, p1n, p2n, p3n, p4n, ncol=4) 
```




The general tendency is that, when the world is in state 1, the more experts are asked, the higher the posterior probability of hypothesis H (say, that you need treatment). Visually, the red bars tend to move rightwards. By contrast, when the world is in state 0, the more experts are asked, the lower the posterior probability of hypothesis H. Visually, the blue bars tend to move leftwards.



### Threshold, Sensitivity and Specificity 

Suppose our beliefs are formed based on a cut-off probabilistic threshold, that is, if the estimated probability is above a threshold t, the belief is formed. 
The function 'above_threshold_after_expert' checks whether the estimated 
posterior probability of hypothesis H is above 
threshold t and returns 1 or 0 accordingly.

```{r}
above_threshold_after_expert <- function(n, w, t) {
    
        ifelse(odds_to_posterior(n, w) > t, 1, 0)
  
                          }
```

The function 'sensitivity_of_threshold' checks the sensitivity of 
a threshold based rule of belief formation.  Sensitivity is calculated by dividing cases in which the threshold is met (above_threshold_after_expert(n, w, t) == 1) by the total number of simulated cases 's' provided the world is 
in state 1 (otherwise, the function is undefined).

```{r}
sensitivity_of_threshold <- function(n, w, t) { 
  
      if (w == 1)
         return(   (length(which(above_threshold_after_expert(n, w, t) == 1)))/s          
                )            
      else 
          return("Undefined because sensitivity requires the world to be in state 1")
     
                    }
```

The function 'specificity_of_threshold' checks the specificity of threshold based rule of belief formation. Specificity is calculated by dividing cases in which the threshold is not met (above_threshold_after_expert(n, w, t) == 0) by the total number of simulated cases 's' provided the world is 
in state 0 (otherwise, the function is undefined).

```{r}
specificity_of_threshold <- function(n, w, t) { 
  
  if (w == 0)
    return(   (length(which(above_threshold_after_expert(n, w, t) == 0)))/s          
    )            
  else 
    return("Undefined because specificity requires the world to be in state 0")
  
}
```

The function 'accuracy_of_threshold' combines sensitivity and specificity, giving each equal weight (0.5). This function gives the overall accuracy score of a belief forming process. 


```{r}
accuracy_of_threshold <- function(n, t) { 
  
  0.5*sensitivity_of_threshold(n, 1, t) + 0.5*specificity_of_threshold(n, 0, t)  
  
}
```

 Below are the graphs representing sensitivity, specificity and overall accuracy, depending on the number of experts being asked. The threshold is set at 0.5 and the estimated sensitivity and specificity for each expert, namely sr1 and sr2, are kept constant at 0.7 and 0.7 respectively. 
 
```{r}
 
 t   <-  0.5
 sr1 <-  0.7
 sr2 <-  0.7
 prior <- 0.5
 
 
```
 
 
```{r, echo=FALSE}


sen <- c(
  sensitivity_of_threshold(1, 1, t), 
  sensitivity_of_threshold(2, 1, t),
  sensitivity_of_threshold(3, 1, t),
  sensitivity_of_threshold(4, 1, t),
  sensitivity_of_threshold(5, 1, t),
  sensitivity_of_threshold(6, 1, t),
  sensitivity_of_threshold(7, 1, t),
  sensitivity_of_threshold(8, 1, t),
  sensitivity_of_threshold(9, 1, t),
  sensitivity_of_threshold(10, 1, t),
  sensitivity_of_threshold(11, 1, t),
  sensitivity_of_threshold(12, 1, t),
  sensitivity_of_threshold(13, 1, t)
        )

spe <- c(
  specificity_of_threshold(1, 0, t), 
  specificity_of_threshold(2, 0, t),
  specificity_of_threshold(3, 0, t),
  specificity_of_threshold(4, 0, t),
  specificity_of_threshold(5, 0, t),
  specificity_of_threshold(6, 0, t),
  specificity_of_threshold(7, 0, t),
  specificity_of_threshold(8, 0, t),
  specificity_of_threshold(9, 0, t),
  specificity_of_threshold(10, 0, t),
  specificity_of_threshold(11, 0, t),
  specificity_of_threshold(12, 0, t),
  specificity_of_threshold(13, 0, t)
        )


acc <- c(
  accuracy_of_threshold(1, t), 
  accuracy_of_threshold(2, t), 
  accuracy_of_threshold(3, t), 
  accuracy_of_threshold(4, t), 
  accuracy_of_threshold(5, t), 
  accuracy_of_threshold(6, t), 
  accuracy_of_threshold(7, t), 
  accuracy_of_threshold(8, t), 
  accuracy_of_threshold(9, t), 
  accuracy_of_threshold(10, t), 
  accuracy_of_threshold(11, t), 
  accuracy_of_threshold(12, t), 
  accuracy_of_threshold(13, t)
      )
  

n <- c(1:13)

df_sen <- data.frame(n, sen)
df_spe <- data.frame(n, spe)
df_acc <- data.frame(n, acc)

sen_a <- ggplot(df_sen, aes(x=n, y=sen)) + 
      geom_point(colour="red") + 
      xlab("Number of experts") +
      ylab("Sensitivity") +
      xlim(1, 13) +
      ylim(0, 1) 


spe_a <- ggplot(df_spe, aes(x=n, y=spe)) + 
      geom_point(colour="blue") + 
      xlab("Number of experts") +
      ylab("Specificity") +
      xlim(1, 13) +
      ylim(0, 1)

acc_a <- ggplot(df_acc, aes(x=n, y=acc)) + 
      geom_point(colour="black") + 
      xlab("Number of experts") +
      ylab("Acuracy") +
      xlim(1, 13) +
      ylim(0, 1)

grid.arrange(sen_a, spe_a, acc_a, ncol=3)
```

Overall accuracy tends to increaese as the opinions of more experts are taken into account, but sensitivity or specificity do not always increase. In fact, senstivity decreases from one to two experts and even further 
from two to three experts, to increase again when four experts are asked for their opinion. Also, the sensitivity of five experts is not as good as the sentivity of four experts. Specificity does not always increaes either as more expert opinions are taken into account, although the pattern is quite diferent from the one associated with sensitivity. For example, while going from one to two experts lowers sensitivty, it does bring up specificity, but while going from three to four experts increases sentivity, it brings down specificity. 

How robust are these results? 

Suppose we change the base rates (priors), all else being the same. This impacts accuracy, sensitivity and specificity significantly.  

```{r}

prior <- 0.6


sen <- c(
  sensitivity_of_threshold(1, 1, t), 
  sensitivity_of_threshold(2, 1, t),
  sensitivity_of_threshold(3, 1, t),
  sensitivity_of_threshold(4, 1, t),
  sensitivity_of_threshold(5, 1, t),
  sensitivity_of_threshold(6, 1, t),
  sensitivity_of_threshold(7, 1, t),
  sensitivity_of_threshold(8, 1, t),
  sensitivity_of_threshold(9, 1, t),
  sensitivity_of_threshold(10, 1, t),
  sensitivity_of_threshold(11, 1, t),
  sensitivity_of_threshold(12, 1, t),
  sensitivity_of_threshold(13, 1, t)
        )

spe <- c(
  specificity_of_threshold(1, 0, t), 
  specificity_of_threshold(2, 0, t),
  specificity_of_threshold(3, 0, t),
  specificity_of_threshold(4, 0, t),
  specificity_of_threshold(5, 0, t),
  specificity_of_threshold(6, 0, t),
  specificity_of_threshold(7, 0, t),
  specificity_of_threshold(8, 0, t),
  specificity_of_threshold(9, 0, t),
  specificity_of_threshold(10, 0, t),
  specificity_of_threshold(11, 0, t),
  specificity_of_threshold(12, 0, t),
  specificity_of_threshold(13, 0, t)
        )


acc <- c(
  accuracy_of_threshold(1, t), 
  accuracy_of_threshold(2, t), 
  accuracy_of_threshold(3, t), 
  accuracy_of_threshold(4, t), 
  accuracy_of_threshold(5, t), 
  accuracy_of_threshold(6, t), 
  accuracy_of_threshold(7, t), 
  accuracy_of_threshold(8, t), 
  accuracy_of_threshold(9, t), 
  accuracy_of_threshold(10, t), 
  accuracy_of_threshold(11, t), 
  accuracy_of_threshold(12, t), 
  accuracy_of_threshold(13, t)
      )
  

n <- c(1:13)

df_sen <- data.frame(n, sen)
df_spe <- data.frame(n, spe)
df_acc <- data.frame(n, acc)

sen_b <- ggplot(df_sen, aes(x=n, y=sen)) + 
      geom_point(colour="red") + 
      xlab("Number of experts") +
      ylab("Sensitivity") +
      xlim(1, 13) +
      ylim(0, 1) 


spe_b <- ggplot(df_spe, aes(x=n, y=spe)) + 
      geom_point(colour="blue") + 
      xlab("Number of experts") +
      ylab("Specificity") +
      xlim(1, 13) +
      ylim(0, 1)

acc_b <- ggplot(df_acc, aes(x=n, y=acc)) + 
      geom_point(colour="black") + 
      xlab("Number of experts") +
      ylab("Acuracy") +
      xlim(1, 13) +
      ylim(0, 1)

grid.arrange(sen_b, spe_b, acc_b, ncol=3)
```


But can we close the gap between sensitivity/specificity when the base rates are different?

```{r}


grid.arrange(sen_a, sen_b, ncol=2)
grid.arrange(spe_a, spe_b, ncol=2)
grid.arrange(acc_a, acc_b, ncol=2)
```


The general tendency is that overall accuracy improves as more experts are asked for their opinion. This is true if one asks many experts in a row, but it is not always true if one decides to ask two or three experts instead of just one. In this case, as the last scenario shows, there could actually be a decline in overall accuracy. 



### Brier score as distance from truth


There is a precise way to see that the more experts are being asked for an opinion, the more the posterior probability estimates converge to their true values (0 or 1, depending on the true state of the world). The average distance between the posterior probability estimates and the true value (1 or 0) can be measured using the so-called Brier score. The lower the Brier score, the closer to the true value the estimate.




```{r}

sr1 <- 0.7
sr2 <- 0.7


brier_score <- function(n, w) {
  
  if (w == 1)
    return(
            mean((1-odds_to_posterior(n, w))^2)
          )
  
  else 
    return(
            mean((0-odds_to_posterior(n, w))^2)    
          )
  }
  
```

The Brier score decreases as the number of experts being asked increases, if the world is in state 1 (red  dots) or state 0 (blue dots).

```{r, echo=FALSE}

bs <- c(
  brier_score(1, 1), 
  brier_score(2, 1),
  brier_score(3, 1),
  brier_score(4, 1),
  brier_score(5, 1),
  brier_score(6, 1),
  brier_score(7, 1), 
  brier_score(8, 1),
  brier_score(9, 1),
  brier_score(10, 1),
  brier_score(11, 1),
  brier_score(12, 1),
  brier_score(13, 1)
    )

bsn <- c(
  brier_score(1, 0), 
  brier_score(2, 0),
  brier_score(3, 0),
  brier_score(4, 0),
  brier_score(5, 0),
  brier_score(6, 0),
  brier_score(7, 0), 
  brier_score(8, 0),
  brier_score(9, 0),
  brier_score(10, 0),
  brier_score(11, 0),
  brier_score(12, 0),
  brier_score(13, 0)
    )

n <- c(1:13)

df_bs <- data.frame(n, bs)
df_bsn <- data.frame(n, bsn)

bs <- ggplot(df_bs, aes(x=n, y=bs)) + 
      geom_point(colour="red") + 
      xlab("Number of experts") +
      ylab("Brier Score") +
      xlim(1, 13) +
      ylim(0, 0.25) 

bsn <- ggplot(df_bsn, aes(x=n, y=bsn)) + 
      geom_point(colour="blue") + 
      xlab("Number of experts") +
      ylab("Brier Score") +
      xlim(1, 13) +
      ylim(0, 0.25)
  
grid.arrange(bs, bsn, ncol=2)  
```

## All reliable experts

Assume all our experts have a reliability of at least .51. The rest is the same.

```{r}

expert_report_variable <- function(w, s){
  
  r1 <- sample(51:90, 1)/100
  r2 <- sample(51:90, 1)/100
  expert_report(r1, r2, w, s) 
}

```


Do things change significantly? Let's consider sensitivity, specificity and accuracy of threshold.

```{r}

s <- 10000
prior <- 0.5
sr1 <- 0.7
sr2 <- 0.7

```

```{r, echo=FALSE}


sen <- c(
  sensitivity_of_threshold(1, 1, t), 
  sensitivity_of_threshold(2, 1, t),
  sensitivity_of_threshold(3, 1, t),
  sensitivity_of_threshold(4, 1, t),
  sensitivity_of_threshold(5, 1, t),
  sensitivity_of_threshold(6, 1, t),
  sensitivity_of_threshold(7, 1, t),
  sensitivity_of_threshold(8, 1, t),
  sensitivity_of_threshold(9, 1, t),
  sensitivity_of_threshold(10, 1, t),
  sensitivity_of_threshold(11, 1, t),
  sensitivity_of_threshold(12, 1, t),
  sensitivity_of_threshold(13, 1, t)
        )

spe <- c(
  specificity_of_threshold(1, 0, t), 
  specificity_of_threshold(2, 0, t),
  specificity_of_threshold(3, 0, t),
  specificity_of_threshold(4, 0, t),
  specificity_of_threshold(5, 0, t),
  specificity_of_threshold(6, 0, t),
  specificity_of_threshold(7, 0, t),
  specificity_of_threshold(8, 0, t),
  specificity_of_threshold(9, 0, t),
  specificity_of_threshold(10, 0, t),
  specificity_of_threshold(11, 0, t),
  specificity_of_threshold(12, 0, t),
  specificity_of_threshold(13, 0, t)
        )


acc <- c(
  accuracy_of_threshold(1, t), 
  accuracy_of_threshold(2, t), 
  accuracy_of_threshold(3, t), 
  accuracy_of_threshold(4, t), 
  accuracy_of_threshold(5, t), 
  accuracy_of_threshold(6, t), 
  accuracy_of_threshold(7, t), 
  accuracy_of_threshold(8, t), 
  accuracy_of_threshold(9, t), 
  accuracy_of_threshold(10, t), 
  accuracy_of_threshold(11, t), 
  accuracy_of_threshold(12, t), 
  accuracy_of_threshold(13, t)
      )
  

n <- c(1:13)

df_sen <- data.frame(n, sen)
df_spe <- data.frame(n, spe)
df_acc <- data.frame(n, acc)

sen_a <- ggplot(df_sen, aes(x=n, y=sen)) + 
      geom_point(colour="red") + 
      xlab("Number of experts") +
      ylab("Sensitivity") +
      xlim(1, 13) +
      ylim(0, 1) 


spe_a <- ggplot(df_spe, aes(x=n, y=spe)) + 
      geom_point(colour="blue") + 
      xlab("Number of experts") +
      ylab("Specificity") +
      xlim(1, 13) +
      ylim(0, 1)

acc_a <- ggplot(df_acc, aes(x=n, y=acc)) + 
      geom_point(colour="black") + 
      xlab("Number of experts") +
      ylab("Acuracy") +
      xlim(1, 13) +
      ylim(0, 1)

grid.arrange(sen_a, spe_a, acc_a, ncol=3)
```

Let's do it a second time and see if the results are yet different.

```{r, echo=FALSE}


sen <- c(
  sensitivity_of_threshold(1, 1, t), 
  sensitivity_of_threshold(2, 1, t),
  sensitivity_of_threshold(3, 1, t),
  sensitivity_of_threshold(4, 1, t),
  sensitivity_of_threshold(5, 1, t),
  sensitivity_of_threshold(6, 1, t),
  sensitivity_of_threshold(7, 1, t),
  sensitivity_of_threshold(8, 1, t),
  sensitivity_of_threshold(9, 1, t),
  sensitivity_of_threshold(10, 1, t),
  sensitivity_of_threshold(11, 1, t),
  sensitivity_of_threshold(12, 1, t),
  sensitivity_of_threshold(13, 1, t)
        )

spe <- c(
  specificity_of_threshold(1, 0, t), 
  specificity_of_threshold(2, 0, t),
  specificity_of_threshold(3, 0, t),
  specificity_of_threshold(4, 0, t),
  specificity_of_threshold(5, 0, t),
  specificity_of_threshold(6, 0, t),
  specificity_of_threshold(7, 0, t),
  specificity_of_threshold(8, 0, t),
  specificity_of_threshold(9, 0, t),
  specificity_of_threshold(10, 0, t),
  specificity_of_threshold(11, 0, t),
  specificity_of_threshold(12, 0, t),
  specificity_of_threshold(13, 0, t)
        )


acc <- c(
  accuracy_of_threshold(1, t), 
  accuracy_of_threshold(2, t), 
  accuracy_of_threshold(3, t), 
  accuracy_of_threshold(4, t), 
  accuracy_of_threshold(5, t), 
  accuracy_of_threshold(6, t), 
  accuracy_of_threshold(7, t), 
  accuracy_of_threshold(8, t), 
  accuracy_of_threshold(9, t), 
  accuracy_of_threshold(10, t), 
  accuracy_of_threshold(11, t), 
  accuracy_of_threshold(12, t), 
  accuracy_of_threshold(13, t)
      )
  

n <- c(1:13)

df_sen <- data.frame(n, sen)
df_spe <- data.frame(n, spe)
df_acc <- data.frame(n, acc)

sen_a <- ggplot(df_sen, aes(x=n, y=sen)) + 
      geom_point(colour="red") + 
      xlab("Number of experts") +
      ylab("Sensitivity") +
      xlim(1, 13) +
      ylim(0, 1) 


spe_a <- ggplot(df_spe, aes(x=n, y=spe)) + 
      geom_point(colour="blue") + 
      xlab("Number of experts") +
      ylab("Specificity") +
      xlim(1, 13) +
      ylim(0, 1)

acc_a <- ggplot(df_acc, aes(x=n, y=acc)) + 
      geom_point(colour="black") + 
      xlab("Number of experts") +
      ylab("Acuracy") +
      xlim(1, 13) +
      ylim(0, 1)

grid.arrange(sen_a, spe_a, acc_a, ncol=3)
```

What if we are more conservative in our assessment of expert reliability?

```{r}

s <- 10000
prior <- 0.5
sr1 <- 0.55
sr2 <- 0.55

```

```{r, echo=FALSE}


sen <- c(
  sensitivity_of_threshold(1, 1, t), 
  sensitivity_of_threshold(2, 1, t),
  sensitivity_of_threshold(3, 1, t),
  sensitivity_of_threshold(4, 1, t),
  sensitivity_of_threshold(5, 1, t),
  sensitivity_of_threshold(6, 1, t),
  sensitivity_of_threshold(7, 1, t),
  sensitivity_of_threshold(8, 1, t),
  sensitivity_of_threshold(9, 1, t),
  sensitivity_of_threshold(10, 1, t),
  sensitivity_of_threshold(11, 1, t),
  sensitivity_of_threshold(12, 1, t),
  sensitivity_of_threshold(13, 1, t)
        )

spe <- c(
  specificity_of_threshold(1, 0, t), 
  specificity_of_threshold(2, 0, t),
  specificity_of_threshold(3, 0, t),
  specificity_of_threshold(4, 0, t),
  specificity_of_threshold(5, 0, t),
  specificity_of_threshold(6, 0, t),
  specificity_of_threshold(7, 0, t),
  specificity_of_threshold(8, 0, t),
  specificity_of_threshold(9, 0, t),
  specificity_of_threshold(10, 0, t),
  specificity_of_threshold(11, 0, t),
  specificity_of_threshold(12, 0, t),
  specificity_of_threshold(13, 0, t)
        )


acc <- c(
  accuracy_of_threshold(1, t), 
  accuracy_of_threshold(2, t), 
  accuracy_of_threshold(3, t), 
  accuracy_of_threshold(4, t), 
  accuracy_of_threshold(5, t), 
  accuracy_of_threshold(6, t), 
  accuracy_of_threshold(7, t), 
  accuracy_of_threshold(8, t), 
  accuracy_of_threshold(9, t), 
  accuracy_of_threshold(10, t), 
  accuracy_of_threshold(11, t), 
  accuracy_of_threshold(12, t), 
  accuracy_of_threshold(13, t)
      )
  

n <- c(1:13)

df_sen <- data.frame(n, sen)
df_spe <- data.frame(n, spe)
df_acc <- data.frame(n, acc)

sen_a <- ggplot(df_sen, aes(x=n, y=sen)) + 
      geom_point(colour="red") + 
      xlab("Number of experts") +
      ylab("Sensitivity") +
      xlim(1, 13) +
      ylim(0, 1) 


spe_a <- ggplot(df_spe, aes(x=n, y=spe)) + 
      geom_point(colour="blue") + 
      xlab("Number of experts") +
      ylab("Specificity") +
      xlim(1, 13) +
      ylim(0, 1)

acc_a <- ggplot(df_acc, aes(x=n, y=acc)) + 
      geom_point(colour="black") + 
      xlab("Number of experts") +
      ylab("Acuracy") +
      xlim(1, 13) +
      ylim(0, 1)

grid.arrange(sen_a, spe_a, acc_a, ncol=3)
```
