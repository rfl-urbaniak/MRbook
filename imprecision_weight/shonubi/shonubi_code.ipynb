{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# United States v. Shonubi (1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import pyro.ops.stats as stats\n",
    "from pyro.infer import Predictive, SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer.autoguide import AutoMultivariateNormal\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "from functools import partial\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "smoke_test = ('CI' in os.environ)\n",
    "num_iterations = 1500 if not smoke_test else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svi_inference(\n",
    "    model,\n",
    "    num_steps=500,\n",
    "    verbose=True,\n",
    "    lr=0.03,\n",
    "    guide=None,\n",
    "    blocked_sites=None,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    losses = []\n",
    "    # running_loss_means = []\n",
    "    if guide is None:\n",
    "        guide = AutoMultivariateNormal(pyro.poutine.block(model, hide=blocked_sites))\n",
    "    elbo = pyro.infer.Trace_ELBO()(model, guide)\n",
    "\n",
    "    elbo(**model_kwargs)\n",
    "    adam = torch.optim.Adam(elbo.parameters(), lr=lr)\n",
    "    print(f\"Running SVI for {num_steps} steps...\")\n",
    "    for step in range(1, num_steps + 1):\n",
    "        adam.zero_grad()\n",
    "        loss = elbo(**model_kwargs)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        adam.step()\n",
    "        if (step % 100 == 0) or (step == 1) & verbose:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step, loss))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses, label=\"ELBO loss\")\n",
    "    sns.despine()\n",
    "    plt.title(\"ELBO Loss\")\n",
    "    plt.ylim(0, max(losses))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return guide\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "# def get_samples(\n",
    "#     distance,\n",
    "#     proximity,\n",
    "#     how_far,\n",
    "#     model= model_sigmavar_proximity,\n",
    "#     num_svi_iters=num_svi_iters,\n",
    "#     num_samples=num_samples,\n",
    "# ):\n",
    "#     guide = AutoMultivariateNormal(model, init_loc_fn=init_to_mean)\n",
    "#     svi = SVI(\n",
    "#         model_sigmavar_proximity, guide, optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO()\n",
    "#     )\n",
    "\n",
    "#     iterations = []\n",
    "#     losses = []\n",
    "\n",
    "#     logging.info(f\"Starting SVI inference with {num_svi_iters} iterations.\")\n",
    "#     start_time = time.time()\n",
    "#     pyro.clear_param_store()\n",
    "#     for i in range(num_svi_iters):\n",
    "#         elbo = svi.step(distance, proximity, how_far)\n",
    "#         iterations.append(i)\n",
    "#         losses.append(elbo)\n",
    "#         if i % 50 == 0:\n",
    "#             logging.info(\"Elbo loss: {}\".format(elbo))\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     logging.info(\"SVI inference completed in %.2f seconds.\", elapsed_time)\n",
    "\n",
    "#     # uncomment if you want to see the ELBO loss plots\n",
    "#     # fig = px.line(x=iterations, y=losses, title=\"ELBO loss\", template=\"presentation\")\n",
    "#     # labels = {\"iterations\": \"iteration\", \"losses\": \"loss\"}\n",
    "#     # fig.update_xaxes(showgrid=False, title_text=labels[\"iterations\"])\n",
    "#     # fig.update_yaxes(showgrid=False, title_text=labels[\"losses\"])\n",
    "#     # fig.update_layout(width=700)\n",
    "#     # fig.show()\n",
    "\n",
    "#     predictive = Predictive(model, guide=guide, num_samples=num_samples)\n",
    "\n",
    "#     proximity_svi = {\n",
    "#         k: v.flatten().reshape(num_samples, -1).detach().cpu().numpy()\n",
    "#         for k, v in predictive(distance, proximity, how_far).items()\n",
    "#         if k != \"obs\"\n",
    "#     }\n",
    "\n",
    "#     print(\"SVI-based coefficient marginals:\")\n",
    "#     for site, values in ft.summary(proximity_svi, [\"d\", \"p\"]).items():\n",
    "#         print(\"Site: {}\".format(site))\n",
    "#         print(values, \"\\n\")\n",
    "\n",
    "#     return {\n",
    "#         \"svi_samples\": proximity_svi,\n",
    "#         \"svi_guide\": guide,\n",
    "#         \"svi_predictive\": predictive,\n",
    "#     }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# svi = SVI(model, guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for site_name, values in samples.items():\n",
    "        marginal_site = pd.DataFrame(values)\n",
    "        describe = marginal_site.describe(percentiles=[.05, 0.25, 0.5, 0.75, 0.95]).transpose()\n",
    "        site_stats[site_name] = describe[[\"mean\", \"std\", \"5%\", \"25%\", \"50%\", \"75%\", \"95%\"]]\n",
    "    return site_stats\n",
    "\n",
    "def precis(samples):\n",
    "\n",
    "    for site, values in summary(samples).items():\n",
    "        print(\"Site: {}\".format(site))\n",
    "        print(values, \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>dataset</th>\n",
       "      <th>balloons</th>\n",
       "      <th>gross_wt</th>\n",
       "      <th>net_wt</th>\n",
       "      <th>purity</th>\n",
       "      <th>age_yrs</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>742.4</td>\n",
       "      <td>503.2</td>\n",
       "      <td>0.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>901.7</td>\n",
       "      <td>576.9</td>\n",
       "      <td>0.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>800.2</td>\n",
       "      <td>573.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>706.2</td>\n",
       "      <td>439.8</td>\n",
       "      <td>0.75</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>23.1</td>\n",
       "      <td>0.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   obs  dataset   balloons  gross_wt  net_wt  purity  age_yrs  gender\n",
       "0  1.0      1.0       79.0     742.4   503.2    0.51      NaN     NaN\n",
       "1  2.0      1.0       90.0     901.7   576.9    0.32      NaN     NaN\n",
       "2  3.0     12.0       90.0     800.2   573.3    0.85     38.0     1.0\n",
       "3  4.0     12.0        1.0     706.2   439.8    0.75     41.0     1.0\n",
       "4  5.0      1.0        5.0      72.2    23.1    0.62      NaN     NaN"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataset\n",
    "\n",
    "sh = pd.read_csv('ShonubiCaseDataset.csv')\n",
    "sh.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST, STRATEGY WHICH USES POSTERIOR MEANS ONLY\n",
    "\n",
    "\n",
    "note there are 107 cases with gross_wt, but no net_wt. Let's first predict\n",
    "those, then fill them in to predict weight based on balloons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "compData = sh[['gross_wt', 'net_wt']].dropna()\n",
    "\n",
    "data = torch.tensor(compData[[\"gross_wt\", \"net_wt\"]].values,\n",
    "                        dtype=torch.float)\n",
    "\n",
    "gross_wt, net_wt  = data[:, 0], data[:, 1]\n",
    "    \n",
    "    \n",
    "def model(gross_wt, net_wt):\n",
    "    b_a = pyro.sample(\"bA\", dist.Normal(0.8, 0.3))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0., 150.))\n",
    "    mean = b_a * gross_wt\n",
    "    with pyro.plate(\"data\", len(gross_wt)):\n",
    "        pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=net_wt)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "adam = pyro.optim.Adam({\"lr\": 0.03})\n",
    "svi = SVI(model, guide, adam, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elbo loss: 776.4009214639664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elbo loss: 776.560912668705\n",
      "Elbo loss: 775.7735349535942\n"
     ]
    }
   ],
   "source": [
    "# pyro.clear_param_store()\n",
    "# for j in range(num_iterations):\n",
    "#     # calculate the loss and take a gradient step\n",
    "#     loss = svi.step(gross_wt, net_wt)\n",
    "#     if j % 100 == 0:\n",
    "#         print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))\n",
    "        \n",
    "        \n",
    "for i in range(num_iterations):\n",
    "    elbo = svi.step(gross_wt, net_wt)\n",
    "    if i % 500 == 0:\n",
    "        logging.info(\"Elbo loss: {}\".format(elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "predictive = Predictive(model, guide=guide, num_samples=num_samples)\n",
    "svi_samples = {k: v.reshape(num_samples).detach().cpu().numpy()\n",
    "               for k, v in predictive(gross_wt, net_wt).items()\n",
    "               if k != \"obs\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site: bA\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  0.608505  0.016319  0.581728  0.597156  0.608675  0.620185  0.635263 \n",
      "\n",
      "Site: sigma\n",
      "         mean       std          5%         25%        50%         75%  \\\n",
      "0  125.463554  5.080407  116.717117  121.811892  125.95512  129.343185   \n",
      "\n",
      "         95%  \n",
      "0  132.89352   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "precis(svi_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting on creating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a subset of missing net_wt values\n",
    "fillNet = sh.copy()[(~sh['gross_wt'].isna()) & (sh['net_wt'].isna())]\n",
    "fillNet.loc[:, 'net_wt'] = fillNet['net_wt'].fillna(0) # 0s instead of NaNs?\n",
    "\n",
    "# use torch.floats ?\n",
    "gross_wtFILL = torch.tensor(fillNet[\"gross_wt\"].values, dtype=torch.float)\n",
    "net_wtFILL = torch.tensor(fillNet[\"net_wt\"].values, dtype=torch.float) # sim wants tensors\n",
    "\n",
    "fillNetPair = {\"gross_wt\": gross_wtFILL}\n",
    "\n",
    "# using the trained model to fill in missing values\n",
    "netPred = sim(netModel, fillNetPair, n=int(1e4)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example\n",
    "# pred_data = {\"marriage\": R_seq, \"median_age_marriage\": A_avg.expand_as(R_seq)}\n",
    "\n",
    "# # compute counterfactual mean divorce (mu)\n",
    "# mu = link(m5_3, data=pred_data)\n",
    "# mu_mean = mu.mean(0)\n",
    "# mu_PI = stats.pi(mu, 0.89, dim=0)\n",
    "\n",
    "# # simulate counterfactual divorce outcomes\n",
    "# R_sim = sim(m5_3, data=pred_data, n=int(1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
