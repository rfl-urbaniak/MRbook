---
title: "LR material removed from main higher-oder paper"
author: "Rafal Urbaniak and Marcello Di Bello"
date: '`r Sys.Date()`'
format:
  pdf:
    toc: true
    include-in-header: quartoStyle.sty
    mainfont: Times New Roman
    sansfont: Times New Roman
documentClass: scrartcl
classOptions: [dvipsnames, enabledeprecatedfontcommands, headings=big]
font:
  size: 10pt
url:
  color: blue
bibliography: [../../../references/referencesMRbook.bib]
csl: apa-6th-edition.csl
indent: true
numbersections: true
---

  <!-- pdf:
    numberSections: true
    dfPrint: kable 
    keepTex: true
    toc: true
    include-in-header:  quartoStyle.sty
    latex_engine: pdflatex  -->


```{r setup}
#| include: false

knitr::opts_chunk$set(echo = TRUE)
library(dagitty)
library(rethinking)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(tidyr)
library(philentropy)
library(latex2exp)
library(gridExtra)
library(rethinking)
library(bnlearn)
library(gRain)
library(reshape2)
library(truncnorm)
library(ggforce)
library(dplyr)
library(magick)



ps <- seq(0,1, length.out = 1001)
getwd()
source("../../scripts/CptCreate.R")
source("../../scripts/SCfunctions.R")

SCprobsFinal <- readRDS("../../datasets/SCprobsFinal.rds")
attach(SCprobsFinal)      
source("../../scripts/SCplotCPTs.R")
source("../../scripts/SCplotDistros.R")

```




\vspace{2cm}

\noindent \textbf{DISCLAIMER:} \textbf{This is a draft of work in progress, please do not cite or distribute without permission.}

\thispagestyle{empty}

\newpage

\begin{quote} \textbf{Abstract.}  Rational agents are often uncertain about the truth of many propositions. To represent this uncertainty, it is natural to rely on probability theory. Two options are typically on the table, precise and imprecise probabilism, but both fall short in some respect. Precise probabilism is not expressive enough, while imprecise probabilism suffers from belief inertia and the impossibility of proper scoring rules. We put forward a novel version of probabilism, higher-order probabilism, and we show that it outperforms existing alternatives. 

\end{quote}






# LR Evidence aggregation: the simple case - SET ASIDE FOR BOOK

<!--
As we have seen, higher-order probabilism outperforms both precise and imprecise probabilism, at the descriptive as well as the normative level. From a descriptive standpoint, higher-order probabilism can model scenarios that cannot be modeled by the other versions of probabilism. From a normative standpoint, accuracy maximization may sometimes recommend that a rational agent represent their credal state with a distribution over probability values rather than a precise probability measure\todo{add ref to vFraasen in fn; perhaps extend the discussion a bit } (more on this soon).^[Having read van Fraasen's "Laws and Symmetry", you might also worry that going higher order somehow leads to a contradiction; we will address this concern later on.]  \todo{where do we show that accuracy recommends a distribution over probability measures? This bit seems missing.}
-->

 Rational agents are often tasked with aggregating pieces of evidence and assessing their value relative to a hypothesis. In this and the next section, we examine the question of how multiple items of evidence should be evaluated together. This question raises novel difficulties for both precise and imprecise probabilism. We show how higher-order probabilism can handle them. 

For the precise probabilist, a natural measure of the value of the evidence is the likelihood ratio.  This ratio is relative to a pair of competing hypotheses, say $H$ and its negation $\neg H$ (though the two hypotheses need not be one the negation of the other). Relative to these hypotheses, the likelihood ratio of a single piece of evidence $E$ is the probability of $E$ given  $H$ divided by the probability of $E$ given $\neg H$, or in short, $\frac{\pr{E \vert H}}{\pr{E \vert \neg H }}$.
<!--
In qualitative terms, evidence 
$E$ supports $H$ over its negation insofar as the probability of $E$ is 
higher given hypothesis $H$ than given its negation, that is, the likelihood ratio is greater than one. 
-->
Degrees of evidential value (or support, strength) can be expressed as follows:
\begin{quote}
the higher $\frac{\pr{E \vert H}}{\pr{E \vert \neg H }}$ (if greater than one), the more strongly $E$ supports $H$.
\end{quote}

\noindent
The value of the evidence increases whenever $\pr{E \vert H}$ increases or whenever $\pr{E \vert \neg H }$ decreases. The higher $\pr{E \vert H}$, the better the evidence at tracking $H$ (a true positive); the lower $\pr{E \vert \neg H }$, the better the evidence at avoiding $\neg H$ (a true negative).
If the probability of $E$ is the same given hypothesis $H$ as given its negation, that is, the likelihood ratio equals one, the evidence would have no value for $H$.

Likelihood ratios can also be used for assessing the value of multiple pieces of evidence in the aggregate, again relative to a pair of hypotheses of interest. In the simplest case (for more complex cases, see the next section), 
multiple items of evidence all bear on the 
same hypothesis. Then, to obtain their combined evidential value, it is enough to multiply 
their individual likelihood ratios. 
$$
\frac{\pr{E_1 \wedge E_1 \dots E_k \vert H}}{\pr{E_1 \wedge E_2 \dots E_k \vert \neg H}} = \frac{\pr{E_1 \vert H}}{\pr{E_1 \vert \neg H}}\times \frac{\pr{E_2 \vert H}}{\pr{E_2 \vert \neg H}} \times \dots \times \frac{\pr{E_k \vert H}}{\pr{E_k \vert \neg H}}
$$

```{r introStarts-OLD,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE}
ps <- seq(0,1,length.out = 1001)

hairMean <-  29/1148
hairA <- 30
hairB <- 1149

dogMean <- 2/78
dogA <- 3
dogB <- 79


lik0 <- hairMean * dogMean
prior <- seq(0,.3, by = 0.001)
priorH0 <- 1-prior
denomin <- lik0 * priorH0 + prior
num <-  lik0 * priorH0
posterior <- 1- num/denomin
threshold <- min(prior[posterior > .99])

pointImpactPlot <- ggplot()+geom_line(aes(x = prior, y = posterior))+xlim(0,.07)+
  theme_tufte(base_size = 10)+labs(title = "Prior vs. posterior, based on point estimates",
                                   subtitle = "Joint evidence: dog & hair")+
  geom_vline(xintercept = threshold, lty = 2, size =.5, alpha = .7)+
  annotate(geom = "label", label = "posterior > .99", x = .067, y =.95, size = 2.5)+
  theme(plot.title.position = "plot")
```

\noindent
The equality holds provided $E_1, E_2, \dots, E_k$ are probabilistically independent conditional on hypothesis $H$ and its negation. Think, for example, at several diagnostic tests performed by independent laboratories or independent witnesses in a trial testifying about the same issue. 

<!--For the precise probabilist, filling in the precise probabilities in the formula above gives an assessment of the value of aggregate evidence. But, as we will now see, this approach risks over- or under-estimating the value of the evidence. (We will focus on likelihood ratios, but the claim we defend can be generalized to other measures of evidential value and posterior probabilites.^[On different measures of degrees of evidential value (or support, confirmation), see \textbf{CITE FITELSON}. Posterior probabilities are obtained from the likelihood ratios via the prior probabilities. The posterior odds $\frac{\pr{H \vert E}}{\pr{\neg H \vert E }}$ are obtained by multiplying the likelihood ratio $\frac{\pr{E \vert H}}{\pr{E \vert \neg H }}$ by the prior odds $\frac{\pr{H}}{\pr{\neg H}}$. The posterior probability $\pr{H \vert E}$ is obtained from the formula $PO/(1+PO)$, where $PO$ is short for 'posterior odds'. This result only holds if the two hypotheses compared are one the negation of the other.]) To see what the problem is. 
-->

<!--  as follows:
$$
\frac{\pr{H \vert E_1 \wedge E_2\dots E_k}}{\pr{\neg H \vert E_1 \wedge E_1\dots E_k}} = \frac{\pr{E_1 \wedge E_1 \dots E_k \vert H}}{\pr{E_1 \wedge E_2\dots E_k \vert \neg H}} \times \frac{\pr{H}}{\pr{\neg H}} 
$$

We will focus on likelihood ratios for ease of exposition.
--->


To see how likelihood ratios can be deployed, it is worth working through a specific case. In a murder case, the police recover two items of trace evidence, both against the defendant. First, hair found at the crime scene matches the defendant's hair; call this evidence '\textsf{hair}.' Second, the fur of the defendant's dog matches the fur found in a carpet wrapped around one of the bodies; call this evidence '\textsf{fur}.'^[The hair evidence and the dog fur evidence are stylized after two 
 items of  evidence in the notorious 1981 Wayne Williams case [@deadman1984fiber1; @deadman1984fiber2].] 
The two matches favor the hypothesis that the defendant (and the defendant's dog) must be the source of the crime traces; call this hypothesis '$\mathsf{source}$'. If the two matches are independent lines of evidence (conditional on the source hypothesis and its negation), their likelihood ratios can be multiplied:

<!--
[Strictly speaking, it is  possible for $A$ and $B$ to be independent conditional on $H$, but not conditional on $\neg H$. Here, we require both independencies to hold.]
-->
 
$$
\frac{{\pr{\s{fur} \wedge \s{hair}  \vert \s{source}}}}{{\pr{\s{fur} \wedge \s{hair} \vert \neg \s{source}}}} = \frac{{\pr{\s{fur} \vert \s{source}}}}{{\pr{\s{fur}  \vert \neg \s{source}}}} \times
 \frac{{\pr{\s{hair} \vert \s{source}}}}{{\pr{\s{hair} \vert \neg \s{source}}}}
$$



So far so good. But how do we fill in the precise probabilies? The numerators can be equated to one: if the defendant is a contributor, the laboratory will declare a match for sure. This is a simplication, but it will do for our purposes. To fill in the denominators, a trial expert will provide so-called match probabilities. They express the likelihood that, by coincidence, a random person (or a random dog) who is not a contributor would still match.  The match probabilities are approximated by counting how many matches are found in a representative sample of the human population (or the canine population). Suppose the matching hair type occurs  `r round(hairMean,4)` times in a reference database, and the matching dog fur type occurs  `r round(dogMean,4)` times in a reference database (more on how these numbers 
are calculated soon). These frequencies can fill in the match probabilities. 
Putting everything together:
\begin{align*}
\frac{{\pr{\s{dog} \vert \s{source}}}}{{\pr{\s{dog}  \vert \neg \s{source}}}} \times
 \frac{{\pr{\s{hair} \vert \s{source}}}}{{\pr{\s{hair} \vert \neg \s{source}}}}
& =  \frac{1}{`r hairMean`} \times  \frac{1}{`r dogMean`} = {`r 1/(hairMean * dogMean)`}
\end{align*}

\noindent
The resulting ratio is large. The two matches, combined, strongly favor the source hypothesis. 
<!--
```{r impactOfPoint4,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%", warning = FALSE, message = FALSE}
#| label: fig-impactOfPoint
#| fig-cap: "Impact of dog fur and human hair evidence on the prior, point estimates."
#| fig-pos: H

pointImpactPlot
```
-->

```{r hair-OLD,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
#carpetSamples <- sample(ps, 1e4, replace = TRUE, prob = dbeta(ps, carpetA, carpetB))
set.seed(231)
hairSamples <- sample(ps, 1e4, replace = TRUE, prob = dbeta(ps, hairA, hairB))
dogSamples <- sample(ps, 1e4, replace = TRUE, prob = dbeta(ps, dogA, dogB))

#carpetHPDI <- HPDI(carpetSamples, prob  =.9)
hairHPDI <- HPDI(hairSamples, prob  =.99)
dogHPDI <- HPDI(dogSamples, prob  =.99)
```

```{r charitableImpact-OLD,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
lik0l <- .037 * .103
denominL <- lik0l * priorH0 + prior
numL <-  lik0l * priorH0
posteriorL <- 1- numL/denominL
thresholdL <- min(prior[posteriorL > .99])

charitableImpactPlot <- ggplot()+geom_line(aes(x = prior, y = posteriorL))+xlim(0,.32)+
  theme_tufte(base_size = 10)+labs(title = "Prior vs. posterior, charitable reading",
                                   subtitle = "Joint evidence: dog & hair")+
  geom_vline(xintercept = thresholdL, lty = 2, size =.5, alpha = .7)+
  annotate(geom = "label", label = "posterior > .99", x = .305, y =.95, size = 2.5)+ylab(
    "posterior"
  )+
  theme(plot.title.position = "plot")
```


This is the story about evidence aggregation told by the precise probabilist.  But this story misses something crucial. As it happens, the match probability for hair evidence is based on 29 matches found in a sample database of size 1148, while the match probability for the dog evidence is based on finding two matches in a smaller database of size 78. The relative frequencies are about $.025$ in both cases, but the two samples differ in size. The smaller the sample, the greater the uncertainty about the match probabilities. So, for individual pieces of evidence, simply reporting the exact numbers makes it seem as though the evidential value of the matches is the same, but actually, it is not.^[The match probabilities in the Wayne Williams case on which our running example is based were 1/100 for the dog fur, and 29/1148 for Wayne Williams' hair. Match probabilities have been slightly but not unrealistically modified to be closer to each other in order to make a conceptual point: the same first-order probabilities, even when they sound precise, may come with different degrees of second-order uncertainty.] In the aggregate, multiplying the individual likelihood ratios further washes away this difference. 

A better alternative is easily available: the evaluation of multiple items of evidence should take into account higher-order uncertainty. @fig-densities (upper part) depicts higher-order probability distributions of different match probabilities given the sample data---the actual number of matches found in the sample databases. <!---By hypothesis, 29 matches were found in the sample database of human hair of size 1,148, and 2 matches were found in the sample database of dog fur of size 78.---> As expected, some random match probabilities are more likely than others, and since the sizes of the two databases are different, the distributions have different spreads: the smaller the database the greater the spread, the greater the uncertainty about the match probability. In light of this, @fig-densities (lower part) depicts the probability distribution for the joint match probability associated with both items of match evidence, hair and fur evidence. The aggregate value of the two pieces of match evidence, then, is given by a distribution over possible likelihood ratios. \todo{add distributions of LRs figure} The shape of this distribution conveys the degree of higher-order uncertainty about the value of the aggregate evidence. \textbf{Marcello (Rafal/Nikodem to add): Can we have a formula for how the two matches are combined in the higher-order approach? In precise probabilism, you multiply the individual LRs, in higher-order probabilism, what do we do formally? Can we also have a distribution of likelihood ratios? What happens if both the numerator and denominator in the LR are distributions?} 


```{r LR_dist,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%"}

# initial calculations #################

# individuall likelihood ratio distribution

lh_hair <- rep(1, 1e4) / hairSamples
lh_dog <- rep(1, 1e4) / dogSamples

# joined likelihood ratio distribution
lh_joint <- lh_hair * lh_dog

# taking densities, wrangling, visualizing

dens_hair <- density(lh_hair)

dens_hair_df <- data.frame(
  x = dens_hair$x,
  y = dens_hair$y
  )
  
dens_dog <- density(lh_dog)

dens_dog_df <- data.frame(
  x = dens_dog$x,
  y = dens_dog$y
)

disjoined_dists <- ggplot() +
  geom_density(stat = 'identity')  +
  geom_density(data = dens_hair_df, aes(x= x, y= y, color = "hair"), stat = 'identity') +
  geom_density(data = dens_dog_df, aes(x= x, y= y, color = "dog"), stat = 'identity') +
  labs(title= 'Disjoined LR distribution', y = 'Density', x = 'Likelihood Ratio', color= 'LR distribution:') +
  scale_color_manual(values = c("dog" = "steelblue", "hair" = "purple")) +
  guides(color = guide_legend(override.aes = list(fill = c( "steelblue", "purple"))))+ 
  theme_tufte(base_size = 10) +
  theme(legend.position = c(0.6, 0.5)) 

lh_joint_dens <- density(lh_joint)

lh_joint_dens_df <- data.frame(
  x = lh_joint_dens$x,
  y = lh_joint_dens$y
)

df_hpdi <- lh_joint_dens_df %>% 
  filter(x >= HPDI(lh_joint, 0.9)[1]) %>% 
  filter(x <= HPDI(lh_joint, 0.9)[2])

joined_dists <- ggplot() +
  geom_density(data = lh_joint_dens_df, aes(x= x, y = y), stat = 'identity', linewidth= 1)  +
  geom_density(data = df_hpdi ,aes(x = x, y = y), stat = 'identity', fill= '#cccccc', linewidth= 0)+
  labs(title = 'Joined LR distribution', y = "Density", x = 'Likelihood Ratio')+ theme_tufte(base_size = 10)+
  geom_text(aes(x = 20000, y = 0.00015, label = "Mean: 1607"), color = "steelblue")+ # calculated with mean(lh_joint)
  geom_text(aes(x = 20000, y = 0.0001, label = "HPDI 0.9: 326 | 2976"), color = "steelblue") # HPDI(lh_joint, 0.9)

```



```{r LR_dist_vis,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "90%", warning = FALSE, message = FALSE}
#| label: fig-lrdens
#| fig-cap: "Distributions of dog and hair likelihood ratios and the resulting joint likelihood ratio. Created with the samples from beta distributions. Shaded area on the second one represents HPDI with 0.9 credibility."
#| fig-pos: H

grid.arrange(disjoined_dists,joined_dists, ncol = 2 )
```




```{r densitiesEvidence-OLD,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%"}

jointEvidence <- dogSamples * hairSamples


densities1Plot <- ggplot()+
  geom_line(aes(x = ps, y = dbeta(ps, hairA, hairB)), lty  = 2)+
  geom_line(aes(x = ps, y = dbeta(ps, dogA, dogB)), lty = 3)+xlim(0,.15)+
  xlab("probability")+
  ylab("density")+
  theme_tufte(base_size = 10)+
  #  annotate(geom  = "label", label = "carpet", x =  0.045, y = 140)+
  annotate(geom  = "label", label = "hair", x =  0.035, y = 80)+
  annotate(geom  = "label", label = "dog", x =  0.06, y = 15)+
#  labs(title = "Conditional densities for  individual items of evidence if the source hypothesis is false")+
#  theme(plot.title.position = "plot")
labs(title = "Distributions of individual match probabilities")+
  theme(plot.title.position = "plot")


densities2Plot <- ggplot()+
  xlab("probability")+
  ylab("density")+
  theme_tufte(base_size = 10)+
  geom_density(aes(x= jointEvidence))+
  geom_vline(xintercept = 0.002760, lty = 2, linewidth = .5)+
  geom_vline(xintercept = 0.000023, lty = 2, linewidth  = .5)+
  geom_vline(xintercept = 0.000144, lty = 3, linewidth = .8)+
  geom_vline(xintercept = 0.001742, lty = 3, linewidth  = .8)+
  #labs(title = "Conditional density for joint evidence",
  labs(title = "Distribution for the joint match probability",
       subtitle = "(with .99 and .9 HPDIs)")+
  theme(plot.title.position = "plot") +
  geom_vline(xintercept = hairMean * dogMean)
```


```{r Figdensities-OLD,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "80%", warning = FALSE, message = FALSE}
#| label: fig-densities-OLD
#| fig-cap: "Beta densities for individual items of evidence and the resulting joint density with .99 and .9 highest posterior density intervals, assuming the sample sizes as discussed and independence, with uniform priors."
#| fig-pos: H

grid.arrange(densities1Plot,densities2Plot, ncol = 1 )
```


The precise probabilist might insist that the value of the evidence---one item or multiple items of evidence---is naturally captured by a precise likelihood ratio. In our running example, this ratio equals one over the first-order match probability, and our best assessment of this first-order probability is still the relative frequency of matches found in the database, whether large or small. If this is right, our best assessment of the match probabilities for both fur and hair evidence should be about $.025$, based on the relative frequencies 2/78 and 29/1148.   If we were to bet whether a dog or a human picked at random would have the matching fur or hair type, our odds should be $.025$ no matter the size of the database. This argument has some bite when evaluating single items of evidence. In fact, the expected values of the match probabilities for hair and match evidence---based on the higher-order distributions in  @fig-densities (upper part)---still end up being about $.025$. If, as the precise probabilist assumes, first-order probabilities is all we should care about, going higher-order would seem a needless complication.

This line of reasoning, however, breaks down when evaluating two or more items of evidence. What should our betting odds be for the proposition that a human and a dog, both picked at random, would have the matching fur and hair type in question? For the precise probabilist, the answer is straightforward: on the assumption of independence, it is enough to multiply the $.025$ individual match probabilities and obtain a joint match probability of `r hairMean * dogMean`. The higher-order probabilist will proceed differently. In assessing first-order match probabilities, they will retain information about higher-order uncertainty as much as possible. This can done in two steps: first, aggregate the higher-order distributions for the two-match probabilities and obtain a higher-order probability distribution for the joint match probability (see @fig-densities); next, to obtain our best assessment of the first-order joint match probability, take the expected value of this latter distribution. The higher-order probabilist will assign `r mean(jointEvidence)` to the joint match probability, a value greater than what the precise probabilist would assign. 



```{r densitiesEvidenceModified-OLD,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%"}

# example with slightly different numbers 
# illustrate divergence between precise and higher order probabilist 
# on value of first-order joint match probabilities

# sample frequency, very small sample size
ps <- seq(0,1, length.out = 1001)
set.seed(231)

evidence1Mean <-  1/20
evidence1A <- 1
evidence1B <- 20

evidence2Mean <- 1000/20000
evidence2A <- 1000
evidence2B <- 20000

evidence1Samples <- sample(ps, 1e4, replace = TRUE, prob = dbeta(ps, evidence1A, evidence1B))

evidence2Samples <- sample(ps, 1e4, replace = TRUE, prob = dbeta(ps, evidence2A, evidence2B))


jointEvidence1 <- evidence1Samples * evidence1Samples
jointEvidenceThree1 <- evidence1Samples^3
jointEvidenceFive1 <- evidence1Samples^5
jointEvidenceSeven1 <- evidence1Samples^7 

jointEvidence2 <- evidence2Samples * evidence2Samples
jointEvidenceThree2 <- evidence2Samples^3
jointEvidenceFive2 <- evidence2Samples^5
jointEvidenceSeven2 <- evidence2Samples^7 

```


So, the higher-order and precise probabilist will disagree about the betting odds for the proposition that a human and a dog, both picked at random, would have matching fur and hair type. The disagreement will become even starker as a larger number of independent items of evidence are evaluated.^[Consider the simple case of independent items of evidence whose individual match probabilities are $.025$. For three, five and seven items of evidence, the joint match probabilities would be: `r (evidence1Mean)^3`,`r (evidence1Mean)^5` and `r (evidence1Mean)^7` (for the precise probabilist) and `r mean(jointEvidenceThree1)`, `r mean(jointEvidenceFive1)` and `r mean(jointEvidenceSeven1)` (for the higher-order probabilist, based on small databases of size 20).] Who should be trusted? Since the higher-order probabilist takes into account more information---that is, the higher-distributions---there is good reason to think that the higher-order probabilist should be trusted more than the precise probabilist.^[As a further illustration of this point, consider a couple of variations of our running example. First, suppose the match probabilities associated with two matches are both set to $.05$ since they are based on the following relative frequencies: one match occurs in a dog fur database and one match occurs in a human hair database, where both databases are small, say of size 20. By multiplying the individual $1/.05$ likelihood ratios associated with the two matches, their evidential value against the defendant would seem quite strong: $1/.05\times 1/.05=`r 1/(evidence1Mean*evidence1Mean)`$. However the match probabilities are based on frequencies resulting from small databases, so their evidential value should be rather weak. Precise probability here seems to exaggerate the aggregate value of the evidence. Following higher-order probabilism, the joint likelihood ratio would be `r 1/mean(jointEvidence1)`, a significantly smaller value. On the other hand, if the same $.05$ match probabilities were based on larger databases, the evidential value of the two matches should be correspondingly greater, but precise probabilism would make no difference. If, for example, 1,000 hair and fur matches are found in databases of size 20,000, the higher-order probabilist would assign `r 1/mean(jointEvidence2)` to the joint likelihood ratio, a much greater value than before. This outcome agrees with our intuitions.] 

Imprecise probabilism will also run into its own problems when assessing the value of aggregate evidence. Recall that the probability measures in the representor set are those compatible with the evidence.  The problem is that almost any random match probability will be compatible with any sample data---with any number of matches found in a reference database. This point should be familiar from the earlier discussion. Think by analogy to coin tossing: even a coin that has a .99 bias toward tails could come up heads on every toss. This series of outcomes is unlikely but possible. Similarly, a hair type that has a match probability extremely small could still be found several times in a sample population. So, it is not clear how to proceed if one takes seriously the binary notion of compatibility. Imprecise probabilism is too permissive because almost any match probability will be compatible with the data. 

Another option for the imprecise probabilist is to rely on reasonable ranges of match probabilities. Suppose these ranges are (.015,.037) (.002, .103), for hair and fur evidence respectively in our original case.^[These are 99% credible intervals starting with uniform priors. A 99\% credible interval is the narrowest interval to which the   expert thinks the true parameter belongs with probability .99. For a discussion of what credible intervals are, how they differ from confidence intervals, and why confidence intervals should not be used, see @kruschke2015doing.] 
<!-- Now, the representor covers
  the convex hull of the probability measures that result in probabilities that are the edges of the intervals. For this reason, to investigate what span of probabilities the imprecise probabilities will end with,
  -->
As expected, the range is wider for dog fur match evidence than hair match evidence: the uncertainty about the dog fur match probability is greater since the sample database was smaller. This is a desirable feature of the interval approach.  Now, to assess the joint uncertainty, it is enough to focus on what happens at the edges of the two intervals. Reasoning with representor members at the edges of the intervals will yield the most extreme probability measure the impreciser is committed to, the worst-case and best-case scenarios. We end up with a new range for the joint match probabilities, (.00003, .003811).\footnote{Redoing the calculations using the upper bounds of the two intervals, $.037$ and $.103$,  yields the following:
\begin{align*}
\mathsf{P}(\s{dog}\wedge \s{hair} \vert \neg \s{source})   & =  .037 \times .103 =.003811.
\end{align*}

\noindent
 This number is around `r round(.003811/lik0,2)` times greater than the original estimate. Given this number, the two matches are much weaker evidence for the source hypothesis than previously thought.    The calculation for the lower bounds, $.015$ and $.002$, yields the following:
\begin{align*}
\mathsf{P}(\s{dog}\wedge \s{hair} \vert \neg \s{source})   & =  .015 \times .002 =.00003
\end{align*}
   
 \noindent  
This number is around `r round(.0003/lik0,2)` times lower than the original estimate.  Given this number, the two matchs are much stronger evidence for the source hypothesis than previously thought.}
The corresponding likelihood ratios could be as high as  `r round(1/0.00003,4)` or as low as `r round(1/0.003811,4)`. The two matches could be much stronger or much weaker evidence than previously thought.

 <!--Now the prior probability of the source hypothesis needs to be higher than `r thresholdL` for the posterior probability to be above .99 (@fig-impactofcharitable). -->

<!--
```{r FigcharitableImpact75,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%", warning = FALSE, message = FALSE}
#| label: fig-impactofcharitable
#| fig-cap: "Impact of dog fur and human hair evidence on the prior, charitable reading."
#| fig-pos: H

charitableImpactPlot
```
-->


<!--
In general, it is impossible to calculate the credible interval for the joint distribution
 based solely on the individual credible intervals corresponding to the individual events.
   We need additional information: the distributions that were used to calculate the intervals 
   for the probabilities of the individual events. In our example, if you additionally knew, 
   for instance, that the expert used  beta distributions (as, arguably, they should in this context),
    you could in principle calculate the  99\% credible interval for the joint distribution.
     It usually will not be the same as whatever the results of multiplying the individual
      interval edges, and it is unlikely that a human fact-finder would be able to correctly 
      run such calculations in their head even if they knew the functional form of the distributions 
      used.^[Also, in principle, in more complex contexts, we need  further information about how the 
      items of evidence are related if we cannot take them to be independent.] 
      So providing the fact-finder with individual intervals, even if further information 
      about the distributions is provided, might easily mislead.^[Investigation of the 
       extent to which the individual interval presentation is misleading  would be an interesting psychological study.]
-->

<!--
As it turns out, given the reported sample sizes, the 99\% credible interval
 for the probability $\mathsf{P}(\s{dog}\wedge \s{hair} \vert \neg \s{source})$ is $(0.000023,  0.002760)$.
The upper bound of this interval would then require the prior probability of the source hypothesis to be above .215
 for the posterior to be above .99. On this interpretation, the two items of match evidence 
 are still not quite as strong as you initially thought, but stronger than what your second calculation indicated. 
-->



Using plausible ranges for the match probabilities leaves the impression that any value in the interval is just as good as any other. Perhaps we should pick the middle value as representative of the interval. However, relying on the entire interval or the middle value will misrepresent the evidence.  To see why, consider again @fig-densities (lower part) which depicts the probability distribution for the joint match probability. Interestingly, this distribution is not symmetric. So the most likely value (and the bulk of the distribution, really) does not lie in the middle between the edges. <!--Just because the parameter lies in an interval with some posterior probability, it does not mean that the ranges near the edges of the interval are equally likely---the bulk of the density might very well be closer to one of the edges. --> Therefore, only relying on the edges---or taking central values as representative of the interval---can lead to overestimating or underestimating the probabilities at play.\footnote{The calculations for the joint interval assume that because the worst- or best-case probability for one event is $x$ and the worst- or best-case probability for another independent event is $y$, the worst- or best-case probability for their conjunction is  $xy$. However, this conclusion does not follow if the margin of error (credible interval) is fixed. Just because the probability of an extreme value $x$ for one variable $X$ is .01, and so it is for the value $y$ of another independent variable $Y$, it does not follow that the probability that those two independent variables take values $x$ and $y$ simultaneously is the same. In general, it is impossible to calculate the credible interval for the joint distribution based solely on the individual credible intervals corresponding to the individual events.}
 

```{r, lratiosCalc, echo= FALSE, eval=TRUE}

# imprecise intervals: hair - (.015,.037), dog - (.002, .103)
hair_low <- .015
hair_high <- .037

dog_low <- .002
dog_high  <- .103

hair_LR_width <- 1/hair_low - 1/hair_high
dog_LR_width <- 1/dog_low - 1/dog_high

# aggregating two items of evidence

aggregated_low <- 1/ (hair_low * dog_low)
aggregated_high <- 1/ (hair_high * dog_high)

# posteriors ???



```


Another problem in taking intervals as representative of the value of the evidence is that they will tend to widen as more items of evidence are evaluated. The size of the likelihood ratio interval was initially `r round(hair_LR_width, 2)` (hair evidence)  and `r round(dog_LR_width, 2)` (fur evidence). After aggregating the two items of evidence, the likelihood ratio interval widened to `r aggregated_low - aggregated_high`. The size of the match probability interval was initially `r hair_low - hair_high` (hair evidence)  and `r dog_low - dog_high` (fur evidence). 
After aggregating the two items of evidence, the match probability interval narrowed to `r hair_low * dog_low - hair_high * dog_high`. Posterior interval (starting with 1:1 prior odds) was initially `r (1/.015)/(1+1/.015) - (1/.037)/(1+1/.037)` (hair evidence)  and `r (1/.002)/(1+1/.002) - (1/.103)/(1+1/.103)` (fur evidence). After aggregating the two items of evidence, the posterior interval narrowed to `r (1/.00003)/(1+1/.00003) - (1/.003811)/(1+1/.003811)`. 

\todo{LR intervals widen but match and posterior probability intervals do not? How does that work? How can we claim that uncertainty increases?}

 All in all, precise and imprecise probabilism does not fare well in modeling the value of evidence in the aggregate. 
 Instead, the evaluation of multiple items of evidence should take into account higher-order uncertainty (as illustrated in @fig-densities). Whenever probability distributions for the probabilities of interest are available (and they should be available for match evidence and many forms of scientific evidence whose reliability has been studied), those distributions should be reported for assessing the value of the evidence. This approach avoids hiding actual aleatory uncertainties under the carpet. It also allows for a more balanced assessment of the evidence, whereas using point values or intervals may exaggerate or underestimate the value of the evidence.

A couple of clarifications are in order. First, the problem we are highlighting is not confined to match evidence. Say an eyewitness testifies against the defendant: they saw the defendant near the crime scene at the relevant time. To assess the value of this testimony, one should know something analogous to the match probability: if the defendant was not there, how probable is it that the witness would still say the defendant was there? Or suppose a medical test for a disease turns out positive. Here again, to assess the evidential value of the positive test, one should know how probable it is that the test would still turn out positive even when a patient is actually negative. And so on.  These false positive probabilities are usually derived from sample-based frequencies in surveys or experiments: how often witnesses misidentify people; how often tests misdiagnose; etc. So, depending on the sample size, the false positive probabilities will have different degrees of uncertainty, and the latter should be taken into account when evaluating eyewitness testimonies, diagnostic test results, and many other forms of evidence. At the same time---and this is the second clarification---this discussion is not meant to suggest that the problem we are highlighting is confined to differences in sample size; it is broader than that. Probabilities can be subject to uncertainty for other reasons, for example, when they are derived from a probability model for which there is little support, or when the sample size is large but unrepresentative. So, in short, the problem of higher-order uncertainty is widespread and goes beyond match evidence and questions of sample size.



<!--
```{r densLines, echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
prior <- seq(0,1, by = 0.001)
priorH0 <- 1-prior

#-----

jointPosterior <- list()
minima <- numeric(1e4)


# for each likelihood from sample,
# calculate the posterior based on what the prior is
# and find the minimum above threshold
for (s in 1:1e4){
  lik <- jointEvidence[s]
  denomin <- lik * priorH0 + prior
  num <-  lik * priorH0
  posterior <- 1- num/denomin
  jointPosterior[[s]] <- posterior
  minima[s] <- min(prior[posterior > .99])
  }



#minimaPlot <- ggplot()+geom_density(aes(x = minima))+
#  theme_tufte(base_size = 10)+ggtitle("Minimal priors sufficient for posterior >.99")#+xlab("minimal prior")+
#  theme(plot.title.position = "plot")

#minimaGrob <- ggplotGrob(minimaPlot)



#jointPosteriorDF <-   as.data.frame(do.call(cbind, jointPosterior))



#jointPosteriorSubsample <- jointPosteriorDF[,1:300]

#jointPosteriorDF$ps <- ps
#jointPosteriorSubsample$ps <- ps

#jointPosteriorLong <- melt(jointPosteriorSubsample,
#                           id.vars = c("ps"))
#colnames(jointPosteriorLong) <- c("prior", "sample","probability")

#alpha = .25
#size = .2

#densitiesLinesPlot <- ggplot(jointPosteriorLong)+
#  geom_line(aes(x = prior, y = probability,group = sample),
#            alpha = alpha, linewidth = size)+
#  theme_tufte()+xlim(0,.2) +
#  annotation_custom(minimaGrob, xmin = .085, xmax = .185,
#                  ymin = 0.01, ymax = 0.8) +
#  ggtitle("Posterior vs prior (300 sampled lines)") +
#  theme(plot.title.position = "plot")
#densitiesLinesPlot





minimaPlot <- ggplot()+geom_density(aes(x = minima))+
  theme_tufte(base_size = 10)+ggtitle("Minimal priors sufficient for posterior >.99")+xlab("minimal prior")+
  theme(plot.title.position = "plot", plot.title = element_text(size = 10))

jointPosteriorDF <-   as.data.frame(do.call(cbind, jointPosterior))

jointPosteriorSubsample <- jointPosteriorDF[,1:300]

jointPosteriorDF$ps <- ps
jointPosteriorSubsample$ps <- ps

jointPosteriorLong <- melt(jointPosteriorSubsample,
                           id.vars = c("ps"))
colnames(jointPosteriorLong) <- c("prior", "sample","probability")

alpha = .25
size = .2

densitiesLinesPlot <- ggplot(jointPosteriorLong)+
  geom_line(aes(x = prior, y = probability,group = sample),
            alpha = alpha, linewidth = size)+
  theme_tufte()+xlim(0,.2) + theme_tufte(base_size = 10)+
  ggtitle("Posterior vs prior (300 sampled lines)") +
  theme(plot.title.position = "plot", plot.title = element_text(size = 10))

```
-->


<!--
```{r Figlines,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "80%", warning = FALSE, message = FALSE}
#| label: fig-lines
#| fig-cap: "300 lines illustrating the uncertainty about the dependence of the posterior on the prior given aleatory uncertainty about the evidence, with the distribution of the minimal priors required for the posterior to be above .99."
#| fig-pos: H

grid.arrange(densitiesLinesPlot, minimaPlot, ncol= 2)
```
-->







```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
priorA <- 1
priorB <- 1
set.seed(215)
trueH <- runif(1,0,1)

sampleSize <- sample(10:20,size = 1)
testSize <- sampleSize

set.seed(319)
successes <- rbinom(1, sampleSize, trueH)
pointEstimate <- successes/sampleSize

pointPredictions <-  rbinom( 1e4 , size = testSize , prob = pointEstimate )

ps <- seq(0,1,length.out = 1001)

testPredictions <- rbinom( 1e4 , size = testSize , prob = trueH )

posterior <- function(x) dbeta(x, priorA + successes,
                               priorB + sampleSize - successes)

posteriorSample <- sample( ps , size=1e4 ,
                           replace=TRUE , posterior(ps)/sum(posterior(ps)))

posteriorPredictions <- rbinom( 1e4 , size=testSize , prob=posteriorSample )


testProbs <- table(factor(testPredictions, levels = seq(0,sampleSize)))/1e4
pointProbs <- table(factor(pointPredictions, levels = seq(0,sampleSize)))/1e4
posteriorProbs <- table(factor(posteriorPredictions, levels = seq(0,sampleSize)))/1e4

testPlot <- ggplot()+geom_bar(aes(x= testPredictions, y = ..prop..))+
  ggtitle(paste("Predictions based on the true parameter = ", round(trueH,2), sep = ""))+xlim(0,testSize)+ylab("probability")+xlab("successes")+theme_tufte(base_size = 8)+theme(plot.title.position = "plot")


pointPlot <- ggplot()+geom_bar(aes(x= pointPredictions,y = ..prop..))+
  labs(title = paste("Predictions based on the point estimate = ", round(pointEstimate,2)), subtitle = paste(successes, " successes in ", sampleSize, " observations", sep = ""))+xlim(0,testSize)+ylab("probability")+xlab("successes")+theme_tufte(base_size = 8)+theme(plot.title.position = "plot")


samplesPlot <- ggplot()+geom_density(aes(x= posteriorSample))+
  ggtitle(paste("Posterior sample from beta(", 1+successes, ",", 1+testSize - successes,
                ")", sep = ""))+xlab(
                  "parameter value"
                )+theme_tufte(base_size = 8)+theme(plot.title.position = "plot")

posteriorPlot <- ggplot()+geom_bar(aes(x= posteriorPredictions,y = ..prop..))+
  ggtitle("Predictions based on the posterior sample")+xlim(0, testSize)+ylab("probability")+xlab("successes")+theme_tufte(base_size = 8)+theme(plot.title.position = "plot")


```



```{r kldsCalculations,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
klds <- numeric(1000)
for(i in 1:1000){
trueH <- runif(1,0,1)
sampleSize <- sample(10:25,size = 1)
testSize <- sampleSize
successes <- rbinom(1, sampleSize, trueH)
pointEstimate <- successes/sampleSize
pointPredictions <-  rbinom( 1e4 , size = testSize , prob = pointEstimate )
ps <- seq(0,1,length.out = 1001)
testPredictions <- rbinom( 1e4 , size = testSize , prob = trueH )
posteriorSample <- sample( ps , size=1e4 ,
                           replace=TRUE , posterior(ps)/sum(posterior(ps)))
posteriorPredictions <- rbinom( 1e4 , size=testSize , prob=posteriorSample )
testProbs <- table(factor(testPredictions, levels = seq(0,sampleSize)))/1e4
pointProbs <- table(factor(pointPredictions, levels = seq(0,sampleSize)))/1e4
posteriorProbs <- table(factor(posteriorPredictions, levels = seq(0,sampleSize)))/1e4
klds[i] <- kld(testProbs,pointProbs) - kld(testProbs,posteriorProbs)
}

```