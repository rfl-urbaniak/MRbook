---
title: "Second-order Probabilism: Expressive Power and Accuracy"
author: "Rafal Urbaniak and Marcello Di Bello"
date: '`r Sys.Date()`'
format:
  pdf:
    toc: true
    include-in-header: quartoStyle.sty
    mainfont: Times New Roman
    sansfont: Times New Roman
documentClass: scrartcl
classOptions: [dvipsnames, enabledeprecatedfontcommands, headings=big]
font:
  size: 10pt
url:
  color: blue
bibliography: [../../../references/referencesMRbook.bib]
csl: apa-6th-edition.csl
indent: true
numbersections: true
---

  <!-- pdf:
    numberSections: true
    dfPrint: kable 
    keepTex: true
    toc: true
    include-in-header:  quartoStyle.sty
    latex_engine: pdflatex  -->


```{r setup}
#| include: false

knitr::opts_chunk$set(echo = TRUE)
library(dagitty)
library(rethinking)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(tidyr)
library(philentropy)
library(latex2exp)
library(gridExtra)
library(rethinking)
library(bnlearn)
library(gRain)
library(reshape2)
library(truncnorm)
library(ggforce)



ps <- seq(0,1, length.out = 1001)
getwd()
source("../../scripts/CptCreate.R")
source("../../scripts/SCfunctions.R")

SCprobsFinal <- readRDS("../../datasets/SCprobsFinal.rds")
attach(SCprobsFinal)      
source("../../scripts/SCplotCPTs.R")
source("../../scripts/SCplotDistros.R")

```




\vspace{2cm}

\noindent \textbf{DISCLAIMER:} \textbf{This is a draft of work in progress, please do not cite or distribute without permission.}

\thispagestyle{empty}

\newpage

\begin{quote} \textbf{Abstract.}  \todo{need to write one when done}

\end{quote}


# Introduction
\label{sec:introduction}


Precise probabilism (PP) has it that a rational agent's (RA)  uncertainty is to be represented as a single probability measure. The view has been criticized on the ground that RA's degrees of belief are not appropriately evidence-responsive, especially when evidence is scant. Accordingly, an alternative view---imprecise probabilism (IP)---has been proposed, on which RA's uncertainty is to be represented by a set of probability measures, rather than a unique one.

Unfortunately, this view runs into problems as well. (1) It still does not seem to be sufficiently evidence-responsive, (2) it is claimed to get certain comparative probability judgments wrong, (3) it seems to be unable to model learning when the starting point is complete lack of information, and (4) notoriously there exist no inaccuracy measure of an imprecise credal stance if the measure is to satisfy certain straightforward formal conditions. 

\todo{Think about including synergy}
<!-- Moreover, while it seems to handle some cases of opinion pooling better than PP, it still can't capture the phenomenon of synergy, where slightly disagreeing sources or experts jointly in some sense seem to improve the epistemic situation. -->

The main claim of this paper is that the way forward is to use higher-order probabilities to represen't RA's uncertainty in the relevant cases. The key idea is that uncertainty is not a single-dimensional thing to be mapped on a single one-dimensional scale like a real line and that itâ€™s the whole shape of the whole distribution over parameter values that should be taken under consideration. This guiding idea  can be used to resolve many problems and philosophical puzzles raised in the debate between PP and IP. Moreover, Bayesian probabilistic programming already provides a fairly reliable implementation framework of this approach.

\todo{add structure description}


# Precise vs. imprecise probabilisms
\label{sec:three-probabilism}


## Precise probabilism

Precise probabilism (\textsf{PP}) holds that a rational 
agent's  uncertainty about a hypothesis is to be 
represented as a single, precise probability measure. 
This is an elegant and simple theory. But representing our uncertainty about 
a proposition in terms of a single, precise probability runs into a number of difficulties. Precise probabilism---arguably---fails to 
capture an important dimension of how 
our fallible beliefs reflect the evidence 
we have (or have not) obtained. A couple of stylized 
examples should make the point clear. For the sake of simplicity, 
we will use examples featuring coins.

\begin{quote}
\textbf{No evidence v. fair coin}
You are about to toss a coin, but have no evidence 
whatsoever about its bias. You are completely ignorant. 
Compare this to the situation in which you know, 
based on overwhelming evidence, that the coin is fair. 
\end{quote}

\noindent On precise probabilism, both scenarios are represented by assigning a probability of .5 to the outcome \emph{heads}. If you are completely 
ignorant, the principle of insufficient evidence suggests that you assign .5 to both outcomes.  Similarly, if you know for sure the coin is fair, assigning .5 seems the best way to quantify the uncertainty about the outcome. The agent's evidence in the two scenario is quite different, but precise probabilities fail to  capture this difference. 

\begin{quote}
\textbf{Learning from ignorance}
You toss a coin with unknown bias. You toss it 10 times and observe \emph{heads} 5 times. Suppose you toss it further and observe 50 \emph{heads} in 100 tosses. 
\end{quote}

\noindent
Since the coin initially had unknown bias, you should 
presumably assign a probability of .5 to both outcomes if you stick with \textsf{PP}. 
After the 10 tosses, you end up again with an estimate of .5.
You must have learned something, but whatever that is, it is not modeled by precise probabilities. When you toss the coin 100 times and observe 50 heads, you learn something new as well. But your precise probability assessment will again be .5.

These  examples suggest that precise probabilism is not appropriately responsive to evidence when it comes to representing what RA justifiedly believes of has learned. It ends up assigning the same probability in  situations in which one's evidence is quite different:  when no evidence is available about the coin's bias; when there is little evidence that the coin is fair (say, after only 10 tosses); and  when there is strong evidence that the coin is fair (say, after 100 tosses). The general problem is, precise probability captures the value around which your uncertainty should be centered, but fails to capture how centered it should be given the evidence.^[In fact, analogous problems arise even if we do not start with complete lack of evidence; if RA initially weakly believes that the coin is  .6 biased towards heads, as she might still learn more, by confirming her belief  by tossing the coin repeatedly and observing, say, 60 heads in 100 tosses---but this improvement is not mirrored in the precise probability she will assign to heads.]
 
 Precise probabilism, it has been argued, fails also to account for cases in which an agent remains undecided even after some additional evidence has been obtained. Imagine RA doesn't know what the bias of the coin is, which PP represents as $\pr(H)= .5$. Then she  learns that the bias towards heads has been slightly increased  by .001 (in the philosophical literature, this is called \emph{sweetening}. Intuitively, this  might still leave RA equally undecided when it comes to betting on $H$.  that would've been fair even if the actual chance of $H$ was .5 and not .001. The same sweetening, however, should make RA bet on $H$ if their original lack of information  was in fact  correctly captured as a precise credence.
 
 

 <!-- ^[Precise probabilism suffers from other difficulties. For example,  it has problems  with formulating a sensible method of probabilistic opinion  aggregation [@Elkin2018resolving,@Stewart2018pooling].  A seemingly intuitive constraint is that if every member agrees that $X$ and $Y$ are probabilistically independent, the aggregated credence should respect this. But this is hard to achieve if we stick to \s{PP} [@Dietrich2016pooling]. For instance, a \emph{prima facie} obvious method of linear pooling does not respect this. Consider probabilistic measures $p$ and $q$ such that $p(X)  = p(Y)  = p(X\vert Y) = 1/3$ and  $q(X)  =  q(Y) = q(X\vert Y) = 2/3$. On both measures, taken separately, $X$ and $Y$ are independent. Now take the average, $r=p/2+q/2$. Then $r(X\cap Y) = 5/18 \neq r(X)r(Y)=1/4$. This inability to capture an important epistemological difference, the impreciser insists, is a serious limitation. Intuitively, a precise stance is not justified by the very scant evidence available.] -->


## Imprecise probabilism

What if we give up the assumption that probability assignments should be precise? Imprecise probabilism (\textsf{IP}) holds that  an agent's credal stance towards a hypothesis is to be represented by means of a *set of probability measures*, typically called a  representor $\mathbb{P}$, rather than a single measure $\mathsf{P}$. The representor should include all and only those probability measures which are compatible
with the evidence.  For instance, if an agent knows that the coin is fair, their credal state  would  be represented by the singleton set $\{\mathsf{P}\}$, where $\mathsf{P}$ is a probability measure which assigns $.5$ to \emph{heads}. If, on the other hand, the agent knows nothing about the coin's bias, their credal state would be represented by the set of all probabilistic measures, since none of them is excluded by the available evidence. Note that the set of probability measures does not represent admissible options that the agent could legitimately pick from. Rather, the agent's credal state is essentially imprecise and should be represented by means of the entire set of probability measures.^[For  the development of imprecise probabilism, see @keynes1921treatise; @Levi1974ideterminate; @Gardenfors1982unreliable; @Kaplan1968decision;  @joyce2005probabilities; @VanFraassen2006vague; @Sturgeon2008grain; @walley1991statistical. @bradley2019imprecise is a good source of further references. Imprecise probabilism shares some similarities with what we might call **interval probabilism** [@Kyburg1961; @kyburg2001uncertain]. On interval probabilism, precise probabilities are replaced  by intervals of probabilities. On imprecise probabilism, instead, precise probabilities are replaced by sets of probabilities.  This makes imprecise probabilism more general, since the probabilities of a proposition in the representor set do not have to form a closed interval. In what follows, we will ignore interval probabilism, as intervals do not contain probabilistic information sufficient to guide reasoning with multiple items of evidence.] 


Imprecise probabilism, at least \emph{prima facie}, offers a straightforward picture of learning from evidence, that is a natural extension of the classical Bayesian approach. When faced with new evidence $E$ between time $t_0$ and $t_1$, the representor set  should be updated point-wise,  running the  standard Bayesian updating on each probability measure in the representor:

\begin{align*} \label{eq:updateRepresentor}
\mathbb{P}_{t_1} = \{\mathsf{P}_{t_1}\vert \exists\, {\mathsf{P}_{t_0} \!\in  \mathbb{P}_{t_0}}\,\, \forall\, {H}\,\, \left[\mathsf{P}_{t_1}(H)=\mathsf{P}_{t_0}(H \vert E)\right] \}.
\end{align*}

\noindent
The hope is that, if we start with a range of probabilities that is not extremely wide, point-wise learning will behave appropriately.
<!---^[The hope is also that \s{IP} offers a feasible aggregation method [@Elkin2018resolving;@Stewart2018pooling]: just put all representors together in one set, and voil\'a! However, this is a very conservative method which quickly leads to extremely few points of agreement, and we are not aware of any successful practical deployment of this method.]
--->
For instance, if we start with a prior probability of \emph{heads} equal to .4 or .6, then those measure should be updated to something closer to $.5$ once we learn that a given coin has already been tossed  ten times with the observed number of heads equal 5 (call this evidence $E$). This would mean that if the initial range of values was $[.4,.6]$ the posterior range of values should be more narrow.

But even this seemingly straightforward piece of reasoning is hard to model without using densities. For to calculate $\pr{\s{bias} = k \vert E}$ we need to calculate $\pr{E \vert \s{bias} = k }\pr{\s{bias} = k}$ and divide it by $\pr{E} = \pr{E \vert \s{bias} = k }\pr{\s{bias = k}} + \pr{E \vert  \s{bias} \neq k }\pr{ \s{bias} \neq k}$. The tricky part is obtaining  $\pr{\s{bias} = k}$ or  $\pr{ \s{bias} \neq k}$ in a principled manner without explicitly going second-order, without estimating the parameter value and without using beta distributions. 


The situation is even more difficult if we start with complete lack of knowledge, as imprecise probabilism runs into the problem of **belief inertia**  [@Levi1980enterprise], which arises in situations in which no amount of intuitively relevant evidence could lead the agent to change their belief state, according to a given modeling strategy. To illustrate, how belief inertia might arise in the context of imprecise probabiliism, consider a situation in which you start tossing a coin  knowing nothing about its bias.\todo{added this explanation of belief inertia, check} The range of possibilities is $[0,1]$. After a few tosses, if you observed at least one tail and one heads, you can exclude the measures assigning 0 or 1 to \emph{heads}. But what else have you learned?  If you are to update your representor set point-wise, you will end up with the same representor set.  Consequently, the edges of your resulting interval will remain the same. In the end, it is not clear how you are supposed to learn anything if you start from complete ignorance.

Here's another example of inertia, coming from @Rinard2013against.  Either all the marbles in the urn are green ($H_1$), or exactly one tenth of the marbles are green ($H_2$). Suppose your initial credence about these two hypothesis is complete uncertainty with interval. Next, suppose you learn that a marble drawn at random from the urn is green ($E$). After  using this evidence to condition each probability measure in your representor (which initially contains all possible probability measures over the relevant space) on this evidence, you end up with the same spread of values for $H_1$ that you had  before learning $E$. This holds  no matter  how many marbles are sampled from the urn and found to be green. This is counterintuitive: if you continue drawing green marbles, even if you started with complete uncertainty, you should become more inclided towards the hypothesis that all marbles are green. \todo{tried to explain this more clearly, check}




Some downplay the problem of belief inertia. They insist that vacuous priors should not be used and that imprecise probabilism gives the right results when the priors are non-vacuous. After all, if you started with knowing truly nothing, then perhaps it is right to conclude that you will never learn anything. Another strategy is to say that, in a state of complete ignorance, a special updating rule should be deployed.^[@Lee2017impreciseEpistemology suggests the rule of  \emph{credal set replacement} that recommends that upon receiving evidence the agent should  drop measures rendered implausible, and add all non-extreme plausible probability measures. This, however, is tricky. One  needs a separate account of what makes a distribution plausible or not, as well as a principled account of why one should use a separate special update rule when starting with complete ignorance.] But no matter what we think about belief inertia, other problems plague imprecise probabilism.  Three  problems are particularly pressing. 



One problem is that **imprecise probabilism fails to capture intuitions 
we have about evidence and uncertainty in a number of scenarios.** Consider this example:

\begin{quote}
\textbf{Even v. uneven bias:}
 You have two coins and you know, for sure, that the probability of getting heads is .4, if you toss one coin, and .6, if you toss the other coin. But you do not know which is which. You pick one of the two at random and toss it.  Contrast this with an uneven case. You have four coins and you know that three of them have bias $.4$ and one of them has bias $.6$. You pick a coin at random and plan to toss it. You should be three times more confident that the probability of getting heads is .4. rather than .6.
\end{quote}

\noindent
The first situation can be easily represented by imprecise probabilism. The representor would contain two probability measures, one that assigns .4. and the other that assigns .6 to the hypothesis 'this coin lands heads'.  But imprecise probabilism cannot represent the second situation, at least not without moving to higher-order probabilities or assigning probabilities to chance hypotheses, in which case it is no longer clear whether the object-level imprecision does any heavy lifting.^[Other scenarios can be constructed in which imprecise probabilism fails to capture distinctive intuitions about evidence and uncertainty; see, for example, [@Rinard2013against].  Suppose you know of two urns, \textsf{GREEN} and \textsf{MYSTERY}. You  are  certain \textsf{GREEN} contains only green marbles, but have no information about \textsf{MYSTERY}. A marble will be drawn at random from each. You should be certain that the marble drawn from \textsf{GREEN}  will be green ($G$), and you should be more confident about this than about the proposition that the marble from \textsf{MYSTERY} will be green ($M$). In line with how lack of information is to be represented on \textsf{IP}, for each $r\in [0,1]$  your representor contains a $\mathsf{P}$ with $\pr{M}=r$. But then,  it also contains one with $\pr{M}=1$. This means that it is not the case that for any probability measure  $\mathsf{P}$ in your representor, $\mathsf{P}(G) > \mathsf{P}(M)$, that is, it is not the case that RA is more confident of $G$ than of $M$. This is highly counter-intuitive.] 


Second, besides descriptive inadequacy,  imprecise probabilism fases a  foundational problem. It  arises when we attempt to measure the accuracy of a representor set of probability measures. Workable *scoring rules* exist for measuring the accuracy of a single, precise credence function, such as the Brier score. These rules measure the distance between one's credence function (or probability measure) and the actual value. A  requirement of scoring rules is that they  be \emph{proper}: any agent will score their own credence function to be more accurate than every other credence function. After all, if an agent thought a different credence was more accurate, they should switch to it.  Proper scoring rules are then used to formulate accuracy-based arguments for precise probabilism. These arguments show (roughly) that, if your precise credence follows the axioms of probability theory, no other credence is going to be more accurate than yours whatever the facts are.  Can the same be done for imprecise probabilism? It seems not. Impossibility theorems demonstrate that **no proper scoring rules are available for representor sets**. So, as many have noted, the prospects for an accuracy-based argument for imprecise probabilism look dim  [@seidenfeld2012forecasting; @Mayo-Wilson2016scoring; @Schoenfield2017accuracy; @CampbellMoore2020accuracy]. Moreover, as shown by  @Schoenfield2017accuracy, if an accuracy measure satisfies certain plausible formal constraints, it will never strictly recommend an imprecise stance, as for any imprecise stance there will be a precise one with at least the same accuracy. 

Another problem with imprecise probabilism did not receive attention in the literature, but we find it quite compelling. First, recall that  at the level of conceptual explanation of what probability measures should belong to an agent's representor, the usual phrase is that these should be probability measures compatible with the agents' evidence. The idea is that thanks to this feature, imprecise credal stances are evidence-responsive in a way precise probabilistic stances are not. However,  **degenerate cases aside, it is hard to make sense of the notion of an agent learning that a probabilistic  measure is  incompatible with the evidence.**   But how, exactly, does the evidence exclude probability measures, other than if the evidence is that a proposition is true (false) while the measure assings to it probability zero (one)? \todo{emphasized that this a relatively new objection, check}

This is not a  mathematical question: mathematically [@bradley2012scientific], non-trivial evidential constraints are easy to model. They can take the form, for example, of the \emph{evidence of chances} $\{ \mathsf{P}(X) = x\}$ or $\mathsf{P}(X) \in [x,y]$, or \emph{structural constraints} such as  "$X$ and $Y$ are independent" or "$X$ is more likely than $Y$." While it is clear that these constraints are something that an agent can come to accept if offered such information by an expert to which the agent completely defers, it  is not trivial to explain how  non-testimonial evidence can result in such constraints for an epistemic agent that functions as IP proposes.  

Most of the  examples  in the literature start with the assumption that the agent is told by a believable source that the chances are such-and-such, or that the experimental set-up is such that the agent knows that such and such structural constraint is satisfied. But, besides ideal circumstances, it is unclear how an agent could come to accept such structural constraints upon observation.  The chain of testimonial evidence has to end somewhere. 

Admittedly, there are straightforward degenerate cases: if you see the outcome of a coin toss to be heads, you reject the measure with $\mathsf{P}(H)=0$, and similarly for tails. Another class of cases might arise if you are randomly drawing objects from a finite set where the real frequencies are already known, because this finite set has been inspected. But such extreme cases aside, what else?  Mere consistency constraint wouldn't get the agent very far in the game of excluding probability measures, as way too many probability measures are strictly speaking still consistent with the observations for evidence to result in epistemic progress.

Bradley suggests that "statistical evidence might inform [evidential] constraints (\dots and that evidence) of causes might inform structural constraints" (125-126). This, however, is not a clear account of how exactly this should proceed. One suggestion might be that once a statistical significance threshold is selected, a given set of observations with a selection of background modeling assumptions yields a credible interval. But this is to admit that to reach such constraints, we already have to start with a second-order approach, and drop information about the densities, focusing only on the intervals obtained with fixed margins of errors. But as we will be insisting, if you have the information about densities to start with, there is no clear advantage to going imprecise instead, and there are multiple problems associated with this move. Moreover, such moves require a choice of an error margin, which is extra-epistemic, and it is not clear what advantage there is to  use extra-epistemic considerations of this sort to drop information contained in densities.^[Relatedly, in forensic evidence evaluation even scholars who disagree about the value of going higher-order agree that interval reporting is problematic, as the choice of a limit or uncertainty level is rather arbitrary  [@Taroni2015Dismissal;@Sjerps2015Uncertainty].] 


# Higher-order probabilism

There is, however, a view in the neighborhood that fares better: a higher-order perspective. In fact, some of the comments by the proponents of imprecise probabilism tend to go in this direction. For instance,   Bradley   compares the measures in a representor to committee members, each voting on a particular issue, say the true bias of a coin. As they acquire more evidence, the committee members will often converge on a specific chance hypothesis. He writes:


> \dots the committee members are "bunching up". Whatever measure you put over the set of probability functions---whatever "second order probability" you use---the "mass" of this measure gets more and more concentrated around the true chance hypothesis. [@bradley2012scientific, p. 157]


\noindent
Note, however, that such bunching up cannot be modeled by imprecise probabilism alone.^[Bradley seems to be aware of that, which would explain the use of scare quotes: when he talks about the option of using second-order probabilities in decision theory, he  insists that  'there is no justification for saying that there is more of your representor here or there.' ~[p.~195]] In a similar vein, @joyce2005probabilities, in a paper defending imprecise probabilism,  attempts to explicate something that imprecise probabilism was advertised to handle better than precise probabilism: weight of evidence. But in fact, the explication uses a density over chance hypotheses to account for the notion of evidential weight and conceptualizes the  weight of evidence  as an increase of concentration of smaller subsets of chance hypotheses, without any reference to representors in the explication of the notion of weight.



The idea that one should use  higher-order probabilities  has also been suggested by critics of imprecise probabilism. For example, @Carr2020impreciseEvidence argues that sometimes evidence requires uncertainty about what credences to have. Carr, however, does not articulate this suggestion more fully, does not develop it formally, and does not explain how her approach would fare against the difficulties affecting precise and imprecise probabilism. This is the key goal of this paper.


The underlying idea of the higher-order approach we propose is that **uncertainty is not a single-dimensional thing to be mapped on a single one-dimensional scale such as a real line. It is the whole shape of the whole distribution over parameter values that should be  taken under consideration.**^[Bradley admits this much [@bradley2012scientific, 90], and so does  Konek  [@konek2013foundations, 59]. For instance, Konek disagrees with: (1)  $X$ is more probable than $Y$ just in case $p(X)>p(Y)$, (2)  $D$ positively supports $H$ if $p_D(H)> p(H)$, or (3)  $A$ is preferable to $B$ just in case the expected utility of $A$ w.r.t. $p$ is larger than that of $B$.] From this perspective, when an agent is asked about their credal stance towards $X$, they can refuse to summarize it in terms of a point value $\mathsf{P}(X)$. They can instead express their credal stance in terms of a probability (density) distribution $f_x$ treating  $\mathsf{P}(X)$ as a random variable.  



To be sure, an agent's credal state toward $X$ could sometimes be usefully represented by the expectation, especially when the agent is quite confident about the probability of a given  proposition.  Generally, expectation is  defined as $\int_{0}^{1} x f(x) \, dx$. In the context of our approach here, we can think of  $x$ as the justified degree of belief in a given proposition, and of  $f$ as the density representing the agent's uncertainty about $x$. Another way to conceptualize this is that 
the imprecisers already have intuitions about the compatibility of evidence with a probability measure.  The density here can be thought of a more general take on this notion. The difference, however, is that while it is not quite clear how evidence can completely exclude probability distributions, in non-degenerate cases, Bayesian methods---at least in more straightforward cases---provide guidance as to what shape the posterior density should have given certain evidence and priors.\todo{added some explanation here, check} 
\todo{added this take on relation to compatibility, check}

Perhaps, such an expectation can be used as  the precise, object-level credence in the proposition itself, where $f$ is the probability density over possible object-level probability values. But this need not always be the case. If the probability density $f$ is not sufficiently concentrated around a single value, a one-point summary might fail to do justice to the nuances of the agent's credal state. This approach lines up with common practice in Bayesian statistics, where the primary role of uncertainty representation is assigned to the whole distribution. Summaries such as the mean, mode standard deviation,  mean absolute deviation, or highest posterior density intervals are only succinct ways for representing the uncertainty of a given scenario.  For example, consider again the scenario in which the agent knows that the bias of the coin is  either .4 or .6 but the former is three times more likely. Representing the agent's credal state with the expectation $\mathsf{P}(X) = .75 \times .4 + .25 \times .6 = .45$ would  fail to capture an important feature of RA's  belief---that she believes the two biases to be of hugely different  plausibilities, and that she in fact is certain that the bias is *not* .75. 

This higher-order approach as a technical devise is not very surprising. Bayesian probabilistic programming languages embrace the well-known idea that parameters can be stacked and depend on each other in more or less complicated manners @Bingham2021PPwithoutTears. What is however somehow surprising is that while the technical devise has been available, it hasn't been implemented to model agent's uncertainty, and by the same token to address all the challenging scenarios we discussed so far. 

Once we allow more expressive power in this fashion, we obtain rather straightforwardly more honest representations  of RA's credal states, illustrated in @fig-evidenceResponse. In particular, the scenario in which the two biases of the coin are not equally likely---which imprecise probabilism cannot model---can be easily modeled within high-order probabilism by assigning different probabilities to the two biases.
  

```{r evidenceResponse1,echo=FALSE,eval=TRUE,fig.align = "center", cache=TRUE, fig.show = "hold", out.width = "100%",   message = FALSE, warning = FALSE, results = FALSE}
p <- seq(from=0 , to=1 , by = 0.001)
FairDensity <- ifelse(p == .5, 1, 0)
FairDF <- data.frame(p,FairDensity)
FairPlot <- ggplot(FairDF, aes(x = p, y = FairDensity))+geom_line()+theme_tufte()+
  xlab("parameter value")+ylim(c(0,1))+ylab("probability")+ggtitle("Fair coin")+
  theme(plot.title.position = "plot")

UniformPlot <- ggplot()+xlim(c(0,1))+stat_function(fun = dunif, args = list(min = 0, max = 1))+
  ylim(0,1)+
  theme_tufte()+
  xlab("parameter value")+
  ylab("probability density")+theme(plot.title.position = "plot")+ggtitle("Unknown bias")


p <- seq(from=0 , to=1 , by = 0.001)
TwoDensity <- ifelse(p == .4 | p == .6, .5, 0)
TwoDF <- data.frame(p,TwoDensity)
TwoPlot <- ggplot(TwoDF, aes(x = p, y = TwoDensity))+geom_line()+theme_tufte()+
  xlab("parameter value")+ylim(c(0,1))+ylab("probability")+ggtitle("Two biases (insufficient reason)")+
  theme(plot.title.position = "plot")+scale_x_continuous(breaks = c(.4,.6), labels = c(.4, .6))




p <- seq(from=0 , to=1 , by = 0.001)
TwoUnbalancedDensity <- ifelse(p == .4, .75, ifelse(p ==  .6, .25, 0))
TwoUnbalancedDF <- data.frame(p,TwoUnbalancedDensity)

TwoUnbalancedPlot <- ggplot(TwoUnbalancedDF, aes(x = p, y = TwoUnbalancedDensity))+geom_line()+theme_tufte()+
  xlab("parameter value")+ylim(c(0,1))+ylab("probability")+ggtitle("Two unbalanced biases")+
  theme(plot.title.position = "plot")+scale_x_continuous(breaks = c(.4,.6), labels = c(.4, .6))

```



```{r FigevidenceResponse2,echo=FALSE,eval=TRUE,fig.align = "center", cache=TRUE, fig.show = "hold", out.width = "80%"}
#| label: fig-evidenceResponse
#| fig-cap: "Examples of higher-order distributions for a few  scenarios problematic for both precise and imprecise probabilism."
#| fig-pos: t

grid.arrange(FairPlot,UniformPlot, TwoPlot, TwoUnbalancedPlot)
```



Besides its flexibility in modelling uncertainty, higher-order probabilism does not fall prey to belief inertia. Consider a situation in which you have no idea about the bias of a coin. So you start with a uniform density over $[0,1]$ as your prior. By using binomial probabilities as likelihoods, observing  any non-zero number of heads will exclude 0 and observing any non-zero number of  tails will exclude 1 from the basis of the posterior. The posterior distribution will become more centered around the parameter estimate as the observations come in. 


@fig-intertia2 shows---starting with a uniform prior distribution--- how the posterior distribution changes after successive observations of heads,  heads again, and then tails.^[More generally, learning about frequencies, assuming independence and constant probability for all the observations, is modeled the Bayes way. You start with some prior density $p$ over the parameter values. If you start with complete lack of information, $p$ should be uniform. Then, you observe the data $D$ which is the number of successes $s$ in a certain number of observations $n$. For each particular possible value $\theta$ of the parameter, the probability of $D$ conditional on $\theta$ follows the binomial distribution. The probability of $D$ is obtained by integration. That is:
\begin{align*}
p(\theta \vert D) & = \frac{p(D\vert \theta)p(\theta)}{p(D)}\\
& = \frac{\theta^s (1-\theta)^{(n - s)}p(\theta)}{\int (\theta')^s (1-\theta')^{(n - s)}p(\theta')\,\, d\theta'}.
\end{align*}
] A further advantage of high-order probabilism over imprecise probabilism is that the prospects for accuracy-based arguments are not foreclosed. This is a significant shortcoming of imprecise probabilism, especially because such arguments exist for precise probabilism. One can show that there exist proper scoring rules for higher-order probabilism. These rules can then be used to formulate accuracy-based arguments. Another interesting feature of the framework is that the point made by Schoenfield against imprecise probabilism does not apply: there are cases in which accuracy considerations recommend an imprecise stance (that is, a multi-modal distribution) over a precise one. We will get back to these issues when we talk about accuracy. \todo{Ref to section}

```{r Figinertia2, eval = TRUE, echo=FALSE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", message= FALSE, warning=FALSE}

n <- 1000 #parameters 
s <- 1e5  #sample size

ps <- seq(from=0 , to=1 , length.out=n)

prior <- rep(1/n , n) #uniform prior

likelihood1g <- dbinom( 1 , size=1 , prob=ps)
likelihood2g <- dbinom( 2 , size=2 , prob=ps)
likelihood2g1b <- dbinom( 2 , size=3 , prob=ps)

posterior1g <- likelihood1g * prior
posterior2g <- likelihood2g * prior
posterior2g1b <- likelihood2g1b * prior


posterior1g <- posterior1g / sum(posterior1g)
posterior2g <- posterior2g / sum(posterior2g)
posterior2g1b <- posterior2g1b / sum(posterior2g1b)

upperLimit <- .003

InertiaPriorPlot <- ggplot()+geom_line(aes(x = ps, y = prior))+theme_tufte(base_size = 7)+xlab("parameter")+
  ylab("probability")+theme(plot.title.position = "plot")+ggtitle("Prior")+
  scale_x_continuous()+scale_y_continuous(limits = c(0,upperLimit))


InertiaOneGPlot <- ggplot()+geom_line(aes(x = ps, y = posterior1g))+theme_tufte(base_size = 7)+xlab("parameter")+
  ylab("probability")+theme(plot.title.position = "plot",
                            axis.text.y = element_blank(),
                            axis.title.y = element_blank(),
                          axis.ticks.y =element_blank()
)+ggtitle("Evidence: h")+
  scale_y_continuous(limits = c(0,upperLimit))



InertiaTwoGPlot <- ggplot()+geom_line(aes(x = ps, y = posterior2g))+theme_tufte(base_size = 7)+xlab("p")+
  ylab("probability")+theme(plot.title.position = "plot"#, 
#                        axis.text.y = element_blank(),
#                            axis.title.y = element_blank(),
#                            axis.ticks.y = element_blank()
                  )+ggtitle("Evidence: h, h")+
scale_y_continuous(limits = c(0,upperLimit))

InertiaTwoGoneBluePlot <- ggplot()+geom_line(aes(x = ps, y = posterior2g1b))+theme_tufte(base_size = 7)+xlab("parameter")+
  ylab("probability")+theme(plot.title.position = "plot",     axis.text.y = element_blank(),
  axis.title.y = element_blank(),
      axis.ticks.y = element_blank()
           )+ggtitle("Evidence: h, h, t")+
  scale_y_continuous(limits = c(0,upperLimit))
```



```{r Figinertia3, echo=FALSE, eval=TRUE,fig.align = "center", cache=TRUE, fig.show = "hold", out.width = "80%",   message = FALSE, warning = FALSE, results = FALSE}
#| label: fig-intertia2
#| fig-cap: "As observations of heads, heads and tails come in, extreme parameter values drop out of the picture and the posterior is shaped by the evidence."


grid.arrange(InertiaPriorPlot, InertiaOneGPlot,  InertiaTwoGPlot, InertiaTwoGoneBluePlot, ncol = 2, nrow = 2)
```







<!-- It seems, therefore, that higher-order probabilism outperforms both precise and imprecise probabilism, at the descriptive as well as the normative level. From a descriptive standpoint, higher-order probabilism can easily model a variety of scenarios that cannot be adequately modeled by the other versions of probabilism. From a normative standpoint, accuracy maximization may sometimes recommend that a rational agent represent their credal state with a distribution over probability values rather than a precise probability measure\todo{add ref to vFraasen in fn; perhaps extend the discussion a bit } (more on this soon).^[Having read van Fraasen's "Laws and Symmetry", you might also worry that going higher order somehow leads to a contradiction; we will address this concern later on.]  -->


\todo{The next two passages are somewhat new  and need to be carefully read and revised. Marcello is unhappy about them!}

The reader might be worried. The examples we discussed so far involve estimation of chances or population frequencies; but how are we to conceptualize higher order probabilities in a more general settings when we think of first-order probabilities as RAs degrees of belief? One might argue: since first-order probabilities capture one's uncertainty about a proposition of interest, second-order probabilities are supposed to capture one's uncertainty about how uncertain you are. But seems that agents with a decent amount of introspection should be aware of how uncertain they are, so  "estimating" their first-order uncertainties seems unnecessary.  


Let us propose a somewhat more general picture that we hope will address this concern. In many contexts, evidence justifies first-order probability assignments (for instance, population frequency estimates) to various degrees. For instance, suppose there is no evidence about the bias of a coin. Then, each first-order point uncertainty about it would be equally (un)-justified. If, instead, we know the coin is fair, the evidence clearly selects one preferred value, .5. But often and with respect to propositions other than straightforward propositions about a frequency, evidence is stronger than the former case and weaker than the latter case. The evidence justifies different values of first-order uncertainty to various degrees. On our picture, second order probabilities can be conceptualized in such a context as densities capturing the extent to which different first-order uncertainties are supported by the evidence.


The unavailability of a proper scoring rule was another weak spot of imprecise probabilism. Let us now turn to investigating how higher-order probabilism handles it. 





# Accuracy in the second-order setting



## Accuracy

As we already discussed, one challenge for the imprecisers is providing a workable scoring rule that would be a
counterpart of, say, the Brier scoring rule for the precise case.
While the imprecisers have hard time defining what the accuracy of a set of measures is, that is not the case for the second-order approach.
Already some work has been done on the notion of accuracy of continuous probability distributions (@HersbachDecomp2000, @Pettigrew2012Epistemic-Utili, @GneitRafter2007). One key notion in use is that of continuous ranked probability score (CRPS) of a distribution $p$
 with respect to a possible world $w$:
\begin{align*}
I(p,w) &= \int_{-\infty}^\infty \vert \mathsf{P}(x) - \mathbf{ 1 }(x\geq V(w))\vert ^2 \, dx
\end{align*}
\noindent where $\mathsf{P}$ is the cumulative probability corresponding to a given density, and
\begin{align*}
\mathbf{ 1 }(x \geq V(w)) & = \begin{cases} 1 & \text{ if } x \geq V(w)\\
0 & \text{ o$\,$/w. }
\end{cases}
\end{align*}
\noindent  
The CRPS score takes the Cramer-Von-Mises measure of distance between 
densities, defined in terms of the area under the squared euclidean distances between the corresponding cumulative density functions:
\begin{align*}
\mathcal{C}(p,q) & = \int_{0}^{1} \vert P(x) - Q(x)\vert^2 \, dx
\end{align*}
\noindent and uses it to measure distance to an epistemically omniscient chance hypothesis,
 which either puts full weight on 0, if a given proposition is false, or on 1, otherwise. We will start building by reflecting on this approach. ^[For the computational ease, we will be using a grid approximation of the densities, as in practice we are unable to work with infinite precision anyway (note for instance that there are no readily computable solutions to the integral used in the definition of CRPS, although it can sometimes be evaluated in closed form) [ @GneitRafter2007, p. 366].]

Let's consider a scenario similar to Schoenfield's (EMS) \todo{What does EMS stand for?}, with an added layer of uncertainty, we have the following situation: An opponent has two coins. One of these coins has a normal distribution of Heads centered around $.3$, and the other is centered around $.5$. Both coins have a standard deviation of $.05$. The opponent randomly selects one of these coins and flips it. The RA knows all the details of this setup.

Now, let's consider three possible stances that RA could take, although there are many possible options:


```{r calculationsEMC,echo=FALSE,eval=TRUE,fig.align = "center", cache=TRUE, fig.show = "hold", out.width = "100%",   message = FALSE, warning = FALSE, results = FALSE}

plotDistroPlain <- function(distro, title, mult = 1.2) {
  plot <-  ggplot()+theme_tufte(base_size = 7)+xlab("parameter values")+
    ylab("probability")+theme(plot.title.position = "plot")+ylim(0,1)+
    ggtitle(title)+
    geom_line(aes(x = ps,y = distro))+
    ylim(c(0,mult * max(distro)))
  return(plot)
}


kld <- function(p,q) kullback_leibler_distance(p,q, testNA = TRUE, unit = "log2",
                                               epsilon = 0.00001)

n <- 1000
ps <- seq(0,1,  length.out =n)
ch1 <- c(rep(0,n-1),1)
indicator1 <- as.numeric(ps >= 1)
ch0 <- c(1, rep(0,n-1))
indicator0 <- as.numeric(ps>=0)
a <- dnorm(ps, .3, .05)
b <- dnorm(ps, .5, .05)
c <- ifelse(ps <= .4, a, b) 

bimodal <- c / sum(c)
bimodalCum <- cumsum(bimodal)
distBi1 <- (bimodalCum - indicator1)^2
distBi0 <- (bimodalCum - indicator0)^2

centered <-   dnorm(ps, .4, .05)
centered <- centered/sum(centered)
centeredCum <- cumsum(centered)
distCe1 <- (centeredCum - indicator1)^2
distCe0 <- (centeredCum - indicator0)^2

aw <- dnorm(ps, .2, .05)
bw <- dnorm(ps, .6, .05)
cw <- ifelse(ps <= .4, aw, bw) 
bimodalWide <- cw / sum(cw)
bimodalWideCum <- cumsum(bimodalWide)
distBiW1 <- (bimodalWideCum - indicator1)^2
distBiW0 <- (bimodalWideCum - indicator0)^2

#now CVM distances from truth and falsehood
#Bi wins in particular distances
dc1 <- sum(distCe1)
db1 <- sum(distBi1)
dbw1 <- sum(distBiW1)
dc0 <- sum(distCe0)
db0 <- sum(distBi0)
dbw0 <- sum(distBiW0)

#now expected values
expBi <- sum(ps * bimodal)
expCe <- sum(ps * centered)
expBiW <- sum(ps * bimodalWide)

expCVMbi <- expBi * db1 + (1-expBi) * db0
expCVMbW <- expBiW * dbw1 + (1-expBiW) * dbw0
expCVMCe <- expCe * dc1 + (1-expCe) * dc0


# now with KLD to omniscient function
kldBi1 <- kld(ch1,bimodal)
kldBi0 <- kld(ch0,bimodal)

kldBiW1 <- kld(ch1,bimodalWide)
kldBiW0 <- kld(ch0,bimodalWide)

kldCe1 <- kld(ch1,centered)
kldCe0 <- kld(ch0,centered)


expKLDbi <- expBi * kldBi1 + (1-expBi) * kldBi0
expKLDbW <- expBiW * kldBiW1 + (1-expBiW) * kldBiW0
expKLDCe <- expCe * kldCe1 + (1-expCe) * kldCe0
```



```{r figEMC,echo=FALSE,eval=TRUE,fig.align = "center", cache=TRUE, fig.show = "hold", out.width = "100%",   message = FALSE, warning = FALSE, results = FALSE}
#| label: fig-emc
#| fig-cap: 'Three (out of many) candidates in a vague EMS scenario. All distributions are built from normal  distributions with standard deviation $.5$, the bimodal ones are "glued" in the middle.'
#| fig-pos: H

grid.arrange(plotDistroPlain(bimodal, "Bimodal, with modes at .3 and .5"),
plotDistroPlain(centered, "Centered around .4"),
plotDistroPlain(bimodalWide, "Wide bimodal, with modes at .2 and .6"), ncol = 1)
```


An impreciser might be inclined to say that it is the bimodal distribution that's appropriately evidence-responsive.
The centered one, while centering on the expected value, definitely gets the chances wrong, 
while the wide bimodal has its guesses too close to truth values and too far from the actual known chances.
 Now, is this in any way mirrored by CRPS and expected CRPS calculations? It turns out it isn't. 

\begin{table}[H]
\centering
\begin{tabular}{lrrrrrr}
\toprule
distribution & CRPS1 & CRPS0 & KLD1 & KLD0 & ExpCRPS & ExpKLD\\
\midrule
bimodal & 534.7305 & 334.9305 & 80.06971 & 33.90347 & 414.8505 & 52.36997\\
centered & 571.2192 & 371.4192 & 110.84220 & 53.13440 & 451.3392 & 76.21752\\
wide bimodal & 485.4052 & 285.6177 & 54.13433 & 19.50965 & 365.5340 & 33.35974\\
\bottomrule
\end{tabular}
\caption{CPRS and KLD inaccuracies of the three distributions to the TRUE and FALSE omniscient functions, with expected inaccuracies (average assessment of how well the distribution aligns with both omniscient functions setups).}
\label{tbl:comp1}
\end{table}

Based on Table \ref{tbl:comp1}, it is worth noting that the expected inaccuracy metric recommends the wide bimodal distribution, which does not seem desirable! Furthermore, this conclusion remains consistent even when we use the KL divergence from the omniscient measure instead of the CRPS. This suggests that the choice of the evaluation metric itself is not the root cause of this recommendation.

The problem here is that all these distributions share the same expected value: $.4$, which is used in 
the calculations of the expected inaccuracies. This also means that not only the wide bimodal distribution 
expects itself to be the least inaccurate, \todo{I do not understand this sentence.} but also that other measures expect it to be the least inaccurate! 
This observation raises concerns about the strategy of (i) calculating two distances/divergencies from the two 
extreme omniscient measures and (ii) averaging by plugging in the expected value, because it will not result in a proper inaccuracy score. 

This approach however is clearly against the spirit of our enterprise. If we start with the idea that expected 
values are often not good representations of RA's uncertainty, it is not particularly surprising that they fail to yield reasonable expected inaccuracy calculations. Because all three distributions share the same expected value, the difference in the probabiliies they assign will be insignificant in the weightning stage (ii). This leaves us with a question, how can we adequately account for the complexity of RA's credal state in the expected inaccuracy considerations?  

If we instead treat RA's higher-order probabilities as beliefs regarding which parameter values are the righ ones (e.g. true chances, real population frequencies, the point credences justified by the evidence) we should see how taking these intuitions seriously plays out.     

Rather than measuring inaccuracy in relation to just two omniscient credences that peak at either 0 or 1 and then averaging using expected values, we should instead utilize a set of $n$ potential true probability hypotheses. Each of these hypotheses corresponds to a single bin in our approximation. We would then compute all the inaccuracies with respect to their corresponding omniscient functions and determine the expected inaccuracy scores using the entire distributions rather than relying solely on their expected values.   

For the three distributions we're discussing in this chapter, the inaccuracies calculated using CRPS  and KL 
divergence with respect to various potential true probability distributions look as in @fig-inaccuracies2.


```{r figinaccuracies2,echo=FALSE,eval=TRUE,fig.align = "center", cache=TRUE, fig.show = "hold", out.width = "100%", message = FALSE, warning = FALSE, results = FALSE}
#| label: fig-inaccuracies2
#| fig-cap: 'CLPSR and KL divergence based inaccuracies vs (omniscient functions corresponding to) $n$ true probability hypotheses for the three distributions discussed in this section.'
#| fig-pos: H


point <- function (value) ifelse(abs(ps - value) == min(abs(ps - value)), 1, 0)
indicator <- function(pointf) ifelse( ps >= ps[min(which(pointf != 0))], 1,0  )

cvm <- function(w,p){
  cumw <-  cumsum(w)
  cump <- cumsum(p)
  dist <- (cump - cumw )^2
  return(sum(dist))
}

inaccuracyChancy <- function(distro){
crpss <- numeric(length(ps))
klds <- numeric(length(ps))
for(i in 1:length(ps)){
chance <- point(ps[i])
crpss[i] <- cvm(chance,distro)
klds[i] <- kld(chance,distro)
}
df <- data.frame(ps,distro,crpss,klds)
return(df)
}

InaBi <- inaccuracyChancy(bimodal)
InaCe <- inaccuracyChancy(centered)
InaBiW <- inaccuracyChancy(bimodalWide)


bcplot <- plotDistroPlain(InaBi$crpss, "Bimodal: CRPS")+ylab("inaccuracy")+
            xlab("true probability")

bkplot <- plotDistroPlain(InaBi$klds, "Bimodal: KLD")+
  ylab("inaccuracy")+xlab("true probability")


ccplot <- plotDistroPlain(InaCe$crpss, "Centered: CRPS")+ylab("inaccuracy")+
  xlab("true probability")

ckplot <- plotDistroPlain(InaCe$klds, "Centered: KLD")+ylab("inaccuracy")+
  xlab("true probability")

bwcplot <- plotDistroPlain(InaBiW$crpss, "Bimodal wide: CRPS")+ylab("inaccuracy")+
  xlab("true probability")

bwkplot <- plotDistroPlain(InaBiW$klds, "Bimodal wide: KLD")+ylab("inaccuracy")+
  xlab("true probability")

library(gridExtra)
grid.arrange(bcplot, bkplot, ccplot, ckplot, bwcplot, bwkplot)
```





One important difference transpires between using CRPS rather than KLD. Notice how for chance hypotheses 
between the actual peaks the inaccuracy remains flat. This seems to be an artifice of choosing a squared
distance metric. If instead we go with a more principled, information-theory-inspired KL divergence,
inaccuracy in fact jumps a bit for values in between the peaks for the bimodal distributions, which 
seems intuitive and desirable.


Note that now the expected inaccuracies of the distributions from their perspective look as in \todo{nl: Their perspective?}
\mbox{Table \ref{tbl:expected2}.}

\begin{table}[H]
\begin{tabular}{lrrrrrr}
& \multicolumn{3}{c}{CPRS} & \multicolumn{3}{c}{KLD} \\
\toprule
  & bimodal & centered & wide bimodal & bimodal & centered & wide bimodal\\
\midrule
bimodal & 64.670 & 78.145 & 88.380 & 8.577 & 10.655 & 11.336\\
centered & 41.657 & 28.181 & 85.911 & 9.239 & 7.690 & 15.627\\
wide bimodal & 137.699 & 171.719 & 113.989 & 11.541 & 19.231 & 8.689\\
\bottomrule
\end{tabular}
\caption{Expected inaccuracies of the three distributions from their own perspectives.
 Each row corresponds to a perspective.}
\label{tbl:expected2}
\end{table}

It's worth noting that the results now match our common sense: each distribution recommends itself. But how does the framework capture the idea that it is the bimodal distribution that seems more adequate than the others?

One way to interpret that is by looking at inaccuracy concerning chance hyphoteses given by the testimonoial evidence. In this case, these are $H_3$, where the true chance is $0.3$, and $H_5$, where the true chance is $0.5$. You can find the specific inaccuracies for them in Table \ref{tbl:schoen}.




\begin{table}[H]
\centering
\begin{tabular}{lrrrr}
 & \multicolumn{2}{c}{CRPS} & \multicolumn{2}{c}{KLD} \\
\toprule
&H3 & H5 & H3 & H5\\
\midrule
bimodal &55.475 & 55.378 & 7.935 & 7.935\\
centered &72.281 & 72.090 & 9.836 & 9.825\\
wide bimodal & 86.230 & 86.223 & 10.871 & 10.882\\
\bottomrule
\end{tabular}
\caption{CRPS and KLD inacurracies of the three distributions with respect to the two hypotheses.
 Note that on both inaccuracy measures the bimodal distribution dominates the other two.}
\label{tbl:schoen}
\end{table}


To make sure that this favorable outcome isn't due to not using pointed credences, we can redo the calculations using the pointed version. In the pointed version, all the focus is on 0.4, or the weight is evenly divided between 0.3 and 0.5, or between 0.2 and 0.6. As anticipated, when we consider inaccuracy, both of these setups recommend the bimodal version, regardless of which of the two hypotheses is in play (see Table \ref{tbl:schoen2}).




\begin{table}[H]
\centering
\begin{tabular}{lrrrr}
 & \multicolumn{2}{c}{CRPS} & \multicolumn{2}{c}{KLD} \\
\toprule
 &H3 & H5 & H3 & H5\\ \midrule
pointed bimodal &49.75 & 49.75 & 1.00 & 1.00\\
pointed centered &100.00 & 100.00 & 16.61 & 16.61\\
pointed wide bimodal & 99.75 & 99.75 & 16.61 & 16.61\\
\bottomrule
\end{tabular}
\caption{CRPS and KLD inacurracies of the three pointed distributions with respect to the two hypotheses.}
\label{tbl:schoen2}
\end{table}



The discussion so far, while based on an example, might leave the reader wondering about the strict propriety of the KLD inaccuracy measure. To address this concern, a proof is provided in the paper's appendix. In essence, the argument demonstrates that for a second-order discretized probability mass $p$ over a parameter space $[0,1]$, given that the real probability is $\theta$  as the Kullback-Leibler divergence of $p$ from 
the indicator distribution  of $\theta$  (which assigns 1 to $\theta$ and 0 to all other parameter
values in the parameter space), denoted as $\mathcal{I}_{\dkl}^2$.^[The argument generalizes 
to parameter spaces that correspond to probabilities of multiple propositions which are Cartesian 
products of parameter spaces explicitly used in the argument in this section.] It turns out that 
this is a strictly proper inaccuracy measure: each $p$ expects itself to be the least 
inaccurate distribution.\footnote{ The argument has four key moves:  

   
\begin{enumerate}
\item the inaccuracy of $p$ w.r.t. to parameter $\theta$ is just $- \log_2 p(\theta)$,
\item  the expected inaccuracy of $p$ from the perspective of $p$ is the entropy of $p$, $H(p)$,
\item  the inaccuracy of $q$ from the perspective of $p$ is the cross-entropy $H(p,q)$,
\item and it is an established result that cross-entropy is strictly larger than entropy as soon as $p\neq q$.
  \end{enumerate}
}




 While imprecisers face challenges in formulating appropriate scoring rules, the second-order approach, which incorporates KLD and centers its focus on the distribution of all conceivable hypotheses, emerges as a more coherent framework. 









\todo{This section has been revised here and there to flow better as an argument against imprecise probabilism, read more carefully.}


# Handling evidence aggregation


This is not the end of the story. Beyond the difficulties discussed in the literature, we also think that both precise and imprecise probabilities have difficulties when it comes to evidence aggregation when it comes to more realistic cases. In this section we illustrate this with a running example of two pieces of evidence. In the next section, we discuss how the approach we proposed can exploit existing computational methods to handle even more complex cases. Our strategy is to  go over a stylized, but fairly realistic case in which  it makes an important difference whether we approach the problem from the precise, imprecise, or higher-order
 perspective. 

```{r introStarts,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE}
ps <- seq(0,1,length.out = 1001)

hairMean <-  29/1148
hairA <- 30
hairB <- 1149

dogMean <- 2/78
dogA <- 3
dogB <- 79

lik0 <- hairMean * dogMean
prior <- seq(0,.3, by = 0.001)
priorH0 <- 1-prior
denomin <- lik0 * priorH0 + prior
num <-  lik0 * priorH0
posterior <- 1- num/denomin
threshold <- min(prior[posterior > .99])

pointImpactPlot <- ggplot()+geom_line(aes(x = prior, y = posterior))+xlim(0,.07)+
  theme_tufte(base_size = 10)+labs(title = "Prior vs. posterior, based on point estimates",
                                   subtitle = "Joint evidence: dog & hair")+
  geom_vline(xintercept = threshold, lty = 2, size =.5, alpha = .7)+
  annotate(geom = "label", label = "posterior > .99", x = .067, y =.95, size = 2.5)+
  theme(plot.title.position = "plot")
```


A defendant in a criminal case may face multiple 
items of incriminating evidence whose strength 
can at least  sometimes be assessed using probabilities. 
For example, consider a murder case 
in which the police recover trace 
evidence that matches the defendant. Hair found at the crime 
scene matches the defendant's hair (call this evidence \textsf{hair}).
In addition, the defendant owns a dog whose 
fur matches the dog fur found in a carpet wrapped around 
one of the bodies (call this evidence \textsf{dog}).^[The
 hair evidence and the dog fur evidence are stylized after two 
 items of  evidence in the notorious 1981 Wayne Williams case [@deadman1984fiber1; @deadman1984fiber2].] 
The two matches suggest that the defendant (and the defendant's dog) 
must be the source of the crime traces (call this hypothesis $\mathsf{source}$).
 But how strong is this evidence, really?  What are the fact-finders to make of it? 


The standard story among legal 
probabilists goes something like this @sep-legal-probabilism. To evaluate the strength of the two items of match evidence, 
we must find the value of the likelihood ratio:

$$
\frac{{\pr{\s{dog} \wedge \s{hair} \vert \s{source}}}}{{\pr{\s{dog} \wedge \s{hair} \vert \neg \s{source}}}}
$$


For simplicity, the numerator can be equated to one. 
To fill in the denominator, an expert provides the relevant random match probabilities.
 Suppose the expert testifies that the probability of a random person's hair matching the reference sample is about `r round(hairMean,4)`, and the probability of a random dog's hair matching the reference sample happens to be about the same, `r round(dogMean,4)`.^[Probabilities have been slightly but not unrealistically modified to be closer to each other in order 
to make a conceptual point. 
The original probabilities were  1/100 for the dog fur, and 29/1148 for Wayne Williams' hair. 
We modified the actual reported probabilities slightly to emphasize the point that
 we will elaborate further on: the same first-order probabilities, even when they sound precise, may come with different degrees of  second-order uncertainty.] 

Presumably, the two matches are independent lines of evidence. In other words, their random match probabilities
 must be independent of each other conditional on either possible truth value of the source hypothesis.^[It is
  possible for $A$ and $B$ to be independent conditional on $C$, but not conditional on $\neg C$. Here, we require both 
  independencies to hold.]
Then, to evaluate the overall impact of the evidence on the source hypothesis, you calculate: 

\begin{align*}
\pr{\s{dog}\wedge \s{hair} \vert \neg \s{source}} & = \pr{\s{dog} \vert \neg \s{source}} 
\times \pr{\s{hair} \vert \neg \s{source}} \\
& =  `r hairMean` \times  `r dogMean` = `r hairMean * dogMean`
\end{align*}

This is a very low number. Two such random matches would be quite a coincidence. 
The expert  facilitates your understanding  of how this low number should be interpreted: they show you how
the  items of match evidence change the probability of the source 
hypothesis given a range of possible priors (@fig-impactOfPoint).  
The posterior of .99 is reached as soon as the prior is higher than  `r threshold`.^[These calculations 
assume that the probability of a match if the suspect and the suspect's dog are the sources is one.]
While perhaps not sufficient for outright belief in the source hypothesis, the evidence seems extremely strong:
 a minor additional piece of evidence could make the case against the defendant overwhelming. 



```{r impactOfPoint4,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%", warning = FALSE, message = FALSE}
#| label: fig-impactOfPoint
#| fig-cap: "Impact of dog fur and human hair evidence on the prior, point estimates."
#| fig-pos: H

pointImpactPlot
```


Unfortunately, this  analysis 
leaves out something crucial. You reflect on what you have been told and ask the expert:
 how can you know the random match probabilities 
with such precision? Shouldn't we also be mindful of the uncertainty that may affect these numbers? 
The expert agrees, and tells you that in fact the random match probability for the hair evidence
  is based on 29 matches found in a database of size 1148, while the random match probability 
  for the dog evidence is based on finding two matches in a reference database of size 78. 


```{r hair,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
#carpetSamples <- sample(ps, 1e4, replace = TRUE, prob = dbeta(ps, carpetA, carpetB))
set.seed(231)
hairSamples <- sample(ps, 1e4, replace = TRUE, prob = dbeta(ps, hairA, hairB))
dogSamples <- sample(ps, 1e4, replace = TRUE, prob = dbeta(ps, dogA, dogB))

#carpetHPDI <- HPDI(carpetSamples, prob  =.9)
hairHPDI <- HPDI(hairSamples, prob  =.99)
dogHPDI <- HPDI(dogSamples, prob  =.99)
```

```{r charitableImpact,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
lik0l <- .037 * .103
denominL <- lik0l * priorH0 + prior
numL <-  lik0l * priorH0
posteriorL <- 1- numL/denominL
thresholdL <- min(prior[posteriorL > .99])

charitableImpactPlot <- ggplot()+geom_line(aes(x = prior, y = posteriorL))+xlim(0,.32)+
  theme_tufte(base_size = 10)+labs(title = "Prior vs. posterior, charitable reading",
                                   subtitle = "Joint evidence: dog & hair")+
  geom_vline(xintercept = thresholdL, lty = 2, size =.5, alpha = .7)+
  annotate(geom = "label", label = "posterior > .99", x = .305, y =.95, size = 2.5)+ylab(
    "posterior"
  )+
  theme(plot.title.position = "plot")
```


The expert's answer makes apparent that the precise random match probabilities do not tell the whole story.
Perhaps, the information about sample sizes is good enough  and now you know how to
use the evidence properly.^[This is what, effectively, @Taroni2015Dismissal seem to suggest when they insist 
the fact-finders should be simply given point estimates and information about the study set-up,
 such as sample size. We disagree.] But if you are like most human beings, you can't. What to do, then?  


If you want to approach this from the impreciser's perspective, it is quite unclear how to proceed forward if 
one takes the binary notion of compatibility to one's heart. After all, pretty much all precise estimates of
 frequencies are 
still compatible with the evidence so far, except for the degenerate case. Perhaps, a less principled but
 defensible strategy is to defer to an expert (see however our earlier discussion of how the chain 
 of deference should end somewhere). 

So you ask the expert for guidance:  what are reasonable ranges of the random match probabilities? 
What are the worst-case and best-case scenarios? 
The expert responds with 99% credible intervals---specifically, starting with uniform priors,
 the ranges of the random match probabilities are (.015,.037) for hair evidence and (.002, .103) for 
 fur evidence.^[Roughly, the 99\% credible interval is the narrowest interval to which the
  expert thinks the true parameter belongs with probability .99. For a discussion of what 
  credible intervals are, how they differ from confidence intervals, and why confidence
  intervals should not be used, see @kruschke2015doing.]  Now, your representor set is supposed to cover
  the convex hull of the probability measures that result in probabilities that are the edges of the intervals. 
  For this reason, to investigate what span of probabilities the imprecise probabilities will end with, it is enough
  to focus on what happens at the edges of the interval. Reasoning with representor members at the edges of the intervals will
  lead you to the most extreme probability measure the impreciser is going to be committed to. 
  So you redo your calculations  using the upper bounds of the two intervals: $.037$ and $.103$ (assuming conditional
   independence). The rationale for 
  choosing the upper bounds is that these numbers result in random match probabilities that 
  are most favorable to the defendant. Your new calculation yields the following:

\begin{align*}
\mathsf{P}(\s{dog}\wedge \s{hair} \vert \neg \s{source})   & =  .037 \times .103 =.003811.
\end{align*}

 This number is around `r round(.003811/lik0,2)` times greater than the original estimate.
  Now the prior probability of the source hypothesis needs to be higher than `r thresholdL` for the
   posterior probability to be above .99 (@fig-impactofcharitable). So you are no longer 
   convinced that the two items of match evidence are strongly incriminating.


```{r FigcharitableImpact75,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%", warning = FALSE, message = FALSE}
#| label: fig-impactofcharitable
#| fig-cap: "Impact of dog fur and human hair evidence on the prior, charitable reading."
#| fig-pos: H

charitableImpactPlot
```


This result is puzzling.  Are the two items of match evidence strongly incriminating evidence 
(as you initially thought) or somewhat weaker (as the new calculation suggests)? For one thing,
  using precise random match probabilities might be too unfavorable toward the defendant. 
  On the other hand, your new assessment of the evidence based on the upper bounds might be
   too *favorable* toward them. Is there a middle way that avoids overestimating and underestimating 
   the strength of the evidence?



To see what this middle path looks like, 
we should reconsider the calculations you just did. 
You made an important blunder: you assumed that because the worst-case probability 
for one event is $x$ and the worst-case probability for another independent event is $y$, 
the worst-case probability  for their conjunction is  $xy$. But this conclusion does not follow i
f the margin of error (credible interval) is fixed.
 The intuitive reason is  simple: just because the probability of an extreme 
 (or larger absolute) value $x$ for one variable $X$ is .01, and so it is for the value 
  $y$ of another independent variable $Y$, it does not follow that the probability that 
  those two independent variables take values $x$ and $y$ simultaneously is the same.
   This probability is actually much smaller.  The interval presentation instead of doing us good led us into error. 


In general, it is impossible to calculate the credible interval for the joint distribution
 based solely on the individual credible intervals corresponding to the individual events.
   We need additional information: the distributions that were used to calculate the intervals 
   for the probabilities of the individual events. In our example, if you additionally knew, 
   for instance, that the expert used  beta distributions (as, arguably, they should in this context),
    you could in principle calculate the  99\% credible interval for the joint distribution.
     It usually will not be the same as whatever the results of multiplying the individual
      interval edges, and it is unlikely that a human fact-finder would be able to correctly 
      run such calculations in their head even if they knew the functional form of the distributions 
      used.^[Also, in principle, in more complex contexts, we need  further information about how the 
      items of evidence are related if we cannot take them to be independent.] 
      So providing the fact-finder with individual intervals, even if further information 
      about the distributions is provided, might easily mislead.^[Investigation of the 
       extent to which the individual interval presentation is misleading  would be an interesting psychological study.]


As it turns out, given the reported sample sizes, the 99\% credible interval
 for the probability $\mathsf{P}(\s{dog}\wedge \s{hair} \vert \neg \s{source})$ is $(0.000023,  0.002760)$.
The upper bound of this interval would then require the prior probability of the source hypothesis to be above .215
 for the posterior to be above .99. On this interpretation, the two items of match evidence 
 are still not quite as strong as you initially thought, but stronger than what your second calculation indicated. 



We still should think of credible intervals as rough summaries, which  might be useful if
 the underlying distributions are fairly symmetrical, or narrow enough.
 But in our case, they might not be. For instance, @fig-densities depicts beta densities
  for dog fur and human hair, together with sampling-approximated density for the joint evidence.  
  The distribution for the joint evidence is not symmetric. 
 If you were only informed about the edges of the interval, you would be oblivious to the
  fact that the most likely value (and the bulk of the distribution, really) does not simply 
  lie in the middle between the edges. Just because the parameter lies in an interval with some 
  posterior probability, it does not mean that the ranges near the edges of the interval are equally 
  likely---the bulk of the density might very well be closer to one of the edges. Therefore, only 
  relying on the edges  can lead one to either overestimate or underestimate the probabilities at play. 
This also means that---following our advice on how to illustrate the impact of evidence on
 prior probabilities---a better representation of the dependence of the posterior on the prior 
 should comprise multiple possible sampled lines whose density mirrors the density around the
  probability of the evidence (@fig-lines).


```{r densitiesEvidence,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%"}

jointEvidence <- dogSamples * hairSamples


densities1Plot <- ggplot()+
  geom_line(aes(x = ps, y = dbeta(ps, hairA, hairB)), lty  = 2)+
  geom_line(aes(x = ps, y = dbeta(ps, dogA, dogB)), lty = 3)+xlim(0,.15)+
  xlab("probability")+
  ylab("density")+
  theme_tufte(base_size = 10)+
  #  annotate(geom  = "label", label = "carpet", x =  0.045, y = 140)+
  annotate(geom  = "label", label = "hair", x =  0.035, y = 80)+
  annotate(geom  = "label", label = "dog", x =  0.06, y = 15)+
  labs(title = "Conditional densities for  individual items of evidence if the source hypothesis is false")+
  theme(plot.title.position = "plot")

densities2Plot <- ggplot()+
  xlab("probability")+
  ylab("density")+
  theme_tufte(base_size = 10)+
  geom_density(aes(x= jointEvidence))+
  geom_vline(xintercept = 0.002760, lty = 2, linewidth = .5)+
  geom_vline(xintercept = 0.000023, lty = 2, linewidth  = .5)+
  geom_vline(xintercept = 0.000144, lty = 3, linewidth = .8)+
  geom_vline(xintercept = 0.001742, lty = 3, linewidth  = .8)+
  labs(title = "Conditional density for joint evidence",
       subtitle = "(with .99 and .9 HPDIs)")+
  theme(plot.title.position = "plot")
```




```{r Figdensities,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "80%", warning = FALSE, message = FALSE}
#| label: fig-densities
#| fig-cap: "Beta densities for individual items of evidence and the resulting joint density with .99 and .9 highest posterior density intervals, assuming the sample sizes as discussed and independence, with uniform priors."
#| fig-pos: H

grid.arrange(densities1Plot,densities2Plot, ncol = 1 )
```



```{r densLines, echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE}
prior <- seq(0,1, by = 0.001)
priorH0 <- 1-prior

#-----

jointPosterior <- list()
minima <- numeric(1e4)


# for each likelihood from sample,
# calculate the posterior based on what the prior is
# and find the minimum above threshold
for (s in 1:1e4){
  lik <- jointEvidence[s]
  denomin <- lik * priorH0 + prior
  num <-  lik * priorH0
  posterior <- 1- num/denomin
  jointPosterior[[s]] <- posterior
  minima[s] <- min(prior[posterior > .99])
  }



#minimaPlot <- ggplot()+geom_density(aes(x = minima))+
#  theme_tufte(base_size = 10)+ggtitle("Minimal priors sufficient for posterior >.99")#+xlab("minimal prior")+
#  theme(plot.title.position = "plot")

#minimaGrob <- ggplotGrob(minimaPlot)



#jointPosteriorDF <-   as.data.frame(do.call(cbind, jointPosterior))



#jointPosteriorSubsample <- jointPosteriorDF[,1:300]

#jointPosteriorDF$ps <- ps
#jointPosteriorSubsample$ps <- ps

#jointPosteriorLong <- melt(jointPosteriorSubsample,
#                           id.vars = c("ps"))
#colnames(jointPosteriorLong) <- c("prior", "sample","probability")

#alpha = .25
#size = .2

#densitiesLinesPlot <- ggplot(jointPosteriorLong)+
#  geom_line(aes(x = prior, y = probability,group = sample),
#            alpha = alpha, linewidth = size)+
#  theme_tufte()+xlim(0,.2) +
#  annotation_custom(minimaGrob, xmin = .085, xmax = .185,
#                  ymin = 0.01, ymax = 0.8) +
#  ggtitle("Posterior vs prior (300 sampled lines)") +
#  theme(plot.title.position = "plot")
#densitiesLinesPlot





minimaPlot <- ggplot()+geom_density(aes(x = minima))+
  theme_tufte(base_size = 10)+ggtitle("Minimal priors sufficient for posterior >.99")+xlab("minimal prior")+
  theme(plot.title.position = "plot", plot.title = element_text(size = 10))

jointPosteriorDF <-   as.data.frame(do.call(cbind, jointPosterior))

jointPosteriorSubsample <- jointPosteriorDF[,1:300]

jointPosteriorDF$ps <- ps
jointPosteriorSubsample$ps <- ps

jointPosteriorLong <- melt(jointPosteriorSubsample,
                           id.vars = c("ps"))
colnames(jointPosteriorLong) <- c("prior", "sample","probability")

alpha = .25
size = .2

densitiesLinesPlot <- ggplot(jointPosteriorLong)+
  geom_line(aes(x = prior, y = probability,group = sample),
            alpha = alpha, linewidth = size)+
  theme_tufte()+xlim(0,.2) + theme_tufte(base_size = 10)+
  ggtitle("Posterior vs prior (300 sampled lines)") +
  theme(plot.title.position = "plot", plot.title = element_text(size = 10))

```





```{r Figlines,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "80%", warning = FALSE, message = FALSE}
#| label: fig-lines
#| fig-cap: "300 lines illustrating the uncertainty about the dependence of the posterior on the prior given aleatory uncertainty about the evidence, with the distribution of the minimal priors required for the posterior to be above .99."
#| fig-pos: H

grid.arrange(densitiesLinesPlot, minimaPlot, ncol= 2)
```


This, then, is the main claim illustrated in this section: higher-order approach to evidence evaluation
is more reliable and more honest about the uncertainties involved. Whenever density estimates 
for the probabilities of interest are available 
(and they should be available for match evidence and many other 
items of scientific evidence if the reliability of a given type of evidence has been properly studied),
 those densities should be reported for assessing the strength of the evidence. 
 This approach avoids hiding actual aleatory uncertainties under the carpet. It also 
allows for a balanced assessment of the evidence, whereas using point estimates  
or intervals may exaggerate or underestimate the value of the evidence.

Mathematically, we do not
 propose anything radically new---we just put together some of the items from the standard Bayesian toolkit.
  The novelty is rather in our arguing that that these tools are under-appreciated in formal epistemology and in 
  the legal scholarship and should be properly used to incorporate second-order uncertainties 
  in evidence evaluation and incorporation.

<!-- # Objections -->
<!-- \label{sec:objections} -->


<!-- This section addresses a number of conceptual difficulties that may arise in using higher-order probabilities, with focus on  those brought up by prominent legal evidence scholars. In discussing these conceptual issues, we will formulate an accuracy-based argument that higher-order probabilities are preferable to precise probabilities. -->


<!-- ## The Taroni-Sjerps debate -->

<!-- Our treatment will be centered around a discussion initiated by -->
<!-- @Taroni2015Dismissal, who argue extensively that trial experts should avoid report higher-order densities, and should only report point estimates. Their point of departure is a reflection on match evidence. -->


<!-- Say an expert reports at trial that the sample from the crime scene matches the defendant. The significance of this match should be evaluated in light of the population frequency $\theta$ of the matching profile.  This frequency, however, cannot be known -->
<!-- for sure and must instead be estimated. -->


<!-- The expert will estimate the true parameter $\theta$ by means of a probability distribution $p(\theta)$ over its possible values. For example, if the observations are realizations of independent and identically distributed Bernoulli trails given $\theta$, the expert's uncertainty about $\theta$ can be modeled as $\s{beta}(\alpha + s + 1 ,\beta + n - s)$, where $s$ is the number of observed successes, $n$ the number of observations in the database (1 is added to the first shape parameter to include the match with the suspect), and $\alpha$ and $\beta$ reflect the expert's priors. -->

<!-- However, they insist, in front of the fact-finders, the expert is supposed to report their epistemic uncertainty, which is to be understood according to the subjective interpretation.  On this interpretation, probabilities express an agent's epistemic attitude towards a proposition, their uncertainty about its truth value. Such probabilities, @Taroni2015Dismissal insist, unlike frequencies, are not states of nature, but states of mind associated with individuals. For this reason, they claim, it makes no sense to talk about second-order uncertainty about subjective probabilities, as there is no underlying state of the nature to estimate. Further, they insist this also applies to likelihood ratios: -->
<!-- \begin{quote} -->
<!-- \dots there is no meaningful state of nature equivalent for the likelihood ratio in its entirety, as it is given by a ratio of two conditional probabilities. [p. 12] -->
<!-- \end{quote} -->
<!-- \noindent In line with this perspectives, in the elicitation of probabilities @Taroni2015Dismissal recommend investigating an agent's betting preferences, and that a proper elicitation of this form will lead to a single number.^["Clearly, -->
<!-- one can adjust the measure of belief of success in the reference gamble in such -->
<!-- a way that one will be indifferent with respect to the truth of the event about -->
<!-- which one needs to give oneâ€™s probability. This understanding is fundamental, as -->
<!-- it implies that probability is given by a single number. It may be hard to define, -->
<!-- but that does not mean that probability does not exist in an individualâ€™s mind. One cannot logically have two different numbers because they -->
<!-- would reflect different measures of belief." [p. 7]] -->
<!-- \noindent Moreover, while they claim  that it is fine to use distributions to talk about chance, they deny this possibility for  personal uncertainty, under the threat of infinite regress: -->
<!-- \begin{quote} -->
<!-- One can, in fact, have probabilities for events, or probabilities for propositions, but not probabilities of probabilities, otherwise one would have an infinite regression. [p. 8] -->
<!-- \end{quote} -->
<!-- \noindent Accordingly,  @Taroni2015Dismissal insist that given a frequentist estimation of the probability of the evidence given the defense hypothesis, \dots -->
<!-- \begin{quote} -->
<!-- \dots personal beliefs [\dots] can be computed as: -->
<!-- \begin{align*}\pr{E} & = \int_{\theta} \pr{E\vert \theta} \pr{\theta}\,\, d\theta \\ -->
<!-- & =  \int_\theta  \theta \pr{\theta}\,\, d\theta -->
<!-- \end{align*} -->
<!-- \end{quote} -->
<!-- \noindent In particular, for the DNA match example, they advice that the personal belief  is the expected value of the $\s{beta}$ distribution, which reduces to $\nicefrac{\alpha + s + 1}{\alpha + \beta +n + 1}$. They claim that it satisfactorily expresses the posterior uncertainty about $\theta$, and that it is  solely this probability that should be used in the denominator in the calculation and reporting of the likelihood ratio. -->


<!-- Nothing so far should be controversial. However, the question arises of how the expert should report their own uncertainty about $\theta$, especially in the light of the usual practice of reporting likelihood ratios. -->

<!-- To fix the notation, let the prosecution hypothesis $H_p$ be that the suspect is the source of the trace, and the defense hypothesis $H_d$ that another person, unrelated to the suspect, is the source. For simplicity, assume that if $H_p$ holds, the laboratory will surely report a match $M$, so that $\pr{M\vert H_p}=1$. -->
<!-- The likelihood ratio, then,  reduces to $\nicefrac{1}{\pr{M \vert H_d}}$---but given that $\theta$ was estimated using density over its possible values, it is not obvious how a single value $\pr{M \vert H_d}$ is to be obtained and whether its use in the reporting does not hide the uncertainty involved in the estimation of $\theta$ under the carpet. -->


<!-- @Taroni2015Dismissal claim  that the point estimate for the match evidence given the defense hypothesis should be calculated as follows: -->
<!-- \begin{align*}\pr{M \vert H_d} & = \int_{\theta} \pr{M\vert \theta} \pr{\theta}\,\, d\theta \\ -->
<!-- & =  \int_\theta  \theta \pr{\theta}\,\, d\theta -->
<!-- \end{align*} -->
<!-- In case of a DNA match, they recommend that the expert report the expected value of the $\s{beta}$ distribution, which reduces to $\nicefrac{\alpha + s + 1}{\alpha + \beta +n + 1}$. They claim that this number satisfactorily expresses the posterior uncertainty about $\theta$. For them, it is this probability alone that should be used in the denominator in the calculation and reporting of the likelihood ratio. -->


<!-- @Sjerps2015Uncertainty disagree. In reporting a single value, the expert would refrain from providing the fact-finders with relevant information  that can make a difference in the proper evaluation of the evidence.  There is a difference between (a) an expert who is certain $\theta$ is $.1$; (b) an expert whose best estimate of $\theta$ is $.1$ based on thousands of observations; and (c) an expert whose best estimate of $\theta$ is again $.1$ but based on only ten observations. -->

<!-- These three scenarios mirror scenarios we discussed earlier: (a) the bias of a coin is known for sure; (b) the bias is estimated on the basis of a large number of tosses; and (c) the bias is estimated using a small set of observations. As our critique of precise probabilism makes clear, a simple point estimate (or precise probability) would fail to capture the differences among the three scenarios. This concern might be slightly mitigated by the fact that @Taroni2015Dismissal admits that the expert, besides providing a point estimate, should also informally explain how the estimate was arrived at. They grant that this additional information can be helpful so long as the recipients are instructed on "the nature of probability, the importance of an understanding of it and its proper use in dealing with uncertainty" [p. 16]. But why stop at an informal presentation? It is unclear why the fact-finders should be deprived of quantifiable information about the aleatory uncertainty of the parameter of interest and only be given an informal description of what the expert did, along with some remarks about the nature of probability. It is wildly optimistic to assume that an informal description of how the point estimate has been arrived at is enough to secure a proper assessment of the evidence. We hope to have convinced the reader already in the introduction that informal treatment and bare intuitions are not good enough  even when it  comes to the evaluation of the impact of a rather simple combination of  two items of evidence if all the fact-finder has to go by is point estimates and and informal description of how the estimates have been obtained. -->






<!-- Somewhat surprisingly, most of the concerns raised by  @Taroni2015Dismissal are  philosophical. They argue that if  probabilities express an agent's epistemic attitude towards a proposition probabilities are not states of nature, but states of mind associated with individuals. They think this claim  has two consequences. First, it makes no sense sense to talk about second-order uncertainty about subjective probabilities, as there is no "underlying state of the nature" to estimate. Second, if these subjective probabilities can be elicited by examining an agent's betting preferences, a proper elicitation will lead to a single number.^[They write: "Clearly, one can adjust the measure of belief of success in the reference gamble in such a way that one will be indifferent with respect to the truth of the event about which one needs to give oneâ€™s probability. This understanding is fundamental, as it implies that probability is given by a single number. It may be hard to define, but that does not mean that probability does not exist in an individualâ€™s mind. One cannot logically have two different numbers because they would reflect different measures of belief." [@Taroni2015Dismissal, p. 7] ] -->


<!-- In response to the philosophical argument, @Dahlman2022Information have also emphasized that the distinction is not so clear-cut. They argue that, if a probability assessment is a subjective attitude that is elicited via a betting preference, a probability assessment is itself a state of nature, "the formation of a betting preference by a certain person at a certain time" [p. 15]. While we will have something to say about the philosophical dimension of this debate, let us first develop  a less philosophically involved  argument for the position taken by @Sjerps2015Uncertainty. -->


<!-- ## An accuracy-based argument -->


<!-- \todo{see M'scomment in boldface} -->
<!-- \textbf{M's comment: This accuracy-based argument is evocative and intriguing, but what does it show really? What is the significance of using PMF based on a point estimate versus a posterior predictive PMF? Does this correspond to something that is done in court? How? The more interesting question is whether using higher-order probabilities reduces errors, say the rate of false convictions or false acquittals. Does it? If so, how?} -->

<!--  With this argument, we hope to break the stalemate in the debate by proving an argument to which both parties should be receptive. -->
<!-- It is an accuracy-based argument in favor of using higher-order probabilities---roughly, it says, if you discard relevant information that you already have contained in the densities resulting from the estimation and rely on point estimates only, your predictions about the world will be less accurate in a very precise and quantifiable sense. -->

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
priorA <- 1
priorB <- 1
set.seed(215)
trueH <- runif(1,0,1)

sampleSize <- sample(10:20,size = 1)
testSize <- sampleSize

set.seed(319)
successes <- rbinom(1, sampleSize, trueH)
pointEstimate <- successes/sampleSize

pointPredictions <-  rbinom( 1e4 , size = testSize , prob = pointEstimate )

ps <- seq(0,1,length.out = 1001)

testPredictions <- rbinom( 1e4 , size = testSize , prob = trueH )

posterior <- function(x) dbeta(x, priorA + successes,
                               priorB + sampleSize - successes)

posteriorSample <- sample( ps , size=1e4 ,
                           replace=TRUE , posterior(ps)/sum(posterior(ps)))

posteriorPredictions <- rbinom( 1e4 , size=testSize , prob=posteriorSample )


testProbs <- table(factor(testPredictions, levels = seq(0,sampleSize)))/1e4
pointProbs <- table(factor(pointPredictions, levels = seq(0,sampleSize)))/1e4
posteriorProbs <- table(factor(posteriorPredictions, levels = seq(0,sampleSize)))/1e4

testPlot <- ggplot()+geom_bar(aes(x= testPredictions, y = ..prop..))+
  ggtitle(paste("Predictions based on the true parameter = ", round(trueH,2), sep = ""))+xlim(0,testSize)+ylab("probability")+xlab("successes")+theme_tufte(base_size = 8)+theme(plot.title.position = "plot")


pointPlot <- ggplot()+geom_bar(aes(x= pointPredictions,y = ..prop..))+
  labs(title = paste("Predictions based on the point estimate = ", round(pointEstimate,2)), subtitle = paste(successes, " successes in ", sampleSize, " observations", sep = ""))+xlim(0,testSize)+ylab("probability")+xlab("successes")+theme_tufte(base_size = 8)+theme(plot.title.position = "plot")


samplesPlot <- ggplot()+geom_density(aes(x= posteriorSample))+
  ggtitle(paste("Posterior sample from beta(", 1+successes, ",", 1+testSize - successes,
                ")", sep = ""))+xlab(
                  "parameter value"
                )+theme_tufte(base_size = 8)+theme(plot.title.position = "plot")

posteriorPlot <- ggplot()+geom_bar(aes(x= posteriorPredictions,y = ..prop..))+
  ggtitle("Predictions based on the posterior sample")+xlim(0, testSize)+ylab("probability")+xlab("successes")+theme_tufte(base_size = 8)+theme(plot.title.position = "plot")


```

<!-- First, let us go over a particular example. Suppose we randomly draw a true population frequency from the uniform distribution. In our particular case, we obtained `r round(trueH,3)`. Then, we randomly draw a sample size as a natural number between 10 and 20. In our particular case, it is `r sampleSize`. Next, we simulate an experiment in which we draw that number of observations from the true distribution. We observe `r successes` successes and use this number to calculate the point estimate of the parameter, which is `r` round(pointEstimate,3)`. -->

<!-- What is the probability mass function (PMF) for all -->
<!-- possible outcomes of an observation of the same size? -->
<!-- Two PMF are initially relevant: first, the true probability mass based on the true parameter; second, the probability mass function based on the point estimate which is binomial around the point estimate. This latter PMF, however, does not take into account the uncertainty about the point estimate. To take this uncertainty seriously, continuing our example, we take a sample distribution of size 16 of possible parameter values from the posterior  $\s{beta}(1+\s{successes}, 1+\s{sample size} - \s{successes})$ distribution (we assume uniform prior for the sake of an example). Then, we use this sample of parameter values to simulate observations, one simulation for each parameter value in the sample. This simulation yields the so-called *posterior predictive distribution* (or posterior predictive PMF), which instead of a point estimate, propagates the uncertainty about the parameter value into the predictions about the outcomes of possible observations. Finally, we take simulated frequencies as our estimates of probabilities. This distribution is more honest about uncertainty and wider than the one obtained using the point estimate. The three PMFs are displayed in Figure \ref{fig:posteriorPrediction}. -->



<!-- \begin{figure}[H] -->
<!-- ```{r FigposteriorPrediction2, echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "80%", warning=FALSE, message = FALSE, dpi = 800, warning = FALSE, message = FALSE, output = FALSE} -->
<!-- grid.arrange(testPlot,pointPlot,samplesPlot,posteriorPlot, ncol = 1) -->
<!-- ``` -->


<!-- \caption{Real probability mass, probability mass calculated using a point estimate, sampling distribution from the posterior, and the posterior predictive distribution based on this sampling distribution.} -->
<!-- \label{fig:posteriorPrediction} -->
<!-- \end{figure} -->

<!-- The PMF based on a point estimate is further off from the real PMF than the posterior predictive distribution. For instance, if we ask about the probability of the outcome being at least 9 successes, the true answer is `r mean(testPredictions >= 9)`, the point estimate PMF tells us it is `r mean(pointPredictions >= 9 )`, while the posterior predictive distribution gives a somewhat better guess at `r mean(posteriorPredictions >= 9 )`. A similar thing happens when we ask about the probability of the outcome being at most 9 successes. The true answer is `r mean(testPredictions <= 9)`, the point-estimate-based answer is `r mean(pointPredictions <= 9 )`, while the posterior predictive distribution yields  `r mean(posteriorPredictions <= 9 )`. More generally, we can use an information-theoretic measure, Kullback-Leibler divergence,  to quantify how far the point-estimate PMF and the posterior predictive PMF are from the true PMF.\footnote{A bit of explanation of this divergence measure. Suppose we are dealing with a variable $X$ with $n$ distinct possible discrete states $x_1, \dots, x_n$ and consider two probability mass functions $p$ and $q$ which express uncertainty about the true value of $X$ so that, say, on $p$, $\pr{X=x_i}=p_i$. First, the uncertainty of a given distribution $p$, its \emph{entropy}, is given by the sum of the logarithms of surprise $\nicefrac{1}{p_i}$ for all the possible values,  $H(p) = \sum x_i \log \frac{1}{p_i} = - \sum p_i \log p_i$.  Next, suppose events arise according to $p$, but we predict them -->
<!-- using $q$. The \emph{cross-entropy}  is then  $\mathsf{H}(p, q)  = \sum p_i \log(q_i)$. This value is going to be higher than the entropy of $p$ if $q$ is different from it. Think of it as the uncertainty involved in using $q$ to predict events that arise according to $p$.  Third, \emph{Kullback-Leibler}  divergence is the additional entropy introduced by -->
<!-- using $q$ instead of $p$ itself, that is, the difference between cross-entropy and entropy: -->
<!-- \begin{align*} -->
<!-- \mathsf{DKL}(p, q) & = H(p, q) - H(p)\\ -->
<!-- &= - \sum p_i \log q_i   - \left(   - \sum p_i \log p_i \right) \\ -->
<!-- & = - \sum p_i\left( \log q_i - \log p_i\right)\\ -->
<!-- & =  \sum p_i\left( \log p_i - \log q_i\right)\\ -->
<!-- & = \sum p_i \log \left( \frac{p_i}{q_i}\right) -->
<!-- \end{align*} \noindent  As it turns out, KL divergence is also the expected -->
<!-- difference in log probabilities. In particular, if -->
<!-- \(p=q\) we get -->
<!-- \(DKL(p,p) = \sum p_i (\log p_i - \log p_i) = 0\), -->
<!-- which works out as it intuitively should be.} -->


<!--  In our particular case, the former distance is `r` kld(testProbs,pointProbs)` and the latter is `r kld(testProbs,posteriorProbs)`. The posterior predictive distribution is information-theoretically closer to the true distribution. -->

```{r kldsCalculations,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
klds <- numeric(1000)
for(i in 1:1000){
trueH <- runif(1,0,1)
sampleSize <- sample(10:25,size = 1)
testSize <- sampleSize
successes <- rbinom(1, sampleSize, trueH)
pointEstimate <- successes/sampleSize
pointPredictions <-  rbinom( 1e4 , size = testSize , prob = pointEstimate )
ps <- seq(0,1,length.out = 1001)
testPredictions <- rbinom( 1e4 , size = testSize , prob = trueH )
posteriorSample <- sample( ps , size=1e4 ,
                           replace=TRUE , posterior(ps)/sum(posterior(ps)))
posteriorPredictions <- rbinom( 1e4 , size=testSize , prob=posteriorSample )
testProbs <- table(factor(testPredictions, levels = seq(0,sampleSize)))/1e4
pointProbs <- table(factor(pointPredictions, levels = seq(0,sampleSize)))/1e4
posteriorProbs <- table(factor(posteriorPredictions, levels = seq(0,sampleSize)))/1e4
klds[i] <- kld(testProbs,pointProbs) - kld(testProbs,posteriorProbs)
}

```


<!-- This was just one example, but the phenomenon generalizes.  We repeat the simulation 1000 times, each time with a new true parameter, a new sample size, and a new sample. Every time the three PMFs are constructed using the methods we described and their KL divergence from the true distribution is calculated.  Figure \ref{fig:kldsPlots} displays the empirical distribution of the results of such a simulation. A positive value indicates that the distribution based on the point-estimate was further from the true PMF than the posterior predictive distribution based on the same observed sample. Notably, the mean difference is `r` round(mean(klds),3)`, the median difference is `r round(median(klds),3)`, and the distribution is asymmetrical, as there are multiple cases of large differences favoring posterior predictive distributions over point-based predictions.  All in all, accuracy-wise, point-estimate-based PMFs are systematically worse than the posterior predictive distribution. -->






<!-- \begin{figure}[H] -->
<!-- ```{r FIgkldsPlots, echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", warning=FALSE, message = FALSE, dpi = 800, warning = FALSE, message = FALSE, output = FALSE} -->
<!-- ggplot()+geom_density(aes(x = klds))+labs( -->
<!--   title = "Point-estimates vs. posterior predictive distributions", -->
<!--   subtitle = "Differences in Kullback-Leibler divergencies from true PMFs")+ -->
<!--   theme_tufte(base_size = 10)+theme(plot.title.position = "plot")+ylab("empirical density")+xlab("difference in klds") -->
<!-- ``` -->
<!-- \caption{Differences in Kullback-Leibler divergencies from the true distributions, comparing the distributions obtained using point estimates and posterior predictive distributions. Positive values indicate the point-estimate-based PMF was further from the true distribution than the posterior predictive distribution.} -->
<!-- \label{fig:kldsPlots} -->
<!-- \end{figure} -->



<!-- ## Conceptual issues -->

<!-- \todo{M's comment in boldface} -->
<!-- \textbf{M's comments: this subsection looks muddled to me. It goes around in circles. I cannot follow the argument. What is the key point made in this subection? The key point seems to be this: just like we compare the plausibility/probability of propositions like "defendant was at crime scene" and "defedant was not at crime scenbe", we compare the plausibility/probability of propositions like "random match probability is .0001" and "random match probability of .0002". The second comparison requires higher-order probabilities. Is this the point? But Taroni is saying that we can average over all possible random match probabilities and get a point estimate. What do we say in response to that?} -->


<!-- Accuracy considerations aside, we will now engage with the more  conceptual points. -->
<!-- @Taroni2015Dismissal argue that since first-order probabilities capture your uncertainty about a proposition of interest, second-order probabilities are supposed to capture your uncertainty about how uncertain you are, and that "estimating" your first-order uncertainties is unnecessary.  They think that you can simply figure out your fair odds in a suitable bet on the proposition in question, and the fair odds track your unique, first-order uncertainty without any uncertainty about it. But this point can be questioned. For one thing, the betting interpretation of probability is not uncontroversial.^[See textbooks in formal epistemology [@bradley2015critical; @Titelbaum2020Fundamentals-of].] -->
<!-- Even assuming the betting interpretation, there seems to be nothing wrong in  saying that sometimes  we are\todo{add ref to Williamson} uncertain about what we think the fair bets are.^[On a related note, the introspective axioms in epistemic logic---that is,  if an agent knows (or doesn't know) $p$, they also know that they (don't) know $p$---are by no means uncontroversial. See, for example, Williamson 2000 (chapter 5)'s argument against the KK principle of positive introspection.] But admittedly, this answer while undermines the betting argument for the sufficiency of point estimates, does not cast much light on what the appropriate relatively uncontroversial interpretation of higher-order uncertainty should be. \todo{M's: I don't understand the argument here. What is a "relatively uncontroversial interpretation"?} -->


<!-- Think again about an expert who gathers information about the allelic frequency $f$ of DNA matches in an available database, and starts with a defensible \s{beta} prior with parameters $\alpha, \beta$. Say the expert observes $s$ matches in a database of size $n$. So the population relative frequency the experts is estimating should follow the $\s{beta}(\alpha + s + 1 ,\beta + n - s)$ distribution.  So far, nothing controversial happens---the expert is estimating the relevant population frequency. -->


<!-- But subjective uncertainty that is to be reported by the expert,   @Taroni2015Dismissal complain, is not about the frequency, but  about their attitude towards a proposition (supposedly expressing a "state of nature")---and, they insist, it makes no sense for an agent to attach uncertainty to their own uncertainty about a proposition. -->

<!-- Assuming the conditions are pristine (the expert has no modeling uncertainty, rules out laboratory errors, and so on), the beta distribution can be used to pretty directly inform the expert's subjective uncertainty. But uncertainty about what? The (estimated) population frequency, for instance, can underlie a  probability assignment to the proposition \emph{a match is observed  if another person, unrelated to the suspect, is the source of the trace}. Admittedly, if only this proposition is being considered, it is yet not clear what second-order uncertainties would be uncertainties about. But the expert also considers a continuum of propositions, each of the form \emph{the true population frequency is $\theta$} for each $\theta\in [0,1]$. A density over $\theta$ models the comparative plausibility that the expert assigns to such propositions in light of the evidence.\footnote{Moreover, and normalization allows them to calculate their subjective probabilities for $\theta$ belonging to various sub-intervals of $[0,1]$.} So if one were worried that there were no propositions that the expert could be "second-order" uncertain about, there actually are plenty. \todo{M: who was worried that there were no second-order propositions? What argument in the literature is this a response to? I cannot follow.} In particular, if $\theta$ is a population frequency, gauging which density captures the extent to which the evidence justifies various estimates of that frequency is the same as gauging the comparative plausibility of the corresponding propositions about possible population frequencies.^[Perhaps, this should no longer be called "estimation", but the the connection with estimation is strong enough to justify this terminology. In the end, this is a verbal discussion that we will not get into.] -->


<!-- More generally, in many contexts, evidence justifies first-order probability assignments (population frequency estimates) to various degrees. Suppose there is no evidence about the bias of a coin. Then, each first-order uncertainty about it would be equally (un)-justified. (If you like to think in terms of bets, the evidence would give no reason to prefer any particular odds as fair.) If, instead, we know the coin is fair, the evidence clearly selects one preferred value, .5. (Again, if you like the betting metaphor, 1:1 would be the  unique recommended betting odds.) But often the evidence is stronger than the former case and weaker than the latter case. Consider, for example, propositions about population frequencies in light of the results of observations. In such circumstances, the evidence justifies different values of first-order uncertainty to various degrees, and densities simply capture the extent to which different first-order uncertainties are supported by the evidence. -->


<!-- We conclude this section by examining two additional points raised by  @Taroni2015Dismissal. The first---which we already alluded to earlier---is that first-order probabilities  are not "states of nature" and so cannot be estimated. It is unclear why the authors insist that only states of nature can be estimated. Mathematicians use approximate methods to estimate answers to fairly abstract questions, not obviously related to "states of nature", whatever these are.  So, estimation should make sense whenever there are some objective answers that we can approximate to a greater or lesser extent. If there is some objectivity to what the ideal evidence would support, or to the extent to which the actual evidence supports various competing hypotheses, we can be more or less wrong about such things, and so it is not implausible to say that  there is a clear sense in which we can estimate them.^[@Taroni2015Dismissal make the same point for likelihood ratios. They argue that there is no "meaningful state of nature equivalent for the likelihood ratio in its entirety, as it is given by a ratio of two conditional probabilities?" But if it is meaningful to estimate two conditional probabilities (that is, frequencies in the population), or to  compare the relative plausibility of various propositions about them in terms of density, it is equally meaningful to estimate any function of the numbers involved.  Otherwise it would also be meaningless to try to estimate the body mass index (BMI) of an average 21 years old male student in the USA just because BMI is a ratio of other quantities. There are reasons not to care about BMI, but it not being a state of nature because it is a function  of other values is not one of them.] -->

<!-- \todo{added back the claim about not worrying about BMI, as some readers might be sensitive to anyone bringing BMI up} -->

<!-- Second, @Taroni2015Dismissal argue that once we allow second-order probability, we run into the threat of infinite regress. But do we? Surely, they would agree that one can be uncertain about a statistical model. But this can be the case even if this model spits out a point estimate rather than a density. If you think the possibility of putting uncertainty on top of propositions about possible values of a first-order parameter leaves us in an  epistemically hopeless situation, you might have hard time explaining why your point estimation is in a better situation. After all, if asking further questions about probabilities up the hierarchy is always justified, we can keep asking about the probability of a point-estimate-spitting model, the probability of that probability, <!---(and the way we have reached it) being adequate  and so on.--->  

<!---Alternatively, we could concede that while our situation is often epistemically difficult, it is not hopeless. Of course, the mathematical and statistical models we use are exactly those: models, which can be more or less epistemically helpful. And when we decide which models to use, we always face a trade-off between various factors.---> 

<!-- Perhaps the problem at issue is just one of complexity. Admittedly, second-order estimation is more complex than relying on point estimates. But we hope to have convinced the reader this complexity is worth the effort. What about more complex models going third-order? If a workable approach can accomplish that---and the additional complexity pays off---we are all for going third-order. <!---In fact, some of the model selection methods can be thought of this way. --->

<!-- The fact that more complex models can always be built hardly lead us into a vicious infinite regress. Rather, it is an indication that our models of uncertainty can---in principle---always be improved. -->



<!-- # Legal Applications -->
<!-- \label{sec:legal-applications} -->

<!-- \inbook{Add carpet evidence in the Wayne Williams case} -->


<!-- Our discussion so far has been mostly theoretical. We made a case  that higher-order probabilism outperforms precise probabilism  on both descriptive and normative grounds. We also staved off a number of conceptual difficulties with going higher-order. It is time to extend our discussion from the introduction to a further illustration of how higher-order probabilism can be of service  in evaluating evidence at trial. We present -->
<!-- here two examples. -->


<!-- ## False Positives in DNA Identification -->

<!-- One important topic is that of errors  in the process of  DNA match evidence evaluation. As already known, the probability of a -->
<!-- false positive caused by contamination, laboratory or evidence collection or storage error has serious impact on   the value of DNA match evidence. As @Thomason2003How-the-Probabi have shown, -->
<!-- the probability of false positives, even when  seemingly low,  has a non-negligible impact: -->

<!-- \begin{quote} -->
<!-- If, as commentators have suggested, the rate of false positives is between 1 in 100 and 1 in 1000, or even less, then one might argue that the jury can safely rule out -->
<!-- the prospect that the reported match in their case is due to error and can proceed to consider the probability of a coincidental match \dots this argument is fallacious and profoundly misleading \dots  the probability that a reported match occurred due to error in a particular case can be much higher, or lower, than the false positive probability. -->
<!-- \end{quote} -->

<!-- \noindent -->
<!-- We are particularly interested in the passing remark that the rate of false positives is between 1 in 100 and 1 in 1000. This difference is not negligible. The simplest option would be to use the upper bound of the [0.001, 0.01] interval. This choice would be the most favorable toward the defendant. But, as already noted in the introduction, doing so would lead to an overly conservative evaluation of the evidence. It is much preferable to have a sensible distribution to work with. -->

<!-- To fix ideas, the posterior probability of the -->
<!-- source hypothesis ($S$) conditional -->
<!-- on the match evidence ($E$) is, with some idealization, as follows: -->

<!-- \begin{align*} -->
<!-- \pr{S \vert E} &   =  \frac{\pr{E\vert S} \pr{S} } {\pr{E}}\\ -->
<!-- & = \frac{\overbrace{\pr{E\vert S}}^1 \pr{S}}{\underbrace{\pr{E\vert S}}_1 \pr{S} + \underbrace{\pr{E \vert RM}}_1 \pr{RM} + \underbrace{\pr{E \vert FP}}_1 \pr{FP}} \\ & = \frac{\pr{S}}{\pr{S} + \pr{RM} + \pr{FP}} -->
<!-- \end{align*} -->

<!-- \noindent -->
<!-- For simplicity, the false negative rate is assumed to be zero, or in other words, $\pr{E\vert S} =1$. The other assumption is that the evidence could come about if: (1) the source hypothesis is true; (2) a random match ($RM$) occurred; or (3) a false positive match occurred ($FP$). -->


<!-- ```{r fpPointCalculations,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} -->
<!-- ps <- seq(0,1, length.out = 100001) -->
<!-- prior <- seq(0,1, by = 0.001) -->
<!-- posterior <- function(prior, rmp = 10e-9, fpp){ -->
<!--   return ( prior/(prior + rmp + fpp) ) -->
<!-- } -->
<!-- pristinePosterior <- posterior(prior, rmp = 1e-9, fpp = 0) -->
<!-- charitablePosterior <-  posterior(prior, rmp = 1e-9, fpp = 0.01) -->
<!-- ``` -->

<!-- Suppose the random match probability for the DNA match evidence  is rather low, say $10^{-9}$, and there is no uncertainty associated -->
<!-- with this number. Consider now two ways of assessing the DNA match. -->
<!-- First, disregarding the possibility of a false positive---setting $FP$ to $0$---makes -->
<!-- the match evidence appear extremely strong. In this case, the minimal prior sufficient for the posterior to be above .99 is only `RR min(prior[pristinePosterior >= .99])`, where the relation between the prior probabilities and the posterior probabilities of the source hypothesis -->
<!-- is given by the dashed orange line in Figure \ref{fig:fplinesPlot}. What happens after -->
<!-- taking into account the possibility of a false positive match? This depends on how this possibility of error is quantified. Assume the false positive rate corresponds to the upper bound of the [0.001, 0.01] interval. This assumption completely changes the assessment of the match evidence. Now the posterior of $.99$ is reached only if the prior is above .99. The match evidence  appears to be extremely weak. So which is it?  As already seen in the introduction, the point estimate exaggerates the value of the match evidence, while using the upper bound of the false positive rate has the opposite effect.  What happens within  the [0.001, 0.01] interval cannot be ignored. -->


<!-- To take into consideration the values within the edges, it would be best to have a good density estimate of the false positive errors frequency, as we should, if the issue had been properly studied. But we do not. For now, we will illustrate the consequences of taking  two different approaches. On one approach, any value between   the edges is considered  equally likely (and we add a little leeway on top). On another approach, not all values are equally likely---for example, suppose you think it is 50\% likely that the false positive rate is below .0033. In addition, suppose the  distribution, while being centered closer to zero, is long-tailed (we used a truncated normal distribution here).  These two distributions are displayed in Figure \ref{fig:fppdistros}. On both approaches, we assume that the false positive rate is between 0.001 and 0.01 with  99\% certainty. The uniform distribution---which regards all false positive rates in the interval as equally likely---leads to a rather conservative evaluation of the match evidence, much more so than the truncated normal distribution. This is apparent from Figure \ref{fig:fppMinima} and \ref{fig:fplinesPlot}, which show the prior probabilities of the source  hypothesis needed to secure a posterior probability above .99.  Working with a distribution---more so if it is not a uniform distribution---affords a more balanced assessment of the evidence than simply relying on the edges of an interval. -->

<!-- The lingering question, however, is how these distributions can be obtained.  Admittedly, studies on false positives are limited and only give an incomplete picture. More studies  are needed. This does not mean, however, that until then using point estimates and interval edges is preferable. After deciding on the functional form of a distribution---such as truncated normal or beta---only a few numbers need to be elicited from experts for constructing a density.^[For instance, assuming the distribution is a truncated normal, it is enough for the expert to assert that both the 99\% interval is as the one we used, and that they believe with more than 50\% confidence the false positive rates to be below $.033$ for the curve to be determined.] Having to rely on such elicitation is not without problems, but it is better than asking experts for single point estimates and relying on these [@o2006uncertain]. -->


<!-- ```{r fpSampling,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} -->
<!-- set.seed(1233) -->
<!-- fpdN <- ifelse(ps > 0 & ps <= .01015, 1, 0) -->
<!-- fpdN <- fpdN/sum(fpdN) -->
<!-- fpdNsample <- sample(ps, 1e5, replace = TRUE, prob = fpdN) -->
<!-- set.seed(1233) -->
<!-- fpdWsample <- sample(ps, 1e5, replace = TRUE, -->
<!--                      prob = dtruncnorm(ps, a = 0, b = 1, -->
<!--                                        mean = 0.0005, -->
<!--                                        sd = .004)) -->

<!-- fpnPlot <- ggplot()+geom_density(aes(x =  fpdNsample))+xlim(0,.02)+theme_tufte(base_size  = 10) + theme(plot.title.position =  "plot")+ labs(title = "Uniform (0, .01015)", subtitle = "Approximated with 10e5 sampled values")+ylab("density")+xlab("false positive rate") -->


<!-- fpwPlot <- ggplot()+geom_density(aes(x =  fpdWsample))+xlim(0,.02)+theme_tufte(base_size  = 10) + theme(plot.title.position =  "plot")+ labs(title = "Truncated normal (.0005, .004) ", subtitle = "Approximated with 10e5 sampled values")+ylab("density")+xlab("false positive rate") -->

<!-- # fpGrid <- grid.arrange(fpnPlot, fpwPlot,ncol=2,top=textGrob("Examples of FP densities with the same interval")) -->
<!-- ``` -->





<!-- \begin{figure}[H] -->


<!-- ```{r Figfppdistros, echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "80%", warning=FALSE, message = FALSE, dpi = 800, warning = FALSE, message = FALSE, output = FALSE} -->
<!-- grid.arrange(fpnPlot, fpwPlot,ncol=2,top=textGrob("Examples of FP densities with the same interval")) -->
<!-- ``` -->


<!-- \caption{Two examples of assumptions about the false positive rates, both having pretty much the same 99\% highest density intervals. Left: all error rates are equally likely. Right: the most likely values are closer to 0, but also some high values while unlikely are possible.} -->

<!-- \label{fig:fppdistros} -->

<!-- \end{figure} -->




<!-- ```{r fpMinima,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"} -->
<!-- set.seed(1233) -->
<!-- fpdNsample <- sample(ps, 1e4, replace = TRUE, prob = fpdN) -->
<!-- fpdWsample <- sample(ps, 1e4, replace = TRUE, -->
<!--                      prob = dtruncnorm(ps, a = 0, b = 1, -->
<!--                                        mean = 0.0005, -->
<!--                                        sd = .004)) -->
<!-- posteriorN <- list() -->
<!-- posteriorW <- list() -->
<!-- minimaN <- numeric(1e4) -->
<!-- minimaW <- numeric(1e4) -->

<!-- for (s in 1:1e4){ -->
<!--   fppN <-    fpdNsample[s] -->
<!--   fppW <-    fpdWsample[s] -->

<!--   posteriorN[[s]] <- posterior(prior = prior, -->
<!--                                fpp = fpdNsample[s]) -->
<!--   posteriorW[[s]] <- posterior(prior = prior, -->
<!--                                fpp = fpdWsample[s]) -->
<!-- } -->

<!-- for (s in 1:1e4){ -->
<!-- minimaN[s] <- ifelse(sum(posteriorN[[s]]> .99) >0, -->
<!--                      min(prior[posteriorN[[s]] > .99]), 1) -->
<!-- minimaW[s] <- ifelse(sum(posteriorW[[s]]> .99) >0, -->
<!--                      min(prior[posteriorW[[s]] > .99]), 1) -->
<!-- } -->

<!-- posteriorNDF <-   do.call(cbind, posteriorN) -->
<!-- posteriorWDF <-   do.call(cbind, posteriorW) -->
<!-- minimasDF <- data.frame(minima = c(minimaN, minimaW), -->
<!--                         prior = c(rep("uniform", 1e4) , -->
<!--                                   rep("trnormal", 1e4)) ) -->

<!-- minimaPlot <- ggplot(minimasDF)+ -->
<!--   geom_histogram(aes(x = minima, fill = prior, y = ..count../sum(..count..)), -->
<!--                  bins = 50, position = "dodge2")+ -->
<!--   theme_tufte(base_size = 10)+ -->
<!--   ggtitle("Minimal priors sufficient for posterior >.99")+ -->
<!--   ylab("Second-order probability")+theme(legend.position = c(0.9,.9), -->
<!--                                          plot.title.position = "plot")+ -->
<!--                                          scale_fill_brewer(type = "qual") -->
<!-- ``` -->







<!-- \begin{figure}[H] -->


<!-- ```{r FigfppMinima, echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "80%", warning=FALSE, message = FALSE, dpi = 800, warning = FALSE, message = FALSE, output = FALSE} -->
<!-- minimaPlot -->
<!-- ``` -->


<!-- \caption{The distribution of minimal priors sufficient for obtaining a posterior above .99 on the two distributions of false positive rates. The truncated normal distribution has its bulk towards the left, but at the same time has higher ratio of evens in which this posterior is never reached. } -->

<!-- \label{fig:fppMinima} -->

<!-- \end{figure} -->




<!-- ```{r fpLines,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%", warning = FALSE, message = FALSE} -->
<!-- posteriorNDFsubset <- as.data.frame(posteriorNDF[,1:300]) -->
<!-- posteriorNDFsubset$prior <- prior -->
<!-- posteriorNDFsubsetLong <- melt(posteriorNDFsubset, id.vars = "prior") -->

<!-- posteriorWDFsubset <- as.data.frame(posteriorWDF[,1:300]) -->
<!-- posteriorWDFsubset$prior <- prior -->
<!-- posteriorWDFsubsetLong <- melt(posteriorWDFsubset, id.vars = "prior") -->

<!-- alpha = .05 -->
<!-- size = .2 -->


<!-- posteriorNplot <- ggplot()+ -->
<!--   geom_line(data = posteriorNDFsubsetLong, -->
<!--             aes(x = prior, -->
<!--                 y = value, group = variable), -->
<!--                 alpha = alpha, size = size)+ -->
<!--   theme_tufte(base_size = 10)+ylab("posterior")+ -->
<!--   geom_line(aes(x = prior, y = pristinePosterior), -->
<!--             color = "orangered", lty = 3, linewidth = 1.1)+ -->
<!--   geom_line(aes(x = prior, y = charitablePosterior), -->
<!--             color = "dodgerblue4", lty = 2, linewidth = 1.1)+ -->
<!--   labs(title = "Uniform fpp density")+theme(plot.title.position = "plot")+ -->
<!--   xlim(0,.25) -->



<!-- posteriorWplot <- ggplot()+ -->
<!--   geom_line(data = posteriorWDFsubsetLong, -->
<!--             aes(x = prior, -->
<!--                 y = value, group = variable), -->
<!--             alpha = alpha, linewidth = size)+ -->
<!--   theme_tufte(base_size = 10)+ylab("posterior")+ -->
<!--   geom_line(aes(x = prior, y = pristinePosterior), -->
<!--             color = "orangered", lty = 3, linewidth = 1.1)+ -->
<!--   geom_line(aes(x = prior, y = charitablePosterior), -->
<!--             color = "dodgerblue4", lty = 2, linewidth = 1.1)+ -->
<!--   labs(title = "Trnormal fpp density")+theme(plot.title.position = "plot")+ -->
<!--   xlim(0,.25) -->
<!-- ``` -->





<!-- \begin{figure}[H] -->
<!-- ```{r FigfplinesPlot3, echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "80%", warning=FALSE, message = FALSE, dpi = 800, warning = FALSE, message = FALSE, output = FALSE} -->
<!-- grid.arrange(posteriorNplot, posteriorWplot,  ncol = 1, -->
<!--      top=textGrob("Posterior vs prior, impact of two fpp densities")) -->
<!-- ``` -->
<!-- \caption{Impact of prior on the posterior assumign two different densitites for false positive rates. Note how both the "pristine" error-free point estimate (orange) and the charitable version (blue) are quite far from where the bulks of the distributions in fact are. Note also how the trnormal density allows for even more charitable cases, which results from it being long-tailed.} -->
<!-- \label{fig:fplinesPlot} -->
<!-- \end{figure} -->

















# Computational and representational considerations

<!---The reader might be worried: how can we handle the computational complexity that comes with moving to higher-order probabilities? The  answer is, as long as we have decent ways of either basing densities on sensible priors and data, or eliciting densities from experts [@o2006uncertain],  implementation is not computationally unfeasible, as we can approximate densities using sampling. 
--->

The higher-order framework we are advocating is 
not only applicable to the evaluation of 
individual pieces of evidence. Complex bodies of 
evidence and hypotheses---for example, those often represented by 
Bayesian networks---can also be approached from this perspective. The general strategy is this: (1) capture the uncertainties involving the individual items of evidence in a modular fashion using the standard tools for statistical inference. (2) Elicit other probabilities or densities from experts^[For expert elicitation of densities in a parametric fashion and the discussion of the improvement to which doing so instead of eliciting point values leads, see [@o2006uncertain].], (3) put those together using a structure similar to that of a Bayesian network, except allowing for uncertainties of various levels to be put together --- a usual tool for such a representation is a probabilistic program [@Bingham2021PPwithoutTears], and (4) perform inference evaluating the relevant probabilities or densities of interest. 




If the reader is more used to thinking in terms of Bayesian networks,  a somewhat restrictive  but fairly straightforward way to conceptualize a large class of such programs is to imagine a probabilistic program as stochastically generating Bayesian networks using our uncertainty about the parameter values, update with the evidence, and propagate  uncertainty to approximate the marginal posterior  for nodes of interest.


As an illustration, let us start with a 
simplified Bayesian network developed by @Fenton2018Risk.
The network is reproduced in Figure \ref{fig-scbnplot} and 
represents the key items of evidence in the infamous British case R. v. Clark (EWCA Crim 54, 2000).^[Sally Clark's first son died in 1996 soon after birth, and her second son died in similar circumstances a few years later in 1998.  At trial, the pediatrician Roy Meadow testified that the probability that a child from such a  family would die of Sudden Infant Death Syndrome (SIDS) was 1 in 8,543.  Meadow calculated that therefore the probability of both children dying of SIDS was approximately  1 in 73 million. Sally Clark was convicted of murdering her  infant sons. The conviction was reversed on appeal. The case of appeal was based on new evidence: signs of a potentially lethal disease were found in one of the bodies.] 

In a Bayesian network the  arrows  depict direct relationships  of  influence  between  variables, and nodes---conditional on their parents---are taken to be independent of their non-descendants. \textsf{Amurder} and \textsf{Bmurder} are binary nodes corresponding to whether Sally  Clarkâ€™s  sons,   call  them A and B, were murdered. These  nodes influence  whether  signs of disease (\textsf{Adisease} and \textsf{Bdisease}) and bruising (\textsf{Abruising} and \textsf{Bbruising}) were present. Also, since A's death preceded in time B's death, whether A was murdered casts some light on the probability that B was also murdered.



The choice of the probabilities in the network is quite specific, and 
it is not clear where such precise values come from.  The standard response invokes 
*sensitivity analysis*: a range of plausible values is tested. As already discussed, this approach ignores the shape of the underlying distributions. Sensitivity analysis  does not make any difference between probability measures (or point estimates) in terms of their plausibility, but some will be more plausible than others. Moreover, if the sensitivity analysis is guided by extreme values, these might play an undeservedly strong role. 
These concerns can be addressed, at least in part, by recourse to higher-order probabilities.  In a precise Bayesian network, each  node is associated with a probability table determined by a finite list of numbers (precise probabilities). But suppose that, instead of precise numbers, we have densities over parameter values for the numbers in the probability tables.^[The densities of interests can then be approximated by (1) sampling parameter values from the specified distributions, (2) plugging them into the construction of the BN, and (3) evaluating the probability of interest in that precise BN. The list of the probabilities thus obtained will approximate the density of interest. In what follows we will work with sample sizes of 10k.]   An example  for the Sally 
Clark case is represented in @fig-scwithhop. 


```{r scBN,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%", warning=FALSE, message = FALSE, dpi = 800}
#create SC DAG
#define the structure of the Sally Clark BN
SallyClarkDAG <- model2network("[Abruising|Acause][Adisease|Acause][Bbruising|Bcause][Bdisease|Bcause][Acause][Bcause|Acause][NoMurdered|Acause:Bcause][Guilty|NoMurdered]")
SCdag <- model2network("[Abruising|Acause][Adisease|Acause][Bbruising|Bcause][Bdisease|Bcause][Acause][Bcause|Acause][NoMurdered|Acause:Bcause][Guilty|NoMurdered]")
#plot 
#graphviz.plot(SallyClarkDAG)


#CPTs as used in Fenton & al.
AcauseProb <-prior.CPT("Acause","SIDS","Murder",0.921659)
AbruisingProb <- single.CPT("Abruising","Acause","Yes","No","SIDS","Murder",0.01,0.05)
AdiseaseProb <- single.CPT("Adisease","Acause","Yes","No","SIDS","Murder",0.05,0.001)
BbruisingProb <- single.CPT("Bbruising","Bcause","Yes","No","SIDS","Murder",0.01,0.05)
BdiseaseProb <- single.CPT("Bdisease","Bcause","Yes","No","SIDS","Murder",0.05,0.001)
BcauseProb <- single.CPT("Bcause","Acause","SIDS","Murder","SIDS","Murder",0.9993604,1-0.9998538)

#E goes first; order: last variable through levels, second last, then first
NoMurderedProb <- array(c(0, 0, 1, 0, 1, 0, 0,1,0,1,0,0), dim = c(3, 2, 2),dimnames = list(NoMurdered = c("both","one","none"),Bcause = c("SIDS","Murder"), Acause = c("SIDS","Murder")))

#this one is definitional
GuiltyProb <-  array(c( 1,0, 1,0, 0,1), dim = c(2,3),dimnames = list(Guilty = c("Yes","No"), NoMurdered = c("both","one","none")))

# Put CPTs together
SallyClarkCPTfenton <- list(Acause=AcauseProb,Adisease = AdiseaseProb,
                      Bcause = BcauseProb,Bdisease=BdiseaseProb,
                      Abruising = AbruisingProb,Bbruising = BbruisingProb,
                      NoMurdered = NoMurderedProb,Guilty=GuiltyProb)

# join with the DAG to get a BN
SCfenton <- custom.fit(SallyClarkDAG,SallyClarkCPTfenton)
```




```{r scBNplot2,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "80%", warning=FALSE, message = FALSE, dpi = 800, fig.height= 5, fig.width= 7}
#| label: fig-scbnplot
#| fig-cap: "Bayesian network for the Sally Clark case, with marginal prior probabilities."
#| fig-pos: H


graphviz.chart(SCfenton, type = "barprob", layout = "dot", #draw.labels = TRUE,
  grid = FALSE, scale = c(0.75, 1.1), col = "black", 
  text.col = "black", bar.col = "black", main = NULL,
  sub = NULL)
```



```{r scStages,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "80%", warning=FALSE, message = FALSE, dpi = 800}

SCfJN <- compile(as.grain(SCfenton))

priorFenton <- querygrain(SCfJN, node = "Guilty")[[1]][1]

SCfJNAbruising <- setEvidence(SCfJN, nodes = c("Abruising"), states = c("Yes"))
AbruisingFenton <- querygrain(SCfJNAbruising, node = "Guilty")[[1]][1]

SCfJNAbruisingBbruising <- setEvidence(SCfJN, nodes = c("Abruising","Bbruising"),
                                states = c("Yes","Yes"))
AbruisingBbruisingFenton <- querygrain(SCfJNAbruisingBbruising, node = "Guilty")[[1]][1]

SCfJNAbruisingBbruisingNoDisease <- setEvidence(SCfJN, nodes = c("Abruising","Bbruising","Adisease","Bdisease"),
                                         states = c("Yes","Yes", "No", "No"))
AbruisingBbruisingFentonNoDiseaseFenton <-   querygrain(SCfJNAbruisingBbruisingNoDisease, node = "Guilty")[[1]][1]


SCfJNAbruisingBbruisingDiseaseA <- setEvidence(SCfJN, nodes = c("Abruising","Bbruising","Adisease","Bdisease"), 
                                               states = c("Yes","Yes", "Yes", "No"))
AbruisingBbruisingFentonDiseaseAFenton <- querygrain(SCfJNAbruisingBbruisingDiseaseA, node = "Guilty")[[1]][1]


SCfentonTable <- data.frame(stage = factor(c("prior", "bruising in A", "bruising in both",
                                      "bruising in both, no disease", "bruising in both, disease on A only"),
                                      levels = c("prior", "bruising in A", "bruising in both",
                                                 "bruising in both, no disease", "bruising in both, disease on A only")),
                            probability = c(priorFenton,AbruisingFenton,AbruisingBbruisingFenton,
                                            AbruisingBbruisingFentonNoDiseaseFenton,AbruisingBbruisingFentonDiseaseAFenton))
```


<!---
\begin{figure}[H]
```{r SCfentonTable2,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%", warning=FALSE, message = FALSE, dpi = 800}
ggplot(SCfentonTable) + geom_point(aes(x = stage, y = probability, size = probability * .9))+
  scale_x_discrete(limits=rev, expand = c(0, 2)) +coord_flip()+theme_tufte(base_size = 10)+ scale_size(guide="none")+
  theme(plot.title.position = "plot")+ggtitle("Impact of evidence according to Fenton's BN for the Sally Clark case") +
  geom_text(aes(x = stage, y= probability * 1.05, label= round(probability,2) ,hjust=-.3, vjust=-.3), size  = 3.5)+
  scale_y_continuous(breaks = seq(0,.7, by =.1), limits = c(0,.8))

```

\caption{The prior and posterior probabilities for Fenton's Sally Clark BN.}

\label{fig:SCfentonTable}

\end{figure}
--->

<!---
\begin{figure}[H]
```{r FigSCwithHOPa, echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%", warning=FALSE, message = FALSE, dpi = 800}

grid.arrange(AbruisingIfSidsPlot+xlim(0,.4),AbruisingIfMurderPlot+xlim(0,.4))
```
\caption{Example of approximated uncertainties about conditional probabilities in the Sally Clark case.}
\label{fig:SCwithHOPa}
\end{figure}
--->


```{r SCwithHOP,  echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, fig.show = "hold", out.width = "125%", out.height= "230%", warning=FALSE, message = FALSE, dpi = 800, fig.height=5 , fig.width= 7}
#| label: fig-scwithhop
#| fig-cap: "An illustration of a probabilistic program for the Sally Clark case."
#| fig-pos: h


# out.extra='angle=90'               does not seem to work in quarto
# , 


#SCprobsFinal <- readRDS("../datasets/SCprobsFinal.rds")
#attach(SCprobsFinal)
#source("../scripts/SCfunctions.R")
#source("../scripts/SCplotCPTs.R")
#source("../scripts/SCplotDistros.R")


AsidsPriorPlotGrob <- ggplotGrob(AsidsPriorPlot+theme_tufte(base_size = 5))
BcauseSidsIfAsidsPlotGrob <- ggplotGrob(BcauseSidsIfAsidsPlot+theme_tufte(base_size = 5))
BcauseSidsIfAmurderPlotGrob<- ggplotGrob(BcauseSidsIfAmurderPlot+theme_tufte(base_size = 5)) 

AbruisingIfSidsPlotGrob <- ggplotGrob(AbruisingIfSidsPlot+theme_tufte(base_size = 5)) 
AbruisingIfMurderPlotGrob <- ggplotGrob(AbruisingIfMurderPlot+theme_tufte(base_size = 5)) 


AdiseaseIfSidsPlotGrob <- ggplotGrob(AdiseaseIfSidsPlot+theme_tufte(base_size = 5)) 
AdiseaseIfMurderPlotGrob <- ggplotGrob(AdiseaseIfMurderPlot+theme_tufte(base_size = 5)) 


BbruisingIfSidsPlotGrob <- ggplotGrob(BbruisingIfSidsPlot+theme_tufte(base_size = 5)) 
BbruisingIfMurderPlotGrob <- ggplotGrob(BbruisingIfMurderPlot+theme_tufte(base_size = 5)) 


BdiseaseIfSidsPlotGrob <- ggplotGrob(BdiseaseIfSidsPlot+theme_tufte(base_size = 5)) 
BdiseaseIfAmurderPlotGrob <- ggplotGrob(BdiseaseIfAmurderPlot+theme_tufte(base_size = 5)) 


daigramToRotate <- ggplot(data.frame(a=1)) + xlim(1, 40) + ylim(1, 60)+theme_void()+
  annotation_custom(AsidsPriorPlotGrob, xmin = 9, xmax = 15, ymin = 49, ymax = 59)+
  geom_label(aes(label = "Bcause", x = 25, y = 41),
              size = 3 )+
  geom_label(aes(label = "Acause", x = 12, y = 48),
            size = 3 )+
  geom_curve(aes(x = 13.5, y = 48.2, xend = 25, yend = 42.5), curvature = -.18,size = .3,
             arrow = arrow(length = unit(.015, "npc")))+
  annotation_custom(BcauseSidsIfAsidsPlotGrob, xmin = 17, xmax = 23, ymin = 47, ymax = 57)+
  annotation_custom(BcauseSidsIfAmurderPlotGrob, xmin = 15, xmax = 21, ymin = 37, ymax = 47)+
  geom_label(aes(label = "Abruising", x = 2, y = 41),
             size = 3 )+
  geom_curve(aes(x = 10.5, y = 48.2, xend = 2, yend = 42.5), curvature = .2,size = .3,
             arrow = arrow(length = unit(.015, "npc")))+
  annotation_custom(AbruisingIfSidsPlotGrob, xmin = 2, xmax = 8, ymin = 47, ymax = 57)+
  annotation_custom(AbruisingIfMurderPlotGrob, xmin = 5, xmax = 11, ymin = 37, ymax = 47)+
  geom_label(aes(label = "Adisease", x = 6, y = 21),
             size = 3 )+
  geom_curve(aes(x = 13, y = 46.2, xend = 6, yend = 23), curvature = -.45,size = .3,
             arrow = arrow(length = unit(.015, "npc")))+
  annotation_custom(AdiseaseIfSidsPlotGrob, xmin = 4.5, xmax = 10.5, ymin = 27, ymax = 37)+
  annotation_custom(AdiseaseIfMurderPlotGrob, xmin = 9, xmax = 15, ymin = 17.5, ymax = 27.5)+
  geom_label(aes(label = "Bbruising", x = 14, y = 12),
             size = 3 ) +
  geom_curve(aes(x = 24, y = 39.5, xend = 15.5, yend = 13.5), curvature = -.2,size = .3,
                                    arrow = arrow(length = unit(.015, "npc")))+
  annotation_custom(BbruisingIfSidsPlotGrob, xmin = 15.5, xmax = 21.5, ymin = 25, ymax = 35)+
  annotation_custom(BbruisingIfMurderPlotGrob, xmin = 18.5, xmax = 24.5, ymin = 10, ymax = 20)+
  geom_label(aes(label = "Bdisease", x = 33, y = 18),
             size = 3 )  +
  geom_curve(aes(x = 26, y = 39.5, xend = 33, yend = 19.5), curvature = -.2,size = .3,
             arrow = arrow(length = unit(.015, "npc")))+
  annotation_custom(BdiseaseIfSidsPlotGrob, xmin = 31, xmax = 37, ymin = 27, ymax = 37)+
  annotation_custom(BdiseaseIfAmurderPlotGrob, xmin = 24.5, xmax = 30.5, ymin = 20, ymax = 30)

print(daigramToRotate, vp=viewport(angle=90), width=7, height=5)


```

\todo{N: I am still searching for a good fix of that plot}



Using the probabilistic program, 
we can investigate the impact of different 
items of evidence on Sally Clark's probability of guilt  (@fig-scwithhop). The starting point is 
the prior density for the \s{Guilt} node (first graph). Next, the network 
is updated with evidence showing signs of bruising on both children (second graph). 
Next, the assumption that both children lack signs of 
potentially lethal disease is added (third graph). Finally, we consider the state of the evidence at the time of the appellate case: signs of bruising existed on both children, but signs of lethal disease were discovered only on the first child. Interestingly, in the strongest scenario against Sally Clark (third graph), the median of the posterior distribution is above .95, 
but the uncertainty around that median is still too wide to warrant a conviction.^[The lower limit of the 89\% Highest Posterior Density Intervals (HPDI) is at .83.] This underscores the fact that relying on point estimates can lead to overconfidence. Paying attention to the higher-order uncertainty about the first-order probability can make a difference to trial decisions.




\todo{nl: This plot is not referenced anywhere, should it be visible?}
```{r SCwithHOP2, echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%", warning=FALSE, message = FALSE, dpi = 800, warning = FALSE, message = FALSE, results= 'hide'}
#| label: fig-scwithhop2
#| fig-cap: 'Impact of incoming evidence in the Sally Clark case.'
#| fig-pos: H

grid.arrange(GuiltPriorPlot, GuiltABbruisingPlot, GuiltABbruisingNoDiseasePlot, GuiltABbruisingDiseaseAPlot, ncol =2)
```



One question that arises is how this approach relates to the standard method of using likelihood ratios to report the value of the evidence. On this approach, the conditional probabilities that are used in the likelihood ratio calculations are estimated and come in a package with an uncertainty about them. Accordingly, these uncertainties propagate: to estimate the likelihood ratio while keeping track of the uncertainty involved, we can   sample probabilities from the selected distributions appropriate for the conditional probabilities needed for the calculations, then divide the corresponding samples, obtaining a sample of likelihood ratios, thus approximating the density capturing the recommended uncertainty about the likelihood ratio. Uncertainty about likelihood ratio is just propagated uncertainty about the involved conditional probabilities. For instance, we can use this tool to gauge our uncertainty about the likelihood ratios corresponding to the signs of bruising in son A and the presence of the symptoms of a potentially lethal disease in son A (@fig-sclrs).





```{r SClrs, echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "90%", warning=FALSE, message = FALSE, dpi = 800, warning = FALSE, message = FALSE, results= 'hide'}
#| label: fig-sclrs
#| fig-cap: 'Likelihood ratios forbruising and signs of disease in child A in the Sally Clark case.'
#| fig-pos: H

AbruisingLR <- AbruisingIfMurder / AbruisingIfSids

AbruisingLRPlot <- plotSample(AbruisingLR,title = "LR: Abruising",  paste("median =", 
                                                            round(median(AbruisingLR),2), ", 89%HPDI = ", 
                                                            round(HPDI(AbruisingLR),2)[1],"-",round(HPDI(AbruisingLR),2)[2],
                                                            sep = "") )+xlim(0,30)

AdiseaseLR <- AdiseaseIfMurder / AdiseaseIfSids

AdiseaseLRPlot <- plotSample(AdiseaseLR,title = "LR: Adisease",  paste("median =", 
                                                                           round(median(AdiseaseLR, na.rm = TRUE),2), ", 89%HPDI = ", 
                                                                           round(HPDI(AdiseaseLR),2)[1],"-",round(HPDI(AdiseaseLR),2)[2],
                                                                           sep = "") )+xlim(0,3)
 grid.arrange(AbruisingLRPlot, AdiseaseLRPlot, ncol =2)
```










# Discussion

Our approach does involve multiple parameters, uncertainty about them, along with a dependency structure between random variables. So it is only natural to ask whether what we propose 
is not just an old wolf in a new sheep's clothing, as one might think that what looks like a DAG and quacks
like a DAG is always a hierarchical model. In this section we briefly clarify what the answer to this question is.

First, we need some clarity on what a Bayesian hierarchical model is. In the widest sense of the word,
 these are mathematical descriptions involving multiple parameters such that credible values for some of 
 them meaningfully depend on the values of other parameters, and that dependencies can be re-factored into
  a chain of dependencies.  For instance, think about a joint parameter  space   for two parameters $\theta$
   and $\omega$, where $p(\theta, \omega \vert D) \propto p(D \vert \theta, \omega)p(\theta, \omega)$.
    If, further, some independence-motivated re-factoring of the right-hand side---for instance
     as $p(D\vert \theta)p(\theta \omega)p(\omega)$---is possible, we are dealing with a hierarchical
      model in the wide sense of the word.

Such models usually come useful when we are dealing with clustered data, such as a cohort study with
repeated measures, or some natural groupings at different levels of analysis. Then, lower-level parameters
are treated as i.i.d. and share the same parameter distribution characterized by some hyper-parameters in
turn characterized by a prior distribution. As a simple example consider a scenario in which we are
dealing with multiple coins created by one mint---each coin has its own bias $\theta_i$, but also
there is some commonality as to what these biases are in this mint, represented by a higher-level 
parameter $\theta$.
Continuing the example,  assume $\theta_i \sim \mathsf{Beta}(a, b)$ and 
$y_{i\vert s} \sim \mathsf{Bern}(\theta_s)$, where the former distribution can be re-parametrized as 
$\mathsf{Beta}(\omega(k-2)+1, (1-\omega)(k-2)+1)$. Let's  keep $k$ fixed,  $\omega$ is our expected value of
the $\theta_i$ parameters, with some dispersion around it determined by $k$. Now, if we also are uncertain
about $\omega$ and express our uncertainty about it in terms a density $p(\omega)$, we got ourselves a
hierarchical model with joint prior distribution over parameters $\prod p(\theta_i \vert \omega) p(\omega)$. 


As  another example, one can develop a multilevel regression model of the distributions of the random levels 
in various counties, where both the intercept and the slope vary with counties by taking  
$y_i\sim \mathsf{Norm}(\alpha_{\mbox{j[i]}}\, + \beta_{\mbox{j[i]}}  x_i, \sigma^2_y )$, where $j$ is a county index, 
$\alpha_j \sim \mathsf{Norm}(\mu_\alpha,\sigma_\alpha^2 )$,  and
$\beta_j \sim \mathsf{No}rm(\mu_\beta,\sigma_\beta^2 )$. Then, running the regression one
estimates both the county-level coefficients, and the higher-level parameters.

Our approach is similar to the standard hierarchical models in the most general sense:  
there is a meaningful independence structure and distributions over parameter values that we are working with.
However, our  approach is unlike such models in a few respects. For one, we are not dealing with clustered data, 
and the random variables are mostly propositions and their truth values. Given a hypothesis $H$ and an item
of evidence $E$ for it, there seems to be no interesting conceptualization on which the underlying data would
be clustered. For example, considering stains at a crime scene as a subgroup of crimes being committed doesn't make logical sense.
Yes, there is dependency between these phenomena, 
but describing it as clustering would be at least misleading.  Second, the dependencies proceed through
 the values of the random variables which are **not** parameters, but rather truth-values, and require 
 also conditional uncertainties regarding the dependencies between these truth-values. 

Again, continuing the hypothesis-evidence example, we have $H \sim \mathsf{Bern}(p_h)$, 
$p_h \sim \mathsf{Beta}(a_h, b_h)$, and  $E\sim \mathsf{Bern}(p_e)$. But then we also have the beta distributions
for the probability of the evidence conditional on the actual values of the random variables---the truth-values---thus 
$p_e \vert H = 1 \sim beta(a_{+}, b_{+} )$ and  $p_e \vert H = 0 \sim \mathsf{Beta}(a_{-}, b_{-})$.
But the re-factoring in terms of the actual values of the random variables (which just happen to resemble 
probabilities because they are truth values) makes it quite specific,  at the same time allowing for the 
computational use of a probabilistic program.  Finally, the reasoning we describe is not  a regression the
 way it is normally performed:  the learning task is delegated to the bottom level of whatever happens to the 
 Bayesian networks once updated with evidence. 
We would prefer to reserve the term \emph{hierarchical model} for a class of models dealing with interesting 
cluster structures in the data. A more fitting term for the representation tool we propose should be used 
here is \emph{probabilistic programs}. We do not claim any originality in devising this tool: it's an 
already existing tool. What we argue for, though, is its ability for being usefully deployed in the
 context of forensic evidence evaluation and integration with other assumptions and hypotheses. 





Perhaps, you might dislike the idea of going higher-order for theoretical reasons. One might be that you don't like 
the complexity. This seems to be the line taken by Bradley, who refuses to go higher-order for the following reason:

\begin{quote}
Why is sets of probabilities the right level to stop the regress at? Why not sets of sets? Why not second-order 
probabilities? Why not single probability functions? This is something of a pragmatic choice. The further we 
allow this regress to continue, the harder it is to deal with these belief representing objects. So let's not
 go further than we need. 131-132\end{quote}

We have argued extensively, that given the difficulties of both PP and IP and how the current approach handles it, 
we are not going further than we need in using higher-order probabilities. We're going where we should be. 
And the supposed pragmatic concerns that one might have are unclear:  parameter uncertainty, approximations
 and other computational methods I have used in fact quite embedded in Bayesian statistical practice and decent 
 computational tools for the framework I propose are available.\footnote{Also, you can insist that instead of
  going higher order we could just take our sample space to be the cartesian product of the original sample
   space and parameter space, or use parameters having certain values as potential states of a bayesian network. 
   If you prefer not to call such approaches first-order, I don't mind, as long as you effectively end up
    assigning probabilities to certain probabilities, the representation means I discussed in this paper
     should be in principle available to you.}



Another concern that you might have is that it is not clear what the semantics of such an approach should 
look like. While a more elaborate account is beyond the scope of this paper, the general gist of the approach 
can be modeled by a slight modification of a framework of 
probabilistic frames [@Dorst2022higher-order;@Dorst2022evidence]. Start with a set of possible worlds $W$.
Suppose you consider a class of probability distributions $D$, a finite list of atomic sentences
$q_1, \dots, q_2$ corresponding to subsets of $W$, and a selection of true probability hypotheses 
$C$ (think of the latter as omniscient distributions, $C\subseteq D$, but in principle this restriction 
can be dropped if need be). Each possible world $w\in W$ and a proposition $p\subseteq W$ come with their 
true probability distribution, $C_{w,p}\in D$ corresponding to the true probability of $p$ in $w$, 
and the distribution that the expert assigns to $p$ in $w$, $P_{w,p}\in D$. Then, various propositions 
involving distributions can be seen as sets of possible worlds, for instance, the proposition that the expert 
assigns $d$ to $p$ is the set of worlds $w$ such that $P_{w,p}=d$.\footnote{There is at least one important 
difference between this approach and that developed by Dorst. His  framework is untyped, which allows for 
an enlightening discussion of the principle of reflection and alternatives to it. In this paper I prefer 
to keep this complexity apart and use an explicitly typed set-up.} 






# Appendix: propriety {-}


# Appendix: the strict propriety of $\mathcal{I}_{\dkl}^2$ {-}

Let us start with a definition.

\begin{definition}[concavity]

A function $f$ is convex over an interval $(a,b)$ just in case for all  $x_1, x_2\in (a,b)$ and 
$0 \leq \lambda \leq 1$ we have:
\begin{align*}
f(\lambda x_1 + (1-\lambda)x_2) \leq \lambda f(x_1) + (1-\lambda)f(x_2)
\end{align*}

\noindent A function $f$  is concave just in case:
\begin{align*}
f(\lambda x_1 + (1-\lambda)x_2) \geq \lambda f(x_1) + (1-\lambda)f(x_2)
\end{align*}
\noindent A function $f$  is strictly concave just in case the equality holds only if either $\lambda = 0$ or $\lambda = 1$.
\end{definition}

For us it is important that if a function is twice differentiable on an interval, then it is (strictly) 
concave just in case its second derivative is non-positive (negative). In particular, as $(\log_2(x))'' = -\frac{1}{x^2 ln(2)}$, $\log_2$ is strictly concave over its domain.\footnote{I line with the rest of the paper, we'll work with $\log$ base 2. We could equally well use any other basis.}





\begin{lemma}[Jensen's inequality]
If $f$ is concave, and $g$ is any function of a random variable, $\mathbb{E}(f(g(x))) \leq f(\mathbb{E}(g(x)))$. If $f$ is 
strictly concave, the equality holds only if $g(x) = \mathbb{E} g(x)$, that is, if $g(x)$ is constant everywhere.
\end{lemma}




\begin{proof}
For the base case consider a two-point mass probability function. Then,
\begin{align*}
p_1f(g(x_1))+ p_2f(g(x_2)) &\leq f(p_1g(x_1) + p_2g(x_2))
\end{align*}
\noindent follows directly from the definition of concativity, if we take $\lambda = p_1$, $(1-\lambda)=p_2$,
 and substitute $g(x_1)$ and $g(x+2)$ for $x_1$ and $x_2$.



Now, suppose that  $ p_1f(g(x_1))+ p_2f(g(x_2)) = f(p_1g(x_1) + p_2g(x_2))$ and that   $f$ is strictly concave.
 That means either $(p_1 = 1\wedge p_2 = 0)$, or $(p_1 = 0 \wedge p_2 =1)$. Then either $x$ always takes
  value $x_1$, in the former case, or always takes value $x_2$, in the latter
   case. $\mathbb{E} g (x) =  p_1 g(x_1) + p_2 g(x_2)$, which equals  $g(x_1)$ in the former case and $g(x_2)$ in the latter.


Now suppose Jensen's inequality and the consequence of strict contativity) holds for $k-1$ mass points. 
Write $p_i' = \frac{p_i}{1-p_k}$ for $i = 1, 2, \dots, k-1$. We now reason:
\begin{align*}
\sum_{i=1}^k p_i f(g(x_i)) & =
 p_kf(g(x_k)) + (1-p_k)\sum_{i =1}^{k-1}p_i'f(g(x_i)) &\\
 & \leq p_k f(g(x_k)) + (1-p_k)f\left( \sum_{i = 1}^{k=i}p_i' g(x_i) \right) & \mbox{\footnotesize by 
 the induction hypothesis}\\ &\leq f\left( p_k(g(x_k)) + (1-p_k)\sum_{i = 1}^{k-1} p_i' g(x_i)\right) & 
 \mbox{\footnotesize by the base case} \\
 & = f \left( \sum_{i}^k p_i g(x_i)\right)
 \end{align*}

Notice also that at the induction hypothesis application stage we know that the equality holds only if 
$p_k =1 \vee p+k = 0$. In the former case $g(x)$ always takes value $x_k = \mathbb{E} g(x)$. In the latter case,
 $p_k$ can be safely ignored and $\sum_{i=1}^{k}p_ig(x_i) = \sum_{i=1}^{k-1}p'g(x_i)$ and by the induction 
 hypothesis we already know that $\mathbb{E} g(x) = g(x)$.


\end{proof}



In particular, the claim holds if we take $g(x)$ to be $\frac{q(x)}{p(x)}$ (were both $p$ and $q$ are 
probability mass functions), and  $f$ to be $\log_2$. Then, given that $A$ is the support set of $p$, we have:
\begin{align*}
\sum_{x\in A}p(x) \log_2 \frac{q(x)}{p(x)} & \leq \log_2 \sum_{x\in A}p(x)\frac{q(x)}{p(x)}
\end{align*}

\noindent Moreover, the equality holds only if $\frac{q(x)}{p(x)}$ is constant, that is, only if $p$ and $q$ 
are the same pmfs. Let's use this in the proof of the following lemma.

\begin{lemma}[Information inequality] For two probability mass functions $p, q$, $\dkl(p,q)\geq 0$ with 
equality iff $p=q$.
\end{lemma}


\begin{proof}
Let $A$ be the support set of $p$, and let $q$ be a probability mass function whose support is $B$.
\begin{align*}
- \dkl(p,q) & = - \sum_{x\in A}p(x) \log_2 \frac{p(x)}{q(x)}& \mbox{\footnotesize (by definition)} \\
&  =  \sum_{x\in A}p(x)  - \left(\log_2 p(x) - \log_2 q(x)\right)& \\
&  =  \sum_{x\in A}p(x)   \left(\log_2 q(x) - \log_2 p(x)\right)& \\
& =  \sum_{x\in A} p(x) \log_2 \frac{q(x)}{p(x)}& \\
& \leq \log_2 \sum_{x\in A} p(x)\frac{q(x)}{p(x)} & \mbox{\footnotesize by Jensen's inequality}\\
& \mbox{(and the equality holds only if $p = q$)}\\
& = \log_2 \sum_{x\in A} q(x)  & \\
& \leq \log_2 \sum_{x\in B} q(x) & \\
& = log (1)  = 0 &\\
\end{align*}
\end{proof}


Observe now that $\dkl$ can be decomposed in terms of cross-entropy and entropy.

\begin{lemma}[decomposition] $\dkl = H(p,q) - H(p)$. \end{lemma}



\begin{proof}
\begin{align*}
\dkl (p, q) & = \sum_{p_{i}} \left( \log_2 p_i - \log_2 q_i \right) \\
& =   - \sum_{p_{i}}\left( \log_2 q_{i} - \log_2 p_{i} \right) \\
& = - \sum_{p_{i}} \log_{2} q_{i} - \sum_{p_{i}} - \log_{2} p_{i}   \\
& -  \underbrace{- \sum_{p_{i}} \log_2 q_{i}}_{H(p,q)}    - \underbrace{- \sum_{p_i}  \log_2 p_{i}}_{H(p)}
\end{align*}
\end{proof}


With information inequality this easily entails  Gibbs' inequality:

\begin{lemma}[Gibbs' inequality] $H(p,q) \geq H(p)$ with identity only if $p = q$.
\end{lemma}



Now we are done with our theoretical set-up. Here is how it entails the propriety of $\mathcal{I}_{\dkl}^2$. 
First, let's systematize the notation.
Consider a discretization of the parameter space $[0,1]$ into $n$ equally spaced values $\theta_1, \dots, \theta_n$. 
For each $i$ the ``true'' second-order distribution if the true parameter indeed is $\theta_i$---we'll call it 
the indicator of $\theta_i$--- is defined by
\begin{align*}
Ind^k(\theta_i) & = \begin{cases} 1 & \mbox{if } \theta_i = \theta_k\\
                        0 & \mbox{otherwise}  \end{cases}
\end{align*}
\noindent I will write $Ind^k_i$ instead of $Ind^k(\theta_i)$.


Now consider a probability distribution $p$ over this parameter space, assigning probabilities $p_1, \dots, p_n$
 to $\theta_1, \dots, \theta_n$ respectively. It is to be evaluated in terms of inaccuracy from the perspective 
 of a given 'true' value $\theta_k$. The inaccuracy of $p$ if $\theta_k$ is the 'true' value, is the 
 divergence between $IndI^k$ and $p$.

\begin{align*}
\mathcal{I}_{\dkl}^2(p, \theta_k) & = \dkl(Ind^k,p) \\
& = \sum_{i=1}^n Ind^k_i \left( \log_2 Ind^k_i - \log_2 p_i \right)
\end{align*}
Note now that for $j \neq k$ we have $Ind^k_j = 0 $  and so $Ind^k_j \left( \log_2 Ind^k_j - \log_2 p_j \right)=0$. 
Therefore we continue:
\begin{align*}
& = Ind^k_k \left( \log_2 Ind^k_k - \log_2 p_k \right)
\end{align*}
Further, $Ind^k_k= 1$ and therefore $\log_2 Ind^k_k =0$, so we simplify:
\begin{align*}
& =  - \log_2 p_k
\end{align*}

\noindent Now, let's think about expected values. First, what the inaccuracy of $p$ as expected by $p$, $EI(p,p)$?

\begin{align*}
EI(p,p) & = \sum_{i =1}^n p_i \mathcal{I}_{\dkl}^2(p, \theta_k) \\
& = \sum_{i =1}^n  p_i - \log_2 p_k \\
& = - \sum_{i =1}^n  p_i  \log_2 p_k = H(p)
\end{align*}

\noindent Analogously, the inaccuracy of $q$ as expected from the perspective of $p$ is:

\begin{align*}
EI(p, q) & =   \sum_{i =1}^n p_i \left( - \log_2 q_i\right)\\
& = -  \sum_{i =1}^n p_i  \log_2 q_i) = H(p,q)
\end{align*}


But that means, by Gibbs' inequality, that $EI(p,q) \geq EI(p,p)$ unless $p=q$, which completes the proof.



#  References {-}