---
title: "Higher-order (Precise) Probabilism" 
date: 'May 15, 2024'
format:
  pdf:
    toc: false
    include-in-header: quartoStyle.sty
    mainfont: Times New Roman
    sansfont: Times New Roman
documentClass: scrartcl
classOptions: [dvipsnames, enabledeprecatedfontcommands, headings=big]
font:
  size: 10pt
url:
  color: blue
bibliography: [../../../references/referencesMRbook.bib]
csl: apa-6th-edition.csl
indent: true
numbersections: true
---

  <!-- pdf:
    numberSections: true
    dfPrint: kable 
    keepTex: true
    toc: true
    include-in-header:  quartoStyle.sty
    latex_engine: pdflatex  -->


```{r setup}
#| include: false

knitr::opts_chunk$set(echo = TRUE)
#library(dagitty)
#library(rethinking)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(tidyr)
#library(philentropy)
#library(latex2exp)
#library(gridExtra)
#library(rethinking)
#library(bnlearn)
#library(gRain)
#library(reshape2)
#library(truncnorm)
#library(ggforce)
library(dplyr)
# library(magick)


```

\begin{quote} \textbf{Abstract.}  Rational agents are often uncertain about the truth of many propositions. To represent this uncertainty, it is natural to rely on probability theory. Two options are typically on the table, precise and imprecise probabilism, but both fall short in some respect. Precise probabilism is not expressive enough, while imprecise probabilism suffers from belief inertia and the impossibility of proper scoring rules. We put forward a novel version of probabilism, higher-order probabilism, and we argue that it outperforms existing alternatives.

Keywords: Probabilism; Imprecise probabilities; Evidence; Probability; Belief inertia; Bayesian networks; Proper scores.

\end{quote}

# Introduction
\label{sec:introduction}


<!--we form beliefs about a variety of propositions on the basis of the evidence available to us. But believing a proposition is not an all-or-nothing affair; it is a matter of degrees. -->


 As rational agents, we are uncertain about the truth of many propositions since our evidence is often fallible. To represent this uncertainty, it is natural to rely on probabilities. Two options are typically on the table: precise and imprecise probabilism. Precise probabilism models an agent's state of uncertainty (or credal state) with a single probability measure: each proposition in the algebra is assigned a probability value between 0 and 1 (a sharp credence).  The problem is that a single probability measure is not expressive enough to distinguish between intuitively different states of uncertainty (\S \ref{sec:precise-probabilism}).  To avoid this problem, a *set* of probability measures can be used to represent the uncertainty of a rational agent. This approach is known as imprecise probabilism. It outperforms precise probabilism in some respects, but also runs into its own problems, such as belief inertia and the impossibility of defining proper scoring rules (\S \ref{sec:imprecise-probabilism}). 
 
 To make progress, this paper argues that a rational agent's uncertainty should be represented  <!---neither by a single probability measure nor a set of measures, but rather-->
 by a set of precise probability measures defined over an algebra of propositions, and in addition, a probability distribution over parameter values interpreted as probability measures. We call this proposal \emph{high-order (precise) probabilism}. <!---The key insight is that a rational agent's uncertainty (or credal state) is not single-dimensional and thus cannot be mapped onto a one-dimensional scale like the real line. Uncertainty is best modeled by the shape of a probability distribution over multiple probability measures. --->  The theory we propose is not mathematically novel, nor are philosophers unfamiliar with higher-order probabilities **CITE SOME: Skyrms, Gaifmann, Domotor, Uchii, Pearl, Peijnenburg and Atkinson**. Here we address specifically how higher-order probabilism can overcome the problems that plague precise and imprecise probabilism (\S \ref{sec:higher-order} and \S \ref{sec:proper-scores}). We also argue that higher-order probabilism fares better than existing versions of probabilism when the probability of multiple propositions, dependent or independent, is to be assessed (\S \ref{sec:higher-order-conjunction} and \ref{sec:higher-order-networks}).  Many of the examples in this paper are about coin tosses, but in the final two sections, we discuss legal examples and illustrate the broader applicability of higher-order probabilism.
  

# Precise probabilism
\label{sec:precise-probabilism}

Precise probabilism holds that a rational 
agent's uncertainty about a proposition is to be represented as a single, precise probability measure. 
Bayesian updating regulates how the prior probability measure should change in light of new evidence that the agent learns. This is an elegant and simple theory, 
but representing our uncertainty about a proposition with a single, precise probability measure fails to 
capture an important dimension of how 
our fallible beliefs reflect the evidence 
we have (or have not) obtained. A couple of stylized 
examples featuring coin tosses should make the point clear. 

\begin{quote}
\textbf{No evidence v. fair coin}
You are about to toss a coin but have no evidence 
about its bias. You are completely ignorant. 
Compare this to the situation in which you know, 
based on overwhelming evidence, that the coin is fair. 
\end{quote}

\noindent On precise probabilism, both scenarios are represented by assigning a probability of .5 to the proposition \emph{the coin will land heads on the next toss} (or \emph{heasd} for short). If you are completely ignorant, the principle of insufficient evidence suggests that you assign .5 to this proposition. Similarly, if you are sure the coin is fair, assigning again .5 seems the best way to quantify your uncertainty about the outcome. The agent's evidence in the two scenarios is quite different, but the precise probability of \emph{heasd} cannot capture this difference. 

\begin{quote}
\textbf{Learning from ignorance}
You toss a coin with unknown bias. You toss it 10 times and observe 5 heads. You toss it further and observe 50 heads in 100 tosses. 
\end{quote}

\noindent
Since the coin initially had an unknown bias, you should 
presumably assign a probability of .5 to the proposition \emph{heads}, if you stick with precise probabilism.  After the 10 tosses, you again assess the probability to be .5. You must have learned something, but whatever that is, it is not reflected in the precise probability of \emph{heads}. When you toss the coin 100 times and observe 50 heads, you also learn something new, but your precise probability does not change.^[Another problem for precise probabilism is known as 
\emph{sweetening} [@sweetening2010]. Imagine a rational agent who does not know the bias of the coin. For precise probabilism, this state of uncertainty is represented by a .5 probability assignment to heads. Next, the agent learns that the bias towards heads, whatever the bias is, has been slightly increased, say by .001. Intuitively, the new information should leave the agent equally undecided about betting on heads or tails. After sweetening, the agent still does not know much about the actual bias of the coin. But, according to precise probabilism, sweetening should make the agent bet on heads: if the probability of heads was initially .5, it should now be slightly above .5.]

<!--These examples suggest that precise probabilism is not appropriately responsive to evidence.--> <!--Precise probabilism assigns the same probability in situations in which one's evidence is quite different:  when no evidence is available about a coin's bias; when there is little evidence the coin is fair (say, after 10 tosses); when there is strong evidence the coin is fair (say, after 100 tosses). Analogous problems arise for evidence that the coin is not fair. If a rational agent starts with a weak belief that the coin is  .6 biased towards heads, they can strengthen that belief by tossing the coin repeatedly and observing, say, 60 heads in 100 tosses. But this improvement in their evidence is not mirrored in the .6 probability they should assign to *heads*.
-->

These examples show that the precise probability of \emph{heads} is not appropriately responsive to evidence. But instead of focusing on this probability, the precise probabilist might respond that we should extend the algebra and include propositions about the bias of the coin. As evidence accumulates that the coin is fair, the (precise) probability of the proposition \emph{the coin has a .5 bias} should go up, even though the (precise) probability of \emph{heads} should remain .5. We think this response is on the right track, but how does it generalize beyond cases of coin tosses? <!--It is one thing not to know much about whether a proposition is true, for example, whether an individual is guilty of a crime. It is another thing to have strong evidence that favors a hypothesis and equally strong evidence that favors its negation, for example, strong evidence favoring the guilt hypothesis and equally strong evidence favoring the hypothesis of innocence. Despite this difference, precise probabilism would recommend that a probability of .5 be assigned to both hypotheses in either case. Or -->

 Suppose that, given a certain stock of evidence, $A$ is more likely than $B$. Further, suppose that the acquisition of new evidence does not change the probabilities. Admittedly, something has changed in the agent's state of uncertainty: the quantity of evidence on which the agent can assess whether $A$ is more probable than $B$ has become larger. And yet, this change is not reflected in the precise probabilities assigned to $A$ and $B$.^[The distinction here is sometimes formulated in terms of the *balance* of the evidence (that is, whether the evidence available tips in favor a hypothesis or another) as opposed to its *weight* (that is, the overall quantity of evidence regardless of its balance); see @keynes1921treatise and @joyce2005probabilities.] 
The precise probabilist might recommend we extend the algebra and include propositions of the form \emph{the probability of event X is such and such}. Since such propositions contain a probability, assigning a probability to them effectively amounts to using higher-order probabilities.^[Interestingly, **PERAL ADD REFERENCE** argues that the semantics of higher-order probability statements can still be expressed in the language of first-order probability.]  Before going higher-order, however, we should explore another view in the literature.



# Imprecise probabilism
\label{sec:imprecise-probabilism}

<!---What if we give up the assumption that probability assignments should be precise? 
-->
Imprecise probabilism holds that a rational agent's credal stance towards a hypothesis is to be represented by a set of probability measures, typically called a representor $\mathbb{P}$, rather than a single measure $\mathsf{P}$. The representor should include all and only those probability measures which are compatible
with the evidence (more on this point later).^[For the development of imprecise probabilism, see @keynes1921treatise; @Levi1974ideterminate; @Gardenfors1982unreliable; @Kaplan1968decision;  @joyce2005probabilities; @VanFraassen2006vague; @Sturgeon2008grain; @walley1991statistical. @bradley2019imprecise is a good source of further references. Imprecise probabilism is closely related to what we might call interval probabilism [@Kyburg1961; @kyburg2001uncertain]. In interval probabilism, precise probabilities are replaced by intervals of probabilities. Imprecise probabilism is more general since the representor set need not be an interval.] 
Modeling an agent's credal state by sets of probability measures can easily accommodate the scenarios in the previous section without adding propositions about coin biases. For instance, if an agent is sure the coin is fair, their credal state would be represented by the singleton set $\{\mathsf{P}\}$, where $\mathsf{P}$ is a probability measure that assigns $.5$ to \emph{heads}. If, on the other hand, the agent knows nothing about the coin's bias, their credal state would be represented by the set of all probabilistic measures, since none of them is excluded by the available evidence. <!--Note that the set of probability measures does not represent admissible options that the agent could legitimately pick from. Rather, the agent's credal state is essentially imprecise and should be represented by means of the entire set of probability measures.-->

So far so good. But now consider this scenario: 
<!--just as precise probabilism fails to be appropriately 
evidence-responsive in certain scenarios, imprecise probabilism runs into similar difficulties 
in other scenarios. 
-->

\begin{quote}
\textbf{Even v. uneven bias:}
 You have two coins and you are sure the probability of \emph{heads} is .4, if you toss one coin, and .6, if you toss the other. But you cannot tell which is which. You pick one coin at random and toss it.  Contrast this with an uneven case. You have four coins and you are sure three of them have bias $.4$ and one of them bias $.6$. You pick a coin at random and toss it. You should be three times more confident that the probability of getting heads is .4. rather than .6.
\end{quote}

\noindent
The first scenario can be easily modeled by imprecise probabilism. The representor would contain two probability measures; one assigns .4. and the other assigns .6 to the proposition \emph{the coin will land heads}.
Yet imprecise probabilism cannot model the second scenario. Since the probability measures in the set are all compatible with the agent's evidence, no probability measure can be assigned a greater (higher-order) probability than any other.^[Other scenarios can be constructed in which imprecise probabilism fails to capture distinctive intuitions about evidence and uncertainty; see, for example, [@Rinard2013against].] 

<!-- Suppose you know of two urns, \textsf{GREEN} and \textsf{MYSTERY}. You are certain \textsf{GREEN} contains only green marbles, but have no information about \textsf{MYSTERY}. A marble will be drawn at random from each. You should be certain that the marble drawn from \textsf{GREEN}  will be green ($G$), and you should be more confident about this than about the proposition that the marble from \textsf{MYSTERY} will be green ($M$). For each $r\in [0,1]$  your representor will contain a $\mathsf{P}$ with $\pr{M}=r$. But then,  it also contains one with $\pr{M}=1$. This means that it is not the case that for any probability measure  $\mathsf{P}$ in the representor set, $\mathsf{P}(G) > \mathsf{P}(M)$, that is, it is not the case that a rational agent should be more confident of $G$ than of $M$. This is counter-intuitive. 
-->

These scenarios show that imprecise probabilism is not always expressive enough. <!--We believe that the solution is to add a probability distribution over the probability measures (more on this in the next section).--> 
<!--Defenders of imprecise probabilism might concede this point but prefer their account for reasons of simplicity. They could also point out that imprecise probabilism models scenarios that precise probabilism cannot model, for example, a state of complete lack of evidence. In this respect, imprecise probabilism outperforms precise probabilism in expressive power, and also retains theoretical simplicity. But this is not quite true as 
-->
Worst still, imprecise probabilism suffers from three shortcomings that do not affect precise probabilism: first, the idea of compatibility between the available evidence and the probability measures in the representator set is not clearly defined; second, updating imprecise probabilities can run into belief inertia; and third, no proper scoring rules exist for imprecise probabilities. We consider each in turn.

The first shortcoming has not received extensive discussion in the literature. For imprecise probabilism, an agent's state of uncertainty is represented by those probability measures that are *compatible* with the agent's evidence, <!--- How should the notion of compatibility be understood here? The idea is that thanks to this feature, imprecise credal stances are evidence-responsive in a way precise probabilistic stances are not.---> but the notion of compatibility lacks a clear definition in the literature. If we think of it as the fact that the agent's evidence does not outright rule out the probability measure in question, the problem is that observations, evidence and data will often allow almost any probability measure. Admittedly, there will be clear-cut cases: if you see the outcome of a coin toss is heads, you reject the measure with $\mathsf{P}(H)=0$, and similarly for tails. Another class of cases might arise while randomly drawing objects from a finite set where the objective chances are known.^[Probability measures can be inconsistent with evidential constraints that agents believe to be true <!--Mathematically, non-trivial evidential constraints are easy to model -->[@bradley2012scientific],
<!-- These constraints can be about \emph{evidence of chances} $\{ \mathsf{P}(X) = x\}$ or $\mathsf{P}(X) \in [x,y]$, or they can be,
-->
 for example,  \emph{structural constraints} such as  "$X$ and $Y$ are independent". <!--These constraints are something that an agent can come to accept outright, but only if offered such information by an expert whom the agent completely defers.---> <!---Most of the examples in the literature start with the assumption that the agent is told by a believable source that the chances are such-and-such, or that certain structural constraints hold.--->  But, unless they come from an oracle, there will usually be some degree of uncertainty about the acceptability of these constraints.]
But clear-cut cases aside, what else?

<!---
Bradley suggests that "statistical evidence might inform [evidential] constraints (\dots and that evidence) of causes might inform structural constraints" (125-126). This, however, is not a clear account of how exactly this should proceed. One suggestion might be that once a statistical significance threshold is selected, a given set of observations with a selection of background modeling assumptions yields a credible interval. But this is to admit that to reach such constraints, we already have to start with a second-order approach, and drop information about the densities, focusing only on the intervals obtained with fixed margins of errors. But as we will be insisting, if you have the information about densities to start with, there is no clear advantage to going imprecise instead, and there are multiple problems associated with this move. Moreover, such moves require a choice of an error margin, which is extra-epistemic, and it is not clear what advantage there is to  use extra-epistemic considerations of this sort to drop information contained in densities.^[Relatedly, in forensic evidence evaluation even scholars who disagree about the value of going higher-order agree that interval reporting is problematic, as the choice of a limit or uncertainty level is rather arbitrary  [@Taroni2015Dismissal;@Sjerps2015Uncertainty].] 
--->

A second, related problem for imprecise probabilism is belief inertia [@Levi1980enterprise]. Precise probabilism offers an elegant model of learning from evidence: Bayesian updating. Imprecise probabilism,  at least \emph{prima facie}, offers an equally elegant model of learning from evidence, richer and more nuanced. <!--It is a natural extension of the classical Bayesian approach that uses precise probabilities-->. When faced with new evidence $E$ between time $t_0$ and $t_1$, the representor set should be updated point-wise, running the  standard Bayesian updating on each probability 
measure in the representor:
\begin{align*} \label{eq:updateRepresentor}
\mathbb{P}_{t_1} = \{\mathsf{P}_{t_1}\vert \exists\, {\mathsf{P}_{t_0} \!\in  \mathbb{P}_{t_0}}\,\, \forall\, {H}\,\, \left[\mathsf{P}_{t_1}(H)=\mathsf{P}_{t_0}(H \vert E)\right] \}.
\end{align*}

<!--The hope is that, if we start with a range of probabilities that is not extremely wide, point-wise learning will behave appropriately.-->
<!---^[The hope is also that \s{IP} offers a feasible aggregation method [@Elkin2018resolving;@Stewart2018pooling]: just put all representors together in one set, and voil\'a! However, this is a very conservative method which quickly leads to extremely few points of agreement, and we are not aware of any successful practical deployment of this method.]
--->
<!--For instance, if we start with a prior probability of \emph{heads} equal to .4 or .6, then those measures should be updated to something closer to $.5$ once we learn that a given coin has already been tossed ten times with the observed number of heads equal 5 (call this evidence $E$). This would mean that if the initial range of values was $[.4,.6]$ the posterior range of values should be narrower.
-->
<!---But even this seemingly straightforward piece of reasoning is hard to model within the confines of imprecise probabilism. For to calculate $\pr{\s{bias} = k \vert E}$ we need to calculate $\pr{E \vert \s{bias} = k }\pr{\s{bias} = k}$ and divide it by $\pr{E} = \pr{E \vert \s{bias} = k }\pr{\s{bias = k}} + \pr{E \vert  \s{bias} \neq k }\pr{ \s{bias} \neq k}$. The tricky part is obtaining  $\pr{\s{bias} = k}$ or  $\pr{ \s{bias} \neq k}$ in a principled manner without explicitly going second-order, without estimating the parameter value and without using beta distributions. Unfortunately, the narrowing of the range of values becomes impossible whenever the starting point is a complete lack of knowledge. 
--->

\noindent
Belief inertia arises in situations in which no amount of evidence can lead the agent to change their initial belief state, according to a given modeling strategy. Consider a situation in which you start tossing a coin knowing nothing about its bias, so the range of possibilities is $[0,1]$. After a few tosses, if you observed at least one tail and one head, you can exclude the measures assigning 0 or 1 to \emph{heads}. But else what can you learn?  <!---If you are to update the representor set point-wise, you will end up with the same representor set.--> For any sequence of outcomes that you can obtain and any probability value in $(0, 1),$ there will exist a probability measure (conditional on the outcomes) that assigns that probability to *heads*. Consequently, the edges of your resulting interval will remain the same. Some downplay the problem of belief inertia and blame the uniform prior [0, 1].^[Another strategy is to say that, in a state of ignorance, a special updating rule should be deployed. @Lee2017impreciseEpistemology suggests the rule of  \emph{credal set replacement} that recommends that upon receiving evidence the agent should drop measures rendered implausible, and add all non-extreme plausible probability measures. This, however, is tricky. One  needs a separate account of what makes a distribution plausible from a principled account of why one should use a separate special update rule when starting with complete ignorance.] Just as contingent propositions should not be assigned (precise) probabilities of 0 or 1 since these extreme values are unrevisable, by the same token the uniform prior [0, 1] should not be used for imprecise probabilities whenever it is impervious to revision [@moss2020Imprecise]. More generally, it is unsurprising you will not learn anything when you start from a place of ignorance [@Joyce2010defense]. But, as we will see in the next section, uniform priors are not necessarily unrevisable and can be a starting point for learning. The problem lies with imprecise probabilities, not with uniform priors as such.

<!--^[Here is another example of belief inertia by 
@Rinard2013against.  Suppose all the marbles in the urn are green ($H_1$), or exactly one-tenth of the marbles are green ($H_2$). Your initial credence about these hypotheses is completely uncertain, the interval $[0,1]$. Next,  you learn that a marble drawn at random from the urn is green ($E$). After conditioning on this evidence, you end up with the same spread of values for $H_1$ that you had before learning $E$. This result holds no matter how many green marbles are drawn. This is counterintuitive: if you keep drawing green marbles, the hypothesis that all marbles are green should become more probable.]
-->


<!--Joyce (2010) writes:
> You cannot learn anything in cases of pronounced ignorance simply because a prerequisite for learning is to have prior views about how potential data should alter your beliefs. (p. 291) [@Joyce2010defense] 
--->

<!--
\noindent
The upshot is that uniform priors should not be used and that imprecise probabilism gives the right results when the priors are non-vacuous. @moss2020Imprecise arrives at the same conclusion by drawing the following parallelism. If contingent propositions should not be assigned probabilities of 0 or 1 whenever these extreme values are unrevisable, then by the same token the uniform interval [0, 1] should not be used for imprecise probabilities whenever it is impervious to revision.
-->

<!---The challenge, however, is to explain in a principled manner which types of priors a rational agent is justified in assigning and why. What is the reason not to assign uniform priors except that they are unrevisable and thus cause belief inertia? While it is true that one cannot learn anything in a state of complete ignorance, the scenarios giving rise to belief inertia are not like that. The agent knows that the coin is two-sided, that the bias of the coin does not change from one toss to the next, etc. ---> 

The third problem for imprecise probabilism is to find suitable proper scoring rules. In the precise case, scoring rules measure the distance between a rational agent's probability measure and the actual value. The Brier score is the most common scoring rule for precise probabilities.^[The Brier score is defined as the squared distance between the true state and the probability forecast, or formally,  $(p(x)-V(x, w))^2$, where $p(x)$ is the probability forecast and $V(x, w)$ determines if a proposition obtains at $w$ ($V(x, w)=1$) or not ($V(x, w)=0$). If, for example, the proposition 'rain' obtains at $w$, the forecast 'rain with .9 probability' would be more accurate at $w$ than the forecast 'rain with .8 probability'. If, on the other hand, the proposition 'not rain' obtained at $w$, the latter forecast would be more accurate.] A requirement of scoring rules is \emph{propriety}: any rational agent will expect their probability measure to be more accurate than any other. A bit more formally, the expected inaccuracy of $p$ from the perspective of $p$ should always be smaller than the expected inaccuracy of $p$ from the perspective of another distribution $q$. After all, if an agent thought a different probability measure was more accurate, they should switch to it.  Well-known results demonstrate the strict propriety of the Brier score for precise 
probabilities.^[Besides propriety, other common requirements (which the Brier score also satisfies) are: the score $I(p, w)$ should be a function of the probability distribution $p$ and the true state $w$ (extensionality); and the score should be a continuous function around $p$ (continuity).] 
<!---
More specifically, let $I(p, w)$ be an inaccuracy score of a probability distribution $p$ relative to the true state $w$. The score $I(p, w)$ is strictly proper if, for any other probability distribution $q$ different from $p$, the following holds: 

$$\sum_{w \in W} p(w) I(p, w) < \sum_{w}  q(w)I(p, w).$$

\noindent 
That is, the expected inaccuracy of $p$ from the perspective of $p$ should always be smaller than the expected inaccuracy of $p$ from the perspective of another distribution $q$. To calculate the expected accuracy of $p$, first you need to calculate its inaccuracy $I(p, w)$ at every possible true state $w$, and then factor in the probabilities $p(w)$ of the true states according to $p$.
-->
Can similar results be established for imprecise probabilities? The answer is likely to be negative. <!--To start, a plausible scoring rule for imprecise probabilities is not easy to define. Suppose a forecaster assigns the [.8, .9] probability interval to the outcome that it would rain tomorrow, where the true state is 'rain'. Would the wider [.6, .99] interval be more accurate since its .99 upper bound is closer to the true state? If it were, the trivial interval [0, 1] would always be more accurate than any other interval since either of its edges are closer to whatever the true state turns out to be. So an inaccuracy score for imprecise probabilities should depend both on the closeness to the true state and the size of the interval. If, for example, the inaccuracy score were to increase as the size of the interval increases, this would block the result that the [0, 1] interval is always the least inaccurate. Still, even if a well-behaved score for imprecise probabilities can be found, a further problem arises in defining its expected inaccuracy.--> In the precise case, let $I(p, w)$ be an inaccuracy score of a probability distribution $p$ relative to the true state $w$. Its expected inaccuracy, from the perspective of $p$ equals
$$\sum_{w \in W}p(w)I(p, w).$$ 
\noindent
Now, let $I([p_-, p_+], w)$ be the inaccuracy score for the interval $[p_-, p_+]$. What is its expected inaccuracy from the perspective of, say, the interval $[p_-, p_+]$ itself? There is no straightforward answer to this question because $I([p_-, p_+], w)$ cannot be multiplied by $[p_-, p_+]$, in the way in which $I(p, w)$ can be multiplied by $p(w)$. The expected inaccuracy of $[p_-, p_+]$ can be evaluated from the perspective of the precise probabilities at its edges, either $p_-$ or $p_+$.^[So the expected inaccuracy would equal 
$\sum_{w\in W} p_-(w)I([p_-, p_+], w)$ or  $\sum_{w\in W} p_+(w)I([p_-, p_+], w).$] 
The problem is that, in this case, finding a proper inaccuracy score that is also continuous is mathematically impossible [@seidenfeld2012forecasting].

Proper scoring rules are often used to formulate accuracy-based arguments for precise probabilism. These arguments show (roughly) that, if your precise measure follows the axioms of probability theory, no other non-probabilistic measure is going to be more accurate than yours whatever the facts are.  So, without proper scoring rules for imprecise probabilities, the prospects for an accuracy-based argument for imprecise probabilism look dim  [@Mayo-Wilson2016scoring; @CampbellMoore2020accuracy].^[Moreover, as shown by  @Schoenfield2017accuracy, if an accuracy measure satisfies certain plausible formal constraints, it will never strictly recommend an imprecise stance, as for any imprecise stance there will be a precise one with at least the same accuracy.]


# Higher-order probabilism
\label{sec:higher-order}

Let us take stock. Imprecise probabilism is expressive enough to model the difference between a state in which there is no evidence about a proposition (or its negation) and a state in which the evidence for and against a proposition is in equipoise. However, imprecise probabilism has its own expressive limitations, for example, it cannot model the case of uneven bias. In addition, imprecise probabilism faces difficulties that do not affect precise probabilism: the notion of compatibility between a probability measure and the evidence is too permissive; belief inertia makes it impossible for a rational agent to learn via Bayesian updating; and no proper scoring rules exist for imprecise probabilism. In this section and the next, we show that higher-order probabilism overcomes the expressive limitations of imprecise probabilism without falling prey to any such difficulties.  

Proponents of imprecise probabilism already hinted at the need to rely on higher-order probabilities. For instance, Bradley compares the measures in a representor to committee members, each voting on a particular issue, say the true chance or bias of a coin. As they acquire more evidence, the committee members will often converge on a chance hypothesis.

> \dots the committee members are "bunching up". Whatever measure you put over the set of probability functions---whatever "second order probability" you use---the "mass" of this measure gets more and more concentrated around the true chance hypothesis. [@bradley2012scientific, p. 157]


\noindent
But such bunching up cannot be modeled by imprecise probabilism alone: a probability distribution over chance hypotheses is needed.^[In a similar vein, @joyce2005probabilities, in a paper defending imprecise probabilism,  explicates the notion of weight of evidence using a probability distribution over chance hypotheses. 
<!---something that imprecise probabilism was advertised to handle better than precise probabilism: weight of evidence. \todo{briefly add quote from Keynes to clarify weight of evidence} The explication uses a density over chance hypotheses and conceptualizes the  weight of evidence  as an increase of concentration of smaller subsets of chance hypotheses. ---> Oddly, representor sets play no central role in Joyce's account of the weight of evidence.] That one should use higher-order probabilities has also been suggested by critics of imprecise probabilism. @Carr2020impreciseEvidence argues that sometimes evidence requires uncertainty about what credences to have. Carr, however, does not articulate this suggestion more fully; does not develop it formally; and does not explain how her approach would fare against the difficulties affecting precise and imprecise probabilism. We now set out to do precisely that. 

<!--Such bunching up cannot be modeled by imprecise probabilism alone. A probability distribution over chance hypotheses is needed. Bradley seems to be aware of that, which would explain the use of scare quotes: when he talks about the option of using second-order probabilities in decision theory, he  insists that  'there is no justification for saying that there is more of your representor here or there.' ~[p.~195]--> 

<!--
\todo{Rafal to correct technical mistakes in this paragraph (e.g. concept of distribution over probability measures seems incorrect for our purposes)}
\todo{Revised this bit, take a look.}
-->

 The central idea of higher-order probabilism is this:  a rational agent's uncertainty cannot be mapped onto a one-dimensional scale such as the real line. Uncertainty is best modeled by the shape of a probability distribution. In some straightforward cases of narrow and symmetric distributions, we can get away with using point probabilities, but such approximations will fail to be useful in more complex cases. So, special cases aside, a rational agent's state of uncertainty (or credal stance) towards a proposition $x$ is not represented by a single probability value $p(x)$ between 0 and 1, but by a probability density $f(p(x))$, where the first-order probability of $x$ is the parameter in question and is treated as a random variable. Crucially, this representation is general. While the examples used so far may not indicate this, the first-order probability of $x$ is not restricted to chance hypotheses or the bias of a coin. The probability density $f(p(x))$ assigns a second-order probability (density) to all possible first-order probabilities $p(x)$.^[For computational ease, we will use a higher-order density that is discretized and ranges over 1000 first-order probabilities.]

<!---^[Bradley admits this much [@bradley2012scientific, 90], and so does  Konek  [@konek2013foundations, 59]. For instance, Konek disagrees with: (1)  $X$ is more probable than $Y$ just in case $p(X)>p(Y)$, (2)  $D$ positively supports $H$ if $p_D(H)> p(H)$, or (3)  $A$ is preferable to $B$ just in case the expected utility of $A$ w.r.t. $p$ is larger than that of $B$.]
---> 

<!---
The reader might be worried. The examples we discussed so far involve estimation of chances or population frequencies; but how are we to conceptualize higher order probabilities in a more general settings when we think of first-order probabilities as RAs degrees of belief?
--->

How should these second-order probabilities be understood? It is helpful to think of higher-order probabilism as a generalization of imprecise probabilism. Imprecisers already admit that some probability measures are compatible and others incompatible with the agent's evidence at some point. Compatibility is a coarse notion; it is an all-or-nothing affair. <!---For instance, suppose there is no evidence about the bias of a coin. Then, any first-order point probability about *heads* would be compatible with the evidence. If, instead, we know the coin is fair, the evidence clearly selects one preferred value, .5, and all other first-order probabilities would be incompatible with the evidence. But often, evidence is stronger than the former case and weaker than the latter case. --->
But, as seen earlier, evidence can hardly exclude a probability measure in a definitive manner except in clear-cut cases. Just as it is often a matter of degrees whether the evidence supports a proposition, the compatibility between evidence and a probability measure can itself be a matter of degrees. In this picture, the evidence justifies different values of first-order probability to various degrees. So, second-order probabilities express the extent to which the first-order probabilities are supported by the evidence. 


This higher-order approach at the technical level is by no means novel. Bayesian probabilistic programming languages embrace the well-known idea that parameters can be stacked and depend on each other [@Bingham2021PPwithoutTears]. But, while the technical machinery has been around for a while, it has not been deployed by philosophers to model a rational agent's uncertainty or credal state. Because of its greater expressive power, higher-order probabilism can represent uncertainty in a more fine-grained manner, as illustrated in @fig-evidenceResponse. In particular, the uneven coin scenario in which the two biases of the coin are not equally likely---which imprecise probabilism cannot model---can be easily modeled within high-order probabilism by assigning different probabilities to the two biases.
  

```{r evidenceResponse1,echo=FALSE,eval=TRUE,fig.align = "center", cache=TRUE, fig.show = "hold", out.width = "60%",   message = FALSE, warning = FALSE, results = FALSE}
p <- seq(from=0 , to=1 , by = 0.001)
FairDensity <- ifelse(p == .5, 1, 0)
FairDF <- data.frame(p,FairDensity)
FairPlot <- ggplot(FairDF, aes(x = p, y = FairDensity))+geom_line()+theme_tufte()+
  xlab("parameter value")+ylim(c(0,1))+ylab("probability")+ggtitle("Fair coin")+
  theme(plot.title.position = "plot")

UniformPlot <- ggplot()+xlim(c(0,1))+stat_function(fun = dunif, args = list(min = 0, max = 1))+
  ylim(0,1)+
  theme_tufte()+
  xlab("parameter value")+
  ylab("probability")+theme(plot.title.position = "plot")+ggtitle("Unknown bias")


p <- seq(from=0 , to=1 , by = 0.001)
TwoDensity <- ifelse(p == .4 | p == .6, .5, 0)
TwoDF <- data.frame(p,TwoDensity)
TwoPlot <- ggplot(TwoDF, aes(x = p, y = TwoDensity))+geom_line()+theme_tufte()+
  xlab("parameter value")+ylim(c(0,1))+ylab("probability")+ggtitle("Two biases (insufficient reason)")+
  theme(plot.title.position = "plot")+scale_x_continuous(breaks = c(.4,.6), labels = c(.4, .6))

p <- seq(from=0 , to=1 , by = 0.001)
TwoUnbalancedDensity <- ifelse(p == .4, .75, ifelse(p ==  .6, .25, 0))
TwoUnbalancedDF <- data.frame(p,TwoUnbalancedDensity)

TwoUnbalancedPlot <- ggplot(TwoUnbalancedDF, aes(x = p, y = TwoUnbalancedDensity))+geom_line()+theme_tufte()+
  xlab("parameter value")+ylim(c(0,1))+ylab("probability")+ggtitle("Two unbalanced biases")+
  theme(plot.title.position = "plot")+scale_x_continuous(breaks = c(.4,.6), labels = c(.4, .6))

```

<!---
```{r}
mean(TwoUnbalancedDensity)
mean(TwoDensity)
mean(TwoDensity)*mean(TwoDensity)
mean(TwoDensity*TwoDensity)
mean(TwoUnbalancedDensity*TwoDensity)
mean(TwoUnbalancedDensity)*mean(TwoDensity)

```
--->

```{r FigevidenceResponse2,echo=FALSE,eval=TRUE,fig.align = "center", cache=TRUE, fig.show = "hold", out.width = "65%"}
#| label: fig-evidenceResponse
#| fig-cap: "Examples of higher-order distributions for a few  scenarios problematic for both precise and imprecise probabilism."
#| fig-pos: t

grid.arrange(FairPlot,UniformPlot, TwoPlot, TwoUnbalancedPlot)
```

An agent's uncertainty could---perhaps, should---sometimes be represented by a single probability value. Higher-order probabilism does not prohibit that. For example, there may well be cases in which an agent's uncertainty is aptly represented by the expectation.^[The 
expectation is usually defined as $\int_{0}^{1} x f(x) \, dx$. In the context of our approach here, $x$ is the first-order probability of a given proposition and $f$ is the density representing the agent's uncertainty about $x$.]
<!---can be used as  the precise, object-level credence in the proposition itself, where $f$ is the probability density over possible object-level probability values.--->  But this need not always be the case. If the probability distribution is not sufficiently concentrated around a single value, a one-point summary will fail to do justice to the nuances of the agent's credal state.^[This approach lines up with common practice in Bayesian statistics, where the primary role of uncertainty representation is assigned to the whole distribution. Summaries such as the mean, mode standard deviation,  mean absolute deviation or highest posterior density intervals are only succinct ways of representing uncertainty.] For example, consider again the scenario in which the agent knows that the bias of the coin is either .4 or .6 but the former is three times more likely. Representing the agent's credal state with the expectation $.75 \times .4 + .25 \times .6 = .45$ would fail to capture the agent's different epistemic attitudes towards the two biases. The agent believes the two biases have different probabilities, and is also certain the bias is *not* .45. 


Besides its greater expressive power in modeling uncertainty, higher-order probabilism does not fall prey to belief inertia. Consider a situation in which you have no idea about the bias of a coin.  You start with a uniform distribution over $[0,1]$ as your prior. Observing any non-zero number of heads will exclude 0 and observing any non-zero number of tails will exclude 1 from the basis of the posterior. The posterior distribution will become more centered as the observations come in. This result is a straightforward application of Bayesian updating. Instead of plugging sharp probability values into the formula for Bayes's theorem, the factors to be multiplied in the theorem will be probability densities (or ratios of densities as needed). @fig-intertia2 illustrates---starting with a uniform prior distribution---how the posterior (beta) distribution changes after successive observations.^[Assuming independence and constant probability for all the observations, learning is modeled the Bayesian way. You start with some prior density $p$ over the parameter values. If you start with a complete lack of information, $p$ should be uniform. Then, you observe the data $D$ which is the number of successes $s$ in a certain number of observations $n$. For each particular possible value $\theta$ of the parameter, the probability of $D$ conditional on $\theta$ follows the binomial distribution. The probability of $D$ is obtained by integration. That is:
\begin{align*}
p(\theta \vert D) & = \frac{p(D\vert \theta)p(\theta)}{p(D)}\\
& = \frac{\theta^s (1-\theta)^{(n - s)}p(\theta)}{\int (\theta')^s (1-\theta')^{(n - s)}p(\theta')\,\, d\theta'}.
\end{align*}
] 

```{r Figinertia2, eval = TRUE, echo=FALSE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "70%", message= FALSE, warning=FALSE}

n <- 1000 #parameters 
s <- 1e5  #sample size

ps <- seq(from=0 , to=1 , length.out=n)

prior <- rep(1/n , n) #uniform prior

likelihood1g <- dbinom( 1 , size=1 , prob=ps)
likelihood2g <- dbinom( 2 , size=2 , prob=ps)
likelihood2g1b <- dbinom( 2 , size=3 , prob=ps)

posterior1g <- likelihood1g * prior
posterior2g <- likelihood2g * prior
posterior2g1b <- likelihood2g1b * prior


posterior1g <- posterior1g / sum(posterior1g)
posterior2g <- posterior2g / sum(posterior2g)
posterior2g1b <- posterior2g1b / sum(posterior2g1b)

upperLimit <- .003

InertiaPriorPlot <- ggplot()+geom_line(aes(x = ps, y = prior))+theme_tufte(base_size = 7)+xlab("parameter")+
  ylab("probability")+theme(plot.title.position = "plot")+ggtitle("Prior")+
  scale_x_continuous()+scale_y_continuous(limits = c(0,upperLimit))


InertiaOneGPlot <- ggplot()+geom_line(aes(x = ps, y = posterior1g))+theme_tufte(base_size = 7)+xlab("parameter")+
  ylab("probability")+theme(plot.title.position = "plot",
                            axis.text.y = element_blank(),
                            axis.title.y = element_blank(),
                          axis.ticks.y =element_blank()
)+ggtitle("Evidence: h")+
  scale_y_continuous(limits = c(0,upperLimit))



InertiaTwoGPlot <- ggplot()+geom_line(aes(x = ps, y = posterior2g))+theme_tufte(base_size = 7)+xlab("p")+
  ylab("probability")+theme(plot.title.position = "plot"#, 
#                        axis.text.y = element_blank(),
#                            axis.title.y = element_blank(),
#                            axis.ticks.y = element_blank()
                  )+ggtitle("Evidence: h, h")+
scale_y_continuous(limits = c(0,upperLimit))

InertiaTwoGoneBluePlot <- ggplot()+geom_line(aes(x = ps, y = posterior2g1b))+theme_tufte(base_size = 7)+xlab("parameter")+
  ylab("probability")+theme(plot.title.position = "plot",     axis.text.y = element_blank(),
  axis.title.y = element_blank(),
      axis.ticks.y = element_blank()
           )+ggtitle("Evidence: h, h, t")+
  scale_y_continuous(limits = c(0,upperLimit))
```



```{r Figinertia3, echo=FALSE, eval=TRUE,fig.align = "center", cache=TRUE, fig.show = "hold", out.width = "75%",   message = FALSE, warning = FALSE, results = FALSE}
#| label: fig-intertia2
#| fig-cap: "As observations of heads, heads and tails come in, extreme parameter values drop out of the picture and the posterior is shaped by the evidence."


grid.arrange(InertiaPriorPlot, InertiaOneGPlot,  InertiaTwoGPlot, InertiaTwoGoneBluePlot, ncol = 2, nrow = 2)
```


The impossibility of defining proper scoring rules was another weakness of imprecise probabilism.  This is a significant shortcoming, especially because proper scores do exist for precise probabilism. Fortunately, one can show that there exist proper scoring rules for higher-order probabilism. <!--These rules can then be used to formulate accuracy-based arguments. In addition, recall the point made by  @Schoenfield2017accuracy:  an accuracy measure  will not usually recommend an imprecise stance. This argument 
fails against imprecise probabilism: there are cases in which accuracy considerations recommend an imprecise stance (that is, a multi-modal distribution) over a precise one. --> We defend this claim in the next section. The argument, however, will be more formal and can be skipped upon first reading.
