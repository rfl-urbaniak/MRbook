---
title: "Comments: Higher-Order Legal Probabilism paper"
author: ""
date: "July 2023"
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: [../../references/referencesMRbook.bib]
csl: [../../references/apa-6th-edition.csl]
indent: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose of this document


- Summarize comments received at Lund conference in June 2023 on the presentatin about Higher Order Legal Probabilism. 
To make sense of these comments, it is best to check out the slides of the presentation. 

- Outline the structure of the paper on higher-order legal probabilism. 


# Comments received in Lund

## Alex Biederman

- There is a difference between a RMP based on a small database and a RMP based on a larger database.
But that difference is best modeled by the **resilience** or **robustness** of the RMP: a RMP based on a larger database will be more resilient/robust in light of future data compared to a RMP based on a smaller database. We do not need to go higher-order. Everything can be done using the notion of resilience/robustness. (Note that this same point was made by Brian Skyrms in his well-known paper on resilience). **Challenge**: what theoretical or practical advantages does the higher-order approach have above and beyond the notion of resilience or robustness?

- It is odd to place a probability on top of a probability. There cannot be uncertainty about uncertainty. More specifically, in the DNA database example, there is uncertainty about the **proportion**, but not about the RMP probability itself. We might have a more or less precise estimate of the proportion (which is a **parameter**), but it makes no sense to talk about uncertainty about a probability.

- In the slides (see presentation slides), $\theta$ is not a probability. It is a proportion. It is okay to place a distribution over a proportion. It makes no sense to place a distribution over a probability. Hence, there is a mistake in the slides because $\theta$ must be a proportion, not a probability.

## Marjan Sjerps

- She agrees with the general line of argument, at least in practice.

- However, she thinks that the **mathematics** is not correct (for reasons similar to Alex Biederman). A probability cannot be placed on top of a probability. This is mathematically incorrect. One needs to show that this higher-order probability is a probability (it satisfies the axioms, it behaves as intended, etc.).

- Hierarchical Bayesian models use hyper-parameters and distributions over them, so they use distributions over parameters and distributions over hyper parameters, but they never use distributions over probabilities. So the higher order approach isn't really Bayesian. It is okay to place distributions over parameters (or hyper parameters), but not over probabilities themselves. 

- *Challenge*: How can we ensure that the mathematics of higher-order probabilities behaves as intended?

- It is unclear whether $\theta$ is a parameter or a probability in the slides. It should be a parameter, not a probability. 

## Franco Taroni and Silvia Bozza

- It is correct to say that the fact-finders should not make decisions on just the RMP alone. 
RMP must be accompanied by information about the **model**, the data collection and any other 
information that may be useful. 

- Using higher-order probabilities to capture this additional information is not necessarily the best formal tool, especially given the technical and mathematical problems with higher-order probabilities.

## Christian Dahlman

- He agrees with the general line of argument, but has problem with the use of **weight** in one of the slides.

- One the slides said that, if a RMP is based on little data, then when this item is combined with other evidence this aspect should be taken into account, for example, by seeking more evidence or by giving more weight to other evidence. (see slides 26 "If $\theta$ is small but its distribution spread out, this is a good reason for the triers of fact to seek more information or give more weight to other incriminating evidence"). This is not correct. 

## Colin Aitken

- If we think of $\theta$ as a parameter, we are simply talking about uncertainty about a parameter and standard statistical tools apply: confidence intervals, etc. 

- But the issue is not about the uncertainty of $\theta$ as a parameter, the issue is whether we should place a probability over a probability, and that is not possible. 



# Structure of paper

As I see it, the argument of the paper should be structured around three key points:

- **First**: The debate among forensic scientists and proposals on the table

- **Second**: The higher order approach as a novel proposal, how it works, etc. 

- **Third**: Why the higher order approach is better than existing approaches

This is standard structure for any scholarly paper. Below each 
point is developed more precisely. 

## Charitable reconstruction of the debate in the literature

- The paper should begin with a **charitable reconstruction** of the debate in the forensic 
science literature. (See, e.g., the 2016 special issue of *Science and Justice*, "Special issue on measuring and reporting the precision of forensic likelihood ratios", edited by G.S. Morrison. All the papers are in a special folder called 2016ScienceJustruce-SpecIssue-LR etc.)  The Lund slides try to do that, but -- in retrospect -- they do not do succeed. 

- Here is a charitable reconstruction: 

- (i) Everybody agrees that LRs (as averages, expectations) do not convey all the underlying uncertainty. There is uncertainty originating with the sample size (call it, *data uncertainty*) and there is uncertainty about the model assumptions (call it, *model uncertainty*). So clearly, LRs do not capture data uncertainty or model uncertainty. Call these additional forms of uncertainty *meta-uncertainty*. (An interesting philosophy paper that might be good to reference on meta-uncertainty is "Meta-uncertainty and the proof paradoxes" by
Katie Steele and Mark Colyvan, Philosophical Studies, in particular sections 4 and 5.)

- (ii) Everybody agrees that meta-uncertainty should be communicated by the experts to the fact-finders besides the first-order uncertainty captured by LRs. 

- (iii) The question is, how should this meta-uncertainty be conveyed? What is the best way to convey it? This is where the disagreement starts. 

- (iv) Many proposals are on the table:  describe data collection and model assumptions (Taroni/Bozza); place confidence intervals over LRs; model possible differences in terms of robustness (Biederman); etc.

- (v) The interval approach is criticized because LRs are not parameters to be estimated.

## The higher order approach

The next section of the paper should outline the higher approach as a novel answer to the question of how to model (at least part of) the meta-uncertainty that cannot be modeled by LRs alone. Some key points:

- The **mathematics** should be clear. Are higher order probabilities well-behaved probabilities? Is the mathematics all working properly? How do we show that? (The paper by Philip Dawid "Forensic likelihood ratio: Statistical problems and pitfalls" in the 2016 special issue might be a good starting point)

- How much of the meta-uncertainty is **captured** by distribution over probabilities and how much of the meta-uncertainty is still **left out** of the formal model?

- Parallels with the debate between precise/imprecise probabilism in philosophy. (Not sure if this is really necessary though.)

-  Good to show that everything that can be done with LRs (e.g. combine them) can be done using higher order LRs and higher order Bayesian networks. So the higher order approach combines reasoning about first-order uncertainty with reaosning about meta-uncertainty. 


## Comparing higher order approach to existing approaches 

In what way is the higher order approach different and better than other existing approaches for conveying meta-uncertainty to fact-finders?

- One key argument in favor of the higher-order approach concerns how different pieces of evidence (with different meta-uncertainties) can be combined. No one of the existing approaches has anything to say about how to combine different pieces and keep track of the meta-uncertainty. For example, it is not clear how the resilience approach or the interval approach could do that correctly (explain why).

- Respond to objections against the higher order approach, e.g. higher order probabilities do not exist or make no sense mathematically. 









