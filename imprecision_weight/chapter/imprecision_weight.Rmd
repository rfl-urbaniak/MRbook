---
title: "“Weight of Evidence, Evidential Completeness and Accuracy”"
author: "Rafal Urbaniak and Marcello Di Bello"
output:
  pdf_document:
    number_sections: yes
    df_print: kable
    keep_tex: yes
    toc: no
    includes:
      in_header: Rafal_latex7.sty
  html_document:
    toc: yes
    df_print: paged
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: ../../references/referencesMRbook.bib
csl: [../../references/apa-6th-edition.csl]
indent: yes
---



  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dagitty)
library(rethinking)
```




# Motivations

## Balance vs. weight

Suppose we want to represent our uncertainty about a proposition in terms of a single probability that we assign to it. It is not too difficult to inspire the intuition that this representation does not capture an important dimension of how our uncertainty connects with the evidence we have or have not obtained.  In a 1872 manuscript of \emph{The Fixation of Belief} (W3 295) C. S. Peirce gives an example meant to do exactly that.  

\begin{quote} When we have drawn a thousand times, if about half have been white, we have great confidence in this result. We now feel pretty sure that, if we were to make a large number of bets upon the color of single beans drawn from the bag, we could approximately insure ourselves in the long run by betting each time upon the white, a confidence which would be entirely wanting if, instead of sampling the bag by 1000 drawings, we had done so by only two.
\end{quote}

\noindent The objection is not too complicated. Your best estimate of the probability of $W=$`the next bean will be white' is .5 if half of the beans you have drawn randomly so far have been white, no matter whether you have drawn a thousand or only two of them. But this means that expressing your uncertainty about $W$ by locutions such as "my confidence in $W$ is .5' does not capture this intuitively important distinction. 

Similar remarks can be found in Peirce's 1878 \emph{Probability of Induction}. There, he also proposes to represent uncertainty by at least two numbers, the first depending on the inferred probability, and the second measuring the amount of knowledge obtained; as the latter, Peirce proposed to use some dispersion-related measure of error (but then suggested that an error of that estimate should also be estimated and so, so that ideally more numbers representing errors would be needed). 

Peirce himself did not call this the weight of evidence (and in fact, used the phrase rather to refer to the balance of evidence, W3 294) [CITE KASSER 2015]. However, his criticism of such an oversimplified representation of uncertainty anticipated what came to be called weight of evidence by Keynes in his 1921 \emph{A Treatise on Probability}:

\begin{quote}
As the relevant evidence at our disposal increases, the magnitude of the
probability of the argument may either increase or decrease, according as the new knowledge strengthens the unfavourable or the favourable evidence; but something seems to have increased in either case,—we have a more substantial basis upon which to rest our conclusion. I express this by saying that an accession of new evidence increases the weight of an argument. New evidence will sometimes decrease the probability of an argument but it will always increase its `weight.' (p. 71)
\end{quote}

\noindent The key point is the same [CITE LEVI 2001]: the balance of probability alone cannot characterize all important aspects of evidential appraisal. Keynes also considered measuring weight of evidence in terms of the variance of the posterior distribution of a certain parameter, but was quite attached to the idea that weight should increase with new information, even if the dispersion increase with new evidence [TP 80-82], and so he proposed only a very rough sketch of  a positive sketch. Moreover, as he was uncertain how a measure of weight should be incorporated in further decision-making, the was skeptical about the practical significance of the notion.  [TP 83]

But what is this positive sketch? On one hand, Keynes [TP 58-59] connects the notion of weight with relevance. Call evidence $E$ relevant to $X$ given $K$ just in case $\mathsf{Pr}(X\vert K \wedge E) \neq \mathsf{Pr}(X \vert K)$.^[As discussed in [CITE RUNDE 1990], Keynes also uses a slightly more convoluted notion of relevance to avoid equally strong items of opposite evidence turning out to be irrelevant; here we do not need to be concerned with this complication.] One postulate than can be found in the \emph{Treatise} [TP 84] is:^[RUNDE 1990 283 suggests Keynes allows for weight of evidence to decrease when new evidence increases the range of alternatives, but this is based on Keynes' claim that weight is increased when the number of alternatives is reduced, and Keynes does not directly say anything about the possibility of an increase of the number of alternatives.]

\begin{tabular}{lp{11cm}}
(R-monotonicity) & If $E$ is relevant to $X$ given $K$, where $K$ is background knowledge, $V(X\vert K \wedge E) > V(X\vert K)$, where $V$ is the weight of evidence.
\end{tabular}




[RUNDE 1990, 280] suggests that Keynes at some point calls weight the completeness of information. This however, is a bit hasty, as Keynes only says that \emph{the degree of completeness of the information on which a probability is based does seem to be relevant, as well as the actual magnitude of the probabiltiy, in making practical decisions}. As later on we will argue that it is actually useful to distinguish evidential weight (how much evidence do we have?) and evidential completeness (do we have all the evidence that we would expect in a given case?), we rather prefer to extract a more modest postulate:

\begin{tabular}{lp{11cm}}
(Completeness) & If $E_1$ and $E_2$ are relevant items of evidence, and $E_2$ is (in a sense to be discussed) more complete than $E_1$,  $V(X\vert K \wedge E_2) > V(X\vert K \wedge E_1)$.
\end{tabular}

\noindent If we conceptualize $E_2$ being complete and $E_1$ being incomplete as $E_2$ being a maximal relevant conjunction of relevant claims one of which is $E_1$, (Completeness) follows from (R-monotonicity).

Now, some requirements on how weight of evidence is related to the balance of probability. For one thing, Keynes insists that new (relevant) evidence might decrease probability but will always increase weight [TP 77]. Since (R-monotonicity) already captures the idea that weight will always increase, here we extract the other part of the claim:

\begin{tabular}{lp{11cm}}
(Possible decrease) & It is possible that   $V(X\vert K \wedge E) > V(X\vert K)$ while $\pr{X \vert K \wedge E} <  \pr (X\vert K)$.
\end{tabular}

Clearly, Keynes also endorsed the following two requirements of a very similar form:

\begin{tabular}{lp{11cm}}
(Possible increase) & It is possible that   $V(X\vert K \wedge E) > V(X\vert K)$ while $\pr{X \vert K \wedge E} >  \pr (X\vert K)$. \\
(Possibly no change ) & It is possible that   $V(X\vert K \wedge E) > V(X\vert K)$ while $\pr{X \vert K \wedge E} =  \pr (X\vert K)$.
\end{tabular}














Keynes
is not referring to the sheer number of statements on the right hand side of a conditional probability P (H | E) or the sheer bulk of information that these statements
contain. By “relevant evidence”, Keynes is only referring to the extent that E pro-
vides information that is pertinent to H in particular. [Pedden3]



## Examples and informal desiderata  

- Go over Nance in particular, Cohen, some other sources?


### Monotonicity of weight

Runde, Joyce, Weatherson, Peden

## Hamer's weight of evidence


## Good's weigh of evidence and the information value

- present Good, discuss 

<!-- https://docs.tibco.com/data-science/GUID-44739B00-E85F-4CE7-8404-24F9B775ADE8.html -->

- compare to pointwise mutual information

- evaluate in light of the desiderata

<!-- https://stats.stackexchange.com/questions/16945/why-do-people-use-the-term-weight-of-evidence-and-how-does-it-differ-from-poi -->

<!-- w(e:h)=logp(e|h)p(e|h¯¯¯) -->
<!-- where e is evidence, h is hypothesis. -->

<!-- Now, I want to know what is the main difference with PMI (pointwise mutual information) -->

<!-- pmi(e,h)=logp(e,h)p(e)∗p(h) -->

<!-- h is something different in PMI and in WOE -->
<!-- Notice the term p(h) in PMI. This implies that h is a random variable of which you can compute the probability. For a Bayesian, that's no problem, but if you do not believe that hypotheses can have a probability a priori you cannot even write PMI for hypothesis and evidence. In WOE, h is a parameter of the distribution and the expressions are always defined. -->

<!-- PMI is symmetric, WOE is not -->
<!-- Trivially, pmi(e,h)=pmi(h,e). However, w(h:e)=logp(h|e)/p(h|e¯) need not be defined because of the term e¯. Even when it is, it is in general not equal to w(e:h). -->

<!-- Other than that, WOE and PMI have similarities. -->

<!-- The weight of evidence says how much the evidence speaks in favor of a hypothesis. If it is 0, it means that it neither speaks for nor against. The higher it is, the more it validates hypothesis h, and the lower it is, the more it validates h¯. -->

<!-- Mutual information quantifies how the occurrence of an event (e or h) says something about the occurrence of the other event. If it is 0, the events are independent and the occurrence of one says nothing about the other. The higher it is the more often they co-occur, and the lower it is the more they are mutually exclusive. -->

<!-- What about the cases where the hypothesis h is also a random variable and both options are valid? For example in communiction over a binary noisy channel, the hypothesis is h the emitted signal to decode and the evidence is the received signal. Say that the probability of flipping is 1/1000, so if you receive a 1, the WOE for 1 is log0.999/0.001=6.90. The PMI, on the other hand, depends on the proability of emitting a 1. You can verify that when the probability of emitting a 1 tends to 0, the PMI tends to 6.90, while it tends to 0 when the probability of emitting a 1 tends to 1. -->

<!-- This paradoxical behavior illustrates two things: -->

<!-- None of them is suitable to make a guess about the emission. If the probability of emitting a 1 drops below 1/1000, the most likely emission is 0 even when receiving a 1. However, for small probabilities of emitting a 1 both WOE and PMI are close to 6.90. -->

<!-- PMI is a gain of (Shannon's) information over the realization of the hypothesis, if the hypothesis is almost sure, then no information is gained. WOE is an update of our prior odds, which does not depend on the value of those odds. -->

## Skyrms and resilience?


## Imprecision and  weight with intervals

Keynes' later works and Peden's paper

### Sharpening by richness

### Sharpening by specificity

### Sharpening by precision


## Imprecision: a second-order approach



## Information-theoretic weight of evidence

## Completeness tends to improve weight

## Weight tends to improve accuracy









# Literature to discuss


Kasser, 2016, Two Conceptions of Weight of Evidence in Peirce’s Illustrations of the Logic of Science [DOWNLOADED]

Feduzi, 2010, On Keynes’s conception of the weight of evidence [READ]

Cohen 1986, Twelve Questions about Keynes's Concept of Weight [READ]

Pedden, William 2018, Imprecise probability and the measurement of Keynes' weight of arguments

Levi 2011, the weight of argument [DOWNLOADED]

Skyrms 1977 resiliency, propensities [DOWNLOADED]

Synthese 186 (2) 2012, volume on Keynesian weight [CHECKED, NOT MUCH ON WEIGHT ACTUALLY, NO NEED TO READ]


Good, weight of evidence, survey

Good, PROBABILITY AND THE
WEIGHING OF EVIDENCE


David Hamer, Probability, anti-resilience, and the weight of expectation [READ]


William Peden, Imprecise Probability and the Measurement of Keynes's "Weight of Arguments"


Runde, Keynesian Uncertainty and the weight of arguments [DOWNLOADED]

Weatherson, 2002, Keynes, uncertainty and interest rates  [DOWNLOADED]

Jeffrey M. Keisler, Value of information analysis: the state of application

Edward C. F. Wilson, A Practical Guide to Value of Information Analysis

Joyce JM (2005) How probabilities reflect evidence.


