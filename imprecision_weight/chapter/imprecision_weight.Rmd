---
title: "“Weight of Evidence, Evidential Completeness and Accuracy”"
author: "Rafal Urbaniak and Marcello Di Bello"
output:
  pdf_document:
  number_sections: true
df_print: kable 
keep_tex: true
includes:
  in_header:
  - style.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 11pt
documentclass: scrartcl
urlcolor: blue
bibliography: referencesMRbook.bib
csl: apa-6th-edition.csl
indent: true
---



  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dagitty)
library(rethinking)
```




# Philosophical motivations

## Keynes' conception of weight of evidence

Early example from CS Peirce (cited in Peden) and Keynes' similar example

Keynes introduces weight as follows. ‘As the relevant evidence at our disposal increases, the magnitude of probability may
either decrease or increase, according as the new knowledge strengthens the unfavourable or favourable evidence; but
something seems to have increased in either case—we have a more substantial basis on which to rest our conclusion . . . New
evidence will sometimes decrease the probability of [the hypothesis] but will always increase its ‘weight’ (Keynes, 1921: 77).


## Informal desiderata  and Keynsian weight

- Go over Nance in particular, Cohen, some other sources?


### Monotonicity of weight

Runde, Joyce, Weatherson, Peden

## Hamer's weight of evidence


## Good's weigh of evidence and the information value

- present Good, discuss 

<!-- https://docs.tibco.com/data-science/GUID-44739B00-E85F-4CE7-8404-24F9B775ADE8.html -->

- compare to pointwise mutual information

- evaluate in light of the desiderata

<!-- https://stats.stackexchange.com/questions/16945/why-do-people-use-the-term-weight-of-evidence-and-how-does-it-differ-from-poi -->

<!-- w(e:h)=logp(e|h)p(e|h¯¯¯) -->
<!-- where e is evidence, h is hypothesis. -->

<!-- Now, I want to know what is the main difference with PMI (pointwise mutual information) -->

<!-- pmi(e,h)=logp(e,h)p(e)∗p(h) -->

<!-- h is something different in PMI and in WOE -->
<!-- Notice the term p(h) in PMI. This implies that h is a random variable of which you can compute the probability. For a Bayesian, that's no problem, but if you do not believe that hypotheses can have a probability a priori you cannot even write PMI for hypothesis and evidence. In WOE, h is a parameter of the distribution and the expressions are always defined. -->

<!-- PMI is symmetric, WOE is not -->
<!-- Trivially, pmi(e,h)=pmi(h,e). However, w(h:e)=logp(h|e)/p(h|e¯) need not be defined because of the term e¯. Even when it is, it is in general not equal to w(e:h). -->

<!-- Other than that, WOE and PMI have similarities. -->

<!-- The weight of evidence says how much the evidence speaks in favor of a hypothesis. If it is 0, it means that it neither speaks for nor against. The higher it is, the more it validates hypothesis h, and the lower it is, the more it validates h¯. -->

<!-- Mutual information quantifies how the occurrence of an event (e or h) says something about the occurrence of the other event. If it is 0, the events are independent and the occurrence of one says nothing about the other. The higher it is the more often they co-occur, and the lower it is the more they are mutually exclusive. -->

<!-- What about the cases where the hypothesis h is also a random variable and both options are valid? For example in communiction over a binary noisy channel, the hypothesis is h the emitted signal to decode and the evidence is the received signal. Say that the probability of flipping is 1/1000, so if you receive a 1, the WOE for 1 is log0.999/0.001=6.90. The PMI, on the other hand, depends on the proability of emitting a 1. You can verify that when the probability of emitting a 1 tends to 0, the PMI tends to 6.90, while it tends to 0 when the probability of emitting a 1 tends to 1. -->

<!-- This paradoxical behavior illustrates two things: -->

<!-- None of them is suitable to make a guess about the emission. If the probability of emitting a 1 drops below 1/1000, the most likely emission is 0 even when receiving a 1. However, for small probabilities of emitting a 1 both WOE and PMI are close to 6.90. -->

<!-- PMI is a gain of (Shannon's) information over the realization of the hypothesis, if the hypothesis is almost sure, then no information is gained. WOE is an update of our prior odds, which does not depend on the value of those odds. -->


## Imprecision and  weight with intervals

Keynes' later works and Peden's paper

### Sharpening by richness

### Sharpening by specificity

### Sharpening by precision


## Imprecision: a second-order approach



## Information-theoretic weight of evidence

## Completeness tends to improve weight

## Weight tends to improve accuracy









# Literature to discuss


Feduzi, 2010, On Keynes’s conception of the weight of evidence

Cohen 1986, Twelve Questions about Keynes's Concept of Weight

Peden, WIlliam 2018, Imprecise probablity and the measurement of Keynes' weight of arguments

Synthese 186 (2) 2012, volume on Keynesian weight [TO DOWNLOAD]


Good, weight of evidence, survey

Good, PROBABILITY AND THE
WEIGHING OF EVIDENCE


David Hamer, Probability, anti-resilience, and the weight of expectation


William Peden, Imprecise Probability and the Measurement of Keynes's "Weight of Arguments"


Runde, Keynesian Uncertainty and the weight of arguments [TO DOWNLOAD]

Weatherson, 2002, Keynes, uncertainty and interest rates  [TO DOWNLOAD}]

Jeffrey M. Keisler, Value of information analysis: the state of application

Edward C. F. Wilson, A Practical Guide to Value of Information Analysis

Joyce JM (2005) How probabilities reflect evidence.


