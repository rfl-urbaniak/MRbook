---
title: "“Weight of Evidence, Evidential Completeness and Accuracy”"
author: "Rafal Urbaniak and Marcello Di Bello"
output:
  pdf_document:
    number_sections: yes
    df_print: kable
    keep_tex: yes
    toc: no
    includes:
      in_header: Rafal_latex7.sty
  html_document:
    toc: yes
    df_print: paged
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 10pt
documentclass: scrartcl
urlcolor: blue
bibliography: ../../references/referencesMRbook.bib
csl: [../../references/apa-6th-edition.csl]
indent: yes
---



  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dagitty)
library(rethinking)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(tidyr)

```




# Balance vs. weight

Suppose we want to represent our uncertainty about a proposition in terms of a single probability that we assign to it. It is not too difficult to inspire the intuition that this representation does not capture an important dimension of how our uncertainty connects with the evidence we have or have not obtained.  In a 1872 manuscript of \emph{The Fixation of Belief} (W3 295) C. S. Peirce gives an example meant to do exactly that.  

\begin{quote} When we have drawn a thousand times, if about half have been white, we have great confidence in this result. We now feel pretty sure that, if we were to make a large number of bets upon the color of single beans drawn from the bag, we could approximately insure ourselves in the long run by betting each time upon the white, a confidence which would be entirely wanting if, instead of sampling the bag by 1000 drawings, we had done so by only two.
\end{quote}

\noindent The objection is not too complicated. Your best estimate of the probability of $W=$`the next bean will be white' is .5 if half of the beans you have drawn randomly so far have been white, no matter whether you have drawn a thousand or only two of them. But this means that expressing your uncertainty about $W$ by locutions such as "my confidence in $W$ is .5' does not capture this intuitively important distinction. 

Similar remarks can be found in Peirce's 1878 \emph{Probability of Induction}. There, he also proposes to represent uncertainty by at least two numbers, the first depending on the inferred probability, and the second measuring the amount of knowledge obtained; as the latter, Peirce proposed to use some dispersion-related measure of error (but then suggested that an error of that estimate should also be estimated and so, so that ideally more numbers representing errors would be needed). 

Peirce himself did not call this the weight of evidence (and in fact, used the phrase rather to refer to the balance of evidence, W3 294) [CITE KASSER 2015]. However, his criticism of such an oversimplified representation of uncertainty anticipated what came to be called weight of evidence by Keynes in his 1921 \emph{A Treatise on Probability}:

\begin{quote}
As the relevant evidence at our disposal increases, the magnitude of the
probability of the argument may either increase or decrease, according as the new knowledge strengthens the unfavourable or the favourable evidence; but something seems to have increased in either case,—we have a more substantial basis upon which to rest our conclusion. I express this by saying that an accession of new evidence increases the weight of an argument. New evidence will sometimes decrease the probability of an argument but it will always increase its `weight.' (p. 71)
\end{quote}

\noindent The key point is the same [CITE LEVI 2001]: the balance of probability alone cannot characterize all important aspects of evidential appraisal. Keynes also considered measuring weight of evidence in terms of the variance of the posterior distribution of a certain parameter, but was quite attached to the idea that weight should increase with new information, even if the dispersion increase with new evidence [TP 80-82], and so he proposed only a very rough sketch of  a positive sketch. Moreover, as he was uncertain how a measure of weight should be incorporated in further decision-making, the was skeptical about the practical significance of the notion.  [TP 83]

But what is this positive sketch? On one hand, Keynes [TP 58-59] connects the notion of weight with relevance. Call evidence $E$ relevant to $X$ given $K$ just in case $\mathsf{Pr}(X\vert K \wedge E) \neq \mathsf{Pr}(X \vert K)$.^[ Keynes also uses a slightly more convoluted notion of relevance to avoid equally strong items of opposite evidence turning out to be irrelevant (this objection has also been brought up by [COHEN 1986 TWELVE]). The more complex version is that  a proposition $E_1$ is relevant to $X$ given $K$   just in case it entails a proposition $E_2$ such that $\pr{X\vert K \wedge E_2} \neq \mathsf{Pr}(X \vert K)$. [COHEN 1986 TWELVE] complaints that this still runs into difficulties. Ignore $K$, take an irrelevant proposition $Z$. It entails $Z\vee X$ and $\pr{Z \vee X\vert X \wedge E}=1$. Now, by Bayes' theorem we have $\pr{X \vert E \wedge (Z \vee X)} = \frac{\pr{X \vert E}\times \pr{Z \vee X \vert X \wedge E}}{\pr{Z \vee X \vert E}} = \frac{\pr{X \vert E}}{\pr{Z \vee X \vert E}}$. If the denominator differs from 1, the result differs from the numerator. We will ignore such difficulties, as they are not of key importance for the development of this chapter.] One postulate than can be found in the \emph{Treatise} [TP 84] is:^[RUNDE 1990 283 suggests Keynes allows for weight of evidence to decrease when new evidence increases the range of alternatives, but this is based on Keynes' claim that weight is increased when the number of alternatives is reduced, and Keynes does not directly say anything about the possibility of an increase of the number of alternatives.]

\begin{tabular}{lp{11cm}}
(Monotonicity) & If $E$ is relevant to $X$ given $K$, where $K$ is background knowledge, $V(X\vert K \wedge E) > V(X\vert K)$, where $V$ is the weight of evidence.
\end{tabular}




[RUNDE 1990, 280] suggests that Keynes at some point calls weight the completeness of information. This however, is a bit hasty, as Keynes only says that \emph{the degree of completeness of the information on which a probability is based does seem to be relevant, as well as the actual magnitude of the probabiltiy, in making practical decisions}. As later on we will argue that it is actually useful to distinguish evidential weight (how much evidence do we have?) and evidential completeness (do we have all the evidence that we would expect in a given case?), we rather prefer to extract a more modest postulate:

\begin{tabular}{lp{11cm}}
(Completeness) & If $E_1$ and $E_2$ are relevant items of evidence, and $E_2$ is (in a sense to be discussed) more complete than $E_1$,  $V(X\vert K \wedge E_2) > V(X\vert K \wedge E_1)$.
\end{tabular}

\noindent If we conceptualize $E_2$ being complete and $E_1$ being incomplete as $E_2$ being a maximal relevant conjunction of relevant claims one of which is $E_1$, (Completeness) follows from (Monotonicity).


Similar requirements seem to be inspired by the urn example. We put them in two forms, a weaker and a stronger one. 

  
  \begin{tabular}{lp{11cm}}
  (Weak increase) & In cases analogous to the urn example, the weight obtained by a larger sample is higher, if the frequencies in the samples remain the same.\\
  (Strong increase) & In cases analogous to the urn example, the weight obtained by a larger sample is higher.
  \end{tabular}




Now, some requirements on how weight of evidence is related to the balance of probability. For one thing, Keynes insists that new (relevant) evidence might decrease probability but will always increase weight [TP 77]. Since (Monotonicity) already captures the idea that weight will always increase, here we extract the other part of the claim:

\begin{tabular}{lp{11cm}}
(Possible decrease) & It is possible that   $V(X\vert K \wedge E) > V(X\vert K)$ while $\pr{X \vert K \wedge E} <  \pr {X\vert K}$.
\end{tabular}

Clearly, Keynes also endorsed the following two requirements of a very similar form:

\begin{tabular}{lp{11cm}}
(Possible increase) & It is possible that   $V(X\vert K \wedge E) > V(X\vert K)$ while $\pr{X \vert K \wedge E} >  \pr {X\vert K}$. \\
(Possibly no change ) & It is possible that   $V(X\vert K \wedge E) > V(X\vert K)$ while $\pr{X \vert K \wedge E} =  \pr {X\vert K}$.
\end{tabular}




Now, back to the urn example. You might think the actually frequency observed should contribute more to the balance, not to the weight, at least in the sense that with the same number of observations, more extreme frequencies should not have lower weight:

\begin{tabular}{lp{11cm}}
(Frequency monotonicity) & For a fixed number of observations in a binomial experiment, for two observed frequencies $f_1$ and $f_2$, if $f_2$ is closer to $.5$ than $f_1$, the weight of observing $f_1$ is not less than that of observing $f_2$.
\end{tabular}



Interestingly, Keynes for quite a few years did not attempt to provide anything close to a formal explication of the notion, and did not spend too much time studying the issue. Various reasons for this has been proposed the literature, a prominent one   [CITE FEDUZI 2010] being that from the decision-theoretic perspective no clear stopping rule emerged as to whether the evidence is weighty enough to make a decision. \todo{Should I talk about other theories here, or should we leave it as is without getting into interpretative details.} Later on we will see a sort of revival---some ideas later developed by Keynes has been used to explicate the notion of weight formally, and we will take a closer look at this proposal. \todo{REF section}



<!-- Runde (1990), Keynes provides three deﬁnitions of evidential weight in the TP, -->

<!-- is not referring to the sheer number of statements on the right hand side of a conditional probability P (H | E) or the sheer bulk of information that these statements -->
<!-- contain. By “relevant evidence”, Keynes is only referring to the extent that E pro- -->
<!-- vides information that is pertinent to H in particular. [Pedden3] -->


<!-- PEDDEN 678: Jochen Runde notes that Keynes -->
<!-- presupposes that weight always increases monotonically with additions of relevant -->
<!-- evidence to a body of evidence [6]. -->


# Examples and informal desiderata  

- Go over Nance in particular,  some other sources?


- first check for completeness, then evaluate

- what do you mean: are there items of relevant evidence that you could reasonably obtain
- destroyed?

## Monotonicity of weight

Before we move on, let us ponder whether (Monotonicity) is actually desirable. Is it always the case, as some formulations from Keynes would suggest, that any item of relevant of evidence, when obtained, leads to a higher weight of evidence?


Here are two examples from [WEATHERSON 2002], one qualitative, one quantitative.\todo{CITE B. Weatherson. Keynes, uncertainty and interest rates. Cambridge Journal of Eco-
nomics, 26 (1): 47-62, 2002.]} First. you are playing poker and wonder if one of the other players, Lydia, has a straight flush (five cards of sequential rank in the same suite). There are 40 possible straight flush hands out of 2,958,960 possible hands, so you estimate the probability of this event to be $\nicefrac{40}{2,958,960}$. But then you look at her facial expressions, listen to her tone of voice, past bluffing behavior, and this makes you more confused about the issue. It seems, obtaining this additional information diluted your original calculated first stab at the problem. Second. You are drawing  from an urn with 10 blue and 90 black lottery tickets. Your initial assessment of the probability of drawing a blue ticket is .1. Then, you learn that the proportion of the tickets at the top is somewhere between .2 and 1. You acquired new evidence, but your evidence became imprecise. In both cases, it seems intuitive that the weight of evidence should decrease, as the evidence becomes less telling.



<!-- Runde, Joyce, -->

<!-- PEDEN 679: Runde argues that, under some circumstances, new ev- -->
<!-- idence might reveal that our evidence is more limited than we thought [6]. James M. -->
<!-- Joyce implicitly rejects the monotonicity presupposition in his discussion of weight -->
<!-- [7]. Brian Weatherson argues against the monotonicity presupposition and provides -->
<!-- an elegant example to support his denial [8]. -->

# Hamer's weight of evidence


# Good's weigh of evidence and the information value

One notion in the vicinity also called \emph{weight of evidence} has been introduced by Good [CITE PROBABILITY AND THE
WEIGHING OF EVIDENCE 1950]. Let $W(H:E)$ be the Good's weigh of evidence in favor of $H$ provided by $E$ (if we want to explicitly conditionalize on some background knowledge $K$, we write $W(H:E\vert K)$).  One assumption about $W$ taken by Good is as follows:\todo{pays attention to different values of different items of evidence, which is better than just counting or supersets}

\begin{tabular}{lp{11cm}}
(Function) & ``It is natural to assume that $W(H:E)$ is some function of $\pr{E\vert H}$ and of $\pr{E\vert \neg H}$, say $f[\pr{E\vert H}, \pr{E \vert \neg H}]$. I cannot see how anything can be relevant to the weight of evidence other than the probability of the evidence given guilt and the probability given innocence.'' [cite Good 1985 p 250]
\end{tabular}

The other two are:

\begin{tabular}{lp{11cm}}
(Independence) & $\pr{H\vert E} $ should depend only on the weight of evidence and on the prior: $\pr{H \vert E} = g[W(H:e), \pr{H}]$.\\
(Additivity)  & $W(H: E_1 \wedge E_2)  = W(H:E_1) + W(H:E_2 \vert E_1)$
\end{tabular}
\noindent The three conditions can be simultaneously satisfied by only one function (up to a constant factor), which leads to Good's definition of weight of evidence:^[To be fair, logarithms of the ratio of posterior odds to prior odds have been used Jeffrey in 1936, [CITE] and  the use of logarithm to ensure additivity has been suggested by Turing [CITE 1950 o 63]. Good's measure differs from Jeffrey's by taking the ratio of likelihoods rather than odds. In fact, the former ratio is identical to $\nicefrac{O(H\vert E)}{O(H)}$, the ratio of conditional odds of $H$ to the prior odds of $H$.]
\begin{align*}
W(H:E) & = \log \frac{\pr{E \vert H}}{\pr{E\vert \neg H}}
\end{align*}

The natural question that arises is the extent to which Good's weight satisfies the desiderata related to Keynes' notion of weight. First, let us think about weight increase with sample size. If in an experiment the observations $E_1, \dots, E_K$ are independent given $H$ and independent given $\neg H$, the resulting joint likelihood is  the result of the multiplication of the individual likelihoods, and so the resulting joint weight is the result of adding the individual weights. 

For example, suppose a die is selected at random from a hat containing nine fair dice and one loaded die with the chance $\nicefrac{1}{3}$ of obtaining a six. The initial uniform distribution gives you weight of evidence for the die being loaded of $log_{10}(.1)$, that is `r log10(.1)` (Good and Turing would say, it is -10 db). Now, every time you toss it and obtain a six, you gain $log_{10}(\frac{\nicefrac{1}{3}}{\nicefrac{1}{6}})= log_{10}(2)$, that is `r log10(2)`, and every time you toss it and obtain something else, the weight changes by $log_{10}(\frac{\nicefrac{2}{3}}{\nicefrac{5}{6}})= log_{10}(.8)$, that is `r log10(.8)`. Let us inspect the weights in db (that is, multiplied by 10) for all possible outcomes of up to 20 tosses (Figure \ref{fig:goodWeight}).



\begin{figure}
```{r goodWEights,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
six <- seq(0,10, by = 1)
others <- seq(0, 10, by  = 1)
options <- expand.grid(six = six, others = others)
options$weight  <- round(10 *( log10(.1) + options$six * log10(2) +
                                 options$others * log10(.8)),1)

ggplot(options, aes(x = six, y = others, fill = weight)) +
  geom_tile(color = "white", lwd = .1,
            linetype = 1)+
  geom_text(aes(label = round(weight,2)), color = "white", size = 3)+
  scale_fill_gradient(low = "black", high = "orangered")+
  theme_tufte()+
  ggtitle("Good's weights for up to 20 die tosses (db)")+
  scale_x_continuous(breaks = seq(0,10, by = 1))+
  scale_y_continuous(breaks = seq(0,10, by = 1))
```
\caption{Good's weights in dbs, rounded, for all possible outcomes of up to 20 tosses of a die randomly selected from 10 dice nine of which were fair, and one is \nicefrac{1}{3} loaded towards six. $H=$`the die is loaded'.}
\label{fig:goodWeight}
\end{figure}

Two facts are notable. (1) Weight can drop with sample size: for instance the weight for 4 others and 5 sixes is 1.2db, and it is .2db for 5 others and 5 sixes. (2) Weight can drop while the sample size increases even if the proportion of sixes remains the same. For instance, if none of the observations are sixes, the weights go from -10 to -19.7 as the sample size goes from 0 to 10. Less trivially, the observation of one six in five leads to weight of -10.9, while the observation of two sixes in ten tosses leads to weight -11.7. That is, (Monotonicity), (Completeness), (Weak increase) and (Strong increase) all fail for Good's measure.



Moreover, there is a conceptual difficulty in the neighborhood. Suppose you are trying to ascertain the bias 
$\theta$ of a coin, but you do not restrict yourself to two hypotheses as in the dice example, but rather initially take any bias to be equally likely. For each particular hypothesis 
$\theta = x$ and any set of observations $E$ you can use the binomial distribution to calculate 
$\pr{E \vert \theta = x}$. But to deploy Good's definition, you also need 
$\pr{E \vert \theta \neq x}$, which is less trivial, as now you have to integrate to calculate the expected probability of the evidence given an infinite array of possible values of $y$. Suppose you have no problem calculating such items. Now imagine you observe 10 heads in 20 tosses. The question `how weighty is the evidence' makes no sense here, as Good's weight needs a hypothesis (and its negation) to be plugged in. For this reason, in such a situation, we can at best talk about a continuum of Good's weights, one for each particular value of 
$\theta$. 




- compare to pointwise mutual information

- evaluate in light of the desiderata

<!-- https://stats.stackexchange.com/questions/16945/why-do-people-use-the-term-weight-of-evidence-and-how-does-it-differ-from-poi -->

<!-- w(e:h)=logp(e|h)p(e|h¯¯¯) -->
<!-- where e is evidence, h is hypothesis. -->

<!-- Now, I want to know what is the main difference with PMI (pointwise mutual information) -->

<!-- pmi(e,h)=logp(e,h)p(e)∗p(h) -->

<!-- h is something different in PMI and in WOE -->
<!-- Notice the term p(h) in PMI. This implies that h is a random variable of which you can compute the probability. For a Bayesian, that's no problem, but if you do not believe that hypotheses can have a probability a priori you cannot even write PMI for hypothesis and evidence. In WOE, h is a parameter of the distribution and the expressions are always defined. -->

<!-- PMI is symmetric, WOE is not -->
<!-- Trivially, pmi(e,h)=pmi(h,e). However, w(h:e)=logp(h|e)/p(h|e¯) need not be defined because of the term e¯. Even when it is, it is in general not equal to w(e:h). -->

<!-- Other than that, WOE and PMI have similarities. -->

<!-- The weight of evidence says how much the evidence speaks in favor of a hypothesis. If it is 0, it means that it neither speaks for nor against. The higher it is, the more it validates hypothesis h, and the lower it is, the more it validates h¯. -->

<!-- Mutual information quantifies how the occurrence of an event (e or h) says something about the occurrence of the other event. If it is 0, the events are independent and the occurrence of one says nothing about the other. The higher it is the more often they co-occur, and the lower it is the more they are mutually exclusive. -->

<!-- What about the cases where the hypothesis h is also a random variable and both options are valid? For example in communiction over a binary noisy channel, the hypothesis is h the emitted signal to decode and the evidence is the received signal. Say that the probability of flipping is 1/1000, so if you receive a 1, the WOE for 1 is log0.999/0.001=6.90. The PMI, on the other hand, depends on the proability of emitting a 1. You can verify that when the probability of emitting a 1 tends to 0, the PMI tends to 6.90, while it tends to 0 when the probability of emitting a 1 tends to 1. -->

<!-- This paradoxical behavior illustrates two things: -->

<!-- None of them is suitable to make a guess about the emission. If the probability of emitting a 1 drops below 1/1000, the most likely emission is 0 even when receiving a 1. However, for small probabilities of emitting a 1 both WOE and PMI are close to 6.90. -->

<!-- PMI is a gain of (Shannon's) information over the realization of the hypothesis, if the hypothesis is almost sure, then no information is gained. WOE is an update of our prior odds, which does not depend on the value of those odds. -->

# Weight and completeness

<!-- Completeness, inclusion entails weight comparison, very limited applicability -->

A question similar to "how weighty is the evidence" is "how complete is it"? These are conceptually different: the former asks about how much information 
pertinent to a given hypotheses the evidence provides, or, about the amount of evidence relevant to that hypothesis, the latter seems to  suggest a comparison  to some ideal list of what such items would be needed for the evidence to be complete. While we think that these notions, albeit related, should be clearly distinguished, the distinction has not always been made clearly in the  literature, starting with Keynes himself, who suggested that in their evaluation of the evidence an agent should consider "the degree of completeness of the information upon which a probability is based." [TP p. 345]\todo{REF}

This picture of ideal-list-of-evidence-relative notion of weight has been explored by   [CITE FEDUZI 2010]. \todo{CITE NANCE HERE?} Let us first present the view following [CITE FEDUZI 343],  $\Omega$ stands for the set of all items of possible evidence relevant for estimating the probability of the hypothesis $H$. Let $K$ be the agent's knowledge, the set of items of evidence already obtained by the agent, $K \subseteq \Omega$. Then her relevant ignorance is $I = \Omega \setminus K$.

Then, Feduzi, following [CITE RUNDE 1990], proposes to define the weight of information $E$ provides about $H$, $V(H/E)$ as follows:
\begin{align} \tag{Vdiv}  V(H/E) & = \frac{K}{K+I}.\end{align}
\noindent While literally it does not make sense to divide sets by sets, we might charitably interpreting Feduzi as using  $\Omega, K$ and $I$ the symbols ambiguously, standing for both the sets of items of evidence, and the amount of relevant information that the sets contain. The obvious difficulty is that it is not a successful explication (at least not yet), as we are not told how to get $K$ and $K+I$ as numerical values to be used in the division. But however we get them, let us see whether (Vdiv) can result in any insights. 



First, one advantage of the completeness approach is that the resolution of the stopping problem is more or less automatic: the agent should make the decision if the evidence 
is complete, and should collect more evidence if it is not. Later on, when discussing Nance's approach to the notion, we will see a complication: obtaining further evidence might be practically unfeasible, and so it makes sense to distinguish ideal completeness from reasonable completeness and base the practical stopping rule on the former. For now, we put this complication aside.


Second, it might be the case that obtaining further evidence while providing more information results in the decrease of weight. 
Here is an example illustrating this due to Feduzi [CITE 345]. Joan in her research tries to establish who is the most quoted author in the literature on decision theory under ambiguity. $\Omega$ is the set of all $n$ papers (though of as items of evidence $E_1, \dots, E_n$). $K_0$ contains the $m$ papers that Joan inspected so far ($E_1, \dots, E_m$, $m<n$. $I_0$ is the set of papers she did not look at yet, $\Omega = K_0 \cup I_0$. However, Joan is aware only of a part of $\Omega$, the papers in the field she believes exist, $S$. Thus, her objective ignorance, $I_0$, and her subjective ignorance, $I_S = S- K$, diverge, as she underestimates the amount of papers that she has not yet encountered. Joan's assessment of weight is going to be $\nicefrac{K}{K+I_s}$.  Say Joan formulates a hypothesis, $H$: "Ellsberg is the most highly cited author in the ambiguity literature" and that she is quite confident that the papers she had not looked at yet would not significantly affect the probability of $H$. She thinks she has read enough, say $\pr{H \vert K} = .7$ and $V(H/K) = .8$. Then, she looks at another paper, somewhat increasing $K$, but that paper contains reference to many papers she has not heard of in journals that she has not heard of, thus increasing her estimation of $S$ quite a lot--- the ultimate impact of the new evidence is a drop in weight as the denominator in (Vdiv) will grow much more than the numerator.  Thus, (Monotonicity) \todo{check crossref} can fail on this approach.

However, even putting the conceptual difference between weight and completeness that we have already mentioned aside, there are concerns about using degrees of completeness as our explication of the notion of weight of evidence.

To start with, we have not really explicated the notion of the amount of evidence employed in (Vdiv). Sure, we could simply count propositions. One simple strategy, to be used if we do not want to use (Vdiv) would  be to simply count the relevant propositions included in the evidence---this would validate (Monotonicity) \todo{check crossref}. Another strategy along these lines would be to assign sizes to sets of propositions and use these as numbers in (Vdiv), invalidating (Monotonicity) in the process. Either way, the strategy is not viable, as it is too syntax-sensitive. Different propositions, intuitively, can contain hugely different amount of nevertheless relevant information \todo{is this trivial or do we need an example?}, and the individuation of propositions is too arbitrary a matter to take such an approach seriously. On one hand,  without some measure of assigning numerical values to sets of evidence, we have no way to deploy (Vdiv). On the other hand, if we could meaningfully assign numbers expressing the "amount of evidence" prior to any application of (Vdiv), there are no clear reasons why we should take these number to express weights of evidence, especially given the second concern with the completeness approach.

So the second difficulty is that  on this approach the weight of evidence becomes very sensitive not only to what the actual evidence is, but also to what an ideal evidence in a given case should be. unless as clear and epistemologically principled guidance as to how to formulate such ideal lists is available, this seems to open a gate to arbitrariness. Change of awareness of one's own ignorance, without any major chance to the actual evidence obtained, might lead to overconfidence or under-confidence in one's judgment. Moreover, it is not clear how disagreement about weight arising between agents not due to evidential differences, but rather due to differences in their list of ideal items of evidence should be adjudicated.




# Skyrms and resilience?


- 
- relation to law Davidson Pargetter 1986, perhaps Nance, who else?




# Evidential probability and  weight 

[PEDDEN 2018] \todo{REF} follows a suggestion from [KYBURG 1961]\todo{REF Kyburg. Probability and the Logic of Rational Belief. Wesleyan University Press, Mid-
dletown Connecticut, 1961 and H. E. Kyburg and C. M. Teng. Uncertain Inference. Cambridge University Press, Cam-
bridge, 2001.} He proposed using the degree of imprecision
of the intervals in his probability system called Evidential Probability (EP). The key idea in EP is that evidential probabilities should be imprecise, and so accordingly an evidential probability function $\mathsf{EP}$ is of the form $\mathsf{EP}(H \vert E \wedge K) = [x,y]$, where the right-hand is the closed interval expressing the objective degree of support that $E\wedge K$ provide for $H$. 

How is this interval to be determined, though? Kyburg's proposal is the following, if the hypothesis is about 
a single object $o$ and a predicate $P$. For the reference classes to which  $o$ is known to belong and for which  $K$ contains  frequency information (possibly imprecise, in the interval form) for objects with $P$, enumerate the corresponding frequency statements, $r_1, \dots, r_n$. Now, you are facing a reference class problem. Apply sequentially the following rules:

\begin{tabular}{lp{10cm}}
(Sharpening by richness) & If $r_j$ conflicts with $r_i$ and $r_i$ has been obtained from a marginal distribution while $r_j$ from a full joint distribution, ignore $r_i$. 
\end{tabular}


As the formulation might be somewhat cryptic, let us illustrate the recommendation with an example.\todo{make sure these examples are better made sense of the HOP way!}

\begin{tabular}{lp{10cm}}
(Richness example) &suppose you are drawing a card from one of two decks of cards, $H:=$ `you will draw the Ace of Spades'. You know that Deck 1 is a regular deck, and Deck 2 is a regular Deck with the Ace of Spades removed. First you toss a fair die and use Deck 1 if the die lands on 1 or 2, and use Deck 2 otherwise. You have at least two frequencies to consider:
\begin{itemize}
\item The frequency of Aces of Spades in the total number o cards ($\nicefrac{1}{103}$), which is your marginal-distribution-based-probability.
\item The one obtained by using the information about the die, and about the frequencies in the decks. There is probability $\nicefrac{1}{3}$ of using Deck 1 which is the only deck containing the card, in which the probability of drawing it is $\nicefrac{1}{52}$, so the probability to be used is $\nicefrac{1}{3}\nicefrac{1}{52}= \nicefrac{1}{156}$.
\end{itemize}
\end{tabular}

One can easily observe that $\nicefrac{1}{103}$ simply is not the probability of drawing the Ace of Spades in the setup. After all, we are not drawing a random card from the joint decks, but have to factor in the uneven probabilities of the decks being chosen, and once we do so, the correct probability is $\nicefrac{1}{156}$. The second strategy is this:


\begin{tabular}{lp{10cm}}
(Sharpening by specificity) & If among the remaining intervals $r_j$ conflicts with $r_i$ and $r_j$ is a proper subset of $r_i$, choose $r_j$ over $r_i$.
\end{tabular}

\noindent This mirrors the idea that one should use more specific information. The third rule is:


\begin{tabular}{lp{10cm}}
(Sharpening by precision) & If there is a single interval that is a proper sub-interval  of every other interval, this is the evidential probability. Otherwise, the evidential probability is the shortest possible cover of these intervals.
\end{tabular}



With this system in the background, [PEDDEN 2018 681] proposes the following definition of the weight of the argument for $H$ given $E$ and $K$, where $\mathsf{EP}(H \vert E \wedge K) = [x,y]$: 
\begin{align}
\tag{WK} & \mathsf{WK(H\vert E\wedge K)} & = 1 - (y-x)
\end{align}
That is, the weight of the evidence is the spread of the evidential probability, transformed to scale between 0 and 1, reaching 1 when the spread is 0 and 0 when the spread is 1. 




How is this approach to be applied to examples such as the one by C. S. Peirce (recall: drawing balls with replacement from an urn, with observed frequency of white balls .5, in one scenario the sample size is 1000 in another it is 2)? 

[PEDDEN 2018 686] proposes the following analysis, of an example analogous to that by Peirce. You are drawing from an urn full of black and red beans (the proportion is unknown). First, abbreviations:

\begin{tabular}{lp{10cm}}
$H$ & $49.5-50.5\%$ of the beans are red. \\
$E_1$ & 2 sampled beans are red. \\
$E_2$ & 3000 sample beans are red. 
\end{tabular}


\noindent Further, imagine you have enough information to calculate that between $2\%$ and $100\%$ of two-fold samples of any large finite population will be matching samples, that is they will match the population with a margin of error of $1\%$. Then $\mathsf{EP}(H \vert E_1 \wedge K) = [.02, 1]$. Similarly, Pedden invites us to suppose that we can calculate the relative frequency of 3000-fold samples that match any large finite population within a margin of error of $1\%$ to somewhere between $72.665\%$ and  $100\%$, so accordingly  $\mathsf{EP}(H \vert E_2 \wedge K) = [.72665, 1]$. Then, the corresponding values of $\mathsf{WK}$ are .02 and .72665 (this is  because in both cases $y =1$ and $1 - (1 -x) = x$). 


What are we to make of this? Are imprecise probabilities promising when it comes to the explication of weight of evidence? Some progress has been made, but note the following limitations.

- The edges of the intervals are what contributes to \textsf{WK}. They are highly sensitive to the choice of the margin of error, but what margin of error to choose and why remains a mystery, and what margin of error has been chosen does not function anywhere in the $\mathsf{EP}$ representation of uncertainty.

- Relatedly, it may easily happen that for two different distributions the $1\%$ intervals will be identical while the 78\%$ intervals will not. Such differences will obviously not be captured by the 1\% margin of error intervals.

- The calculations of such intervals might be easy for simple combinatorial cases, but it is far from obvious how similar intervals are to be obtained for more complicated real-life cases. Emphatically, classical statistical confidence intervals are not ranges within which the true parameter lies with a certain probability (and if you interpret confidence intervals this way, you behave as if you were running a Bayesian reasoning with a uniform prior, which is often unjustified and prone to over-fitting).


Before we abandon the idea, though, let us know that over the last 30 years we have observed a revival of imprecise probabilities, and so it is only fair that we should take its most recent versions for a ride. Hence our next interest: imprecise probabilism, its motivations, the difficulties it runs into. 


# Imprecise probabilities and weight


The point of departure for imprecise probabilism (\textsf{IP}) is the precise probabilism (\textsf{PP}), which we are already familiar with. On the latter view, a rational agent's  uncertainty is to be represented as a single probability measure. \textbf{Imprecise probabilism}, in contrast with PP,  holds that  an agent's credal stance towards a hypothesis $H$ is to be represented by means of a set of probability measures, called a  representor, $\mathbb{P}$, rather than a single measure $\mathsf{P}$. The idea is that  the representor should include all and only those probability measures which are compatible with this evidence.  For instance, if an agent knows that the coin is fair, their credal stance  would  be captured by $\{\mathsf{P}\}$, where $\mathsf{P}$ is simply a probability measure which assigns $.5$ to $H$. If, on the other hand, the agent knows nothing about the coin's bias, their stance would rather be represented by means of the set of all probabilistic measures, as none of them is excluded by the available evidence. Note that on IP it is not the case that the set represents admissible options and the agent can legitimately pick any precise measure from the set. Rather, the agent's credal stance is essentially imprecise and has to be represented by means of the whole set.^[For  the development of IP see [@keynes1921treatise;@Levi1974ideterminate; @Gardenfors1982unreliable; @Kaplan1968decision;  @joyce2005probabilities; @VanFraassen2006vague; @Sturgeon2008grain; @walley1991statistical], [@bradley2019imprecise] is a good source of literature.] 
The literature contains an array of arguments for IP. Let us take a look at the main ones.

-  \textsf{PP} does not seem appropriately evidence-responsive, especially when evidence is limited. Following \textsf{PP}, in Peirce's example, the agent's uncertainty about $W:=$"the next drawn ball is going to be white' is $.5$ no matter whether you have drawn two balls one of which was white, or a thousand balls, five hundred of which were white.

- Indifference is not sensitive to sweetening (improving the chances of $H$ only slightly), while \textsf{PP} predicts such sensitivity. For instance, if you do not know  what the bias of a coin is, learning that it now has been slightly modified to increase the probability of heads by .001 will still leave you  unwilling to bet on heads in a bet  that would've been fair  if the actual chance of $H$ was .5 and not .001.

-  \textsf{PP} has problems representing complete lack of knowledge.  Suppose you start tossing the coin  starting with knowing only that the coin bias is in $[0,1]$  and then observe the outcome of ten tosses, half of which turn out to be heads. This is some evidence for the real bias being around .5. How do you represent your stances before and after the observations?  If you deploy the principle of insufficient evidence, you start with $\mathsf{P}_0(H)=.5$  and end with $\mathsf{P}_1(H)=.5$, as if nothing changed. If you do not deploy the principle of insufficient evidence, what do you do?


- \textsf{PP}  has problems  with formulating a sensible method of probabilistic opinion  aggregation [@Elkin2018resolving,@Stewart2018pooling].  A seemingly intuitive constraint is that if every member agrees that $X$ and $Y$ are probabilistically independent, the aggregated credence should respect this. But this is hard to achieve if we stick to  PP [@Dietrich2016pooling]. For instance, a \emph{prima facie} obvious method of linear pooling does not respect this. Consider probabilistic measures $p$ and $q$ such that $p(X)  = p(Y)  = p(X\vert Y) = 1/3$ and  $q(X)  =  q(Y) = q(X\vert Y) = 2/3$. On both measures, taken separately, $X$ and $Y$ are independent. Now take the average, $r=p/2+q/2$. Then $r(X\cap Y) = 5/18 \neq r(X)r(Y)=1/4$. 






One key difference between Kyburg's \textsf{EP} and \textsf{IP} is that on the latter we use sets of probability measure instead of intervals. This makes the approach not only more general (as now, for instance, the resulting probabilities of a proposition in question do not have to form a closed interval), but also provides a more general and less idiosyncratic picture of   learning from evidence, that is a a natural extension of the classical Bayesian approach.  When faced with new evidence $E$ between time $t_0$ and $t_1$, RA's representor should be updated point-wise,  running the standard Bayesian updating on each probability measure in the representor: 
\begin{align*} \label{eq:updateRepresentor}
\mathbb{P}_{t_1} = \{\mathsf{P}_{t_1}\vert \exists\, {\mathsf{P}_{t_0} \!\in  \mathbb{P}_{t_0}}\,\, \forall\, {H}\,\, \left[\mathsf{P}_{t_1}(H)=\mathsf{P}_{t_0}(H \vert E)\right] \}.
\end{align*}


How is weight of evidence to be measured on \textsf{IP} [@Kaplan1996decision; @joyce2005probabilities; @Sturgeon2008grain]? One line, analogous to the one taken by Pedden is   to take the edges of the resulting interval that captures changes in the weight of evidence  [@walley1991statistical]. If you know the proportion of beans is between .02 and 1, your interval is wider than it is if you know the proportion is between 0.72665. In this sense, this approach, admittedly, handles cases such as the beans example.


The main question here, though, is that it is not clear how you are supposed to learn that the proportion of beans is such and such.  This is related to the problem of  belief inertia  [@Levi1980enterprise].  Say  you start drawing beans  knowing only that the true proportion of red beans  is in $(0,1)$  and then draw two beans both of which are red.  On \textsf{IP} your  initial credal state  is to be modeled by the set of all possible probability measures over your algebra of propositions.  Once you observe the two beans, each particular measure from your initial representor gets updated to a different one that assigns a higher probability to "red", but also each  measure in your original representor can be obtained by updating   some other measure in your original representor on the evidence (and the picture does not change if you continue with the remaining 2998 observations) . Thus, if you are to update your representor point-wise, you will  end up with the same representor set. Consequently, the edges of your resulting interval will remain the same. 

Relatedly, recall that the main selling point for \textsf{IP} was its ability to account for how credal states are responsive to the evidence and the amount thereof. But it is not clear how to make sense of evidential constraints in a way that makes them go beyond testimonial evidence. On \textsf{IP} the representors are somehow to obey the evidential constraints: they are supposed to be point-wise updated on the evidence, and to  contain only those probabilistic measures that are not excluded by the evidence obtained so far. But how exactly does the evidence exclude probability measures?  This is not a  mathematical question: mathematically [@bradley2012uncertaintyPhD], evidential constraints are fairly easy to model, as they can take the form of the \emph{evidence of chances} $\{ \mathsf{P}(X) = x\}$ or $\mathsf{P}(X) \in [x,y]$, or be \emph{structural constraints} such as  "$X$ and $Y$ are independent" or "$X$ is more likely than $Y$." While it is clear that these constraints are something that an agent can come to accept if offered such information by an expert to which the agent completely defers, it  is not trivial to explain how  non-testimonial evidence can result in such constraints. 

Most of the  examples  in the literature start with the assumption that the agent is told by a believable source that the chances are such-and-such, or that the experimental set-up is such that the agent  knows that such and such structural constraint is satisfied. But outside of such ideal circumstances what observations exactly would need to be made to come to accept such constraints remains unclear. And the question is urging: even if you were lucky enough to run into an expert that you completely trust that provides you with a constraint like this, how exactly did the expert come to learn the constraint? The chain of testimonial evidence has to end somewhere!

Admittedly, there are straightforward degenerate cases: if you see the outcome of a coin toss to be heads, you reject the measure with $\mathsf{P}(H)=0$, and similarly for tails. Another class of cases might arise if you are randomly drawing objects from a finite set where the real frequencies are already known, because this finite set has been inspected. But such extreme cases aside, what else? Mere consistency constraint wouldn't get the agent very far in the game of excluding probability measures, as way too many probability measures are strictly speaking still consistent with the observations for evidence to result in epistemic progress. 

Bradley suggests that "statistical evidence might inform [evidential] constraints [\dots and that evidence] of causes might inform structural constraints" [125-126]. This, however, is far cry from a clear account of how exactly this should proceed. Now, one suggestion might be that once a statistical significance threshold is selected, a given set of observations with a selection of background modeling assumptions yields a confidence interval, and perhaps that the bounds of this confidence interval should be the lower  and upper envelope---this is in line with the example used by Pedden (and our objections to such a use of conidence intervals apply here as well). Moreover, notice that whatever problems  Bayesian statisticians raise against classical statistics apply here. To mention a few: the approach uses MLE and so is not sensitive to priors (or, in other words, is equivalent to always taking maximally uninformative priors), the estimates are sensitive to stopping intention, and there are no clear methods for combining various pieces of information of this sort (if this was easy, there would be no need for meta-analysis in statistical practice).\footnote{Admittedly, there are formulae for calculating confindence intervals based on two confidence intervals if they are based on separate independent observations in an experiment of exactly the same design, but this is a very idealized setup.}

Even supposing that a sensible mechanism of the exclusion in question has been proposed, measuring weight along the lines of (WK) or by the absolute distance between the edges of the interval has a weakness we already signaled. It is a function of two points and is completely insensitive to what happens between them. If we are looking at a somewhat monolithic class of distributions, say all beta distributions, looking at where the  1st and the  99th centiles are located might be a good rough guide to what is happening in between. But without such a restriction, not so much.

Here is an example of how easily more complicated cases that could be misrepresented by looking only at the envelopes might arise.  Suppose you  start with knowing that the coin bias lies within $(.4, .6)$ (Situation A). Then you hear from two equally reliable witnesses: one tells you that the real bias is exactly .4 and the other one tells them the real bias is exactly .6 (Situation B). It seems that you now have more evidence than before, but it is unclear how this difference is to be captured by the edges of the interval of non-excluded values, as the edges are exactly the same.


Another stab at explicating weight of evidence within the \textsf{IP} framework has been made by @joyce2005probabilities. Joyce uses a density over chance hypotheses to account for the notion of evidential weight.    He conceptualizes the  weight of evidence  as an increase of concentration of smaller subsets of chance hypotheses:
\begin{align}
\tag{Joyce} w(X,E) & = \sum_x \vert c(ch(X) = x  \vert E) \times (x - c(X\vert E))^2 - c(ch(X) = x) \times (x - c(X))^2\vert
\end{align}

\noindent This looks a bit complicated, so let us take this slow. Suppose you only consider three chance hypotheses, that the coin bias is one of $.4, .5,$ and $.6$, that is, the hypotheses are $ch(X) = .4, ch(X)=.5,$ and $ch(X)=.6$. For each $x\in \{.4, .5, .6\}$) you attach a prior credence  $c(ch(X) = x)$ to the corresponding hypothesis. Say you start with equal priors, that is for all $x\in \{.4, .5, .6\}$ you have $c(ch(X) =x) = \nicefrac{1}{3}$. Then, your expected value of $X$, which Joyce takes to be your credence in $X$ simpliciter is $\sum_x c(Ch(X)=x)x$, which is .5. 


Now consider your evidence: you tossed the coin and observed, say, seven heads out of ten tosses. We need $c(ch(X)=x \vert E)$. By Bayes, we have:
\begin{align*}
c(ch(X)=x \vert E) & = \frac{c(E \vert ch(X) =x) c(ch(X)=x )}{c(E)},
\end{align*}
\noindent so we need to calculate the likelihoods, $c(E \vert ch(X) =x)$. We assume you are probabilistically coherent, that you defer to chances, and know the experimental setup, so that the likelihoods are calculated using the binomial distribution, i.e. if the evidence is $a$ heads and $b$ tails: 
\begin{align*}
c(E \vert ch(X) =x) & = {a+b \choose a}\,x ^a (1-x)^{b}
\end{align*}
\noindent Accordingly, in our example, the likelihoods (rounded) are .042, .117$, and $.214$ respectively. The denominator is calculated by taking $c(E) =  \sum_x c(E \vert ch(X) =x) c(ch(X)=x)$, which in our case turns out to be $.124$. Putting these together, the values of $c(ch(X)=x \vert E)$ are $.113$ $.312$, and  $.573$ (rounded). Then, your expected value, which Joyce to be your credence in $X$ simpliciter conditional on $E$ is $\sum_x c(Ch(X)=x\vert E)x$, which is  $.54$.


Once that we went over an example illustrating the quantities employed in (Joyce), before we plug these into the final formula,  let us try to understand Joyce's motivations for such an explication. The idea here is that weighty evidence should make the credence resilient, and resilience makes the difference between the posterior credence in chances $c(ch(X)=x \vert E)$ and the prior credence in chances $c(ch(X)=x)$. The complication is that the impact of this difference should be lower for for those values of $x$ that are close to $c(X\vert E)$ for the posterior and close to $c(X)$ for the prior. Hence, the formula for $w$ is takes (the absolute value of) the difference between posteriors and priors weighed by, these (squared) distances. The weightier the evidence, the smaller $w$ is supposed to be.

Accordingly, in our example the weights for the prior are $-.1^2, 0, .1^2 = 0.01, 0, .01$, the weights for the posterior are   $.021330539, 0.002120582 0.0029$ and w is $0.003241822$. For comparison, if instead we observed 70 heads in 100 tosses, $w$ would be 0.006689603.

There are various issues with this approach. One is that now to evaluate the weight of evidence $E$ with respect to proposition $X$ now you need to have and use in your calculations your estimation of chances of $X$. Let us put aside the worry that it is not obvious that we can meaningfully talk about chances of arbitrary propositions. Even then, the name of the game for the imprecise probabilist was to express the uncertainty about $X$ in terms of a representor, a set of probability measures. However, one can have a representor with respect to a set of object-level propositions including $X$ without having a single credence about chances, so now the calculations of weight of $E$ with respect to $X$ do not fall out whatever was supposed to capture the agent's uncertainty about $X$, $E$ and their relationship. 


Moreover, the reader might have observed that the values of $w$ for our example are not very telling. A ten-fold increase in sample size, with frequency being fixed, results in $w$ dropping by $62\%$, and both are small numbers that are hard to interpret.  This raises the question: how does the measure behave with respect to binomial trials and are the outcomes intuitively acceptable? Let us take the measure for a ride. 

First, suppose we keep the same priors, and calculate $w$ depending on how many successes we have observed in the 10 tosses. The results are as follows:


```{r joyceMeasure,echo=FALSE,eval=TRUE,fig.align = "center", cache=FALSE, fig.show = "hold", out.width = "100%",   message = FALSE, warning = FALSE, results = FALSE}
weightJoyce <- function (chanceHypotheses = c(.4, .5, .6),
                         credenceInHypotheses =  c(1/3, 1/3, 1/3),
                         successes = 7,
                         trials = 10){
  
        #chance of evidence given data
        chex <- dbinom(successes, trials,  chanceHypotheses)
        #overall chance of evidence (denumerator)
        che <-  sum(chex * credenceInHypotheses)
        #chance of x given evidence (by Bayes)
        chxe <- (chex * credenceInHypotheses ) / che
        
        #credence in X before and after
        cX <- sum (credenceInHypotheses * chanceHypotheses)
        cXe <- sum (chxe * chanceHypotheses)
        
        multiplier <- (chanceHypotheses - cX)^2
        multiplierE <- (chanceHypotheses - cXe)^2
        
        top <-  chxe   * multiplierE
        bottom <- credenceInHypotheses * multiplier 
        
        weight <- sum ( abs( top - bottom) )
        
        return(list(hypotheses = chanceHypotheses, prior = credenceInHypotheses, posterior =  chxe, 
                    cX = cX, cXe = cXe, weight = weight))  
}

outOfTenWeightsEqualPriors <- numeric(10)

for(i in seq(1,11, by  = 1)){
  outOfTenWeightsEqualPriors[i] <- 
  weightJoyce(successes = i-1)$weight
}


outOfTenWeightsLeftPriors <- numeric(10)
for(i in seq(1,11, by  = 1)){
  outOfTenWeightsLeftPriors[i] <-   
  weightJoyce(credenceInHypotheses = c(.5, .3, .2), 
              successes = i-1)$weight
}

outOf10df <- data.frame( successes = seq(0,10,1),
  equal = outOfTenWeightsEqualPriors, ".5, .3, .2" = outOfTenWeightsLeftPriors)

names(outOf10df) <- c("successes", "equal", ".5, .3, .2")


outOf10dfLong  <- gather(data = outOf10df,
                    key = priors, value = w,
                    "equal", ".5, .3, .2", 
                    factor_key=TRUE)



joyce10  <- ggplot(outOf10dfLong)+geom_point(aes(x = successes,
                                    y = w, color = priors) )+
  scale_x_continuous(breaks = seq(0,10))+theme_tufte(base_size = 14)+ylab("w")+
  xlab("successes in ten trials")+
  scale_y_continuous(breaks = seq(0,0.007, by = .001))+
  labs(title = "Joyce's weights change radically by frequency",
       subtitle = "(sample size 10)")+theme(plot.title.position = "plot")
```



\begin{figure}
```{r joyce1,echo=FALSE,eval=TRUE,fig.align = "center", cache=FALSE, fig.show = "hold", out.width = "100%",   message = FALSE, warning = FALSE, results = FALSE}

joyce10

```

\caption{Joyce's $w$ (the lower it is, the higher the weight) for various observed successes in 10 Bernoulli trials. Three chance chypotheses: $.4, .5, .6$, and two sets of priors: equal and $.5, .3, .2$ respectively. Note how the weightiest evidence is obtained with five observed successes and how its drop is fourteen-fold, if the observed frequency is 0.}
\label{fig:joyce1}
\end{figure}




The behavior of $w$ is even more unusual if the sample size is higher. In Figure \ref{fig:joyce2} we illustrate what happens with $n=100$, for various possible outcomes of 100 Bernoulli trials.


```{r joyce2calculations,echo=FALSE,eval=TRUE,fig.align = "center", cache=FALSE, fig.show = "hold", out.width = "100%",   message = FALSE, warning = FALSE, results = FALSE}

outOf100WeightsEqualPriors <- numeric(101)

for(i in seq(1,101, by  = 1)){
  outOf100WeightsEqualPriors[i] <- 
    weightJoyce(successes = i-1, trials = 100)$weight
}


outOf100WeightsLeftPriors <- numeric(101)

for(i in seq(1,101, by  = 1)){
  outOf100WeightsLeftPriors[i] <- weightJoyce(credenceInHypotheses = c(.5, .3, .2),
                                              successes = i-1, trials  = 100)$weight
}

outOf100df <- data.frame( successes = seq(0,100,1),
              equal = outOf100WeightsEqualPriors, 
              ".5, .3, .2" = outOf100WeightsLeftPriors)

names(outOf100df) <- c("successes", "equal", ".5, .3, .2")

outOf100dfLong  <- gather(data = outOf100df,
                         key = priors, value = w,
                         "equal", ".5, .3, .2", 
                         factor_key=TRUE)



joyce100  <- ggplot(outOf100dfLong)+geom_point(aes(x = successes,
                                                 y = w, color = priors) )+
  scale_x_continuous(breaks = seq(0,100, by = 5))+theme_tufte(base_size = 14)+ylab("w")+
  xlab("successes in ten trials")+
  scale_y_continuous(breaks = seq(0,0.01, by = .001))+
  labs(title = "Joyce's weight displays strange patters",
       subtitle = "(sample size 100)")+
  theme(plot.title.position = "plot")
```





\begin{figure}
```{r joyce2,echo=FALSE,eval=TRUE,fig.align = "center", cache=FALSE, fig.show = "hold", out.width = "100%",   message = FALSE, warning = FALSE, results = FALSE}

joyce100

```

\caption{Joyce's $w$ (the lower it is, the higher the weight) for various observed successes in 100 Bernoulli trials. Three chance chypotheses: $.4, .5, .6$, and two sets of priors: equal and $.5, .3, .2$ respectively. Agan, the weightiest evidence is obtained with successes close to the expected value, with  large variation for observed frequencies not too far from the expected values, fairly flat otherwise.}
\label{fig:joyce2}
\end{figure}


So this measure might result in  drastic shift in weights even if the observed frequenciesa are not too far from the chance hypotheses. This we find undesirable. 

In Figure \ref{fig:joyce3} we illustrate two other phenomena, which  might come up when the observed frequencies are kept fixed, but the sample size increases.




```{r joyce3calculations,echo=FALSE,eval=TRUE,fig.align = "center", cache=FALSE, fig.show = "hold", out.width = "100%",   message = FALSE, warning = FALSE, results = FALSE}

s <- seq(1,100)
obs <- seq(10, 1000, by = 10)


weightsBySampleSize <- numeric(length(s))
weightsBySampleSizeLeft <- numeric(length(s))


for (i in  1:100){
weightsBySampleSize[i] <- weightJoyce(successes = s[i],
                                      trials = obs[i])$weight
}

for (i in  1:100){
  weightsBySampleSizeLeft[i] <- weightJoyce(successes = s[i],
                                    trials = obs[i],
                                    credenceInHypotheses = c(.5, .3, .2))$weight
}


wbss <- data.frame( "sample size" = obs,
                          equal = weightsBySampleSize, 
                          ".5, .3, .2" = weightsBySampleSizeLeft)

wbss$frequency <- rep(.1, nrow(wbss))

s2 <- seq(1,500)
obs2 <-2 *s2   

weightsBySampleSize2 <- numeric(length(s))
weightsBySampleSizeLeft2 <- numeric(length(s))


for (i in  1:500){
  weightsBySampleSize2[i] <- weightJoyce(successes = s2[i],
                                        trials = obs2[i])$weight
}

for (i in  1:500){
  weightsBySampleSizeLeft2[i] <- weightJoyce(successes = s2[i],
                                            trials = obs2[i],
                                            credenceInHypotheses = c(.5, .3, .2))$weight
}


wbssHalf <- data.frame( "sample size" = obs2,
                    equal = weightsBySampleSize2, 
                    ".5, .3, .2" = weightsBySampleSizeLeft2)

wbssHalf$frequency <- rep(.5, nrow(wbssHalf))


names(wbss) <- c("sampleSize", "equal", ".5, .3, .2", "frequency")
names(wbssHalf) <- c("sampleSize", "equal", ".5, .3, .2", "frequency")



wbssLong <- gather(data = rbind(wbss, wbssHalf),
                          key = priors, value = w,
                          "equal", ".5, .3, .2", 
                          factor_key=TRUE)


joyceWBSS  <- ggplot(wbssLong)+geom_line(aes(x = sampleSize,
                                y = w, color = priors,
                                lty = as.factor(frequency)) )+
  scale_x_continuous(breaks = seq(0,1000, by = 100))+theme_tufte(base_size = 14)+
  ylab("w")+
  xlab("sample size")+
  #  scale_y_continuous(breaks = seq(0,0.01, by = .001))+
  labs(title = "Joyce's weights  can drop with sample size",
       subtitle = "(eventually they stop growing)",
       lty = "observed frequency")+
  theme(plot.title.position = "plot")


```









\begin{figure}
```{r joyce3plot,echo=FALSE,eval=TRUE,fig.align = "center", cache=FALSE, fig.show = "hold", out.width = "100%",   message = FALSE, warning = FALSE, results = FALSE}
joyceWBSS
```

\caption{Joyce's $w$ (the lower it is, the higher the weight) for two fixed success ratio across various observed successes in  Bernoulli trials (lines are used for smoothing).  Note large shifts with possible decrease in the beginning, and a flattening afterwards.}
\label{fig:joyce2}
\end{figure}

What is the reason for this strange behavior? The shapinig of Joyce's weight is a balancing act. For instance, for frequency .1 with equal priors the weight is maximized at $n=90$ and starts dropping at $n=100$. Why? We start with three chance hypotheses, $.4, .5, .6$ with equal priors. Once the observations have been made, the posterior for the chance hypotheses is focused at the chance hypothesis .4 (its posterior is more or less .99999), and so is the credence in $X$ simpliciter (this expected value is .4000003). Now the weights are build by measuring square distance from the credence in $X$ simpliciter and since the expected value is nearly equal to the lowest chance hypothesis under consideration, the weight for the lowest chance hypothesis is $8.260660e-14$, so while the posterior for this hypothesis is very high, the weight is very low and its contribution to the weight calculation is severely limited. Ultimately, what happens with weight is now a matter of balancing the uneven posteriors with squared penalties (or rewards, really) for the distance from the expected value (which is pretty much the most likely hypothesis once you have made enough observations). Once you observe 10 successes in 1000 trials, the credence in $X$ simpliciter becomes $.4000001$, so the distance of the lowest chance hypothesis to it drops, and this weight drops "faster" than the resulting increase in the probability of the lowest chance hypothesis itself. The stabiliziation is achieved because further on the posterior for this hypothesis can only get closer to one (and closer to zero for all the other hypotheses).















Moreover, this approach is of limited applicability. For one thing, as Joyce admits, it is supposed to work when RA's credence is mediated by chance hypotheses. Depending on applications, such a mediation might be unavailable. Another issue is that  this might work for unimodal distributions when we  only consider the influx of new data points, but it's unlikely to give desired results if, say, the   evidence obtained  is  the testimony of disagreeing witnesses. This is because an essential part of the calculations relies on taking the expected value, and it is not too hard to imagine cases of diverging items of evidence resulting only in a small chance of the expected value. \todo{Should I further elaborate?}

The approach insists that an agent's  stance towards a proposition should be represented as the expected value of the chance hypothesis---and we will argue against this view later on. At this point, what is crucial for us, the proposal employs probabilities or probability densities (if we go continuous) over parameter values. Even if we do not assume these are chances and treat them as, say, parameters that are potentially rational to accept in light of the evidence, by using this approach we no longer can represent uncertainty about a proposition in terms of a set of probability measures over any algebra containing the proposition itself (as the algebra now also needs to contain the chance hypotheses as well!). Perhaps, this is as it should be. But then, as I will argue later one, there are useful ways to go this way without turning to IP---after all, notice how the notion of a representor plays no role in Joyce's explication of weight whatsoever!






# Troubles with imprecise probabilism

- \textsf{IP} has no means of distinguishing the situation in which you are about to toss a coin whose bias is either .4 or .6., and the one in which you are  about to toss a coin whose bias is either .4 or .6, and bias .4 is three times  more likely than .6.



- \textsf{IP} give wrong comparison predictions [@Rinard2013against].  Suppose you know of two urns, \textsf{GREEN} and \textsf{MYSTERY}. You  are  certain \textsf{GREEN} contains only green marbles, but have no information about \textsf{MYSTERY}. A marble will be drawn at random from each. You should be certain that the marble drawn from \textsf{GREEN}  will be green ($G$), and you should be more confident about this than about the proposition that the marble from \textsf{MYSTERY} will be green ($M$). In line with how lack of information is to be represented on \textsf{IP}, for each $r\in [0,1]$  your representor contains a $\mathsf{P}$ with $\pr{M}=r$. But then,  it also contains one with $\pr{M}=1$. This means that it is not the case that for any probability measure  $\mathsf{P}$ in your representor, $\mathsf{P}(G) > \mathsf{P}(M)$, that is, it is not the case that RA is more confident of $G$ than of $M$. This is highly counter-intuitive.


  




## A second-order approach to uncertainty



## Information-theoretic weight of evidence

## Completeness tends to improve weight

## Weight tends to improve accuracy


Here is a question asked by [COHEN 1986 TWELVE p. 276]: is it worth while knowing the weight of an argument without knowing its probability? In our terminology, questions inspired by Cohen's are: what's the point of weight considerations if we already have the distributions? Can weights be put to use if we do not have the distributions?










# Literature to discuss


Kasser, 2016, Two Conceptions of Weight of Evidence in Peirce’s Illustrations of the Logic of Science [COVERED]

Feduzi, 2010, On Keynes’s conception of the weight of evidence COVERED

Cohen 1986, Twelve Questions about Keynes's Concept of Weight [COVERED]

Pedden, William 2018, Imprecise probability and the measurement of Keynes' weight of arguments [GET BACK TO INERTIA, DILUTION ETC.]

Levi 2011, the weight of argument [DOWNLOADED]

Skyrms 1977 resiliency, propensities [DOWNLOADED]

Skyrms causal necessity, chapter on resilience [DOWNLOAD]

Synthese 186 (2) 2012, volume on Keynesian weight [CHECKED, NOT MUCH ON WEIGHT ACTUALLY, NO NEED TO READ]


Good, weight of evidence, survey

Good, PROBABILITY AND THE
WEIGHING OF EVIDENCE


David Hamer, Probability, anti-resilience, and the weight of expectation [READ]


William Peden, Imprecise Probability and the Measurement of Keynes's "Weight of Arguments"


Runde, Keynesian Uncertainty and the weight of arguments [DOWNLOADED]

Weatherson, 2002, Keynes, uncertainty and interest rates  [DOWNLOADED]

Jeffrey M. Keisler, Value of information analysis: the state of application

Edward C. F. Wilson, A Practical Guide to Value of Information Analysis

Joyce JM (2005) How probabilities reflect evidence.

 Kyburg. Probability and the Logic of Rational Belief. Wesleyan University Press, Middletown Connecticut, 1961 
 
 
 H. E. Kyburg and C. M. Teng. Uncertain Inference. Cambridge University Press, Cam-
bridge, 2001.


