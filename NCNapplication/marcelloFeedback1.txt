









Section 2.1 "State of the art" is very long and perhaps much too long. Not all of it is really relevant. Much of it is taken from what we wrote elsewhere. By the way, make sure that this stuff is not already published in a paper somewhere or in the SEP entry, or else there might be copyright issues, even if we wrote it. I would cut this part a lot. All you need to say is what the three paradigms are, how they are different, what is missing from legal probabilism in its current form that the other paradigms seem to have. 

Section 2.2.1 Thi section is again about the state of the art since you talk about (a) Vlek's use of scenario nodes and (b) Fenton's work on averaging Bayesian networks. These are both still state of the art, even though at the very cutting edge. You can say that legal probabilists have already tried to incorporate aspects of other paradigms into the legal probabilistic paradigm, but that these attempts are still inadequate for the reasons you describe. So I think this belongs to section 2.1.


Incidentally, I think point C on page 7 is very strong and very important. You need to emphasize this more. This is something that you uniquely bring to the table -- i.e. familiarity with epistemological debates on the nature of coherence (which legal probabilists like Vlek or Fenton ignore) and familiarity with the details of evidence assessment in legal cases (which formal epistemologists like Fitelson ignore). This combination of expertise should take more center stage and should perhaps be mentioned in the goals, something like integrating the formal epistemology literature and legal probabilism. There is a strong, true interdisciplinarity here that needs mentioning and emphasizing. 


On the difficulties of Fenton's model averaging, everything you say there makes sense, but there might be a deeper problem with Fenton's approach. If what we are modeling is cross-examination, averaging does not seem to be the right way to go about modeling cross-examination. To model cross-examination, I think we need to be able to model the relations of "undercutting" and "rebutting" that are common in argumentation frameworks. And these relations can be modelled by adding nodes in the network in a suitable manner. I am attaching a paper of mine I mentioned to you which I have just finished on cross-examination and Bayesian networks. I know this might take you out of the way, but I thought I would mention it. We can talk about it next time and I am sure that if you work on it, you'll make it more rigorous and formally precise. 


Section 2.2.2 on strategy and novelty is pretty good, but I think you should be careful in explaining more clearly why you are doing what you are doing. I think this section should map more neatly and clearly onto the goals and critique of the existing literature you set up in the earlier sections. Right now this section is not completely connected with the goals and the examination of the existing literature. For example, the bit about "selection criteria" and the formalization of new legal probabilism is cool, but why is this relevant? Why does this help achieve your goals? This part seems to be about decisions, not so much about evidence evaluation, which instead seems to be the focus of most of the project. So you might want to be careful in dividing your discussion between decisions and evidence evaluation, or remove the discussion about legal decisions.

The work plan is underdeveloped. The stages look good, but I think you might want to give titles of papers or titles of chapters and what each chapter or paper addresses. Right now the work plan is vague and readers will not have a clear sense of what you will accomplish. The work plan should clearly map onto the goals you set out to achieve. You might also want to be clear about what your contribution to the project and what my contribution to the project will be. Is the book the output or just one part of it? This is not clear. Again I think you need to specify titles of papers or chapters. 

The methodology part is also too brief and people from other disciplines will likely find it inadequate. You seem to be using four methods: informal conceptual analysis; formal conceptual analysis; computational (R simulations, etc.); and case studies. You might want to walk the reader through each of these in some detail, and perhaps point to papers you have published which have used these methods or a combination of these. There is no discussion of case studies and how you intend to use those, which case studies you are going to use and how you are going to evaluate your framework against these case studies. Your mention of case studies almost seems like an afterthought. But I think it's good to have case studies and perhaps case studies could help address the practical question about aggregating different types of evidence. 



Happy to see another version when it is ready!