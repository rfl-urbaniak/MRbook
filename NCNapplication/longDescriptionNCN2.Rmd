---
title: "Rethinking legal probabilism"
author: "Rafa≈Ç Urbaniak"
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        - Rafal_latex6.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 11pt
documentclass: scrartcl
urlcolor: blue
bibliography: LP-SEP-FINAL6.bib
csl: apa-5th-edition.csl
---




```{r setup, include=FALSE}
library(ggplot2)
library(ggthemes)
knitr::opts_chunk$set(echo = TRUE)
```





\tableofcontents

\thispagestyle{empty}

# Scientific goal

<!-- (description of the problem to be solved, research questions and hypotheses) -->




As many miscarriages of justice indicate, scientific evidence is easily misinterpreted in court. This happens partially due to miscommunication  between the parties involved, but also because incorporating scientific evidence in the context of a whole case can be really hard.    While probabilistic tools for  piecemeal evaluation of scientific evidence and spotting probablistic fallacies in legal contexts are quite well developed,\todo{say sth about replicability crisis in forensic sciences at some point?} the construction of a more general probabilistic model of incorporating such evidence in a wider context of a whole case,  probabilistic explication of and theorizing about evidence evaluation and legal decision standards, remain a challenge. Legal probabilism, for our purpose, is the view that this challenge can and should be met.  This project intends to contribute to further development of this enterprise in a philosophically motivated manner. 
 


The assessment of evidence in the court of law can be viewed from at least three  perspectives: as an interplay of arguments, as an assessment of  probabilities involved, or as an interaction of competing narrations. Each  perspective presents an account of   legal reasoning [@vanEemeren2017; @di2018evidential]. Individually, each of these strains has been investigated. The probablistic approach is the most developed one but legal probabilism is still underdeveloped ---to a large extent this is so in light of various lines of criticism developed by the representatives of the other strains, as  such challenges have not been satisfactorily adressed by the proponents of legal probablism. 





The \textbf{goal of this project} is to contribute to the \textbf{development of legal probabilism}  by formulating its variant that  accomodates important \textbf{insights provided by its critics}. A crucial point of criticism is that the fact-finding process  should  be conceptualized as \textbf{a competition of narrations}. I plan to develop methods that allow the probabilist to take this perspective, and explain how such methods allow the legal probabilist to address various other objections present in the literature. The key idea is that once  \textbf{narrations are represented as bayesian networks, various criteria on,  features of  and operations on narrations can be explicated in terms of corresponding properties of and operations on bayesian networks}. Further, the hypothesis is that such an improved framework will facilitate addressing key objections  raised against legal probabilism in the literature. 

The conceptual developments are accompanied by  technical accounts. \textbf{\textsf{R}} code  capturing to the technical features developed is made available to the reader.   Thus, the output will be a \textbf{unifying extended probabilistic model embracing key aspects of the narrative and argumentative approaches, susceptible to AI implementation.}  The methods employed include: Bayesian statistical methods (including Bayesian approach to higher-order probability), imprecise probabilities and Bayesian networks.

















# Significance 



(state of the art, justification for tackling a specific scientific problem,
justification for the pioneering nature of the project,
the impact of the project results on the development of the research field and scientific discipline);


## State of the art




One of the functions of the trial is to  resolve disputes about  questions of facts. Did the defendant rob the bank? Who is the father of the child? Did this drug cause birth defects? 
To answer these questions, the litigants will present evidence of different kinds: eyewitness testimonies, DNA matches, epidemiological studies, etc. The evidence presented will often be in conflict with other evidence. In a bank robbery case, for example, the prosecution may present eyewitness testimony that the defendant was seen driving a truck  near the bank a few minutes after the robbery took place.  The defense may respond that no traces were found at the crime scene that would match the defendant.  The fact-finders, judges or lay jurors, should address these conflicts by assessing and weighing the evidence,  and on that basis reach a final decision. This is a difficult task. The evidence presented at trial can be complex and open to multiple interpretations, and even when it is assessed carefully, it may still lead to an incorrect verdict.  How should judges and jurors respond to this uncertainty? 


From among the three perspectives mentioned in the beginning, I will focus on the probablistic approach and take it as my point of departure, for various reasons:

- The project is to be informed by and reflect on the actual practice of legal evidence evaluation, and much of  scientific evidence in such contexts  has probabilistic form.

- Probabilistic tools are fairly well-developed both for applications and within formal epistemology, reaching a state of fruition which I think should inspire deeper reflection.

- Statistical computing tools for such methods are avaialable, which makes programming development and preliminary computational and data-driven evaluation of the ideas to be defended a viable enterprise. 


Accoringly, the view in focus of this research is legal probabilism, according to which  probability theory helps to understand  the fallibility of trial decision-making, theorize about and improve upon the assessment of the evidence presented in court.   It  is an ongoing research program that comprises a variety of claims about evidence assessment and decision-making at trial. At its simplest, it comprises two core tenets: first, that the evidence presented at trial can be assessed, weighed and combined by means of probability theory; and second, that legal decision rules,  such as proof beyond a reasonable doubt in criminal cases, can be explicated in probabilistic terms. 


In the Middle Ages, before the advent of probability theory, 
there existed an informal mathematics 
of legal evidence [@wigmore1901number]. Formalistic procedures fixed the number of witnesses required to establish a claim. Lawyers would list  ways in which items of 
evidence could be added or subtracted to  weaken or strengthen one's case. This formalistic system fell into disrepute as the Enlightenment principle of `free proof' gained wide acceptance [@damaska1996free].   Concurrently, the development of probability theory brought forth a new approach to weighing evidence and making decisions under uncertainty. The early  theorists of probability in the 17th and 18th century were as much interested in games of chance as they were interested in the uncertainty of trial decisions  [@Hacking1984, @daston1988, @Franklin2001]. 
 @Bernoulli1713Ars-conjectandi  was  one 
of the first to formulate probabilistic rules for combining different pieces of evidence in legal cases and assessing to what extent they supported a claim of interest. 
 He was also one of the first to suggest that decision rules at trial could be understood 
as probability thresholds. 




Bernoulli's prescient insights attained greater popularity in the 20th century amidst the law and economics movement [@Calabresi1961, @becker1968crime, @Posner1973].  In a seminal article, @Finkelstein1970A gave one of the first systematic analyses of how probability theory, and Bayes' theorem in particular,  can help to weigh evidence at trial. @lempert1977modeling
was one of the first to rely on probability theory, specifically likelihood ratios, 
for assessing the relevance of evidence. Such contributions fueled what has been called the New Evidence  Scholarship, a rigorous way of studying the process of legal proof at trial [@Lempert1986]. After the discovery of DNA fingerprinting in the eighties, many legal probabilists focused on how probability theory could be used to quantify the strength of a DNA match under various circumstances [@kaye1986admissibility; @NRCI1992, @Robertson1995evidence;  @Koehler1996On-Conveying-th; Kaye2010The-Double-Heli].


In response to these developments, @tribe71 attacked what he called `trial by mathematics'.  His critique ranged from listing well-known cases of misuse or probabilities in legal contexts and practical difficulties in assessing the probability of someone's criminal or civil liability to the dehumanization of trial decisions.  After Tribe, many have criticized legal probabilism on a variety of grounds, both theoretical and practical, arguing 
that probabilistic  models are either inadequate or unhelpful [see, for instance, @Underwood1977The-thumb-on-th,cohen86; @brilmayer1986;  @dant1988gambling, @Allen1986A-Reconceptuali]. 

More recently, alternative frameworks for modeling evidential reasoning and decision-making at trial have been proposed. They are based on inference  to the best explanation [@Pardo2008judicial; @Allen2010No-Plausible-Al], narratives and stories [@Pennington1991;
@Allen1986A-Reconceptuali; @allen2001naturalized; @Allen2010No-Plausible-Al; @clermont2015TrialTraditionalProbability,@pardo2018], 
and argumentation theory [@gordon2007; @Walton2002; @bex2011ArgumentsStoriesCriminal].  Those who favor a conciliatory stance have combined  legal probabilism with  other frameworks, 
offering preliminary sketches of hybrid theories [@verheij2014catch, @urbaniak2018narration]. 


Some legal scholars and practitioners have voiced their support for legal probabilism explicitly [@Tillers2007]. Yet skepticism about mathematical and quantitative models of legal evidence is still widespread among prominent legal scholars and practitioners [see, for example, @allen2007problematic].
 Even among legal probabilists, 
few would think it possible to quantify precisely the probability of someone's guilt or civil liability. The probabilistic formalism---@taroni2006bayesian write---`should primarily be considered as an aid to structure and guide one's inferences under uncertainty, rather than a way to reach precise numerical assessments' (p. xv). 

Perhaps the most difficult  challenge for legal probabilism---at least, one that has galvanized philosophical attention in recent years---comes from the paradoxes of legal proof or puzzles of naked statistical evidence. In  a number of seminal papers, [@Nesson1979Reasonable-doub; @Cohen81; and @Thomson86]  formulated  scenarios in which, even if the probability of guilt or civil liability, based on the available evidence, is particularly high, a verdict against the defendant seems unwarranted.  Arguably, these scenarios underscore a theoretical difficulty with probabilistic accounts of legal standards of proof. Many articles have been written on the topic, initially by legal scholars. In the last decade, philosophers have also joined the debate, while interest among legal scholars has waned.^[For  critical surveys  see [@redmayne2008exploring] and [@gardiner2018,pardo2019].] Other conceptual challenges for legal probabilism include the so-called problem of conjunction and the reference class problem.











One important difficulty is that there are various thought experiments in which the probability of guilt is very high and yet conviciton or finding of liability is intuitively unjustified---these are known as  proof paradoxes [@Cohen1977The-probable-an; @redmayne2008exploring]. \todo{Add an example}


At least \emph{prima facie}, then, it seems that some conditions other than high posterior probability of liability have to be satisfied for the decision to penalize (or find liable) to be justified.  Accordingly, various informal notions  have been claimed to be essential for  a proper explication of judiciary decision standards [@wells1992naked; @haack2011legal]. For instance,  evidence is claimed to be insufficient for conviction if it is not \emph{sensitive} to the issue at hand: if it remained the same even if the accused was innocent [@enoch2015sense]. Or, to look at another approach,  evidence is claimed to be insufficient for conviction if it doesn't \emph{normically support} it: if---given the same evidence---no explanation would be needed even if the accused was innocent [@Smith_conviction_mind_2017].   A legal probabilist needs either to show that these notions are  unnecessary or inadequate for the purpose at hand, or to indicate how they can be explicated in probabilistic terms. 




\todo{describe what the probabilistic model is before we get deeper?}

Another point of criticism of the wider proabilist model, legal proceedings are back-and-forth between opposing parties in which  cross-examination is of crucial importance,  reasoning goes not only evidence-to-hypothesis, but also hypotheses-to-evidence [@wells1992naked; @allen2007problematic] in a way that seems analogous to inference to the best explanation [@dant1988gambling], which notoriously is claimed to not be susceptible to probabilistic analysis [@Lipton2004-LIPITT].   An informal philosophical account inspired by such considerations---The \textbf{No Plausible Alternative Story (NPAS)} theory [@Allen2010No-Plausible-Al]---is that the courtroom is a confrontation of competing narrations [@wagenaar1993anchored; @ho2008philosophy] offered by the sides, and  the narrative to be selected should be the most plausible one. The view is conceptually plausible [@di2013statistics], and  finds  support  in psychological evidence [@pennington1991cognitive; @pennington1992explaining]. 



It would be a great advantage of legal probabilism if it could to model phenomena captured by the narrative approach, but how is the legal probabilist to make sense of them?  From her perspective,  the key disadvantage of NPAS is that it abandons the rich toolbox of probabilistic methods and takes the key notion of plausibility to be a primitive notion which should be understood only intuitively. 
 
 
 
 
 
 
 ## Pioneering nature of the project
 

 
 Recent work in Artificial Intelligence made it possible to use probability theory---in the form of Bayesian networks---to weigh and assess complex bodies of evidence consisting of multiple components. 
 
 
 
 
Initial philosophical analysis of the approach has been performed, [@di2013statistics]  pioneering a probabilistic understanding of narrations. 

\todo{add more about Marcello}



## Choice of problem



## Pioneering nature \& impact



# Concept and work plan 

(general work plan, specific research goals,
results of preliminary research, risk analysis);

# Research methodology

(underlying scientific methodology, methods, techniques and research tools,
methods of results analysis, equipment and devices to be used in research);


# References



