% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  11pt,
  dvipsnames,enabledeprecatedfontcommands]{scrartcl}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Beyond Reverse Bayesianism:},
  pdfauthor={Marcello Di Bello and Rafal Urbaniak},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\documentclass{article}

% %packages
\usepackage{booktabs}
%\usepackage[left]{showlabels}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{ragged2e}
\usepackage{etex}
%\usepackage{yfonts}
\usepackage{marvosym}
\usepackage[notextcomp]{kpfonts}
\usepackage{nicefrac}
\newcommand*{\QED}{\hfill \footnotesize {\sc Q.e.d.}}
\usepackage{floatrow}
\usepackage{multicol}

\usepackage[textsize=footnotesize]{todonotes}
\newcommand{\ali}[1]{\todo[color=gray!40]{\textbf{Alicja:} #1}}
\newcommand{\mar}[1]{\todo[color=blue!40]{#1}}
\newcommand{\raf}[1]{\todo[color=olive!40]{#1}}

%\linespread{1.5}
\newcommand{\indep}{\!\perp \!\!\! \perp\!}


\setlength{\parindent}{10pt}
\setlength{\parskip}{1pt}


%language
%\usepackage{times}
\usepackage{mathptmx}
\usepackage[scaled=0.86]{helvet}
\usepackage{t1enc}
%\usepackage[utf8x]{inputenc}
%\usepackage[polish]{babel}
%\usepackage{polski}




%AMS
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{geometry}
 \geometry{a4paper,left=35mm,top=20mm,}


%environments
\newtheorem{fact}{Fact}


% allow page breaks in equations
\allowdisplaybreaks


%abbreviations
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}
\newcommand{\n}{\neg}
\newcommand{\et}{\wedge}
\newcommand{\jt}{\rightarrow}
\newcommand{\ko}[1]{\forall  #1\,}
\newcommand{\ro}{\leftrightarrow}
\newcommand{\exi}[1]{\exists\, {_{#1}}}
\newcommand{\pr}[1]{\ensuremath{\mathsf{P}(#1)}}
\newcommand{\ppr}[2]{\ensuremath{\mathsf{P}^{#1}(#2)}}
\newcommand{\cost}{\mathsf{cost}}
\newcommand{\benefit}{\mathsf{benefit}}
\newcommand{\ut}{\mathsf{ut}}

\newcommand{\odds}{\mathsf{Odds}}
\newcommand{\ind}{\mathsf{Ind}}
\newcommand{\nf}[2]{\nicefrac{#1\,}{#2}}
\newcommand{\R}[1]{\texttt{#1}}
\newcommand{\prr}[1]{\mbox{$\mathtt{P}_{prior}(#1)$}}
\newcommand{\prp}[1]{\mbox{$\mathtt{P}_{posterior}(#1)$}}



\newtheorem{q}{\color{blue}Question}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[fact]


%technical intermezzo
%---------------------

\newcommand{\intermezzoa}{
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}



	\tiny{\sc Optional Content Starts}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}\nopagebreak 
	}


\newcommand{\intermezzob}{\nopagebreak 
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}

	\tiny{\sc Optional Content Ends}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}
	}
	
	
%--------------------






















\newtheorem*{reply*}{Reply}
\usepackage{enumitem}
\newcommand{\question}[1]{\begin{enumerate}[resume,leftmargin=0cm,labelsep=0cm,align=left]
\item #1
\end{enumerate}}

\usepackage{float}

% \setbeamertemplate{blocks}[rounded][shadow=true]
% \setbeamertemplate{itemize items}[ball]
% \AtBeginPart{}
% \AtBeginSection{}
% \AtBeginSubsection{}
% \AtBeginSubsubsection{}
% \setlength{\emergencystretch}{0em}
% \setlength{\parskip}{0pt}






\usepackage[authoryear]{natbib}

%\bibliographystyle{apalike}



\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows}

\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Beyond Reverse Bayesianism:}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Awareness Growth in Bayesian Networks}
\author{Marcello Di Bello and Rafal Urbaniak}
\date{June 13, 2022}

\begin{document}
\maketitle

\begin{abstract}
We examine Steele and Stefánsson's counterexample to Reverse Bayesianism, a popular theory 
that addresses the problem of awareness growth for Bayesianism. We show that Steele and Stefánsson's counterexamples have limited applicability but agree with their skepticism toward Reverse Bayesianism. We strengthen their argument by providing a simpler counterexample that is less prone to objections. In addition, we submit that the problem of awareness growth cannot be tackled in an algorithmic manner because subject-matter assumptions need to be made explicit. Thanks to its ability to express probabilistic dependencies, we think that the theory of Bayesian networks can help to model awareness growth in the Bayesian framework. We offer some illustrations of this claim.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Learning is modeled in the Bayesian framework by the rule of
conditionalization. This rule posits that the agent's new degree of
belief in a proposition \(H\) after a learning experience \(E\) should
be the same as the agent's old degree of belief in \(H\) conditional on
\(E\). That is, \[\ppr{E}{H}=\pr{H \vert E},\] where \(\pr{}\)
represents the agent's old degree of belief (before the learning
experience \(E\)) and \(\ppr{E}{}\) represents the agent's new degree of
belief (after the learning experience \(E\)).

Both \(E\) and \(H\) belong to the agent's algebra of propositions. This
algebra models the agent's awareness state, the propositions taken to be
live possibilities. Conditionalization never modifies the algebra and
thus makes it impossible for an agent to learn something they have never
thought about. This forces a great deal of rigidity on the learning
process. Even before learning about \(E\), the agent must already have
assigned a degree of belief to any proposition conditional on \(E\).
This picture commits the agent to the specification of their `total
possible future experience' (Howson 1976, The Development of Logical
Probability), as though learning was confined to an `initial prison'
(Lakatos, 1968, Changes in the Problem of Inductive Logic).

But, arguably, the learning process is more complex than what
conditionalization allows. Not only do we learn that some propositions
that we were entertaining are true or false, but we may also learn new
propositions that we did not entertain before. Or we may entertain new
propositions---without necessarily learning that they are true or
false---and this change in awareness may in turn change what we already
believe. How should this more complex learning process be modeled by
Bayesianism? Call this the problem of awareness growth.

The algebra of propositions need not be so narrowly construed that it
only contains propositions that are presently under consideration. The
algebra may also contain propositions about which, even though they are
not the object of present consideration, the agent has already formed,
perhaps implicitly, a certain disposition to believe. But even this
expanded algebra will have to be revised sooner or later. The algebra of
propositions could in principle contain anything that could possibly be
conceived, expressed, thought of. Such rich algebra would not need to
change at any point, but this is hardly a plausible model of ordinary
agents with bounded resources such as ourselves.

Critics of Bayesianism and sympathizers alike have been discussing the
problem of awareness growth under different names for quite some time,
at least since the eighties. This problem arises in a number of
different contexts, for example, new scientific theories (Glymour, 1980,
Why I am not a Bayesian; Chihara 1987, Some Problems for Bayesian
Confirmation Theory; Earmann 1992, Bayes of Bust?), language changes and
paradigm shifts (Williamson 2003, Bayesianism and Language Change), and
theories of induction (Zabell, Predicting the Unpredictable).

A proposal that has attracted considerable scholarly attention in recent
years is Reverse Bayesianism (Karni and Viero, 2015, Probabilistic
Sophistication and Reverse Bayesianism; Wenmackers and Romeijn 2016, New
Theory About Old Evidence; Bradely 2017, Decision Theory with A Human
Face) . The idea is to model awareness growth as a change in the algebra
while ensuring that the proportions of probabilities of the propositions
shared between the old and new algebra remain the same in a sense to be
specified.

Let \(\mathcal{F}\) be the initial algebra of propositions and let
\(\mathcal{F}^+\) the algebra after the agent's awareness state has
grown. Both algebras contain the contradictory and tautologous
propositions \(\perp\) and \(\top\), and they are closed under
connectives such as disjunction \(\vee\), conjunction \(\wedge\) and
negation \(\neg\). Denote by \(X\) and \(X^+\) the subsets of these
algebras that contain only basic propositions, namely those without
connectives. \textbf{Reverse Bayesianism} posits that the ratio of
probabilities for any basic propositions \(A\) and \(B\) in both \(X\)
and \(X^+\)---the basic propositions shared by the old and new
algebra---remain constant through the process of awareness growth:
\[\frac{\pr{A}}{\pr{B}} = \frac{\ppr{+}{A}}{\ppr{+}{B}},\] where
\(\pr{}\) represents the agent's degree of belief before awareness
growth and \(\ppr{+}{}\) represents the agent's degree of belief after
awareness growth.

Reverse Bayesianism is an elegant theory that manages to cope with a
seemingly intractable problem. As the awareness state of an an agent
grows, the agent would prefer not to throw away completely the epistemic
work they have done so far. The agent may desire to retain as much of
their old degrees of beliefs as possible. Reverse Bayesianism provides a
simple recipe to do that. It also coheres with the conservative spirit
of conditionalization which preserves the old probability distribution
conditional on what is learned.

Unfortunately, Reverse Bayesianism is not without complications. Steele
and Stefánsson (2021, Belief Revision for Growing Awareness) argue that
Reverse Bayesianism, when suitably formulated, can work in a limited
class of cases, what they call \textit{awareness expansion}, but cannot
work for \textit{awareness refinement} (more on this distinction later).
Their argument rests on a number of ingenious counterexamples.

We share Steele and Stefánsson's skepticism toward Reverse Bayesianism,
but also believe that their counterexamples have limited applicability
(\S ~\ref{sec:counterexamples}). We strengthen their argument by
providing a simpler counterexample that is less prone to objections
(\S ~\ref{sec:better}). At the same time, we conjecture that the problem
of awareness growth cannot be tackled in an algorithmic manner because
subject-matter assumptions, both probabilistic and structural, need to
be made explicit. Thanks to its ability to express probabilistic
dependencies, we think that the theory of Bayesian networks can help to
model awareness growth in the Bayesian framework. We offer two
illustrations of this claim. First, we provide an example of awareness
growth refinement that it is structurally different from other cases of
refinement (\S ~\ref{sec:structural}). Second, we model two scenarios
from Anna Mathani, both intended to challenge Reverse Bayesianism
(\S ~\ref{sec:mathani}).

\hypertarget{counterexamples}{%
\section{Counterexamples}\label{counterexamples}}

In this section, we rehearse two of the ingenious counterexamples to
Reverse Bayesianism by Steele and Stefánsson. One example targets
awareness expansion and the other awareness refinement. We then show why
they make a limited case against Reverse Bayesianism and finally provide
a better counterexample with the aid of Bayesian networks.

\hypertarget{friends-and-movies}{%
\subsection{Friends and Movies}\label{friends-and-movies}}

\label{sec:counterexamples}

The difference between expansion and refinement is intuitively
plausible, but can be tricky to pin down formally. A rough
characterization will suffice here. Suppose, as is customary,
propositions are interpreted as sets of possible worlds, where the set
of all possible worlds is the possibility space. An algebra of
propositions thus interpreted induces a partition of the possibility
space. Refinement occurs when the new proposition added to the algebra
induces a more fine-grained partition of the possibility space.
Expansion occurs when the new proposition is inconsistent with the
existing ones, thus making the old partition no longer exhaustive.

The first counterexample by Steele and Stefánsson targets cases of
awareness expansion:

\begin{quote}
\textsc{Friends}: Suppose you happen to see your partner enter your best
friend's house on an evening when your partner had told you she would
have to work late. At that point, you become convinced that your partner
and best friend are having an affair, as opposed to their being warm
friends or mere acquaintances. You discuss your suspicion with another
friend of yours, who points out that perhaps they were meeting to plan a
surprise party to celebrate your upcoming birthday---a possibility that
you had not even entertained. Becoming aware of this possible
explanation for your partner's behaviour makes you doubt that she is
having an affair with your friend, relative, for instance, to their
being warm friends. (Steele and Stefánsson, 2021, Section 5, Example 2)
\end{quote}

\noindent Initially, the algebra only contains the hypotheses `my
partner and my best friend met to have an affair' (\textit{Affair}) and
`my partner and my best friend met as friends or acquaintances'
(\textit{Friends/acquaintances}). The other proposition in the algebra
is the evidence, that is, the fact that your partner and your best
friend met one night without telling you (\textit{Secretive}). Given
this evidence, \textit{Affair} is more probable than
\textit{Friends/acquaintances}:
\[\pr{\textit{Affair} \vert  \textit{Secretive} }> \pr{\textit{Friends/acquaintances} \vert \textit{Secretive}} \tag{>}.\]
When the algebra changes, a new hypothesis is added which you had not
considered before: your partner and your best friends met to plan a
surprise party for your upcoming birthday (\textit{Surprise}). Given the
same evidence, \textit{Friends/acquaintances} is now more likely than
\textit{Affair}:
\[\ppr{+}{\textit{Affair} \vert  \textit{Secretive} } < \ppr{+}{\textit{Friends/acquaintances} \vert \textit{Secretive}}. \tag{<}\]
This holds assuming that hypothesis \textit{Surprise} is more likely
than the hypothesis \textit{Affair}:
\[\ppr{+}{ \textit{Surprise} \vert \textit{Secretive}}> \ppr{+}{ \textit{Affair} \vert \textit{Secretive}},\]
and, in addition, that \textit{Surprise} implies
\textit{Friends/acquaintances}. After all, in order to prepare a
surprise party, your partner and best friend have to be at least
acquaintances.

The conjunction of (\(>\)) and (\(<\)) violates Reverse Bayesianism
since \textit{Friends/acquaintances} and \textit{Affair} are basic
propositions that do not contain any connectives. But, as Steele and
Stefánsson admits, Reverse Bayesianism can still be made to work with a
slightly different---though quite similar in spirit---condition, called
\textbf{Awareness Rigidity}: \[\ppr{+}{A \vert T^*}=\pr{A},\] where
\(T^*\) corresponds to a proposition that picks out, from the vantage
point of the new awareness state, the entire possibility space before
the episode of awareness growth. In our running example, the proposition
\(\neg\textit{Surprise}\) picks out the entire possibility space in just
this way. And conditional on \(\neg\textit{Surprise}\), the probability
of \textit{Affair} does not change. Thus,
\[\ppr{+}{\textit{Affair} \vert  \textit{Secretive} \& \neg\textit{Surprise} } > \ppr{+}{\textit{Friends/acquaintances} \vert \textit{Secretive} \& \neg\textit{Surprise}}. \]
Awareness Rigidity is satisfied. Reverse Bayesianism---the spirit of it,
not the letter---stands.

This is not the end of the story, however. Steele and Stefánsson offer
another counterexample that also works against Awareness Rigidity, this
time targeting a case of refinement:

\begin{quote}
\textsc{Movies}: ``Suppose you are deciding whether to see a movie at
your local cinema. You know that the movie's predominant language and
genre will affect your viewing experience. The possible languages you
consider are French and German and the genres you consider are thriller
and comedy. But then you realise that, due to your poor French and
German skills, your enjoyment of the movie will also depend on the level
of difficulty of the language. Since it occurs to you that the owner of
the cinema is quite simple-minded, you are, after this realisation, much
more confident that the movie will have low-level language than
high-level language. Moreover, since you associate low-level language
with thrillers, this makes you more confident than you were before that
the movie on offer is a thriller as opposed to a comedy.'' (Steele and
Stefánsson, 2021, Section 5, Example 3)
\end{quote}

\noindent Initially, you did not consider language difficulty. So you
assigned the same probability to the hypotheses \textit{Thriller} and
\textit{Comedy}. But learning that the owner is simple-minded made you
think that the level of linguistic difficulty must be low and the movie
most likely a thriller rather than a comedy (perhaps because thrillers
are simpler---linguistically---than comedies). So, against Reverse
Bayesianism, \textsc{Movies} violates the condition
\(\frac{\pr{\textit{Thriller}}}{\pr{\textit{Comedy}}}=\frac{\ppr{+}{\textit{Thriller}}}{\ppr{+}{\textit{Comedy}}}\).

The counterexample also works against Awareness Rigidity. It is not true
that
\(\pr{\textit{Thriller}}=\ppr{+}{\textit{Thriller} \vert \textit{Thriller}\vee \textit{Comedy}}\).
Note that this counterexample is a case of refinement. First, you
categorize movies by just language and genre, and then you add a further
category, level of difficulty. So the proposition which picks out the
entire possibility space should be the same before and after awareness
growth, for example, \(\textit{Thriller}\vee \textit{Comedy}\). In cases
of awareness growth by refinement, then, Awareness Rigidity mandates
that all probability assignments stay the same. But \textsc{Movies} does
not satisfy this requirement.

This is all well and good, but how strong of a counterexample is this?
Steele and Stefánsson consider an objection:

\begin{quote}It might be argued that our examples are not illustrative of \dots a simple growth in awareness; rather, our examples illustrate and should be expressed 
  formally as complex learning experiences, where first there is a growth in awareness, and then 
  there is a further learning event ... In this way, one could argue that the awareness-growth 
  aspect of the learning event always satisfies Reverse Bayesianism.
\end{quote}

\noindent  Admittedly, \textsc{Movies} can be split into two episodes.
In the first, you entertain a new variable besides language and genre,
namely the language difficulty of the movie. In the second episode, you
learn something you did not consider before, namely that the owner is
simple-minded. Could Reserve Bayesianism still work for the first
episode, but not the second? Steele and Stefánsson do not address this
question explicitly, but insist that no matter the answer both episodes
are instances of awareness growth. We agree with them on this point.
Awareness growth is both \textit{entertaining} a new proposition not in
the initial awareness state of the agent and \textit{learning} a new
proposition. Nonetheless, we should still wonder. Is the second episode
(learning something new) necessary for the counterexample to work
together with the first episode (mere refinement without learning)?

Suppose the counterexample did work only in tandem with an episode of
learning something new. If that were so, defenders of Reverse
Bayesianism or Awareness Rigidity could still claim that their theory
applies to a large class of cases. It applies to cases of awareness
refinement without learning and also to cases of awareness expansion.
For recall that the first putative counterexample featuring awareness
expansion---\textsc{Friends}---did not challenge Reverse Bayesianism
insofar as the latter is formulated in terms of its close cousin,
Awareness Rigidity. So the force of Steele and Stefánsson's
counterexamples would be rather limited.

\hypertarget{lighting}{%
\subsection{Lighting}\label{lighting}}

\label{sec:better}

We claim there is a more straightforward counterexample that only
depicts mere refinement without an episode of learning and that still
challenges Reverse Bayesianism and Awareness Rigidity. To see that this
is indeed the case, we propose to consider the following scenario:

\begin{quote}
\textsc{Lighting:} You have evidence that favors a certain hypothesis,
say a witness saw the defendant around the crime scene. You give some
weight to this evidence. In your assessment, that the defendant was seen
around the crime scene raises the probability that the defendant was
actually there. But now you wonder, what if it was dark when the witness
saw the defendant? You become a bit more careful and settle on this: if
the lighting conditions were good, you should still trust the evidence,
but if they were bad, you should not. Unfortunately, you cannot learn
about the actual lighting conditions, but the mere realization that it
\textit{could} have been dark makes you lower the probability that the
defendant was actually there, based on the same eveidence.
\end{quote}

\noindent This scenario is simpler because it consists of mere
refinement. You wonder about the lighting conditions but you do not
learn what they were.\footnote{Strictly speaking, you are learning that
  it is \emph{possible} that the lighting conditions were bad. However,
  you do not condition on the proposition `the lighting conditions were
  bad' or `the lighting conditions were good' as if you learned it with
  certainty, and thus you do not learn about the lighting conditions in
  the sense in which learning is understood in this paper.} Still, mere
refinement in this scenario challenges Reverse Bayesianism and Awareness
Rigidity. That this should be so is not easy to see. Fortunately, the
theory of Bayesian networks helps to see why.

A Bayesian network is a formal model that consists of a direct acyclic
graph (DAG) accompanied by a probability distribution. The nodes in the
graph represent random variables that can take different values. We will
use `nodes' and `variables' interchangeably. The nodes are connected by
arrows, but no loops are allowed, hence the name direct acyclic graph.
Bayesian networks are relied upon in many fields, but have not been used
to model awareness growth. We think instead they are a good framework
for this purpose. Awareness growth can be modeled as a change in the
graphical network---nodes and arrows are added or erased---as well as a
change in the probability distribution from the old to the new network.

To model \textsc{Lighting} with Bayesian networks, we start with this
graph, which is the usual hypothesis-evidence idiom:

\begin{center}\includegraphics[width=0.5\linewidth,height=0.3\textheight]{ReplyToSteeleStefansson2_files/figure-latex/heDAG-1} \end{center}

\noindent where \(H\) is the hypothesis node and \(E\) the evidence
node. If an arrow goes from \(H\) to \(E\), the probability distribution
associated with the Bayesian network should be defined by conditional
probabilities of the form \(\pr{E=e \vert H=h}\), where uppercase
letters represent the variables (nodes) and lower case letters represent
the values of these variables.\footnote{A major point of contention in
  the interpretation of Bayesian networks is is the meaning of the
  directed arrows. They could be interpreted causally---as though the
  direction of causality proceeds from the events described by the
  hypothesis to event described by the evidence---but they need not be.
  REFERENCES?} Since you trust the evidence, you think that the evidence
is more likely under the hypothesis that the defendant was present at
the crime scene than under the alternative hypothesis:
\[\pr{\textit{E=seen} \vert \textit{H=present}} > \pr{\textit{E=seen} \vert \textit{H=absent}}\]
The inequality is a qualitative ordering of how plausible the evidence
is in light of competing hypotheses. No matter the numbers, by the
probability calculus, it follows that the evidence raises the
probability of the hypothesis \textit{H=present}.

Now, as you wonder about the lighting conditions, the graph should be
amended:

\begin{center}\includegraphics[width=0.5\linewidth,height=0.1\textheight]{ReplyToSteeleStefansson2_files/figure-latex/lighting2DAG-1} \end{center}

\noindent where the node \(L\) can have two values, \textit{L=good} and
\textit{L=bad}. A plausible way to update your assessment of the
evidence is as follows:
\[\ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=good}} > \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=good}}\]
\[\ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{bad}} = \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=bad}}\]

\noindent Here is what you are thinking: if the lighting conditions were
good, you should still trust the evidence like you did before (first
line). But if the lighting conditions were bad, you should regard the
evidence as no better than chance (second line).

Should you now assess the evidence at your disposal---that the witness
saw the defendant at the crime scene---any differently than you did
before? The evidence would have the same value if the likelihood ratios
associated with it relative to the competing hypotheses were the same
before and after awareness growth:
\[\frac{\pr{E=e \vert H=h}}{\pr{E=e \vert H=h'}}= \frac{\ppr{+}{E=e \vert H=h}}{\ppr{+}{E=e \vert H=h'}} \tag{C}.\]
In changing the probability function from \(\pr{}\) to \(\ppr{+}{}\), it
would be quite a coincidence if (C) were true. In our example, many
possible probability assignments violate this equality. If before
awareness growth you thought the evidence favored the hypothesis
\textit{H=present} to some extent, after the growth in awareness, the
evidence is likely to appear less strong.\footnote{By the law of total
  probability, the right hand side of the equality in (C) should be
  expanded, as follows:
  \[\frac{\ppr{+}{E=e \vert H=h}}{\ppr{+}{E=e \vert H=h'}}=\frac{\ppr{+}{\textit{E=seen} \wedge \textit{L=good} \vert \textit{H=present}}+\ppr{+}{\textit{E=seen} \wedge \textit{L=bad} \vert \textit{H=present}}}{\ppr{+}{\textit{E=seen} \wedge \textit{L=good} \vert \textit{H=absent}}+\ppr{+}{\textit{E=seen} \wedge \textit{L=bad} \vert \textit{H=absent}}}.\]
  For concreteness, let's use some numbers:
  \[\pr{\textit{E=seen} \vert \textit{H=present}}=\ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=good}}=.8\]
  \[\pr{\textit{E=seen} \vert \textit{H=absent}}=\ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=good}}=.4\]
  \[\ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=bad}} = \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=bad}}=.5.\]
  \[\ppr{+}{\textit{L=bad}} = \ppr{+}{\textit{L=good}}=.5.\] So the
  ratio
  \(\frac{\pr{\textit{E=seen} \vert \textit{H=present}}}{\pr{\textit{E=seen} \vert \textit{H=absent}}}\)
  equals \(2\). After the growth in awareness, the ratio
  \(\frac{\ppr{+}{\textit{E=seen} \vert \textit{H=present}}}{\ppr{+}{\textit{E=seen} \vert \textit{H=absent}}}\)
  will drop to \(\frac{.65}{.45}\approx 1.44\). The calculations here
  rely on the dependency structure encoded in the Bayesian network (see
  starred step below). \begin{align*}
  \ppr{+}{\textit{E=seen} \vert \textit{H=present}} &= \ppr{+}{\textit{E=seen} \wedge \textit{L=good} \vert \textit{H=present}}+\ppr{+}{\textit{E=seen} \wedge \textit{L=bad} \vert \textit{H=present}}\\
  &= \ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=good}}  \times \ppr{+}{\textit{L=good} \vert  \textit{H=present} }\\ & +\ppr{+}{\textit{E=seen}  \vert \textit{H=present} \wedge \textit{L=bad}} \times \ppr{+}{\textit{L=bad} \vert  \textit{H=present}}\\
  &=^* \ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=good}}  \times \ppr{+}{\textit{L=good}}\\ & +\ppr{+}{\textit{E=seen}  \vert \textit{H=present} \wedge \textit{L=bad}} \times \ppr{+}{\textit{L=bad}}\\
  &= .8 \times .5 +.5 *.5 = .65 
  \end{align*} \begin{align*}
  \ppr{+}{\textit{E=seen} \vert \textit{H=absent}} &= \ppr{+}{\textit{E=seen} \wedge \textit{L=good} \vert \textit{H=absent}}+\ppr{+}{\textit{E=seen} \wedge \textit{L=bad} \vert \textit{H=absent}}\\
  &= \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=good}}  \times \ppr{+}{\textit{L=good} \vert  \textit{H=absent} }\\ & +\ppr{+}{\textit{E=seen}  \vert \textit{H=absent} \wedge \textit{L=bad}} \times \ppr{+}{\textit{L=bad} \vert  \textit{H=absent}}\\
  &=^* \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=good}}  \times \ppr{+}{\textit{L=good}}\\ & +\ppr{+}{\textit{E=seen}  \vert \textit{H=absent} \wedge \textit{L=bad}} \times \ppr{+}{\textit{L=bad}}\\
  &= .4 \times .5 +.5 *.5 = .45 
  \end{align*} This argument can be repeated with many other numerical
  assignments.}

Why does this matter? We have seen that, after awareness growth, you
should typically regard the evidence \textit{E=seen} as one that favors
\textit{H=present} less strongly. Since the prior probability of the
hypothesis should be the same before and after awareness growth, it
follows that
\[\ppr{+}{\textit{H=present} \vert \textit{E=seen}} \neq \pr{\textit{H=present} \vert \textit{E=seen}}.\]
This outcome violates Awareness Rigidity. For recall that in cases of
refinement, Awareness Rigidity requires that the probability of basic
propositions stay fixed.

Reverse Bayesianism is also violated. For example, the ratio of the
probabilities of \textit{H=present} to \textit{E=seen}, before and after
awareness growth, has changed:
\[\frac{\ppr{\textit{E=seen}}{\textit{H=present}}}{\ppr{ \textit{E=seen}}{\textit{E=seen}}} \neq \frac{\ppr{+, \textit{E=seen}}{\textit{H=present}}}{\ppr{+, \textit{E=seen}}{\textit{E=seen}}},\]
where \(\ppr{\textit{E=seen}}{}\) and \(\ppr{+, \textit{E=seen}}{}\)
represent the agent's degrees of belief, before and after awareness
growth, updated by the evidence \(\textit{E=seen}\).

Unlike \textsc{Movies}, the counterexample \textsc{Lighting} works even
though it only depicts a case of awareness growth that consists in
refinement without learning. Defenders of Reverse Bayesianism and
Awareness Rigidity can no longer claim that their theories work when
awareness growth is not intertwined with learning. So, Steele and
Stefánsson's critique of these theories sits now on a firmer ground.

\hypertarget{structural-assumptions}{%
\section{Structural assumptions}\label{structural-assumptions}}

Besides counterexamples that can be leveled against Reverse Bayesianism,
we think there is a more general lesson to be learned. It has to do with
the importance of formalizing structural assumptions and the role of
Bayesian networks in modeling awareness growth. We substantiate this
point with two illustrations. The first shows that the distinction
between refinement and expansion that Steele and Stefánsson rely on is
actually more fine-grained. The second illustration draws on some recent
scenarios formulated by Anna Mathani.

\hypertarget{another-refinement}{%
\subsection{Another refinement}\label{another-refinement}}

\label{sec:structural}

Steele and Stefánsson's argument relies on the distinction between cases
of awareness and cases of expansion. Both \textsc{Movies} and
\textsc{Lighting} are cases of refinement, and they both violate Reverse
Bayesianism. \textsc{Friends}, instead, is a case of expansion and does
not violate Reverse Bayesian (understood as Awareness Rigidity). But
this categorization is too simple. As we shall see, not all cases of
refinement are the same, and it is important to understand and to be
able to model the structural differences that may arise between them. To
illustrate what is at issue, consider this variation of the
\textsc{Lighting} scenario:

\begin{quote}
\textsc{Veracity}: A witness saw that the defendant was around the crime
scene and you initially took this to be evidence that the witness was
actually there. But then you worry that the witness might be lying or
misremembering what happened. Perhaps, the witness was never there, made
things up or mixed things up. But despite that, you do not change
anything of your initial assessment of the evidence.
\end{quote}

\noindent   The rational thing to do here is to stick to your guns and
not change your earlier assessment of the evidence. Why should that be
so? And what is the difference with \textsc{Lighting}? Once again,
Bayesian networks prove to be a good analytic tool here.

The graphical network should initially look like the initial DAG for
\textsc{Lighting}. But, as your awareness grows, the graphical network
should be updated:

\begin{center}\includegraphics[width=0.5\linewidth,height=0.3\textheight]{ReplyToSteeleStefansson2_files/figure-latex/veracityDAG-1} \end{center}

\noindent As before, the hypothesis node \(H\) bears on the whereabouts
of the defendant and has two values, \textit{H=present} and
\textit{H=absent}. Note the difference between \(E\) and \(R\). The
evidence node \(E\) bears on the visual experience had by the witness.
The reporting node \(R\), instead, bears on what the witness reports to
have seen. The chain of transmission from `visual experience' to
`reporting' may fail for various reasons, such as lying or
misremembering.

Even if \textsc{Veracity} is a case of refinement, the old and new
probability functions agree with one another completely. The conditional
probabilities, \(\pr{E=e \vert H=h}\) should be the same as
\(\ppr{+}{E=e \vert H=h}\) for any values \(e\) and \(h\) of the
variables \(H\) and \(E\) that are shared before and after awareness
growth. In comparing the old and new Bayesian network, this equality
falls out from their structure, as the connection between \(H\) and
\(E\) remains unchanged. Thus, Reverse Bayesianism and Awareness
Rigidity are perfectly fine in scenarios such as
\textsc{Veracity}.\footnote{ This does not mean that the assessment of
  the probability of the hypothesis \textit{H=present} should undergo no
  change. If you worry that the witness could have lied, shouldn't this
  still make you less confident about \textit{H=present}? Surely so. But
  \textsc{Veracity} can be interpreted as a scenario in which an episode
  of awareness refinement takes place together with a form of
  retraction. At first, after the learning episode, you update your
  belief based on the \textit{visaul experience} of the witness. But
  after the growth in awareness, you realize that your learning is in
  fact limited to what the witness \textit{reported} to have seen. This
  change is first modeled as a case of refinement: an additional arrow
  is added from the evidence node \(E\) to the new node \(R\). Beside
  refinement, the previous learning episode is retracted and replaced by
  a more careful statement of what you learned: instead of conditioning
  on \textit{E=seen}, you should condition on what the witness reported
  to have seen, \textit{R=seen-reported}. This retraction will affect
  the probability you assign to the hypothesis
  \textit{H=present}---since
  \(\pr{\textit{H=present}\vert \textit{E=seen}}\neq \pr{\textit{H=present}\vert \textit{R=seen-reported}}\)---but
  this does not conflict with Reverse Bayesianism.}

Where does this leave us? Reverse Bayesianism is fine in scenarios like
\textsc{Veracity}, but fails in scenarios like \textsc{Lighting}. Why is
that so? In one scenario, the visual experience could have occurred
under good or bad lighting conditions; in the other, the visual
experience could have been reported truthfully or untruthfully. The two
scenarios are structurally different, and this difference can be
appreciated by looking at the Bayesian networks used to model them. In
\textsc{Veracity}, the new node is added downstream. Since the
conditional probabilities associated with the upstream nodes are
unaffected, Reverse Bayesianism is satisfied. By contrast, in
\textsc{Lighting}, the new node is added upstream. Since the conditional
probabilities associated with the downstream node will often have to
change, Reverse Bayesianism fails here.

This discussion suggests a conjecture: structural features about how we
conceptualize a specific scenario seems to be the guiding principles
about how we update the probability function through awareness growth,
not a formal principle like Reverse Bayesianism. We further elaborate on
this conjecture by drawing on some examples from Anna Mathani.

\hypertarget{mathanis-counterexamples}{%
\subsection{Mathani's counterexamples}\label{mathanis-counterexamples}}

\label{sec:mathani}

Mahtani offers two counterexamples to Reverse Bayesianism. The first
goes like this:

\begin{quote}
\textsc{Tenant}: Suppose that you are staying at Bob's flat which he shares with his landlord. You know
that Bob is a tenant, and that there is only one landlord, and that this landlord also
lives in the flat. In the morning you hear singing coming from the shower room, and
you try to work out from the sounds who the singer could be. At this point you have
two relevant propositions that you consider possible ... $Landlord$ standing for the possibility that the landlord is the singer, and $Bob$ standing for the possibility that Bob is the singer  \dots  Because you know that Bob is a tenant in the flat, you also have a credence in the proposition $Tenant$ that the singer is a tenant. Your credence in $Tenant$ is the same as your credence in $Bob$, for given your state of awareness these two propositions are equivalent ... Now let's suppose the possibility suddenly occurs to you that there might be another tenant living in the same flat, and that perhaps that is the person singing in the shower ($Other$).
\end{quote}

\noindent Initially, you thought the singer could either be the landlord
or Bob, the tenant. Then you come to the realization that a third person
could be the singer, another tenant. Before awareness growth, that Bob
is in the shower and that a tenant is in the shower are equivalent
descriptions. After awareness growth, this equivalence breaks down.

Why is this scenario problematic? Suppose, after you hear singing in the
shower, you become sure someone is in there, but you cannot tell who. So
\(\pr{Landlord} = \pr{Bob} = 1/2\), and since \(Bob\) and \(Tenant\) are
equivalent, also \(\pr{Tenant}\) = 1/2. Now, \(Landlord\), \(Bob\) and
\(Tenant\) are all basic propositions that you were originally aware of,
and thus Reverse Bayesianisn requires that their assigned probabilities
should remain in the same proportion after your awareness grows. But
note that \(Other\) entails \(Tenant\) and \(Bob\) and \(Other\) are
disjoint, so it follows that \(\ppr{+}{Other}\) must have zero
probability.\footnote{If \(\ppr{+}{Other}>0\), either the proportion of
  \(Tenant\) to \(Landlord\) or the prportion of \(Bob\) to \(Landlord\)
  should change.} This is an undesired outcome that rules out the
possibility that there could be a third person in the shower.\footnote{Awareness
  Rigidity is no of help either because it would require that
  \(\ppr{+}{Landlord \vert Landlord \vee Tenant}=\ppr{+}{Bob \vert Landlord \vee Tenant}\)
  both equal \(1/2\), thus forcing
  \(\ppr{+}{Other \vert Landlord \vee Tenant}\) to zero.}

Consider now Mathani's second counterexample:

\begin{quote} 
\textsc{Coin}: You know that I am holding a fair ten pence UK coin which I am about to toss. You
have a credence of 0.5 that it will land $Heads$, and a credence of 0.5 that it will
land $Tails$. You think that the tails side always shows an engraving of a lion. So you
also  have a credence of 0.5 that ($Lion$) it will land with the lion engraving face-up: relative to your state of awareness $Tails$ and $Lion$ are equivalent.... Now let's suppose that you somehow become aware
that occasionally ten pence coins have .... an engraving of Stonehenge on the tails side. 
\end{quote}

\noindent  \(Tails\) and \(Lion\) are equivalent propositions prior to
awareness growth. Suppose you initially gave \(Tails\) and \(Lion\) the
same credence. Since they are both basic propositions, Reverse
Bayesianism requires that their relative proportions should stay the
same after awareness grow. The same applies to \(Heads\) and \(Tails\).
But since \(Lion\) and \(Stonehenge\) are incompatible and the latter
entails \(Tails\), you should have \(\ppr{+}{Stonehenge} = 0\), again an
undesirable conclusion.

Mathani notes that \textsc{Coin} has the same structure as
\textsc{Tenant}. This is true to some extent, but there is also an
interesting asymmetry between the two scenarios. In \textsc{Tenant}, it
is natural to assign \(1/3\) to \(Landlord\), \(Bob\) and \(Other\)
after awareness growth. That someone is singing in the shower is
evidence that someone must be in there, but without any more
discriminating evidence, each person should be assigned the same
probability. Consequently, a probability of 2/3 should be assigned to
\(Tenant\). On this picture, the proportion of \(Landlord\) to
\(Tenant\) changes from 1:1 (before awareness growth) to 1:2 (after
awareness growth). But, in \textsc{Coin}, the relative proportion of
\(Heads\) to \(Tails\) should remain constant throughout, unless
evidence emerges that the coin is not fair. One might have expected that
\(Landlord\) and \(Tenant\) would behave just like \(Heads\) and
\(Tails\), but actually they do not.

Bayesian networks can help to model the asymmetry between these two
scenarios. Consider \textsc{Coin} first. The structure of the scenario
is represented by the following graph:

\begin{center}\includegraphics[width=0.5\linewidth,height=0.3\textheight]{ReplyToSteeleStefansson2_files/figure-latex/tailsDAG-1} \end{center}

\noindent The upstream node \(Outcome\) has two states, \(tails\) and
\(heads\). These two states remain the same throughout. What changes are
the states associated with the \(Imagine\) node downstream. Before
awareness growth, the node \(Image\) has two states: \(lions\) and
\textit{heads-image}.\footnote{The heads side must have some imagine,
  not specified in the scenario.} You assume that \(Image=lions\) is
true if and only if \(Outcome=tails\) is true. Then, you come to the
realization that the imagines for tails include a lion or a stonehenge
engraving. So, after awareness growth, the node \(Image\) contains three
states: \(lion\), \(stonehenge\) and \textit{heads-image}. Consider now
the other scenario, \textsc{Tenant}. We start with the following graph:

\begin{center}\includegraphics[width=0.5\linewidth,height=0.3\textheight]{ReplyToSteeleStefansson2_files/figure-latex/tenantsDAG-1} \end{center}

\noindent Initially, the upstream node \(Person\) has two possible
states, representing who is in the bathroom singing:
\textit{landlord-person} and \(bob\). To simplify things, the assumption
here is that the evidence of singing has already ruled out the
possibility that no one would be in the shower. The downstream node
\(Role\) has also two values, \(landlord\) and \(tenant\). After your
awareness grows, the upstream node \(Person\) should now have one more
possible state, \(other\).

The difference in modeling the two scenarios is this. In \textsc{Coin},
the states of the upstream node remain fixed, whereas in
\textsc{Tenant}, they change. After awareness growth, no new state is
added to \(Outcome\), but an additional state, \(other\), is added to
\(Person\). Plausible probability distributions for the Bayesian
networks associated with the two scenarios are displayed in Table
\ref{table:coin-tenant}. How the networks should be built and which
probabilities should shift is based on our background knowledge. This
knowledge tells us that the equiprobability of \(heads\) and \(tails\)
should not be affected by realizing that \(stonhenge\) is another
possible engraving for the tails side. It also tells us that the
probabilities of \(landlord\) and \(tenant\) should be affected by
realizing that a third person could be in the shower.

We conclude with some programmatic remarks. We think that the awareness
of agents grows while holding fixed certain material structural
assumptions, based on commonsense, semantic stipulations or causal
dependency. To model awareness growth, we need a formalism that can
express these material structural assumptions. This can done using
Bayesian networks, and we offered some illustrations of this strategy.
These material assumptions also guide us in formulating the adequate
conservative constraints, and these will inevitably vary on a
case-by-case basis. The literature on awareness growth from a Bayesian
perspective is primarily concerned with a formal, almost algorithmic
solution to the problem. Insofar as Reverse Bayesianism is an expression
of this formalistic aspiration, we agree with Steele and Stefánsson that
we are better off looking elsewhere.

\begin{table}
\begin{tabular}{clcc}
$\pr{Image \vert Outcome}$ & & \multicolumn{2}{c}{$Outcome$} \\
 &   & $heads$ & $tails$ \\
\multirow{2}{*}{$Image$} & $lion$ & 0 & 1\\
& \textit{heads-image} & 1 & 0 \\
\hline
\hline
$\ppr{+}{Image \vert Outcome}$ & & \multicolumn{2}{c}{$Outcome$} \\
&  & $heads$ & $tails$ \\
\multirow{3}{*}{$Image$} & $lion$ & 0 & 1/2\\ 
& $stonehenge$ & 0 & 1/2 \\
& \textit{heads-image} & 1 & 0 \\
\hline
\hline
$\pr{Outcome}=\ppr{+}{Outcome}$ & \multicolumn{2}{c}{$Outcome$} & \\
&  $heads$ & $tails$ & \\
& 1/2 & 1/2 & \\
\end{tabular}


\begin{tabular}{clccc}
&&&&\\
&&&&\\
$\pr{Role \vert Person}$ & & \multicolumn{3}{c}{$Person$} \\
 &   & \textit{landlord-person}  & \multicolumn{2}{c}{$bob$} \\
\multirow{2}{*}{$Role$} & $tenant$ & 0 & \multicolumn{2}{c}{1}\\
& $landlord$  & 1 & \multicolumn{2}{c}{0} \\
\hline
\hline
$\ppr{+}{Role \vert Person}$ & & \multicolumn{3}{c}{$Person$} \\
&  & \textit{landlord-person} & $bob$ & $other$ \\
\multirow{2}{*}{$Role$} & $tenant$ & 0 & 1/2 & 1/2\\ 
& $landlord$ & 1 & 0 & 0 \\
\hline
\hline
$\pr{Person}$ & \multicolumn{2}{c}{$Person$} & \\
&  \textit{landlord-person} & $bob$ & \\
& 1/2 & 1/2 & \\
\hline
\hline
$\ppr{+}{Person}$ & \multicolumn{2}{c}{$Person$} & \\
&  \textit{landlord-person} & $bob$ & $other$ \\
& 1/3 & 1/3 & 1/3 \\
\end{tabular}
\caption{Top table displays a plausible probability distribution for \textsc{Coin} and bottom table does the same for \textsc{Tenant}.}
\label{table:coin-tenant}
\end{table}

\end{document}
