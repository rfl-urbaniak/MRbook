---
title: "Unanticipated possibilities"
author: "Marcello/Rafal"
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        - style.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 11pt
documentclass: scrartcl
urlcolor: blue
bibliography: [/Users/mdibello/Desktop/Book-Legal-Prob/references/referencesMRbook.bib]
csl: [/Users/mdibello/Desktop/Book-Legal-Prob/references/apa-6th-edition.csl]
indent: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The problem

## Basic idea

How does the Bayesian framework deal with the problem of unanticipated possibilities? 

- There seems to be two sides to this problem:

    a. The space of possibilities could get larger, say, first one considers only two suspects, Michael and Nero, and then a third suspect, Tito, becomes plausible. The priors of M and N were initially 50:50 (or 75:25, whatever), and T got nothing. Actually, T was not in the algebra to begin with. But, now T is in the algebra and gets a non-zero probability.  How can this be handled in the Bayesian framework?

    b. The space of possibilities becomes more fine-grained. Consider a DNA evidence case. At first, one of the hypothesis is that the defendant is the source of the crime stain. But as the investigation proceeds, it becomes clear that it makes a difference whether the defendant is the source of the crime stains on the pillow or on the sofa. Perhaps, the stain on the pillow were left innocently, but the stains on sofa were left by the perpetrator. So the space of possibilities becomes more fine-grained. 

- Not sure if the two items above are actually different, but they seem to be. In the first, the possibilities expand. In the second, the possibilities become more fine-grained. The first can be represented by adding a possible world, while the second, by making the equivalence classes among possible world more fine grained. 


- We need to be careful about what we are trying to represent here. Say we are trying to represent what is going on in the jurors' or judges' minds as they hear evidence in the trial and they process the evidence. We are trying to give a plausible picture of the processing of the evidence and hypotheses at stake. 

- Are we trying to give an empirically adequate picture or just a philosophically or theoretically plausible picture?

## Relevant literature

### Ronald Allen


Quotations from Allen's The Nature of Juridical Proof: Probability as a Tool in Plausible Reasoning:


\begin{quote}
The decision-makers in all legal systems must learn about the issue being litigated, whether this is through serial hearings as in some continental systems or concentrated trials in the American tradition ... in no system does the fact-finder have substantial knowledge about the case until evidence is presented. Obviously. That means, though, that it is literally impossible to form a probability space of mutually exclusive hypotheses, to assign them initial probabilities. (p. 138)

Some of you may think that I overstate the case and that the mutually exclusive hypotheses are
obvious: guilt or innocence or liability or no liability ... Guilt, innocence, liability and no liability are not factual hypotheses; they are legal conclusions that are applied to characterise the implications of the facts that are found. It should be obvious that one cannot condition 'guilt' on some evidence without going through a factual proposition that is the actual object of proof. (p. 138)

Complicating matters further is the \textbf{emergence of new theories} ... Like society itself, the
litigation process is dynamic, not static, and frequently new theories will emerge. When they do, the
probability space must be reconfigured. It is thus pointless to configure the probability space even if it
could be done at some point prior to receiving all the evidence and hearing all the arguments. (p. 139)

And finally, the \textbf{spectre of Old Evidence} raises its head. After the theories of the parties are finally identified and the logical space defined, initial assignments of probability must be made in order for Bayes' Theorem to be employed. But those initial assignments of probability will obviously include
consideration of the trial evidence, as well as pre-existing knowledge of the fact-finder. Once the
probability space is determined and initial probabilities assigned, all the trial evidence is old evidence
that has already been accommodated. There is no work for Bayes' Theorem to do. (p. 139)

What, then, are the \textbf{tasks for the future}? ... For the legal
analysts, there are two issues that should preoccupy us going forward. The first is the perennial problem
of constructing methods to \textbf{decrease the probability that unreliable evidence is being provided} by
purported experts and scientists ... that means engaging with the underlying forensic science to ensure that appropriate methodologies are employed and that the evidence offered at trial does not overleap what those methodologies can deliver. The second is the more fundamental problem noted above of \textbf{integrating that evidence with the rest of the evidence} in the case ... What that means in turn, of course, is to flesh out the nature of rational thought ... The discrete question to be addressed is the implications of rational thought writ large for the legal system. (p. 141) 
\end{quote}


Allen formulates several related challenges:

1. The issues being litigated emerge as the presentation of the evidence proceeds. The Bayesian framework instead assumes that the algebra of propositions (=what is at issue) is defined from the start, which is not a realistic depiction of what happens during a trial. 

2. The issues being litigated cannot simply be guilt/innocent. There are several other, more specific propositions that are the actual object of proof.

3. New theories emerge, for example, by expanding the space of possibilities (say a new suspect could be the perpetrator) or by refining existing possibilities. 

4. The probability space is defined by the evidence that is presented at trial, so all evidence becomes old evidence and its probability must be one. 


### Chihara


### Steele and Stefansson 2021: awareness growth

- They set up the problem as both one of expansion of of 
the space of possibilities and refinement of possibilities. 

- They cite economists and philosophers---in particular the paper Reverse Bayesianism 
by Karni and Vierø, as well as work by Bradley and Romeijn. The idea of reverse 
Bayesianism is simple. For any pair of propositions $A, B$ that are 
basic and inconsistent, their probability ratio in the old algebra, $P(A)/P(B)$, 
should be the same in the new algebra, so $P(A)/P(B)=P^+(A)/P^+(B)$, where $P^+$ is a probability function 
over the expanded or refined algebra of propositions. This is a sort of conservative principle.


- They provide a counterexample of reverse Bayesianism:

> Example 2: Suppose you happen to see your partner enter your best friend’s house on an evening when your partner had told you she would have to work late. At that point, you become convinced that your partner and best friend are having an affair, as opposed to their being warm friends or mere acquaintances. You discuss your suspicion with another friend of yours, who points out that perhaps they were meeting to plan a surprise party to celebrate your upcoming birthday—a possibility that you had not even entertained. Becoming aware of this possible explanation for your partner’s behaviour makes you doubt that she is having an affair with your friend, relative, for instance, to their being warm friends. 9p. 

- The two initial hypotheses, 'affair' and 'friends', are say at a ratio of 2:1 given the evidence available. But the introduction of a third hypothesis 'surprise birthday planning' (possibly a refinement of 'friends'), should plausibly reverse the ratio to 1:2. Counterexample to reverse Bayesianism is false.  Interestingly, this fits  with legal cases and seems an example of  shifts in plausibility between hypotheses. 

- (Idea of explaining away seems also related. The presumably compelling evidence that they met without telling you suggest they were having an affair, but the hypothesis that they were for planning a surprise birthday explain away all that evidence. )

- In Sec. 6, they offer the following diagnosis of what is happening. "What happens in these cases is that the ‘new’ propositions that the agent comes to be aware of change the way she comprehends the ‘old’ propositions, in particular, how these propositions relate to other propositions."

- They offer an "Awareness Rigidity" that might seem better than unconstrained reverse Bayesianism, that is, $P^+(A \vert T^*)=P(A)$, where $T^*$ tracks what was a tautology in the old possibility space. So, for example, if the old possibility space had only two options, F and V, then $F\vee V$ would be $T^*$, but this need not be a tautology in the new possibility space as, another possibility might been added. 

- "The problem is that Awareness Rigidity entails Reverse Bayesianism in cases where awareness grows by refinement (since it effectively requires that the probabilities for all propositions in the old awareness context remain unchanged). We previously argued that Reverse Bayesianism is not plausible even in cases of refinement."

- "Awareness Rigidity might nevertheless be compelling for awareness growth by expansion. After all, when awareness grows by expansion, the new tautology contains possibilities that it did not contain before, and hence, Awareness Rigidity does not entail Reverse Bayesianism in cases of expansion. So, perhaps Awareness Rigidity is plausible when it comes to expansion, even though Reverse Bayesianism is not."

- So their overall take is  negative. There is no clear formal theory of what to do when the space of possibilities changes. 


## Mathani 2020: Awareness Growth and Dispositional Attitudes

- She gives a counterexample to reverse Bayesianiam, different from 
Steele and Stefansson's counterexample. 

- Example 1: You are in a flat where you think other two people live: the landlord and another tenant (Bob). Then, you hear singing in the shower and assume it must be one of the two. Note that here Tenant=Bob. But then, you start considering that, perhaps, there is another tenant besides Bob, so now Tenant could mean Bob or Other.  

- Intuitive outcome (which a modified version of reverse Bayesianism by Steele and Stefansson seems to capture): The probability ratio of Bob and Landlord should not change (say 1:1) even though the probability ratio of Landlord and Tenant does change. Initially, you thought there were just two people, one tenant (Bob) and the landlord, equally likely to be in the shower. Now you think that there could be two tenants, besides the landlord, all equally likely to be in the shower. So, intuitively, you want to say it is 1/3 probability for Landlord, Bob and Other. 

- Example 2: You toss a fair coin, heads and tails with probability ratio 1:1.  You assume that Lion=Tails. You make that identification because you think that Tails is just heads of a lion. But then you consider the possibility that Tails could carry soemthing else, such as Stonehenge. 

- Intuitive outcome (which a modified version of reverse Bayesianism by Steele and Stefansson DOES NOT seem to capture): The probability ratio of Heads/Tails should stay fixed.   But the relative probability of Heads and Lion should change, even though initially it was 1:1. 

- Note how Example 1 differs from Example 2. In the Example 1, Landlord and Bob have fixed probability ratio, not Landlord and Tenant. In Example 2, instead, Heads and Tails have fixed probability ratio, not Heads and Lions. How to explain that? The difference is that, in one case, you keep fixed the ratio of coarse-grained proposition (heads/tails) while in the other you keep foxed the ratio of fine=grained proposition (Landlords/Bob). What explains the asymmetry?

- Mathani invoked underlying attitudes that Bayesianism cannot capture. But she admits her solution is tentative (end of section 3). She does not fully explain what is going on here. Steele and Stefansson would say these are cases of refinement and there is no clear theory about how to handle that. They think reserve Bayesianian holds only for expansion, not refinement. So there seems to be no settled theory here. 

- Possible move. There is an underlying causal structure that is different in the two examples. Example 1: key structural intuition is that each person is equally likely to be the shower. The unit  of analysis are individual people, not the categorization landlord/tenant. So you need to represent this causally with Bayesian network. Example 2: the unit of analysis is heads/tails. Bayesian network and their underlying causal structure seems an apt representation for the difference between the two examples. 



## Wenmackers and Romeijin 2016: New Theory about Old Evidence

- They use a catch-all approach. Same approach is also used by Zabell (urn with a mutator) and Verheij. 

- Not sure their approach can be used. 

- They describe theories as statistical theories, so so basically a theory is associated with a likelihood probability such as $Pr(... \vert H)$. Not sure this framing of the problem is useful. 



## Joe Williamson

- Take up the question of algebra change in the context of language change

- Key idea is twofold: (a) minimize cross-entropy (a particular distance function) between old and new probabilities for propositions that were already contained in the old algebra. For new propositions, (ii) maximize entropy (something like a principle of indifference).

 - Notably, he uses Bayesian networks. 



## Douven-Romeijn 2009: The Judy Benjamin Problem

- The question they tackle is how to update by a conditional statement if/then. 
They examine various distance functions that generalize Bayesian updating. 


- The examine a distance function --- between old and new probability function -- which should minimize the distance in probabilities between old and new probability function, each distance being weighted by how entrenched the propositions is. So proposition that are more entrenched gets priority in distance minimization compared to proposition 
that are less entrenched. 

- Note that in this kind of treatment there is change in the algebra. So distance function arte just generalization of Bayesian updating. 








# Framing the Problem

## Committments of Bayesianism

1. The probability space (algebra of propositions, what is at issue) is defined at the start, it is fixed. Some propositions are about events (hypothesis propositions) and others are about facts (evidence propositions). 

2. Prior probabilities (precise or imprecise) are assigned to  the propositions in the algebra

3. Conditional probabilities are assigned to all Boolean combination of propositions, or more parsimoniously 
following a Bayesian network representation

4. As some propositions become facts (evidence), Bayesian updating is internal to the fixed algebra of propositions 

## Two separate problems

- First problem: What are the questions at issue: who did it; what did the witness say? etc. This first problem is the one that defines the algebra, but as Allen notes, this is a dynamic process that does not take place once and for all at the start. The algebra is continually revised as new evidence comes in. 

- Second problem: Answering the questions. The Bayesian framework helps to assess the evidence and answer the questions once questions are formulated, but does not help to ask the questions.

- Aside one: perhaps the plausibility theory can address both problem since 'plausibility'  guides the choice of questions as well as how the questions are answered. A question is asked if it has plausible answers and a question is answered by the most plausible answer. 

- Aside two: there is the issue of computational complexity. Where do the numbers come from? This seems a separate and different issue, though. 







# Approaches / solutions

## Possible worlds / equivalence classes 

- We start with all possible worlds $S$. This is the entirety of the logical space. Each world is a complete description of how things could have been. 
    - Language approach. Each world is a maximally consistent set of sentences. The language here should be assumed the most informative language possible, or at least, a very expressive language, say English. Anything that cannot be expressed in that language is left out and will never be under consideration. 
    - Model theoretic approach. Note sure. 

- At some point during trial, a common ground $\mathcal{G}=(G, \sim)$ is formed---it becomes clear what is at issue, what questions are being asked, what is not at issue, what is disputed, what is agreed upon, etc. This common ground, formally, has two dimensions:
    a. some possible worlds are removed, i.e. all possible worlds that make true propositions that are demonstrable false, assumed to be false or simply not considered at issue, so we should only focus on a subset of $S$, that is, $G\subseteq S$
    b. an equivalence class $\sim$ is induced over the remaining set $G$ of possible worlds. This equivalence class fixes how fine grained the discussion is going to be (at the level of source/not-source or pillow/sofa, etc.). Worlds that are in the same equivalence classes are indistinguishable and thus treated as equivalent. Notation: let $[G]_\sim$ be the set of all equivalence classes induced by $\sim$ in $G$.

- The probability measure $\pr{}$ applies within the common ground as defined in points (a) and (b) above. So if $[G]_\sim$ is the set of all equivalence classes induced by $\sim$ in $G$,  any probability measure $\pr{}$ is a function from all subsets of $[G]_\sim$ into $[0, 1]$, that is, $\pr{}: \wp([G]_\sim)\to [0,1]$.

- Two side notes:

    - The Bayesian updating, in the traditional sense, occurs within the common ground $\mathcal{G}=(G, \sim)$. Presumably, since the common ground is fairly limited in terms of how much is at under dispute---the litigant at trial cannot discuss everything---the application of probability to this limited domain is not unfeasible. To be discussed further. This might address Allen's complexity concerns about Bayesianism. 

    - Suppose what is being disputed is just M (=Michael did it) versus N (=Nero did it).
Then, M and N form an exhaustive and exclusive pair. Which means that $\nicefrac{\pr{E \vert M}}{\pr{E \ vert N}}$ is the same same thing as $\nicefrac{\pr{E \vert M}}{\pr{E \ vert \neg M}}$. This is true, however, relative to a fixed common ground. It is not true in general since $M$ and $N$ do not cover the entire logical space $S$.

- Unanticipated possibilities may arise (a) either because possible worlds that were taught to be false, assumed to be false or simply excluded become relevant---for example, when Tito becomes a suspect besides Nero and Michael---(b) or because the equivalence class become more fine-grained---for example, when the focus become pillow/sofa and not simply source/non-source.

- There seems to be two dynamic updates going on here:

  a. dynamic update within a fixed common ground, this the classical Bayesian update
  b. meta-level dynamic update about  the common ground 

- Both updates are plausible. For judges and jurors may be asked (a) to assess evidence relative to a fixed common ground or (b) assess the evidence relative to a new common ground (say, new set of hypotheses).

- The formal framework should be able to go back and forth between the two level (simply Bayesian updates within the common ground and meta-level update about the common ground itself). There should bridge rule or constraints that allow to go from level to another without loss of information. 

    - What we want to avoid is that, once a new common ground is formed, the evidence assessment is redone completely afresh from scratch. The epistemic work done previously in asessing evidence relative to the old common ground shouod be retained if possible. 

  - Note that we speak as though the evidence is kept the same in going from one common ground to the next.  This makes sense intuitively. How can we make plausible sense of this sameness of evidence across common grounds? Strictly speaking the evidence is not the same. Since the evidence is a propositions of some kind (=set of possible world), any change in common ground will trigger a change in the evidence as a proposition. So we need to specify conditions by which the evidence stays the same. 
  
- One question  here is whether we should do this syntactically or model-theoretically. The problem of doing it model-theoretically are things like logical omniscience. 

## Bayesian network updating and comparison

- The Bayesian network need not be incompatible with the possible world approach. The two might be complementary.

- Rafal identified two dynamic processes:

    a. Once a Bayesian network is constructed, and all probabilities are assigned, Bayesian updating takes place within the network.
    
    b. A meta-level updating that may consist in (b1) refining the Bayesian network itself by adding arrows and nodes or (b2) comparing a Bayesian network to another Bayesian network.


- Jon Williamson in his paper 'Bayesianism and Language Change' formulates an algorithm to update a Bayesian network and the underlying probability distribution. 

- How does the meta-level network updating (refining of a network or comparison across networks) relate to the meta-level updating of the possible world approach outlined earlier?

### Illustration and problems

- First, we could start we a simple network with just one node $H$, with two values, say, "Mark did it" and "Joe did it".

- Second, we add a witness node $W$, again with two values, ""Witness says they saw Mark around crime scene" and "Witness says they saw Joe around the crime scene".

- Third, we could add a further node $V$ specif icing the visibility condition with two values, "good visibility" and "bad visibility".

- Fourth, we could add a another note with a DNA match, with two values, "Mark is a match" and "Joe is a match". Strictly speaking this node might actually consider of many more notes, the reports by the experts, the actual match and reliability conditions. 

- The challenge is to model this situation appropriately. Presumably, we are dealing with a Bayesian network that is build in four stages as follows:

    - Stage 1: node $H$ alone along with prior probability (precise or imprecise). The algebra here is very simple and $p_1$ is defined over this algebra. 

    - Stage 2: add node $W$, so we get $H \rightarrow W$, along with conditional probabilities. The algebra is slightly more complex and $p_2$ is defined over this new algebra. 

    - Stage 3: add node $V$, so we get $H \rightarrow W \leftarrow V$, along with conditional probabilities.  The algebra is again more complex and $p_3$ is defined over this new algebra.

    - Stage 4: add node $M$, so we get $M \leftarrow H \rightarrow W \leftarrow V$, along with conditional probabilities. The algebra is again more complex and $p_4$ is defined over this new algebra.

- Within each stage we can do a Bayesian update depending on what the facts turns out to be, say the witness said they saw Joe around the crime scene (stage 2) or the visibility conditions were bad (stage 3) or  Mark is a match but  Joe is not (stage 4). So there is an internal evidential updating relative to a fixed network, and an external, meta-updating about the structure of the network itself.

- Question: Where do the numbers associated with the conditional probabilities come from? We can perhaps sidestep this question, but the more pressing question is this. Suppose we have assigned some numbers in stage 1, how do we go about assigning numbers in stage 2 and take into account of the number in stage 1?

- Possible answer: the common answer here is a principle of conservativity (try to preserve as much as possible of then old probability assignments). This is what Jon Williamson does and other along with him. 



## Fleshing out the Bayesian network approach more precisely


- The key ideas are as follows: 

(1) as new propositions become salient or new questions are asked, the Bayesian network is updated. Graphically, this is easy: just add or remove 
arrows and nodes and you see fit. But how should be update the conditional probability tables? How much should we retain of the old conditional probability tables? 

(2) Bayesian networks capture, to some degree, a certain causal structure among propositions. Note sure if we need the causal structure as such, but at least, they capture a probabilistic dependency. So that structure (causal or otherwise) will guide what we should retain of the old conditional probability table as we create a new, updated Bayesian network in response to awareness growth. 


- Start with a simple network\[location\rightarrow perception\]. This is about something that happened (say, defendant was there versus defendant was somewhere else, so $location=there$ and $location=elsewhere$) and that witness seeing that it happened ($perception=sawDef$). We can assume that 
\[\pr{perception=sawDef \vert location=there}>\pr{petrception=sawDef \vert location=elsewhere}\].

- Now suppose someone realizes that, one thing is the perception and another things is the reporting itself, one step further removed from what happened. So the network can be updated downstream by \[location\rightarrow perception \rightarrow reporting\]. The conditional probability table will have to be updated adding numbers about $\pr{reporting=sawDef \vert perception=sawDef}$ and $\pr{reporting=sawDef \vert perception=sawNobody}$. 
But importantly, by the causal structure implicit in the network, the previous numbers, about 
\[\pr{perception=sawDef \vert location=there}>\pr{perception=sawDef \vert location=elsewhere}\] 
need not be changed. The causal structure guides us in retaining something.^[NOTE: That the upstream conditional probabilities are left unaffected by adding downstream conditional probabilities can be proven formally, even though the downstream (absolute) probabilities do affect the upstream (absolute) probabilities.]

- We can make a further modification. Suppose we wonder about the visibility condition at the time of perception. This should bring about a change in the upstream part of the network:
\[location\rightarrow perception \leftarrow visibility\].
This will force to change also the upstream conditional probabilities, without changing anything downstream though. 

- We can also think yet another modification. Suppose we wonder whether the witness has been paid.  This should bring about a change in the downstream part of the network:
\[perception\rightarrow reporting \leftarrow payment\].
This will force to change also the downstream conditional probabilities, without changing anything upstream though. So the causal or dependence structure in the network guides how much we can retain of previous probability assignments.

- The above all cases of refinement, but note something old probability assignment are retained, while other times they are not retained. 

- What about cases of expansion? Here reverse Bayesianism holds and what goes on can be captured by the Bayesian network approach with some tweaks. Start with the simple structure:
\[E\leftarrow H\] 
where $H$ has two values, say $John$ and $Sue$. These are the two suspects. And suppose that given the evidence one is more plausibly the culprit than the other. So, $\pr{E \vert H=John}>>>>\pr{E \vert H=Sue}$. Say the ratio is 10:1. But now another hypothesis comes along, say $H=Ela$, which seems more plausible than either of the other two.  We want to retain that $H=John$ is more plausible than $H=Sue$, but also add the further information about $H=Ela$. This is tricky but doable with a Bayesian network.  Basically: 

    (a) node $H$ is morphed onto =$H^*$ which now has three values instead of just two, adding $Ela$ as a possible value. 

    (b) the arrow from $H$ to $E$ is removed and instead we have an arrow from $H^*$ to $E$. We now assess the evidence relative to $H^*$, no longer just $H$.
    
    \[E\leftarrow H^* \rightarrow H\] 

    (c) we add an arrow that goes from $H^*$ to $H$, ensuring that $\pr{H=Joe \vert H^*=Joe}=1$ and
$\pr{H=Sue \vert H^*=Joe}=0$, $\pr{H=Joe \vert H^*=Sue}=0$ and
$\pr{H=Sue \vert H^*=Sue}=1$, and finally, $\pr{H=Joe \vert H^*=Ela}=0$ and
$\pr{H=Sue \vert H^*=Ela}=0$. So that should ensure that the new value $Ela$ relates to the old values properly. 

    (d) We ensure that $\pr{E \vert H=Sue}:\pr{E \vert H=Joe}=\pr{E \vert H^*=Sue}: \pr{E \vert H^*=Joe}$. So this is the conservativity that we put in place before and after the awareness growth. 

- The more complicated examples by Steele and Stefansson can be handled by a similar construction.


- This approach based on the causal structure of the Bayesian network should also be to handle the Mathani's cases. 
















    
  
  
  
  
 


