% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  11pt,
  dvipsnames,enabledeprecatedfontcommands]{scrartcl}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Unanticipated possibilities},
  pdfauthor={Marcello/Rafal},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\documentclass{article}

% %packages
\usepackage{booktabs}
%\usepackage[left]{showlabels}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{ragged2e}
\usepackage{etex}
%\usepackage{yfonts}
\usepackage{marvosym}
\usepackage[notextcomp]{kpfonts}
\usepackage{nicefrac}
\newcommand*{\QED}{\hfill \footnotesize {\sc Q.e.d.}}
\usepackage{floatrow}
\usepackage{multicol}

\usepackage[textsize=footnotesize]{todonotes}
\newcommand{\ali}[1]{\todo[color=gray!40]{\textbf{Alicja:} #1}}
\newcommand{\mar}[1]{\todo[color=blue!40]{#1}}
\newcommand{\raf}[1]{\todo[color=olive!40]{#1}}

%\linespread{1.5}
\newcommand{\indep}{\!\perp \!\!\! \perp\!}


\setlength{\parindent}{10pt}
\setlength{\parskip}{1pt}


%language
%\usepackage{times}
\usepackage{mathptmx}
\usepackage[scaled=0.86]{helvet}
\usepackage{t1enc}
%\usepackage[utf8x]{inputenc}
%\usepackage[polish]{babel}
%\usepackage{polski}




%AMS
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{geometry}
 \geometry{a4paper,left=35mm,top=20mm,}


%environments
\newtheorem{fact}{Fact}


% allow page breaks in equations
\allowdisplaybreaks


%abbreviations
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}
\newcommand{\n}{\neg}
\newcommand{\et}{\wedge}
\newcommand{\jt}{\rightarrow}
\newcommand{\ko}[1]{\forall  #1\,}
\newcommand{\ro}{\leftrightarrow}
\newcommand{\exi}[1]{\exists\, {_{#1}}}
\newcommand{\pr}[1]{\ensuremath{\mathsf{P}(#1)}}
\newcommand{\cost}{\mathsf{cost}}
\newcommand{\benefit}{\mathsf{benefit}}
\newcommand{\ut}{\mathsf{ut}}

\newcommand{\odds}{\mathsf{Odds}}
\newcommand{\ind}{\mathsf{Ind}}
\newcommand{\nf}[2]{\nicefrac{#1\,}{#2}}
\newcommand{\R}[1]{\texttt{#1}}
\newcommand{\prr}[1]{\mbox{$\mathtt{P}_{prior}(#1)$}}
\newcommand{\prp}[1]{\mbox{$\mathtt{P}_{posterior}(#1)$}}



\newtheorem{q}{\color{blue}Question}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[fact]


%technical intermezzo
%---------------------

\newcommand{\intermezzoa}{
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}



	\tiny{\sc Optional Content Starts}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}\nopagebreak 
	}


\newcommand{\intermezzob}{\nopagebreak 
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}

	\tiny{\sc Optional Content Ends}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}
	}
	
	
%--------------------






















\newtheorem*{reply*}{Reply}
\usepackage{enumitem}
\newcommand{\question}[1]{\begin{enumerate}[resume,leftmargin=0cm,labelsep=0cm,align=left]
\item #1
\end{enumerate}}

\usepackage{float}

% \setbeamertemplate{blocks}[rounded][shadow=true]
% \setbeamertemplate{itemize items}[ball]
% \AtBeginPart{}
% \AtBeginSection{}
% \AtBeginSubsection{}
% \AtBeginSubsubsection{}
% \setlength{\emergencystretch}{0em}
% \setlength{\parskip}{0pt}






\usepackage[authoryear]{natbib}

%\bibliographystyle{apalike}



\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows}

\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Unanticipated possibilities}
\author{Marcello/Rafal}
\date{}

\begin{document}
\maketitle

\hypertarget{the-problem}{%
\section{The problem}\label{the-problem}}

\hypertarget{basic-idea}{%
\subsection{Basic idea}\label{basic-idea}}

How does the Bayesian framework deal with the problem of unanticipated
possibilities?

\begin{itemize}
\item
  There seems to be two sides to this problem:

  \begin{enumerate}
  \def\labelenumi{\alph{enumi}.}
  \item
    The space of possibilities could get larger, say, first one
    considers only two suspects, Michael and Nero, and then a third
    suspect, Tito, becomes plausible. The priors of M and N were
    initially 50:50 (or 75:25, whatever), and T got nothing. Actually, T
    was not in the algebra to begin with. But, now T is in the algebra
    and gets a non-zero probability. How can this be handled in the
    Bayesian framework?
  \item
    The space of possibilities becomes more fine-grained. Consider a DNA
    evidence case. At first, one of the hypothesis is that the defendant
    is the source of the crime stain. But as the investigation proceeds,
    it becomes clear that it makes a difference whether the defendant is
    the source of the crime stains on the pillow or on the sofa.
    Perhaps, the stain on the pillow were left innocently, but the
    stains on sofa were left by the perpetrator. So the space of
    possibilities becomes more fine-grained.
  \end{enumerate}
\item
  Not sure if the two items above are actually different, but they seem
  to be. In the first, the possibilities expand. In the second, the
  possibilities become more fine-grained. The first can be represented
  by adding a possible world, while the second, by making the
  equivalence classes among possible world more fine grained.
\item
  We need to be careful about what we are trying to represent here. Say
  we are trying to represent what is going on in the jurors' or judges'
  minds as they hear evidence in the trial and they process the
  evidence. We are trying to give a plausible picture of the processing
  of the evidence and hypotheses at stake.
\item
  Are we trying to give an empirically adequate picture or just a
  philosophically or theoretically plausible picture?
\end{itemize}

\hypertarget{relevant-literature}{%
\subsection{Relevant literature}\label{relevant-literature}}

\hypertarget{ronald-allen}{%
\subsubsection{Ronald Allen}\label{ronald-allen}}

Quotations from Allen's The Nature of Juridical Proof: Probability as a
Tool in Plausible Reasoning:

\begin{quote}
The decision-makers in all legal systems must learn about the issue being litigated, whether this is through serial hearings as in some continental systems or concentrated trials in the American tradition ... in no system does the fact-finder have substantial knowledge about the case until evidence is presented. Obviously. That means, though, that it is literally impossible to form a probability space of mutually exclusive hypotheses, to assign them initial probabilities. (p. 138)

Some of you may think that I overstate the case and that the mutually exclusive hypotheses are
obvious: guilt or innocence or liability or no liability ... Guilt, innocence, liability and no liability are not factual hypotheses; they are legal conclusions that are applied to characterise the implications of the facts that are found. It should be obvious that one cannot condition 'guilt' on some evidence without going through a factual proposition that is the actual object of proof. (p. 138)

Complicating matters further is the \textbf{emergence of new theories} ... Like society itself, the
litigation process is dynamic, not static, and frequently new theories will emerge. When they do, the
probability space must be reconfigured. It is thus pointless to configure the probability space even if it
could be done at some point prior to receiving all the evidence and hearing all the arguments. (p. 139)

And finally, the \textbf{spectre of Old Evidence} raises its head. After the theories of the parties are finally identified and the logical space defined, initial assignments of probability must be made in order for Bayes' Theorem to be employed. But those initial assignments of probability will obviously include
consideration of the trial evidence, as well as pre-existing knowledge of the fact-finder. Once the
probability space is determined and initial probabilities assigned, all the trial evidence is old evidence
that has already been accommodated. There is no work for Bayes' Theorem to do. (p. 139)

What, then, are the \textbf{tasks for the future}? ... For the legal
analysts, there are two issues that should preoccupy us going forward. The first is the perennial problem
of constructing methods to \textbf{decrease the probability that unreliable evidence is being provided} by
purported experts and scientists ... that means engaging with the underlying forensic science to ensure that appropriate methodologies are employed and that the evidence offered at trial does not overleap what those methodologies can deliver. The second is the more fundamental problem noted above of \textbf{integrating that evidence with the rest of the evidence} in the case ... What that means in turn, of course, is to flesh out the nature of rational thought ... The discrete question to be addressed is the implications of rational thought writ large for the legal system. (p. 141) 
\end{quote}

Allen formulates several related challenges:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The issues being litigated emerge as the presentation of the evidence
  proceeds. The Bayesian framework instead assumes that the algebra of
  propositions (=what is at issue) is defined from the start, which is
  not a realistic depiction of what happens during a trial.
\item
  The issues being litigated cannot simply be guilt/innocent. There are
  several other, more specific propositions that are the actual object
  of proof.
\item
  New theories emerge, for example, by expanding the space of
  possibilities (say a new suspect could be the perpetrator) or by
  refining existing possibilities.
\item
  The probability space is defined by the evidence that is presented at
  trial, so all evidence becomes old evidence and its probability must
  be one.
\end{enumerate}

\hypertarget{chihara}{%
\subsubsection{Chihara}\label{chihara}}

\hypertarget{steele-and-stafansson-awareness-growth}{%
\subsubsection{Steele and Stafansson: awareness
growth}\label{steele-and-stafansson-awareness-growth}}

\begin{itemize}
\item
  They set up the problem as both one of expansion of of the space of
  possibilities and refinement of possibilities.
\item
  They cite economists and philosophers---in particular the paper
  Reverse Bayesianism by Karni and Vier√∏, as well as work by Bradley and
  Romeijn. The idea of reverse Bayesianism is simple. For any pair of
  propositions \(A, B\) that are basic and inconsistent, their
  probability ratio in the old algebra,\(P(A)/P(B)\), should be the same
  as ijn the new algebra, so \(P(A)/P(B)=P^+(A)/P^+(B)\), where \(P^+\)
  is a probability function over a an expanded or refined albegra of
  propositions. This is a sort of conservative principle.
\item
  They provide a counterexample of reverse Bayesianism:
\end{itemize}

\begin{quote}
Example 2: Suppose you happen to see your partner enter your best
friend's house on an evening when your partner had told you she would
have to work late. At that point, you become convinced that your partner
and best friend are having an affair, as opposed to their being warm
friends or mere acquaintances. You discuss your suspicion with another
friend of yours, who points out that perhaps they were meeting to plan a
surprise party to celebrate your upcoming birthday---a possibility that
you had not even entertained. Becoming aware of this possible
explanation for your partner's behaviour makes you doubt that she is
having an affair with your friend, relative, for instance, to their
being warm friends. 9p.
\end{quote}

\begin{itemize}
\item
  The two initial hypotheses, `affair' and `friends,' are say at a ratio
  of 2:1 given the evidence available. But the introduction of a third
  hypothesis `surprise birthday planning' (possibly a refinement of
  `friends'), should plausibly reverse the ratio to 1:2. Counterexample
  to reverse Bayesianism is false. Interestingly, this fits with legal
  cases and seems an example of shifts in plausibility between
  hypotheses.
\item
  (Idea of explaining away seems also related. The presumably compelling
  evidence that they met without telling you suggest they were having an
  affair, but the hypothesis that they were for planning a surprise
  birthday explain away all that evidence. )
\item
  In Sec. 6, they offer the following diagnosis of what is happening.
  ``What happens in these cases is that the `new' propositions that the
  agent comes to be aware of change the way she comprehends the `old'
  propositions, in particular, how these propositions relate to other
  propositions.''
\item
  They offer an ``Awareness Rigidity'' that might seem better than
  unconstrained reverse Bayesianism, that is, \(P^+(A \vert T^*)=P(A)\),
  where \(T^*\) tracks what was a tautology in the old possibility
  space. So, for example, if the old possibility space had only two
  options, F and V, then \(F\vee V\) would be \(T^*\), but this need not
  be a tautology in the new possibility space as, another possibility
  might been added.
\item
  ``The problem is that Awareness Rigidity entails Reverse Bayesianism
  in cases where awareness grows by refinement (since it effectively
  requires that the probabilities for all propositions in the old
  awareness context remain unchanged). We previously argued that Reverse
  Bayesianism is not plausible even in cases of refinement.''
\item
  ``Awareness Rigidity might nevertheless be compelling for awareness
  growth by expansion. After all, when awareness grows by expansion, the
  new tautology contains possibilities that it did not contain before,
  and hence, Awareness Rigidity does not entail Reverse Bayesianism in
  cases of expansion. So, perhaps Awareness Rigidity is plausible when
  it comes to expansion, even though Reverse Bayesianism is not.''
\item
  So their overall take is negative. There is no clear formal theory of
  what to do when the space of possibilities changes.
\end{itemize}

\hypertarget{douven-romeijn-2009-the-judy-benjamin-problem}{%
\subsection{Douven-Romeijn 2009: The Judy Benjamin
Problem}\label{douven-romeijn-2009-the-judy-benjamin-problem}}

\begin{itemize}
\tightlist
\item
  The question they tackle is how to update by a conditional statement
  if/then. They examine various distance functions that generalize
  Bayesian updating.
\end{itemize}

\hypertarget{verheij-zabell-catch-all}{%
\subsection{Verheij, Zabell: Catch all}\label{verheij-zabell-catch-all}}

\hypertarget{framing-the-problem}{%
\section{Framing the Problem}\label{framing-the-problem}}

\hypertarget{committments-of-bayesianism}{%
\subsection{Committments of
Bayesianism}\label{committments-of-bayesianism}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The probability space (algebra of propositions, what is at issue) is
  defined at the start, it is fixed. Some propositions are about events
  (hypothesis propositions) and others are about facts (evidence
  propositions).
\item
  Prior probabilities (precise or imprecise) are assigned to the
  propositions in the algebra
\item
  Conditional probabilities are assigned to all Boolean combination of
  propositions, or more parsimoniously following a Bayesian network
  representation
\item
  As some propositions become facts (evidence), Bayesian updating is
  internal to the fixed algebra of propositions
\end{enumerate}

\hypertarget{two-separate-problems}{%
\subsection{Two separate problems}\label{two-separate-problems}}

\begin{itemize}
\item
  First problem: What are the questions at issue: who did it; what did
  the witness say? etc. This first problem is the one that defines the
  algebra, but as Allen notes, this is a dynamic process that does not
  take place once and for all at the start. The algebra is continually
  revised as new evidence comes in.
\item
  Second problem: Answering the questions. The Bayesian framework helps
  to assess the evidence and answer the questions once questions are
  formulated, but does not help to ask the questions.
\item
  Aside one: perhaps the plausibility theory can address both problem
  since `plausibility' guides the choice of questions as well as how the
  questions are answered. A question is asked if it has plausible
  answers and a question is answered by the most plausible answer.
\item
  Aside two: there is the issue of computational complexity. Where do
  the numbers come from? This seems a separate and different issue,
  though.
\end{itemize}

\hypertarget{approaches-solutions}{%
\section{Approaches / solutions}\label{approaches-solutions}}

\hypertarget{possible-worlds-equivalence-classes}{%
\subsection{Possible worlds / equivalence
classes}\label{possible-worlds-equivalence-classes}}

\begin{itemize}
\item
  We start with all possible worlds \(S\). This is the entirety of the
  logical space. Each world is a complete description of how things
  could have been.

  \begin{itemize}
  \tightlist
  \item
    Language approach. Each world is a maximally consistent set of
    sentences. The language here should be assumed the most informative
    language possible, or at least, a very expressive language, say
    English. Anything that cannot be expressed in that language is left
    out and will never be under consideration.
  \item
    Model theoretic approach. Note sure.
  \end{itemize}
\item
  At some point during trial, a common ground \(\mathcal{G}=(G, \sim)\)
  is formed---it becomes clear what is at issue, what questions are
  being asked, what is not at issue, what is disputed, what is agreed
  upon, etc. This common ground, formally, has two dimensions:

  \begin{enumerate}
  \def\labelenumi{\alph{enumi}.}
  \tightlist
  \item
    some possible worlds are removed, i.e.~all possible worlds that make
    true propositions that are demonstrable false, assumed to be false
    or simply not considered at issue, so we should only focus on a
    subset of \(S\), that is, \(G\subseteq S\)
  \item
    an equivalence class \(\sim\) is induced over the remaining set
    \(G\) of possible worlds. This equivalence class fixes how fine
    grained the discussion is going to be (at the level of
    source/not-source or pillow/sofa, etc.). Worlds that are in the same
    equivalence classes are indistinguishable and thus treated as
    equivalent. Notation: let \([G]_\sim\) be the set of all equivalence
    classes induced by \(\sim\) in \(G\).
  \end{enumerate}
\item
  The probability measure \(\pr{}\) applies within the common ground as
  defined in points (a) and (b) above. So if \([G]_\sim\) is the set of
  all equivalence classes induced by \(\sim\) in \(G\), any probability
  measure \(\pr{}\) is a function from all subsets of \([G]_\sim\) into
  \([0, 1]\), that is, \(\pr{}: \wp([G]_\sim)\to [0,1]\).
\item
  Two side notes:

  \begin{itemize}
  \item
    The Bayesian updating, in the traditional sense, occurs within the
    common ground \(\mathcal{G}=(G, \sim)\). Presumably, since the
    common ground is fairly limited in terms of how much is at under
    dispute---the litigant at trial cannot discuss everything---the
    application of probability to this limited domain is not unfeasible.
    To be discussed further. This might address Allen's complexity
    concerns about Bayesianism.
  \item
    Suppose what is being disputed is just M (=Michael did it) versus N
    (=Nero did it). Then, M and N form an exhaustive and exclusive pair.
    Which means that \(\nicefrac{\pr{E \vert M}}{\pr{E \ vert N}}\) is
    the same same thing as
    \(\nicefrac{\pr{E \vert M}}{\pr{E \ vert \neg M}}\). This is true,
    however, relative to a fixed common ground. It is not true in
    general since \(M\) and \(N\) do not cover the entire logical space
    \(S\).
  \end{itemize}
\item
  Unanticipated possibilities may arise (a) either because possible
  worlds that were taught to be false, assumed to be false or simply
  excluded become relevant---for example, when Tito becomes a suspect
  besides Nero and Michael---(b) or because the equivalence class become
  more fine-grained---for example, when the focus become pillow/sofa and
  not simply source/non-source.
\item
  There seems to be two dynamic updates going on here:

  \begin{enumerate}
  \def\labelenumi{\alph{enumi}.}
  \tightlist
  \item
    dynamic update within a fixed common ground, this the classical
    Bayesian update
  \item
    meta-level dynamic update about the common ground
  \end{enumerate}
\item
  Both updates are plausible. For judges and jurors may be asked (a) to
  assess evidence relative to a fixed common ground or (b) assess the
  evidence relative to a new common ground (say, new set of hypotheses).
\item
  The formal framework should be able to go back and forth between the
  two level (simply Bayesian updates within the common ground and
  meta-level update about the common ground itself). There should bridge
  rule or constraints that allow to go from level to another without
  loss of information.

  \begin{itemize}
  \item
    What we want to avoid is that, once a new common ground is formed,
    the evidence assessment is redone completely afresh from scratch.
    The epistemic work done previously in asessing evidence relative to
    the old common ground shouod be retained if possible.
  \item
    Note that we speak as though the evidence is kept the same in going
    from one common ground to the next. This makes sense intuitively.
    How can we make plausible sense of this sameness of evidence across
    common grounds? Strictly speaking the evidence is not the same.
    Since the evidence is a propositions of some kind (=set of possible
    world), any change in common ground will trigger a change in the
    evidence as a proposition. So we need to specify conditions by which
    the evidence stays the same.
  \end{itemize}
\item
  One question here is whether we should do this syntactically or
  model-theoretically. The problem of doing it model-theoretically are
  things like logical omniscience.
\end{itemize}

\hypertarget{bayesian-network-updating-and-comparison}{%
\subsection{Bayesian network updating and
comparison}\label{bayesian-network-updating-and-comparison}}

\begin{itemize}
\item
  The Bayesian network need not be incompatible with the possible world
  approach. The two might be complementary.
\item
  Rafal identified two dynamic processes:

  \begin{enumerate}
  \def\labelenumi{\alph{enumi}.}
  \item
    Once a Bayesian network is constructed, and all probabilities are
    assigned, Bayesian updating takes place within the network.
  \item
    A meta-level updating that may consist in (b1) refining the Bayesian
    network itself by adding arrows and nodes or (b2) comparing a
    Bayesian network to another Bayesian network.
  \end{enumerate}
\item
  Jon Williamson in his paper `Bayesianism and Language Change'
  formulates an algorithm to update a Bayesian network and the
  underlying probability distribution.
\item
  How does the meta-level network updating (refining of a network or
  comparison across networks) relate to the meta-level updating of the
  possible world approach outlined earlier?
\end{itemize}

\hypertarget{illustration-and-problems}{%
\subsubsection{Illustration and
problems}\label{illustration-and-problems}}

\begin{itemize}
\item
  First, we could start we a simple network with just one node \(H\),
  with two values, say, ``Mark did it'' and ``Joe did it.''
\item
  Second, we add a witness node \(W\), again with two values, "``Witness
  says they saw Mark around crime scene'' and ``Witness says they saw
  Joe around the crime scene.''
\item
  Third, we could add a further node \(V\) specif icing the visibility
  condition with two values, ``good visibility'' and ``bad visibility.''
\item
  Fourth, we could add a another note with a DNA match, with two values,
  ``Mark is a match'' and ``Joe is a match.'' Strictly speaking this
  node might actually consider of many more notes, the reports by the
  experts, the actual match and reliability conditions.
\item
  The challenge is to model this situation appropriately. Presumably, we
  are dealing with a Bayesian network that is build in four stages as
  follows:

  \begin{itemize}
  \item
    Stage 1: node \(H\) alone along with prior probability (precise or
    imprecise). The algebra here is very simple and \(p_1\) is defined
    over this algebra.
  \item
    Stage 2: add node \(W\), so we get \(H \rightarrow W\), along with
    conditional probabilities. The algebra is slightly more complex and
    \(p_2\) is defined over this new algebra.
  \item
    Stage 3: add node \(V\), so we get \(H \rightarrow W \leftarrow V\),
    along with conditional probabilities. The algebra is again more
    complex and \(p_3\) is defined over this new algebra.
  \item
    Stage 4: add node \(M\), so we get
    \(M \leftarrow H \rightarrow W \leftarrow V\), along with
    conditional probabilities. The algebra is again more complex and
    \(p_4\) is defined over this new algebra.
  \end{itemize}
\item
  Within each stage we can do a Bayesian update depending on what the
  facts turns out to be, say the witness said they saw Joe around the
  crime scene (stage 2) or the visibility conditions were bad (stage 3)
  or Mark is a match but Joe is not (stage 4). So there is an internal
  evidential updating relative to a fixed network, and an external,
  meta-updating about the structure of the network itself.
\item
  Question: Where do the numbers associated with the conditional
  probabilities come from? We can perhaps sidestep this question, but
  the more pressing question is this. Suppose we have assigned some
  numbers in stage 1, how do we go about assigning numbers in stage 2
  and take into account of the number in stage 1?
\item
  Possible answer: the common answer here is a principle of
  conservativity (try to preserve as much as possible of then old
  probability assignments). This is what Jon Williamson does and other
  along with him.
\end{itemize}

\end{document}
