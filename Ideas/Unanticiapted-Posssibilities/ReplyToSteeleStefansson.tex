% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  11pt,
  dvipsnames,enabledeprecatedfontcommands]{scrartcl}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Awareness Growth in Bayesian Networks},
  pdfauthor={Marcello/Rafal},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\documentclass{article}

% %packages
\usepackage{booktabs}
%\usepackage[left]{showlabels}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{ragged2e}
\usepackage{etex}
%\usepackage{yfonts}
\usepackage{marvosym}
\usepackage[notextcomp]{kpfonts}
\usepackage{nicefrac}
\newcommand*{\QED}{\hfill \footnotesize {\sc Q.e.d.}}
\usepackage{floatrow}
\usepackage{multicol}

\usepackage[textsize=footnotesize]{todonotes}
\newcommand{\ali}[1]{\todo[color=gray!40]{\textbf{Alicja:} #1}}
\newcommand{\mar}[1]{\todo[color=blue!40]{#1}}
\newcommand{\raf}[1]{\todo[color=olive!40]{#1}}

%\linespread{1.5}
\newcommand{\indep}{\!\perp \!\!\! \perp\!}


\setlength{\parindent}{10pt}
\setlength{\parskip}{1pt}


%language
%\usepackage{times}
\usepackage{mathptmx}
\usepackage[scaled=0.86]{helvet}
\usepackage{t1enc}
%\usepackage[utf8x]{inputenc}
%\usepackage[polish]{babel}
%\usepackage{polski}




%AMS
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{geometry}
 \geometry{a4paper,left=35mm,top=20mm,}


%environments
\newtheorem{fact}{Fact}


% allow page breaks in equations
\allowdisplaybreaks


%abbreviations
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}
\newcommand{\n}{\neg}
\newcommand{\et}{\wedge}
\newcommand{\jt}{\rightarrow}
\newcommand{\ko}[1]{\forall  #1\,}
\newcommand{\ro}{\leftrightarrow}
\newcommand{\exi}[1]{\exists\, {_{#1}}}
\newcommand{\pr}[1]{\ensuremath{\mathsf{P}(#1)}}
\newcommand{\ppr}[2]{\ensuremath{\mathsf{P}^{#1}(#2)}}
\newcommand{\cost}{\mathsf{cost}}
\newcommand{\benefit}{\mathsf{benefit}}
\newcommand{\ut}{\mathsf{ut}}

\newcommand{\odds}{\mathsf{Odds}}
\newcommand{\ind}{\mathsf{Ind}}
\newcommand{\nf}[2]{\nicefrac{#1\,}{#2}}
\newcommand{\R}[1]{\texttt{#1}}
\newcommand{\prr}[1]{\mbox{$\mathtt{P}_{prior}(#1)$}}
\newcommand{\prp}[1]{\mbox{$\mathtt{P}_{posterior}(#1)$}}



\newtheorem{q}{\color{blue}Question}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[fact]


%technical intermezzo
%---------------------

\newcommand{\intermezzoa}{
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}



	\tiny{\sc Optional Content Starts}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}\nopagebreak 
	}


\newcommand{\intermezzob}{\nopagebreak 
	\begin{minipage}[c]{13cm}
	\begin{center}\rule{10cm}{0.4pt}

	\tiny{\sc Optional Content Ends}
	
	\vspace{-1mm}
	
	\rule{10cm}{0.4pt}\end{center}
	\end{minipage}
	}
	
	
%--------------------






















\newtheorem*{reply*}{Reply}
\usepackage{enumitem}
\newcommand{\question}[1]{\begin{enumerate}[resume,leftmargin=0cm,labelsep=0cm,align=left]
\item #1
\end{enumerate}}

\usepackage{float}

% \setbeamertemplate{blocks}[rounded][shadow=true]
% \setbeamertemplate{itemize items}[ball]
% \AtBeginPart{}
% \AtBeginSection{}
% \AtBeginSubsection{}
% \AtBeginSubsubsection{}
% \setlength{\emergencystretch}{0em}
% \setlength{\parskip}{0pt}






\usepackage[authoryear]{natbib}

%\bibliographystyle{apalike}



\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows}

\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Awareness Growth in Bayesian Networks}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Reply to Steele and Stefánsson}
\author{Marcello/Rafal}
\date{}

\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Learning is modeled in the Bayesian framework by the rule of
conditionalization. This rule posits that the agent's new degree of
belief in a proposition \(H\) after a learning experience \(E\) should
be the same as the agent's old degree of belief in \(H\) conditional on
\(E\). That is, \[\ppr{E}{H}=\pr{H \vert E},\] where \(\pr{}\)
represents the agent's old degree of belief (before the learning
experience \(E\)) and \(\ppr{E}{}\) represents the agent's new degree of
belief (after the learning experience \(E\)).

Both \(E\) and \(H\) belong to the agent's algebra of propositions. This
algebra models the agent's awareness state, the propositions taken to be
live possibilities. Conditionalization never modifies the algebra and
thus makes it impossible for an agent to learn something they have never
thought about. This forces a great deal of rigidity on the learning
process. Even before learning about \(E\), the agent must already have
assigned a degree of belief to any proposition conditional on \(E\).
This picture commits the agent to the specification of their `total
possible future experience' (Howson 1976, The Development of Logical
Probability), as though learning was confined to an `initial prison'
(Lakatos, 1968, Changes in the Problem of Inductive Logic).

But, arguably, the learning process is more complex than what
conditionalization allows. Not only do we learn that some propositions
that we were entertaining are true or false, but we may also learn new
propositions that we did not entertain before. Or we may entertain new
propositions---without necessarily learning that they are true or
false---and this change in awareness may in turn change what we already
believe. How should this more complex learning process be modeled by
Bayesianism? Call this the problem of awareness growth.

Critics of Bayesianism and sympathizers alike have been discussing the
problem of awareness growth under different names for quite some time,
at least since the eighties. This problem arises in a number of
different contexts, for example, new scientific theories (Glymour, 1980,
Why I am not a Bayesian; Chihara 1987, Some Problems for Bayesian
Confirmation Theory; Earmann 1992, Bayes of Bust?), language changes and
paradigm shifts (Williamson 2003, Bayesianism and Language Change), and
theories of induction (Zabell, Predicting the Unpredictable).

A proposal that has attracted considerable scholarly attention is
Reverse Bayesianism (Karni and Viero, 2015, Probabilistic Sophistication
and Reverse Bayesianism; Wenmackers and Romeijn 2016, New Theory About
Old Evidence; Bradely 2017, Decision Theory with A Human Face) . The
idea is to model awareness growth as a change in the algebra while
ensuring that the probabilities of the propositions shared between the
old and new algebra remain fixed under suitable constraints.

Let \(\mathcal{F}\) be the initial algebra of propositions and let
\(\mathcal{F}^+\) the algebra after the agent's awareness has grown.
Both contain the contradictory proposition \(\perp\) and tautologous
proposition \(\top\) and they are closed under connectives such as
disjunction \(\vee\), conjunction \(\wedge\) and negation \(\neg\).
Denote by \(X\) and \(X^+\) the subsets of these algebras that contain
only basic propositions, those without connectives.
\textbf{Reverse Bayesianism} posits that the ratio of probabilities for
any basic propositions \(A\) and \(B\) in both \(X\) and \(X^+\)---the
basic propositions shared by the old and new algebra---remain constant
through the process of awareness growth:
\[\frac{\pr{A}}{\pr{B}} = \frac{\ppr{+}{A}}{\ppr{+}{B}},\] where
\(\pr{}\) represents the agent's degree of belief before awareness
growth and \(\ppr{+}{}\) represents the agent's degree of belief after
awareness growth.

Reverse Bayesianism is an elegant theory that manages to cope with a
seemingly intractable problem. As the awareness of an an agent grows,
the agent would prefer not to throw away completely the epistemic work
they have done so far. The agent may desire to retain as much of their
old degrees of beliefs as possible. Reverse Bayesianism provides a
simple recipe to do that. It also coheres with the conservative spirit
of conditionalization which preserves the old probability distribution
conditional on what is learned.

Unfortunately, Reverse Bayesianism is not without complications. Steele
and Stefánsson (2021, Belief Revision for Growing Awareness) argue that
Reverse Bayesianism, when suitably formulated, can work in a limited
class of cases, what they call \textit{awareness expansion}, but cannot
work for \textit{awareness refinement} (more on this distinction later).
Their argument rests on a number of ingenious counterexamples.

We contend, however, that their counterexamples have limited
applicability and thus constitute an overall weak argument against
Reverse Bayesianism (\S ~\ref{sec:counterexamples}). Still, we share
Steele and Stefánsson's skepticism and provide a better counterexample
(\S ~\ref{sec:better}). At the same time, we carve out a class of cases
in which Reverse Bayesianism still holds, and these include not just
cases of expansion but also some cases of refinement
(\S ~\ref{sec:downstream}). Our critique of Reverse Bayesianism is thus
more fine-grained than Steele and Stefánsson's. Ultimately, we
conjecture that the problem of awareness growth cannot be tackled in an
algorithmic manner because subject-matter structural assumptions are
necessary (\S ~\ref{sec:material}). We rely on the theory of Bayesian
networks at several key junctures in our argument.

\hypertarget{counterexamples}{%
\section{Counterexamples?}\label{counterexamples}}

\label{sec:counterexamples}

We begin by rehearsing two of the ingenious counterexamples to Reverse
Bayesianism by Steele and Stefánsson. One targets awareness expansion
and the other awareness refinement. The difference between expansion and
refinement is intuitively plausible, but it can be tricky to pin down
formally. A rough characterization will suffice here. Suppose, as is
customary, propositions are interpreted as sets of possible worlds,
where the set of all possible worlds is the possibility space. An
algebra of propositions thus interpreted induces a partition of the
possibility space. Refinement occurs when the new proposition added to
the algebra induces a more fine-grained partition of the possibility
space. Expansion, instead, occurs when the new proposition shows the
existing partition of the possibility space is not exhaustive.

The first counterexample by Steele and Stefánsson targets cases of
awareness expansion:

\begin{quote}
\textsc{Friends}: Suppose you happen to see your partner enter your best
friend's house on an evening when your partner had told you she would
have to work late. At that point, you become convinced that your partner
and best friend are having an affair, as opposed to their being warm
friends or mere acquaintances. You discuss your suspicion with another
friend of yours, who points out that perhaps they were meeting to plan a
surprise party to celebrate your upcoming birthday---a possibility that
you had not even entertained. Becoming aware of this possible
explanation for your partner's behaviour makes you doubt that she is
having an affair with your friend, relative, for instance, to their
being warm friends. (Steele and Stefánsson, 2021, Section 5, Example 2)
\end{quote}

\noindent Initially, the algebra only contains the hypotheses `my
partner and my best friend met to have an affair' (\textit{Affair}) and
`my partner and my best friend met as friends or acquaintances'
(\textit{Friends/acquaintances}). The other proposition in the algebra
is the evidence, that is, the fact that your partner and your best
friend met one night without telling you (\textit{Secretive}). Given
this evidence, \textit{Affair} is more probable than
\textit{Friends/acquaintances}:
\[\pr{\textit{Affair} \vert  \textit{Secretive} }> \pr{\textit{Friends/acquaintances} \vert \textit{Secretive}} \tag{>}.\]
When the algebra changes, a new hypothesis is added which you had not
considered before: your partner and your best friends met to plan a
surprise party for your upcoming birthday (\textit{Surprise}). Given the
same evidence, \textit{Friends/acquaintances} is now more likely than
\textit{Affair}:\footnote{This holds assuming (a) that hypothesis
  \textit{Surprise} is more likely than the hypothesis \textit{Affair}:
  \[\ppr{+}{ \textit{Surprise} \vert \textit{Secretive}}> \ppr{+}{ \textit{Affair} \vert \textit{Secretive}},\]
  and in addition, (b) that \textit{Surprise} implies
  \textit{Friends/acquaintances}. After all, in order to prepare a
  surprise party, your partner and best friend have to be at least
  acquaintances.}
\[\ppr{+}{\textit{Affair} \vert  \textit{Secretive} } < \ppr{+}{\textit{Friends/acquaintances} \vert \textit{Secretive}}. \tag{<}\]
The conjunction of (\(>\)) and (\(<\)) violates Reverse Bayesianism.

But, as Steele and Stefánsson admits, Reverse Bayesianism can still be
made to work with a slightly different---though quite similar in
spirit---condition, called \textbf{Awareness Rigidity}:
\[\ppr{+}{A \vert T^*}=\pr{A},\] where \(T^*\) corresponds to a
proposition that picks out, from the vantage point of the new awareness
state, the entire possibility space before the episode of awareness
growth. In our running example, the proposition
\(\neg\textit{Surprise}\) picks out the entire possibility space in just
this way. And conditional on \(\neg\textit{Surprise}\), the probability
of \textit{Affair} does not change. Thus,
\[\ppr{+}{\textit{Affair} \vert  \textit{Secretive} \& \neg\textit{Surprise} } > \ppr{+}{\textit{Friends/acquaintances} \vert \textit{Secretive} \& \neg\textit{Surprise}}. \]
Awareness Rigidity is vindicated. Reverse Bayesianism---the spirit of
it, not the letter---stands.

This is not the end of the story, however. Steele and Stefánsson offer
another counterexample that also works against Awareness Rigidity, this
time targeting a case of refinement:

\begin{quote}
\textsc{Movies}: Suppose you are deciding whether to see a movie at your
local cinema. You know that the movie's predominant language and genre
will affect your viewing experience. The possible languages you consider
are French and German and the genres you consider are thriller and
comedy. But then you realise that, due to your poor French and German
skills, your enjoyment of the movie will also depend on the level of
difficulty of the language. Since it occurs to you that the owner of the
cinema is quite simple-minded, you are, after this realisation, much
more confident that the movie will have low-level language than
high-level language. Moreover, since you associate low-level language
with thrillers, this makes you more confident than you were before that
the movie on offer is a thriller as opposed to a comedy. (Steele and
Stefánsson, 2021, Section 5, Example 3)
\end{quote}

\noindent Initially, you did not consider language difficulty. So you
assigned the same probability to the hypotheses \textit{Thriller} and
\textit{Comedy}. But learning that the owner is simple-minded made you
think the level of linguistic difficulty must be low and the movie most
likely a thriller rather than a comedy (perhaps because thrillers are
simpler---linguistically---than comedies). So, against Reverse
Bayesianism, \textsc{Movies} violates the condition
\(\frac{\pr{\textit{Thriller}}}{\pr{\textit{Comedy}}}=\frac{\ppr{+}{\textit{Thriller}}}{\ppr{+}{\textit{Comedy}}}\).

The counterexample also works against Awareness Rigidity. It is not true
that
\(\pr{\textit{Thriller}}=\ppr{+}{\textit{Thriller} \vert \textit{Thriller}\vee \textit{Comedy}}\).
Note that this counterexample is a case of refinement. First, you
categorize movies by just language and genre, and then you add a further
category, level of difficulty. So the proposition which picks out the
entire possibility space should be the same before and after awareness
growth, for example, \(\textit{Thriller}\vee \textit{Comedy}\). In cases
of awareness growth by refinement, then, Awareness Rigidity mandates
that all probability assignments stay the same. But \textsc{Movies} does
not satisfy this requirement.

This is all well and good, but how strong of a counterexample is this?
Steele and Stefánsson consider an objection:

\begin{quote}
It might be argued that our examples are not illustrative of \ldots{} a
simple growth in awareness; rather, our examples illustrate and should
be expressed formally as complex learning experiences, where first there
is a growth in awareness, and then there is a further learning event
\ldots{} In this way, one could argue that the awareness-growth aspect
of the learning event always satisfies Reverse Bayesianism
\end{quote}

\noindent  Admittedly, \textsc{Movies} can be split into two episodes.
In the first, you entertain a new variable besides language and genre,
namely the language difficulty of the movie. In the second episode, you
learn something you did not consider before, namely that the owner is
simple-minded. Could Reserve Bayesianism still work for the first
episode, but not the second? Steele and Stefánsson do not address to
this question explicitly, but they insist that no matter the answer both
episodes are instances of awareness growth. We agree with Steele and
Stefánsson on this point. Awareness growth is both \textit{entertaining}
a new proposition not in the initial awareness state of the agent and
\textit{learning} a new proposition not in the initial awareness state.
Nonetheless, we still wonder. Is the second episode (learning something
new) necessary for the counterexample to work together with the first
episode (refinement without learning)?

Suppose the counterexample did work only in tandem with an episode of
learning something new. If that were so, defenders of Reverse
Bayesianism or Awareness Rigidity could still claim that their theory
applies to a large class of cases. It applies to cases of awareness
refinement without learning and also to cases of awareness expansion.
For recall that the first putative counterexample about awareness
expansion---\textsc{Friends}---did not challenge Reverse Bayesianism
insofar as the latter is formulated in terms of its close cousin,
Awareness Rigidity. So the force of Steele and Stefánsson's
counterexamples would be rather limited.

Or perhaps there is a more straightforward counterexample that only
depicts mere refinement without an episode of learning and that still
challenges Reverse Bayesianism and Awareness Rigidity? As we shall soon
see, the answer to this question is indeed positive.

\hypertarget{a-better-counterexample}{%
\section{A better counterexample}\label{a-better-counterexample}}

\label{sec:better}

Steele and Stefánsson's counterexample to Reverse Bayesianism in the
case of refinement is rather complex, perhaps unnecessarily so. We now
present something simpler:

\begin{quote}
\textsc{Lighting:} You have evidence that favors a certain hypothesis,
say a witness saw the defendant around the crime scene. You give some
weight to this evidence. In your assessment, that the defendant was seen
around the crime scene raises the probability that the defendant was
actually there. But now you wonder, what if it was dark when the witness
saw the defendant? You become a bit more careful and settle on this: if
the lighting conditions were good, you should still trust the evidence,
but if they were bad, you should not. Unfortunately, you cannot learn
about the actual lighting conditions, but the mere realization that it
\textit{could} have been dark makes you lower the probability that the
defendant was actually there, based on the same eveidence.
\end{quote}

\noindent This scenario is simpler because it consists of mere
refinement. You wonder about the lighting conditions but you do not
learn what they were. Still, mere refinement in this scenario challenges
Reverse Bayesianism and Awareness Rigidity. That this should be so is
not easy to see. Fortunately, the theory of Bayesian networks helps to
see why.

A Bayesian network is a formal model that consists of a graph
accompanied by a probability distribution. The nodes in the graph
represent random variables that can take different values. We will use
`nodes' and `variables' interchangeably. The nodes are connected by
arrows, but no loops are allowed, hence the name direct acyclic graph
(DAG). In this framework, awareness growth brings about a change in the
graphical network---nodes and arrows are added or erased---as well as a
change in the probability distribution from the old to the new network.

To model the scenario \textsc{Lighting} with Bayesian networks, we start
with this graph: \[H \rightarrow E,\] where \(H\) is the hypothesis node
and \(E\) the evidence node. If an arrow goes from \(H\) to \(E\), the
probability distribution associated with the Bayesian network should be
defined by conditional probabilities of the form \(\pr{E=e \vert H=h}\),
where uppercase letters represent the variables (nodes) and lower case
letters represent the values of these variables.\footnote{A major point
  of contention in the interpretation of Bayesian networks is is the
  meaning of the directed arrows. They could be interpreted
  causally---as though the direction of causality proceeds from the
  events described by the hypothesis to event described by the
  evidence---but they need not be. REFERENCES?}

Since you trust the evidence, you think that it is more likely under the
hypothesis that the defendant was present at the crime scene than under
the alternative hypothesis:
\[\pr{\textit{E=seen} \vert \textit{H=present}} > \pr{\textit{E=seen} \vert \textit{H=absent}}\]
The inequality is a qualitative ordering of how plausible the evidence
is in light of competing hypotheses. No matter the numbers, by the
probability calculus, it follows that the evidence raises the
probability of the hypothesis \textit{H=present}.

Now, as you wonder about the lighting conditions, the graph should be
amended: \[H \rightarrow E \leftarrow L,\] where the node \(L\) can have
two values, \textit{L=good} and \textit{L=bad}. A plausible way to
update your assessment of the evidence is as follows:
\[\ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=good}} > \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=good}}\]
\[\ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{bad}} = \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=bad}}\]

\noindent Note the change in the probability function from \(\pr{}\) to
\(\ppr{+}{}\). Here is what you are thinking: if the lighting conditions
were good, you should still trust the evidence like you did before. But
if the lighting conditions were bad, you should regard the evidence as
no better than chance.

Should you now assess the evidence at your disposal---that the witness
saw the defendant at the crime scene---any differently than you did
before? The evidence would have the same value if the likelihood ratios
associated with it relative to the competing hypotheses were the same
before and after awareness growth:
\[\frac{\pr{E=e \vert H=h}}{\pr{E=e \vert H=h'}}= \frac{\ppr{+}{E=e \vert H=h}}{\ppr{+}{E=e \vert H=h'}} \tag{C}.\]
But it would be quite a coincidence if (C) were true. For concreteness,
let's use some numbers:
\[\pr{\textit{E=seen} \vert \textit{H=present}}=\ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=good}}=.8\]
\[\pr{\textit{E=seen} \vert \textit{H=absent}}=\ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=good}}=.4\]
\[\ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=bad}} = \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=bad}}=.5.\]
So the ratio
\(\frac{\pr{\textit{E=seen} \vert \textit{H=present}}}{\pr{\textit{E=seen} \vert \textit{H=absent}}}=2\).
Before awareness growth, you thought the evidence favored the hypothesis
\textit{H=present} moderately strongly. That seemed reasonable. But,
after the awareness growth, the ratio
\(\frac{\ppr{+}{\textit{E=seen} \vert \textit{H=present}}}{\ppr{+}{\textit{E=seen} \vert \textit{H=absent}}}=\frac{.65}{.45}\approx 1.44\).\footnote{The
  calculations here rely on the dependency structure encoded in the
  Bayesian network (see starred step below). \begin{align*}
  \ppr{+}{\textit{E=seen} \vert \textit{H=present}} &= \ppr{+}{\textit{E=seen} \wedge \textit{L=good} \vert \textit{H=present}}+\ppr{+}{\textit{E=seen} \wedge \textit{L=bad} \vert \textit{H=present}}\\
  &= \ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=good}}  \times \ppr{+}{\textit{L=good} \vert  \textit{H=present} }\\ & +\ppr{+}{\textit{E=seen}  \vert \textit{H=present} \wedge \textit{L=bad}} \times \ppr{+}{\textit{L=bad} \vert  \textit{H=present}}\\
  &=^* \ppr{+}{\textit{E=seen} \vert \textit{H=present} \wedge \textit{L=good}}  \times \ppr{+}{\textit{L=good}}\\ & +\ppr{+}{\textit{E=seen}  \vert \textit{H=present} \wedge \textit{L=bad}} \times \ppr{+}{\textit{L=bad}}\\
  &= .8 \times .5 +.5 *.5 = .65 
  \end{align*} \begin{align*}
  \ppr{+}{\textit{E=seen} \vert \textit{H=absent}} &= \ppr{+}{\textit{E=seen} \wedge \textit{L=good} \vert \textit{H=absent}}+\ppr{+}{\textit{E=seen} \wedge \textit{L=bad} \vert \textit{H=absent}}\\
  &= \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=good}}  \times \ppr{+}{\textit{L=good} \vert  \textit{H=absent} }\\ & +\ppr{+}{\textit{E=seen}  \vert \textit{H=absent} \wedge \textit{L=bad}} \times \ppr{+}{\textit{L=bad} \vert  \textit{H=absent}}\\
  &=^* \ppr{+}{\textit{E=seen} \vert \textit{H=absent} \wedge \textit{L=good}}  \times \ppr{+}{\textit{L=good}}\\ & +\ppr{+}{\textit{E=seen}  \vert \textit{H=absent} \wedge \textit{L=bad}} \times \ppr{+}{\textit{L=bad}}\\
  &= .4 \times .5 +.5 *.5 = .45 
  \end{align*}} This argument can be repeated with several other
numerical
assignments.\todo{Need a more general argument here. Simulation?} So,
quite often, mere refinement can weaken the evidence, even without
learning anything new.\footnote{If you did learn that the lighting
  conditions were bad, the evidence would become even weaker,
  effectively worthless:
  \[\frac{\ppr{+, \textit{L=bad}}{\textit{E=seen} \vert \textit{H=present}}}{\ppr{+, \textit{L=bad}}{\textit{E=seen} \vert \textit{H=absent}}}=1,\]
  where \(\ppr{+, \textit{L=bad}}{}\) is the new probability function
  after learning that \textit{L=bad}.}

Why does all this matter? We have seen that, after awareness growth, you
should regard the evidence at your disposal as one that favors
\textit{H=present} less strongly. Since the prior probability of the
hypothesis should be the same before and after awareness growth, it
follows that
\[\ppr{+}{\textit{H=present} \vert \textit{E=seen}} \neq \pr{\textit{H=present} \vert \textit{E=seen}}.\]
This outcome violates Awareness Rigidity. For recall that in cases of
refinement, Awareness Rigidity requires that the probability of basic
propositions stay fixed.

Reverse Bayesianism is also violated. For example, the ratio of the
probabilities of \textit{H=present} to \textit{E=seen}, before and after
awareness growth, has changed:
\[\frac{\ppr{\textit{E=seen}}{\textit{H=present}}}{\ppr{ \textit{E=seen}}{\textit{H=seen}}} \neq \frac{\ppr{+, \textit{E=seen}}{\textit{H=present}}}{\ppr{+, \textit{E=seen}}{\textit{H=seen}}},\]
where \(\ppr{\textit{E=seen}}{}\) and \(\ppr{+, \textit{E=seen}}{}\)
represent the agent's degrees of belief, before and after awareness
growth, updated by the evidence \(\textit{E=seen}\).

\hypertarget{downstream-and-upstream-refinement}{%
\section{Downstream and upstream
refinement}\label{downstream-and-upstream-refinement}}

\label{sec:downstream}

Unlike \textsc{Movies}, the counterexample \textsc{Lighting} works even
though it only depicts refinement without learning. Defenders of Reverse
Bayesianism and Awareness Rigidity can no longer claim that their
theories work when awareness growth is not intertwined with learning.
So, Steele and Stefánsson's critique of these theories sits now on
firmer ground. And yet, the scope of this critique should not be
exaggerated. As we shall now see, there are cases of refinement in which
Reverse Bayesianism and Awareness Rigidity are perfectly fine in their
place.

Consider this variation of the \textsc{Lighting} scenario:

\begin{quote}
\textsc{Veracity}: A witness saw that the defendant was around the crime
scene and you initially took this to be evidence that the witness was
actually there. But then you had second thoughts. Instead of worrying
about the lighting conditions, you worry that the witness might be lying
or misremembering what happened. Perhaps, the witness was never there,
made things up or mixed things up. But despite that, you do not change
anything of your initial assessment of the evidence.
\end{quote}

\noindent   The rational thing to do here is to stick to your guns and
not change your earlier assessment of the evidence. Why should that be
so? And what is the difference with \textsc{Lighting}? Once again,
Bayesian networks prove to be a good analytic tool here.

The graphical network should initially look like this:
\[H\rightarrow E\] But, as your awareness grows, the graphical network
should be updated: \[H\rightarrow E \rightarrow R\] The hypothesis node
\(H\) bears on the whereabouts of the defendant. Note the difference
between \(E\) and \(R\). The evidence node bears on what the witness
saw. The reporting node bears on what the witness reports to have seen.
The chain of transmission from `seeing' to `reporting' may fail for
various reasons, such as lying or confusion.

Even if \textsc{Veracity} is a case of refinement, the old and new
probability functions agree with one another completely. The conditional
probabilities, \(\pr{E=e \vert H=h}\) should be the same as
\(\ppr{+}{E=e \vert H=h}\) for any values \(e\) and \(h\) of the
variables \(H\) and \(E\) that are shared before and after awareness
growth. Given the dependency structure of the two Bayesian
networks---first, \(H\rightarrow E\) and then
\(H\rightarrow E \rightarrow R\)---the equality is easy to establish
formally.\footnote{GIVE PROOF} Thus, Reverse Bayesianism and Awareness
Rigidity are perfectly fine in scenarios like \textsc{Veracity}.

A confusion should be eliminated at this point. We do not intend to
suggest that the assessment of the probability of the hypothesis
\textit{H=present} should undergo no change at all. If you worry that
the witness could have lied, shouldn't this affect your belief in what
they said about the defendant's whereabouts? Surely so. But note that in
\textsc{Veracity} an episode of awareness refinement takes place
together with a form of retraction. Initially, what is taken to be
known, after the learning episode, is that the witness \textit{saw} the
defendant around the crime scene. But after awareness growth, you
realize your learning is in fact limited to what the witness
\textit{reported} to have seen. So the previous learning episode is
retracted and replaced by a more careful statement of what you learned.
This retraction will affect the probability you assign to the hypothesis
\textit{H=seen}, but this does not conflict with Reverse Bayesianism or
Awareness Rigidity. In \textsc{Lighting}, instead, no retraction of the
evidence takes place. The evidence that is known remains the fact that
the witness saw the defendant around the crime scene, even though that
experience could have been misleading due to bad lighting conditions.

Where does this leave us? The following are now well-established: (a)
Reverse Bayesianism (or its close cousin Awareness Rigidity) handles
successfully cases of awareness expansion; (b) it also handles
successfully cases of refinement like \textsc{Veracity}; but (c) it does
fail in cases of refinement like \textsc{Lighting}. So, ultimately,
Steele and Stefánsson's critique only targets a subclass of refinement
cases. The scope of this critique is therefore somewhat limited. And
yet, we do not think the prospects for Reverse Bayesianism are good. In
this respect, we tend to agree with Steele and Stefánsson. But we
conjecture that there is a deeper reason why Reverse Bayesianism cannot
ultimately work, besides possible counterexamples that may be leveled
against it. It seeks to provide a formal, almost algorithmic solution to
the problem of awareness growth, and this formal aspiration is likely to
lead us down the wrong path.

To see why, consider again the distinction between the two cases of
refinement. Reverse Bayesianism is perfectly fine in scenarios like
\textsc{Veracity}, but fails in scenarios like \textsc{Lighting}. What
is that so? In one scenario, what the witness saw could have occurred
under good or bad lighting conditions; in the other, what the witness
saw could have been reported truthfully or untruthfully. The two
scenarios are structurally different, and this difference can be
appreciated by looking at the Bayesian networks used to model them.
There may be other, more fine-grained distinctions to be made. In
\textsc{Veracity}, the new node is added downstream. Since the
conditional probabilities associated with the upstream nodes are
unaffected, Reverse Bayesianism is vindicated. By contrast, in
\textsc{Lighting}, the new node is added upstream. Since the conditional
probabilities that are associated with the downstream nodes will often
have to change, Reverse Bayesianism fails here.

This discussion suggests a conjecture: structural---possibly
causal---constraints about how we conceptualize the world seems to be
the guiding principles about how we update the probability function
through awareness growth, not a formal principle like Reverse
Bayesianism. We further elaborate on this conjecture in the final
section.

\hypertarget{material-not-formal-constraints}{%
\section{Material, not formal
constraints}\label{material-not-formal-constraints}}

\label{sec:material}

Those who sympathize with a formal, algorithmic solution to the problem
of awareness growth might offer the following reply. Granted, Reverse
Bayesianism (or its close cousin, Awareness Rigidity) are not general
enough formal constraints. They fail sometimes. But, arguably, a weaker
formal constraint may be immune from counterexamples. We now explore
what this weaker constraint might look like for upstream refinement
cases like \textsc{Lighting}, and show that this strategy is unlikely to
succeed.

Recall that, in \textsc{Lighting}, the probability functions \(\pr{}\)
and \(\ppr{+}{}\) do not assign the same weight to the evidence relative
to the competing hypotheses, except in somewhat exceptional
circumstances. But despite that, the two probability functions agree in
one important respect:
\[\pr{E=e \vert H=h} \geq \pr{E=e \vert H=h'} \textit{ iff } \ppr{+}{E=e \vert H=h} \geq \ppr{+}{E=e \vert H=h'} \tag{$C^*$},\]
where (i) \(E\) and \(H\) are nodes that are part of the graphical
network before and after awareness growth, and (ii) there is a direct
path from \(H\) to \(E\) before and after awareness
growth.\todo{Is the condition of direct path necessary?} In other words,
the plausibility ordering between hypotheses and evidence is preserved.
Condition \((C^*)\) can serve as a conservative constraint that governs
the relationship between \(\pr{}\) and \(\ppr{+}{}\). It is satisfied in
the scenario \textsc{Lighting}, but how general is this condition?

Interestingly, \((C^*)\) holds generally in a class of Bayesian
networks, under minimal, and entirely reasonable, assumptions. Assume
the Bayesian network has a node \(E\) with an incoming arrow from node
\(H\), before and after awareness growth. After awareness growth,
besides \(E\) and \(H\), another variable \(Y\) is added upstream. The
new graph looks like this: \[H\rightarrow E \leftarrow Y.\] For
simplicity, we assume that variables are binaries. All we need is the
following assumption: \[\pr{E=e \vert H=h} \geq \pr{E=e \vert H=h'} \]
\[\textit{ iff }\]
\[\ppr{+}{E=e \vert H=h \wedge Y=y} \geq \ppr{+}{E=e \vert H=h' \wedge Y=y} \tag{EQUAL}\]
\[\textit{ iff }\]
\[\ppr{+}{E=e \vert H=h \wedge Y=y'} \geq \ppr{+}{E=e \vert H=h' \wedge Y=y'}\]
This assumption says that the plausibility ordering remains the same
before and after awareness growth \textit{all else being the same}. It
is a minimal assumption, but enough to establish (\(C^*\)).\footnote{From
  (EQUAL) and via this chain of equivalences:
  \[[a \geq a' \& b\geq b'] \textit{ iff } [ak \geq a'k \& b(1-k)\geq b'(1-k) (\textit{with }k>0)] \textit{ iff }  [ak+b(1-k) \geq a'k+b'(1-k)],\]
  it follows that \[\pr{E=e \vert H=h} \geq \pr{E=e \vert H=h'} \]
  \[\textit{ iff }\]
  \[\ppr{+}{E=e \vert H=h \wedge Y=y}\times \ppr{+}{Y=y} + \ppr{+}{E=e \vert H=h \wedge Y=y'}\times \ppr{+}{Y=y'} \]
  \[\geq\]
  \[\ppr{+}{E=e \vert H=h' \wedge Y=y}\times \ppr{+}{Y=y} + \ppr{+}{E=e \vert H=h' \wedge Y=y'}\times \ppr{+}{Y=y'} \]
  We are done since, by the law of total probability and the
  probabilistic dependencies in the graph, (\(C^*\)) is equivalent to
  the above statement.}

The formal requirement (EQUAL) seems quite general. It should also hold
in Steele and Stefánsson's scenario \textsc{Movies}, another case of
upstream refinement.\footnote{We briefly explain why. At first, the
  graphical network looks like this:
  \[\textit{Genre} \rightarrow \textit{Enjoyment} \leftarrow \textit{Language},\]
  where each node can take two values: \textit{Genre=comedy} and
  \textit{Genre=thriller}; \textit{Language=french} and
  \textit{Language=german}; and \textit{Enjoyment=yes} and
  \textit{Enjoyment=no}. Assume you are ranking the options in terms of
  how they are going to contribute to your enjoyment
  (\textit{Enjoyment=yes}). This ranking can be encoded by conditional
  probability statements of the form
  \[\pr{\textit{Enjoyment=x} \vert \textit{Language=y} \wedge \textit{Genre=z}} \geq \pr{\textit{Enjoyment=x} \vert \textit{Language=y'} \wedge \textit{Genre=z'}}.\]
  The first episode of awareness growth in \textsc{Movies} consists in
  realizing that the linguistic difficulty of the movie could also be a
  factor. So the expanded graphical network now becomes: \begin{align*}
  \textit{Difficulty} &\\
  \downarrow \\
  \textit{Genre} \rightarrow \textit{Enjoyment} &\leftarrow \textit{Language}\\
  \end{align*} \noindent Your ranking of what is likely to give you
  enjoyment should now be updated and made more specific, but much of
  the earlier ordering can be retained, that is:
  \[\pr{\textit{Enjoyment=x} \vert \textit{Language=y} \wedge \textit{Genre=z}} \geq \pr{\textit{Enjoyment=x} \vert \textit{Language=y'} \wedge \textit{Genre=z'}}\]
  \[\textit{ iff }\]
  \[\ppr{+}{\textit{Enjoyment=x} \vert \textit{Language=y} \wedge \textit{Genre=z}} \geq \ppr{+}{\textit{Enjoyment=x} \vert \textit{Language=y'} \wedge \textit{Genre=z'}}.\]
  The difference with condition (\(C^*\)) is that here two propositions,
  not just one, are conditioned on. So (\(C^*\)) should be be
  generalized, accordingly, but the general idea remains the same.} So
the algorithmic solution to the problem of awareness growth might go
like this: for expansion and downstream refinement, uses Awareness
Rigidity, and for downstream refinement, use the weaker (C*).

But this solution cannot ultimately work. There will be cases in which
the plausibility ordering is not preserved because (EQUAL) does not
hold. For suppose you have evidence that---in your judgment---reliably
tracks a hypothesis, say you think that appearance as of hands reliably
tracks the presence of hands:
\[\pr{E=\textit{as-of-hands} \vert H=\textit{hands}} > \pr{E=\textit{as-of-hands} \vert H=\textit{no-hands}} \]
You now entertain a `switching hypothesis': when you see a hand, there
is no hand, and when you do not see a hand, there is a hand. In this
case, (EQUAL) would be violated since
\[\ppr{+}{E=\textit{as-of-hands} \vert H=\textit{hands} \wedge Y=\textit{switching}} < \ppr{+}{E=\textit{as-of-hands} \vert H=\textit{no-hands} \wedge Y=\textit{switching}}\]
This scenario is far-fetched---does the switching hypothesis even make
sense?---but suggests that no matter how weak a formal constraint might
be, there is likely a counterexample.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

We argued that Steele and Stefánsson's case against Reverse Bayesianism
is weaker than it might seem at first. The scenario
\textsc{Movies}---which is their key counterexample---is unconvincing
since it mixes learning and refinement. To avoid this, we constructed a
more clear-cut case of refinement, \textsc{Lighting}, in which both
Awareness Rigidity and Reverse Bayesianism fail unequivocally. At the
same time, we showed that there are cases of downstream refinement like
\textsc{Veracity} in which Reverse Bayesianism and Awareness Rigidity
are perfectly fine in their place. In cases of upstream refinement, like
\textsc{Lighting}, one can be tempted to formulate a weaker formal
constraint that would still vindicate the formalistic aspiration of
Reverse Bayesianism. But no matter the constraint, there are likely to
be counterexamples to it.

We conclude with a few programmatic observations. We think that the
awareness of agents grows while holding fixed certain material
structural assumptions, based on commonsense, semantic stipulations or
causal dependency. To model awareness growth, we need a formalism that
can express these material structural assumptions. This can done using
Bayesian networks, and we offered some illustrations of this strategy,
for example, by distinguish two forms of refinement on the basis of
different structural assumptions. These material assumptions also guide
us in formulating the adequate conservative constraints, and these will
inevitably vary on a case-by-case basis. Our approach stands in stark
contrast with much of the literature on awareness growth from a Bayesian
perspective. This literature is primarily concerned with a formal,
almost algorithmic solution to the problem. We suspect that seeking such
formal solution is doomed to fail. Insofar as Reverse Bayesianism is an
expression of this formalistic aspiration, we agree with Steele and
Stefánsson that we are better off looking elsewhere.

\hypertarget{extra-materials-ignore}{%
\section{Extra Materials -- IGNORE}\label{extra-materials-ignore}}

\hypertarget{expansion}{%
\subsection{Expansion}\label{expansion}}

There remains to examine cases of awareness expansion. They consist in
the addition of another proposition not previously in the algebra, but
that is not a refinement of existing propositions. The addition of the
hypothesis \textit{Surprise} is, however, an ambiguous case. For one
thing, \textit{Surprise} is a novel hypothesis that cannot be subsumed
under \textit{Friends/acquaintances} or \textit{Affair}. On the other,
\textit{Surprise} seems a refinement of \textit{Friends/acquaintances},
since a meeting for planning a surprise is a more specific way to
describe a meeting of acquaintances. A more clear-cut case of awareness
expansion would be the following. The police is investigating a murder
case. There are two suspects under investigation: Joe and Sue. They both
have a motive. The incriminating evidence favors one over the other, but
not overwhelmingly. T hen, a new hypothesis is considered: Ela could be
the perpetrator. The evidence incriminates Ela almost without any doubt.
Any theory of awareness growth should be able to model the difference
between the example provided by Steele and Stefánsson and the criminal
case just outlined. They are both, arguably, cases of expansion, but
they are also different.

Steele and Stefánsson provide a formal definition of the difference
between refinement and expansion. Our observations here are largely
confined at the intuitively level. Our point is that there are a number
of intuitively plausible differences that a formal theory should be able
to capture. The coarse distinction between refinement and expansion
might be, in the end, too coarse. Relying on Bayesian networks, we will
illustrate this point more precisely in the next section.

\hypertarget{steele-and-stefuxe1nsson-example}{%
\subsection{Steele and Stefánsson
example}\label{steele-and-stefuxe1nsson-example}}

Before awareness growth, the Bayesian network has a simple form:
\[H \rightarrow E,\] where the hypothesis variable \(H\) takes two
values, \(H=\textit{Affair}\) and \(H=\textit{Friends/acquaintances}\).
The evidence variable \(E\) can take several values, one of them being
\(E=\textit{Secretive}\). You could have seen other things other than
what you saw, but there is no need to specify the other values
exhaustively. Suppose the prior odds ratio of the hypotheses is 1:1,
say, because you suspected your partner might be cheating on you, and
the likelihood ratio
\[\frac{\pr{E=\textit{Secretive}\vert H=\textit{Affair}}}{\pr{E=\textit{Secretive}\vert H=\textit{Friends/acquaintances}}}\]
is 9:1, because the hypothesis \textit{Affair} is a better explanation
of the evidence than the hypothesis \textit{Friends/acquaintances}.
Then, the posterior probability given the evidence
\[\pr{H=\textit{Affair} \vert E=\textit{Secretive}}\] is quite high,
\(\frac{9}{10}=.9\). So
\(\ppr{E=\textit{Secretive}}{H=\textit{Affair}}=.9\).\footnote{This
  calculation presupposes that the two hypotheses \textit{Affair} and
  \textit{Friends/acquaintances} are exclusive and exhaustive. This
  assumption is justified given the initial awareness state of the
  agent.}

After awareness growth, the Bayesian network should be modified as
follows: \[H \leftarrow H' \rightarrow E,\] where the new hypothesis
node now consists of three values instead of two:

\(H'=\textit{Affair}\)

\(H'=\textit{Friends/acquaintances}\wedge \neg \textit{Surprise}\)

\(H'=\textit{Friends/acquaintances}\wedge\textit{Surprise}\).

\noindent The scenario \(\textit{Friends/acquaintances}\) is split into
the scenario in which your partner and best friend met simply as friends
or acquaintances, and the scenario in which they met to prepare a
surprise party for you. On this interpretation, the counterexample by
Steele and Stefánsson is a case of refinement, not expansion. We will
return to this point later.

The network contains a directed arrow between the old hypothesis node
\(H\) and the new hypothesis node \(H'\) This arrow can be interpreted
as a bridge between the old awareness state limited to two hypotheses
and the new awareness state that contains an additional hypothesis. This
bridge is purely conceptual and can be defined by two sets of
constrains. The first set of constrains posits that \textit{Affair}
under \(H\) has the same meaning as \textit{Affair} under \(H'\):

\(\ppr{+}{H=\textit{Affair} \vert H'=\textit{Affair}}=1\)

\(\ppr{+}{H=\textit{Affair} \vert H'=\textit{\textit{Friends/acquaintances}}}=0\)

\(\ppr{+}{H=\textit{Affair} \vert H'=\textit{Surprise}}=0\)

The second set of constrains posits that hypothesis
\(\textit{Friends/acquaintances}\) under \(H\) can be actually be
interpreted in two ways under \(H'\), as
\(\textit{Friends/acquaintances} \wedge \neg \textit{Surprise}\) and
\(\textit{Friends/acquaintances} \wedge \textit{Surprise}\). So, in
other words, the episode of awareness growth consists in the realization
that \(\textit{Friends/acquaintances}\) can be made precise in two more
specific ways:

\(\ppr{+}{H=\textit{Friends/acquaintances} \vert H'=\textit{Affair}}=0\)

\(\ppr{+}{H=\textit{Friends/acquaintances} \vert H'=\textit{Friends/acquaintances} \wedge \neg \textit{Surprise}}=1\)

\(\ppr{+}{H=\textit{Friends/acquaintances} \vert H'=\textit{Friends/acquaintances} \wedge \textit{Surprise}}=0\)

This bridge between \(H\) and \(H'\) justifies the following
conservativity constraint:

\[\frac{\pr{E=\textit{Secretive}\vert H=\textit{Affair}}}{\pr{E=\textit{Secretive}\vert H=\textit{Friends/acquaintances}}} = \frac{\ppr{+}{E=\textit{Secretive}\vert H=\textit{Affair}}}{\ppr{+}{E=\textit{Secretive}\vert H=\textit{Friends/acquaintances}}}=\frac{9}{1} \]

\hypertarget{expansion-criminal-case-example}{%
\subsection{Expansion: criminal case
example}\label{expansion-criminal-case-example}}

\end{document}
